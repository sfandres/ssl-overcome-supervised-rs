IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 13:42:35,130	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 13:42:35,130	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 13:42:38,090	SUCC scripts.py:747 -- --------------------
2024-01-07 13:42:38,090	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 13:42:38,090	SUCC scripts.py:749 -- --------------------
2024-01-07 13:42:38,090	INFO scripts.py:751 -- Next steps
2024-01-07 13:42:38,090	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 13:42:38,090	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 13:42:38,091	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 13:42:38,091	INFO scripts.py:773 -- import ray
2024-01-07 13:42:38,091	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 13:42:38,091	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 13:42:38,091	INFO scripts.py:791 --   ray status
2024-01-07 13:42:38,091	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 13:42:38,091	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 13:42:38,091	INFO scripts.py:810 --   ray stop
2024-01-07 13:42:38,091	INFO scripts.py:891 -- --block
2024-01-07 13:42:38,092	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 13:42:38,092	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              14578387935232624810
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f2902e130d0>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         imagenet
seed:                42
dropout:             None
transfer_learning:   FT
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [10 10 10 10 10 10 10 10 10 10]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.92
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [10 10 10 10 10 10 10 10 10 10]
Done!
Using ImageNet weights

Supervised model resnet18 with imagenet weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Fine-tuning adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 13:43:21,985	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 13:43:21,995	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 13:43:41,618	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 13:43:41 (running for 00:00:18.67)
Memory usage on this node: 13.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |
| train_5806f_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_5806f_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_5806f_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78294)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=78294)[0m Configuration completed!
[2m[36m(func pid=78294)[0m New optimizer parameters:
[2m[36m(func pid=78294)[0m SGD (
[2m[36m(func pid=78294)[0m Parameter Group 0
[2m[36m(func pid=78294)[0m     dampening: 0
[2m[36m(func pid=78294)[0m     differentiable: False
[2m[36m(func pid=78294)[0m     foreach: None
[2m[36m(func pid=78294)[0m     lr: 0.0001
[2m[36m(func pid=78294)[0m     maximize: False
[2m[36m(func pid=78294)[0m     momentum: 0.99
[2m[36m(func pid=78294)[0m     nesterov: False
[2m[36m(func pid=78294)[0m     weight_decay: 0
[2m[36m(func pid=78294)[0m )
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.2571 | Steps: 4 | Val loss: 2.5330 | Batch size: 32 | lr: 0.0001 | Duration: 4.95s
[2m[36m(func pid=78294)[0m top1: 0.05970149253731343
[2m[36m(func pid=78294)[0m top5: 0.47947761194029853
[2m[36m(func pid=78294)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=78294)[0m f1_macro: 0.03409516878862957
[2m[36m(func pid=78294)[0m f1_weighted: 0.034290185958594455
[2m[36m(func pid=78294)[0m f1_per_class: [0.081, 0.01, 0.0, 0.079, 0.0, 0.02, 0.0, 0.094, 0.023, 0.034]
== Status ==
Current time: 2024-01-07 13:43:52 (running for 00:00:28.96)
Memory usage on this node: 15.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |
| train_5806f_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_5806f_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78677)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78677)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=78677)[0m Configuration completed!
[2m[36m(func pid=78677)[0m New optimizer parameters:
[2m[36m(func pid=78677)[0m SGD (
[2m[36m(func pid=78677)[0m Parameter Group 0
[2m[36m(func pid=78677)[0m     dampening: 0
[2m[36m(func pid=78677)[0m     differentiable: False
[2m[36m(func pid=78677)[0m     foreach: None
[2m[36m(func pid=78677)[0m     lr: 0.001
[2m[36m(func pid=78677)[0m     maximize: False
[2m[36m(func pid=78677)[0m     momentum: 0.99
[2m[36m(func pid=78677)[0m     nesterov: False
[2m[36m(func pid=78677)[0m     weight_decay: 0
[2m[36m(func pid=78677)[0m )
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1118 | Steps: 4 | Val loss: 2.4526 | Batch size: 32 | lr: 0.001 | Duration: 4.70s
[2m[36m(func pid=78677)[0m top1: 0.07136194029850747
[2m[36m(func pid=78677)[0m top5: 0.4906716417910448
[2m[36m(func pid=78677)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=78677)[0m f1_macro: 0.049506012858248946
[2m[36m(func pid=78677)[0m f1_weighted: 0.05341008273285971
[2m[36m(func pid=78677)[0m f1_per_class: [0.147, 0.06, 0.0, 0.107, 0.0, 0.025, 0.0, 0.101, 0.042, 0.013]
== Status ==
Current time: 2024-01-07 13:44:00 (running for 00:00:37.25)
Memory usage on this node: 18.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |
| train_5806f_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79093)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=79093)[0m Configuration completed!
[2m[36m(func pid=79093)[0m New optimizer parameters:
[2m[36m(func pid=79093)[0m SGD (
[2m[36m(func pid=79093)[0m Parameter Group 0
[2m[36m(func pid=79093)[0m     dampening: 0
[2m[36m(func pid=79093)[0m     differentiable: False
[2m[36m(func pid=79093)[0m     foreach: None
[2m[36m(func pid=79093)[0m     lr: 0.01
[2m[36m(func pid=79093)[0m     maximize: False
[2m[36m(func pid=79093)[0m     momentum: 0.99
[2m[36m(func pid=79093)[0m     nesterov: False
[2m[36m(func pid=79093)[0m     weight_decay: 0
[2m[36m(func pid=79093)[0m )
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.2421 | Steps: 4 | Val loss: 2.3975 | Batch size: 32 | lr: 0.01 | Duration: 4.96s
[2m[36m(func pid=79093)[0m top1: 0.03824626865671642
[2m[36m(func pid=79093)[0m top5: 0.6856343283582089
[2m[36m(func pid=79093)[0m f1_micro: 0.03824626865671642
[2m[36m(func pid=79093)[0m f1_macro: 0.06172267082927411
[2m[36m(func pid=79093)[0m f1_weighted: 0.029086654953577974
[2m[36m(func pid=79093)[0m f1_per_class: [0.192, 0.0, 0.265, 0.007, 0.018, 0.063, 0.045, 0.0, 0.028, 0.0]
== Status ==
Current time: 2024-01-07 13:44:09 (running for 00:00:46.07)
Memory usage on this node: 20.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 13:44:16 (running for 00:00:53.86)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |        |            |                      |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  3.112 |       0.05 |                    1 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |        |            |                      |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |        |            |                      |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79525)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=79525)[0m Configuration completed!
[2m[36m(func pid=79525)[0m New optimizer parameters:
[2m[36m(func pid=79525)[0m SGD (
[2m[36m(func pid=79525)[0m Parameter Group 0
[2m[36m(func pid=79525)[0m     dampening: 0
[2m[36m(func pid=79525)[0m     differentiable: False
[2m[36m(func pid=79525)[0m     foreach: None
[2m[36m(func pid=79525)[0m     lr: 0.1
[2m[36m(func pid=79525)[0m     maximize: False
[2m[36m(func pid=79525)[0m     momentum: 0.99
[2m[36m(func pid=79525)[0m     nesterov: False
[2m[36m(func pid=79525)[0m     weight_decay: 0
[2m[36m(func pid=79525)[0m )
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7367 | Steps: 4 | Val loss: 2.4085 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.8724 | Steps: 4 | Val loss: 1.5709 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1435 | Steps: 4 | Val loss: 2.5365 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:44:22 (running for 00:00:58.91)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  3.257 |      0.034 |                    1 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  3.112 |      0.05  |                    1 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.242 |      0.062 |                    1 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |        |            |                      |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.4882 | Steps: 4 | Val loss: 27.0058 | Batch size: 32 | lr: 0.1 | Duration: 5.19s
[2m[36m(func pid=78677)[0m top1: 0.08582089552238806
[2m[36m(func pid=78677)[0m top5: 0.5102611940298507
[2m[36m(func pid=78677)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=78677)[0m f1_macro: 0.0825214370225851
[2m[36m(func pid=78677)[0m f1_weighted: 0.062322383134247795
[2m[36m(func pid=78677)[0m f1_per_class: [0.106, 0.073, 0.207, 0.067, 0.025, 0.117, 0.012, 0.132, 0.087, 0.0]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m top1: 0.06203358208955224
[2m[36m(func pid=78294)[0m top5: 0.4748134328358209
[2m[36m(func pid=78294)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=78294)[0m f1_macro: 0.03698729999399701
[2m[36m(func pid=78294)[0m f1_weighted: 0.04302872973412607
[2m[36m(func pid=78294)[0m f1_per_class: [0.046, 0.051, 0.0, 0.087, 0.0, 0.019, 0.0, 0.093, 0.024, 0.049]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.3978544776119403
[2m[36m(func pid=79093)[0m top5: 0.9361007462686567
[2m[36m(func pid=79093)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=79093)[0m f1_macro: 0.3209528925800723
[2m[36m(func pid=79093)[0m f1_weighted: 0.358070275379645
[2m[36m(func pid=79093)[0m f1_per_class: [0.456, 0.262, 0.421, 0.583, 0.421, 0.253, 0.288, 0.278, 0.065, 0.182]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m top1: 0.01166044776119403
[2m[36m(func pid=79525)[0m top5: 0.5625
[2m[36m(func pid=79525)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=79525)[0m f1_macro: 0.0023062730627306273
[2m[36m(func pid=79525)[0m f1_weighted: 0.0002689217657101944
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4396 | Steps: 4 | Val loss: 2.2902 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.1851 | Steps: 4 | Val loss: 2.2842 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9760 | Steps: 4 | Val loss: 2.4997 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 16.7627 | Steps: 4 | Val loss: 503967285248.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:44:27 (running for 00:01:04.83)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  3.144 |      0.037 |                    2 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  2.44  |      0.102 |                    3 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  1.872 |      0.321 |                    2 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  6.488 |      0.002 |                    1 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78677)[0m top1: 0.11100746268656717
[2m[36m(func pid=78677)[0m top5: 0.6767723880597015
[2m[36m(func pid=78677)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=78677)[0m f1_macro: 0.10186899421409376
[2m[36m(func pid=78677)[0m f1_weighted: 0.0960216240217077
[2m[36m(func pid=78677)[0m f1_per_class: [0.08, 0.031, 0.222, 0.098, 0.037, 0.176, 0.085, 0.208, 0.082, 0.0]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m top1: 0.3376865671641791
[2m[36m(func pid=79093)[0m top5: 0.8759328358208955
[2m[36m(func pid=79093)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=79093)[0m f1_macro: 0.3100056456133704
[2m[36m(func pid=79093)[0m f1_weighted: 0.2455824145148325
[2m[36m(func pid=79093)[0m f1_per_class: [0.562, 0.337, 0.667, 0.492, 0.338, 0.008, 0.012, 0.379, 0.102, 0.203]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.07649253731343283
[2m[36m(func pid=78294)[0m top5: 0.4673507462686567
[2m[36m(func pid=78294)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=78294)[0m f1_macro: 0.0490766280243362
[2m[36m(func pid=78294)[0m f1_weighted: 0.07229299346215122
[2m[36m(func pid=78294)[0m f1_per_class: [0.06, 0.08, 0.0, 0.173, 0.0, 0.012, 0.003, 0.086, 0.044, 0.032]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m top1: 0.1142723880597015
[2m[36m(func pid=79525)[0m top5: 0.5149253731343284
[2m[36m(func pid=79525)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=79525)[0m f1_macro: 0.020510673922143157
[2m[36m(func pid=79525)[0m f1_weighted: 0.023438036897971425
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.205, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8260 | Steps: 4 | Val loss: 2.4663 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0419 | Steps: 4 | Val loss: 2.0529 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5025 | Steps: 4 | Val loss: 4.0941 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 13.0343 | Steps: 4 | Val loss: 11260092416.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:44:33 (running for 00:01:10.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.976 |      0.049 |                    3 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  2.44  |      0.102 |                    3 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  0.503 |      0.194 |                    4 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 16.763 |      0.021 |                    2 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.2560634328358209
[2m[36m(func pid=79093)[0m top5: 0.7010261194029851
[2m[36m(func pid=79093)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=79093)[0m f1_macro: 0.19369957265291013
[2m[36m(func pid=79093)[0m f1_weighted: 0.24221609636168856
[2m[36m(func pid=79093)[0m f1_per_class: [0.307, 0.513, 0.083, 0.408, 0.091, 0.062, 0.0, 0.401, 0.072, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.0960820895522388
[2m[36m(func pid=78294)[0m top5: 0.4566231343283582
[2m[36m(func pid=78294)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=78294)[0m f1_macro: 0.061994043457556436
[2m[36m(func pid=78294)[0m f1_weighted: 0.09608860771087063
[2m[36m(func pid=78294)[0m f1_per_class: [0.049, 0.151, 0.0, 0.205, 0.0, 0.038, 0.0, 0.099, 0.046, 0.032]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.21921641791044777
[2m[36m(func pid=78677)[0m top5: 0.8143656716417911
[2m[36m(func pid=78677)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=78677)[0m f1_macro: 0.21458490736429878
[2m[36m(func pid=78677)[0m f1_weighted: 0.23992211338639854
[2m[36m(func pid=78677)[0m f1_per_class: [0.133, 0.136, 0.488, 0.256, 0.11, 0.086, 0.362, 0.251, 0.12, 0.205]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m top1: 0.05783582089552239
[2m[36m(func pid=79525)[0m top5: 0.5149253731343284
[2m[36m(func pid=79525)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=79525)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=79525)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.3699 | Steps: 4 | Val loss: 1.8543 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0526 | Steps: 4 | Val loss: 6.6535 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7580 | Steps: 4 | Val loss: 2.4199 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 13:44:39 (running for 00:01:15.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.826 |      0.062 |                    4 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.37  |      0.26  |                    5 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  0.503 |      0.194 |                    4 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 13.034 |      0.011 |                    3 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=78677)[0m top1: 0.35027985074626866

[2m[36m(func pid=78677)[0m top5: 0.8376865671641791
[2m[36m(func pid=78677)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=78677)[0m f1_macro: 0.2601045899463502
[2m[36m(func pid=78677)[0m f1_weighted: 0.35781403886098717
[2m[36m(func pid=78677)[0m f1_per_class: [0.435, 0.284, 0.375, 0.409, 0.077, 0.0, 0.569, 0.129, 0.109, 0.214]
[2m[36m(func pid=79093)[0m top1: 0.08162313432835822
[2m[36m(func pid=79093)[0m top5: 0.5783582089552238
[2m[36m(func pid=79093)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=79093)[0m f1_macro: 0.13033509192464954
[2m[36m(func pid=79093)[0m f1_weighted: 0.0823077715329584
[2m[36m(func pid=79093)[0m f1_per_class: [0.299, 0.305, 0.018, 0.023, 0.146, 0.008, 0.003, 0.171, 0.023, 0.308]
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 5.0365 | Steps: 4 | Val loss: 136133696.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.11194029850746269
[2m[36m(func pid=78294)[0m top5: 0.4724813432835821
[2m[36m(func pid=78294)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=78294)[0m f1_macro: 0.08074662813244227
[2m[36m(func pid=78294)[0m f1_weighted: 0.11152912165764621
[2m[36m(func pid=78294)[0m f1_per_class: [0.066, 0.191, 0.1, 0.221, 0.0, 0.054, 0.003, 0.108, 0.041, 0.023]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m top1: 0.05783582089552239
[2m[36m(func pid=79525)[0m top5: 0.5149253731343284
[2m[36m(func pid=79525)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=79525)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=79525)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8087 | Steps: 4 | Val loss: 8.9922 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5746 | Steps: 4 | Val loss: 2.3816 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.1358 | Steps: 4 | Val loss: 1.8622 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=79093)[0m top1: 0.14225746268656717
[2m[36m(func pid=79093)[0m top5: 0.6585820895522388
[2m[36m(func pid=79093)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=79093)[0m f1_macro: 0.13067059510145745
[2m[36m(func pid=79093)[0m f1_weighted: 0.16954786954677942
[2m[36m(func pid=79093)[0m f1_per_class: [0.196, 0.2, 0.036, 0.387, 0.167, 0.0, 0.034, 0.16, 0.046, 0.081]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 10.7973 | Steps: 4 | Val loss: 2883493036032.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:44:44 (running for 00:01:21.47)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.758 |      0.081 |                    5 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.37  |      0.26  |                    5 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  0.809 |      0.131 |                    6 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.036 |      0.011 |                    4 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.11007462686567164
[2m[36m(func pid=78294)[0m top5: 0.49906716417910446
[2m[36m(func pid=78294)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=78294)[0m f1_macro: 0.0822041609170017
[2m[36m(func pid=78294)[0m f1_weighted: 0.11162534622999849
[2m[36m(func pid=78294)[0m f1_per_class: [0.082, 0.195, 0.083, 0.193, 0.0, 0.084, 0.018, 0.084, 0.057, 0.025]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.3255597014925373
[2m[36m(func pid=78677)[0m top5: 0.8526119402985075
[2m[36m(func pid=78677)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=78677)[0m f1_macro: 0.2704513160072271
[2m[36m(func pid=78677)[0m f1_weighted: 0.33894645665598583
[2m[36m(func pid=78677)[0m f1_per_class: [0.459, 0.264, 0.364, 0.509, 0.052, 0.0, 0.391, 0.261, 0.155, 0.25]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m top1: 0.05783582089552239
[2m[36m(func pid=79525)[0m top5: 0.5149253731343284
[2m[36m(func pid=79525)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=79525)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=79525)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5274 | Steps: 4 | Val loss: 2.3306 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.0403 | Steps: 4 | Val loss: 1.9453 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1782 | Steps: 4 | Val loss: 38.7387 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:44:49 (running for 00:01:26.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.527 |      0.082 |                    7 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.136 |      0.27  |                    6 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  0.809 |      0.131 |                    6 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 10.797 |      0.011 |                    5 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.11147388059701492
[2m[36m(func pid=78294)[0m top5: 0.5443097014925373
[2m[36m(func pid=78294)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=78294)[0m f1_macro: 0.08228236418787624
[2m[36m(func pid=78294)[0m f1_weighted: 0.1215526965698352
[2m[36m(func pid=78294)[0m f1_per_class: [0.104, 0.174, 0.069, 0.188, 0.007, 0.1, 0.066, 0.06, 0.053, 0.0]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.3148320895522388
[2m[36m(func pid=78677)[0m top5: 0.8306902985074627
[2m[36m(func pid=78677)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=78677)[0m f1_macro: 0.24121810404329497
[2m[36m(func pid=78677)[0m f1_weighted: 0.265757741530371
[2m[36m(func pid=78677)[0m f1_per_class: [0.333, 0.292, 0.316, 0.51, 0.068, 0.0, 0.114, 0.359, 0.208, 0.213]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m top1: 0.09981343283582089
[2m[36m(func pid=79093)[0m top5: 0.5144589552238806
[2m[36m(func pid=79093)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=79093)[0m f1_macro: 0.07182735926340861
[2m[36m(func pid=79093)[0m f1_weighted: 0.1173463846167167
[2m[36m(func pid=79093)[0m f1_per_class: [0.246, 0.021, 0.034, 0.388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 124.5384 | Steps: 4 | Val loss: 3299031296.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=79525)[0m top1: 0.2980410447761194
[2m[36m(func pid=79525)[0m top5: 0.5149253731343284
[2m[36m(func pid=79525)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=79525)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=79525)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3644 | Steps: 4 | Val loss: 2.2745 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5124 | Steps: 4 | Val loss: 1.9891 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.8121 | Steps: 4 | Val loss: 101.7232 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:44:55 (running for 00:01:32.33)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+---------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |   2.364 |      0.083 |                    8 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |   1.04  |      0.241 |                    7 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |   1.178 |      0.072 |                    7 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 124.538 |      0.046 |                    6 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |         |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |         |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |         |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |         |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |         |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.1142723880597015
[2m[36m(func pid=78294)[0m top5: 0.6152052238805971
[2m[36m(func pid=78294)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=78294)[0m f1_macro: 0.08259037461890367
[2m[36m(func pid=78294)[0m f1_weighted: 0.12748195188228775
[2m[36m(func pid=78294)[0m f1_per_class: [0.109, 0.177, 0.062, 0.159, 0.007, 0.101, 0.116, 0.028, 0.066, 0.0]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.12173507462686567
[2m[36m(func pid=79093)[0m top5: 0.49300373134328357
[2m[36m(func pid=79093)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=79093)[0m f1_macro: 0.04751848051858251
[2m[36m(func pid=79093)[0m f1_weighted: 0.09279443861138044
[2m[36m(func pid=79093)[0m f1_per_class: [0.03, 0.241, 0.0, 0.176, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.3400186567164179
[2m[36m(func pid=78677)[0m top5: 0.7882462686567164
[2m[36m(func pid=78677)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=78677)[0m f1_macro: 0.2825840165565031
[2m[36m(func pid=78677)[0m f1_weighted: 0.2868984651228149
[2m[36m(func pid=78677)[0m f1_per_class: [0.508, 0.505, 0.261, 0.498, 0.112, 0.102, 0.012, 0.384, 0.255, 0.188]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 72.9590 | Steps: 4 | Val loss: 52587404.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=79525)[0m top1: 0.006063432835820896
[2m[36m(func pid=79525)[0m top5: 0.5093283582089553
[2m[36m(func pid=79525)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=79525)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=79525)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2477 | Steps: 4 | Val loss: 2.2261 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 7.9402 | Steps: 4 | Val loss: 1391.3053 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6152 | Steps: 4 | Val loss: 2.0087 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:45:00 (running for 00:01:37.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.248 |      0.097 |                    9 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.512 |      0.283 |                    8 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.812 |      0.048 |                    8 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 72.959 |      0.001 |                    7 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.13712686567164178
[2m[36m(func pid=78294)[0m top5: 0.6618470149253731
[2m[36m(func pid=78294)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=78294)[0m f1_macro: 0.09666110939485842
[2m[36m(func pid=78294)[0m f1_weighted: 0.152068860402355
[2m[36m(func pid=78294)[0m f1_per_class: [0.115, 0.196, 0.058, 0.115, 0.038, 0.129, 0.216, 0.031, 0.068, 0.0]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.06110074626865672
[2m[36m(func pid=79093)[0m top5: 0.5615671641791045
[2m[36m(func pid=79093)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=79093)[0m f1_macro: 0.024448997016134
[2m[36m(func pid=79093)[0m f1_weighted: 0.00881846315037815
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.088, 0.0, 0.0, 0.0, 0.0, 0.126, 0.03, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.38619402985074625
[2m[36m(func pid=78677)[0m top5: 0.7737873134328358
[2m[36m(func pid=78677)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=78677)[0m f1_macro: 0.34553586016677457
[2m[36m(func pid=78677)[0m f1_weighted: 0.3362044244756855
[2m[36m(func pid=78677)[0m f1_per_class: [0.649, 0.562, 0.293, 0.534, 0.196, 0.339, 0.0, 0.397, 0.306, 0.18]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 18.3067 | Steps: 4 | Val loss: 4649022.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0509 | Steps: 4 | Val loss: 2.1619 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=79525)[0m top1: 0.006063432835820896
[2m[36m(func pid=79525)[0m top5: 0.5093283582089553
[2m[36m(func pid=79525)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=79525)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=79525)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 5.2179 | Steps: 4 | Val loss: 17423.9785 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4354 | Steps: 4 | Val loss: 1.8789 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=78294)[0m top1: 0.1791044776119403
[2m[36m(func pid=78294)[0m top5: 0.7033582089552238
[2m[36m(func pid=78294)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=78294)[0m f1_macro: 0.11136388466053385
[2m[36m(func pid=78294)[0m f1_weighted: 0.19094377093461248
[2m[36m(func pid=78294)[0m f1_per_class: [0.129, 0.248, 0.062, 0.096, 0.016, 0.158, 0.328, 0.0, 0.077, 0.0]
[2m[36m(func pid=78294)[0m 
== Status ==
Current time: 2024-01-07 13:45:06 (running for 00:01:43.24)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.051 |      0.111 |                   10 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.615 |      0.346 |                    9 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  7.94  |      0.024 |                    9 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 18.307 |      0.001 |                    8 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.05970149253731343
[2m[36m(func pid=79093)[0m top5: 0.5167910447761194
[2m[36m(func pid=79093)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=79093)[0m f1_macro: 0.055403348554033484
[2m[36m(func pid=79093)[0m f1_weighted: 0.00903303119107658
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.444, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.4146455223880597
[2m[36m(func pid=78677)[0m top5: 0.8409514925373134
[2m[36m(func pid=78677)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=78677)[0m f1_macro: 0.37611743810911713
[2m[36m(func pid=78677)[0m f1_weighted: 0.35819189341364926
[2m[36m(func pid=78677)[0m f1_per_class: [0.674, 0.579, 0.338, 0.572, 0.246, 0.398, 0.0, 0.38, 0.327, 0.247]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 10.1678 | Steps: 4 | Val loss: 1104856.1250 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1027 | Steps: 4 | Val loss: 2.1385 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 6.3094 | Steps: 4 | Val loss: 9858.3389 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=79525)[0m top1: 0.006063432835820896
[2m[36m(func pid=79525)[0m top5: 0.5093283582089553
[2m[36m(func pid=79525)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=79525)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=79525)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2821 | Steps: 4 | Val loss: 1.7180 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:45:11 (running for 00:01:48.62)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.051 |      0.111 |                   10 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.435 |      0.376 |                   10 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.309 |      0.057 |                   11 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 10.168 |      0.001 |                    9 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.16791044776119404
[2m[36m(func pid=79093)[0m top5: 0.6175373134328358
[2m[36m(func pid=79093)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=79093)[0m f1_macro: 0.0573982849206363
[2m[36m(func pid=79093)[0m f1_weighted: 0.07980658650151969
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.166, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.20615671641791045
[2m[36m(func pid=78294)[0m top5: 0.726679104477612
[2m[36m(func pid=78294)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=78294)[0m f1_macro: 0.12590271550675053
[2m[36m(func pid=78294)[0m f1_weighted: 0.21711924183330997
[2m[36m(func pid=78294)[0m f1_per_class: [0.138, 0.273, 0.064, 0.096, 0.035, 0.162, 0.397, 0.0, 0.094, 0.0]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.43050373134328357
[2m[36m(func pid=78677)[0m top5: 0.8955223880597015
[2m[36m(func pid=78677)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=78677)[0m f1_macro: 0.39700689394930194
[2m[36m(func pid=78677)[0m f1_weighted: 0.379894052518814
[2m[36m(func pid=78677)[0m f1_per_class: [0.673, 0.606, 0.393, 0.582, 0.267, 0.413, 0.045, 0.366, 0.273, 0.352]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 12.1771 | Steps: 4 | Val loss: 418238.6875 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 6.6218 | Steps: 4 | Val loss: 6015.7036 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=79525)[0m top1: 0.006063432835820896
[2m[36m(func pid=79525)[0m top5: 0.5093283582089553
[2m[36m(func pid=79525)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=79525)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=79525)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.8867 | Steps: 4 | Val loss: 2.0823 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3879 | Steps: 4 | Val loss: 1.5177 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=79093)[0m top1: 0.34375
[2m[36m(func pid=79093)[0m top5: 0.6012126865671642
[2m[36m(func pid=79093)[0m f1_micro: 0.34375
[2m[36m(func pid=79093)[0m f1_macro: 0.08819623905889458
[2m[36m(func pid=79093)[0m f1_weighted: 0.21481371355000975
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.382, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:45:17 (running for 00:01:54.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  2.103 |      0.126 |                   11 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.282 |      0.397 |                   11 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.622 |      0.088 |                   12 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 12.177 |      0.001 |                   10 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.23694029850746268
[2m[36m(func pid=78294)[0m top5: 0.753731343283582
[2m[36m(func pid=78294)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=78294)[0m f1_macro: 0.1402422095489572
[2m[36m(func pid=78294)[0m f1_weighted: 0.24485166781375167
[2m[36m(func pid=78294)[0m f1_per_class: [0.141, 0.287, 0.087, 0.115, 0.05, 0.165, 0.462, 0.0, 0.096, 0.0]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.4533582089552239
[2m[36m(func pid=78677)[0m top5: 0.9384328358208955
[2m[36m(func pid=78677)[0m f1_micro: 0.4533582089552239
[2m[36m(func pid=78677)[0m f1_macro: 0.4264396094031776
[2m[36m(func pid=78677)[0m f1_weighted: 0.4498770667757264
[2m[36m(func pid=78677)[0m f1_per_class: [0.565, 0.601, 0.48, 0.6, 0.321, 0.42, 0.287, 0.255, 0.243, 0.492]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 17.3838 | Steps: 4 | Val loss: 200069.3125 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=79525)[0m top1: 0.006063432835820896
[2m[36m(func pid=79525)[0m top5: 0.5093283582089553
[2m[36m(func pid=79525)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=79525)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=79525)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 5.8228 | Steps: 4 | Val loss: 3498.9082 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.7138 | Steps: 4 | Val loss: 1.9891 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3173 | Steps: 4 | Val loss: 1.5092 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:45:22 (running for 00:01:59.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  1.887 |      0.14  |                   12 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.388 |      0.426 |                   12 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.823 |      0.006 |                   13 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 17.384 |      0.001 |                   11 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.03311567164179104
[2m[36m(func pid=79093)[0m top5: 0.5251865671641791
[2m[36m(func pid=79093)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=79093)[0m f1_macro: 0.006431159420289855
[2m[36m(func pid=79093)[0m f1_weighted: 0.002129721636383301
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.2980410447761194
[2m[36m(func pid=78294)[0m top5: 0.8003731343283582
[2m[36m(func pid=78294)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=78294)[0m f1_macro: 0.1815411476367086
[2m[36m(func pid=78294)[0m f1_weighted: 0.2923859393911025
[2m[36m(func pid=78294)[0m f1_per_class: [0.168, 0.354, 0.112, 0.153, 0.088, 0.178, 0.532, 0.0, 0.109, 0.121]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.48927238805970147
[2m[36m(func pid=78677)[0m top5: 0.9510261194029851
[2m[36m(func pid=78677)[0m f1_micro: 0.48927238805970147
[2m[36m(func pid=78677)[0m f1_macro: 0.42018243153759727
[2m[36m(func pid=78677)[0m f1_weighted: 0.5054716249862243
[2m[36m(func pid=78677)[0m f1_per_class: [0.421, 0.541, 0.632, 0.582, 0.2, 0.3, 0.606, 0.111, 0.25, 0.56]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.6983 | Steps: 4 | Val loss: 54306.3242 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=79525)[0m top1: 0.020522388059701493
[2m[36m(func pid=79525)[0m top5: 0.5163246268656716
[2m[36m(func pid=79525)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=79525)[0m f1_macro: 0.004356435643564356
[2m[36m(func pid=79525)[0m f1_weighted: 0.0008940446283434313
[2m[36m(func pid=79525)[0m f1_per_class: [0.044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.3220 | Steps: 4 | Val loss: 4235.6060 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.6383 | Steps: 4 | Val loss: 1.9130 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3269 | Steps: 4 | Val loss: 1.8229 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:45:28 (running for 00:02:05.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  1.714 |      0.182 |                   13 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.317 |      0.42  |                   13 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  4.322 |      0.02  |                   14 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.698 |      0.004 |                   12 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.05923507462686567
[2m[36m(func pid=79093)[0m top5: 0.5130597014925373
[2m[36m(func pid=79093)[0m f1_micro: 0.05923507462686567
[2m[36m(func pid=79093)[0m f1_macro: 0.019892040435764057
[2m[36m(func pid=79093)[0m f1_weighted: 0.007489475041125709
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.087]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.33675373134328357
[2m[36m(func pid=78294)[0m top5: 0.8353544776119403
[2m[36m(func pid=78294)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=78294)[0m f1_macro: 0.20195113776908097
[2m[36m(func pid=78294)[0m f1_weighted: 0.3264002000298635
[2m[36m(func pid=78294)[0m f1_per_class: [0.172, 0.354, 0.168, 0.245, 0.119, 0.151, 0.566, 0.0, 0.13, 0.114]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.439365671641791
[2m[36m(func pid=78677)[0m top5: 0.9421641791044776
[2m[36m(func pid=78677)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=78677)[0m f1_macro: 0.3677673170536861
[2m[36m(func pid=78677)[0m f1_weighted: 0.4507956295710156
[2m[36m(func pid=78677)[0m f1_per_class: [0.378, 0.412, 0.647, 0.542, 0.12, 0.064, 0.627, 0.149, 0.228, 0.512]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 16.7236 | Steps: 4 | Val loss: 38213.6172 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 6.7966 | Steps: 4 | Val loss: 9107.4326 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=79525)[0m top1: 0.013992537313432836
[2m[36m(func pid=79525)[0m top5: 0.49580223880597013
[2m[36m(func pid=79525)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=79525)[0m f1_macro: 0.009185568022565585
[2m[36m(func pid=79525)[0m f1_weighted: 0.013747433616935274
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.079, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.4827 | Steps: 4 | Val loss: 1.8478 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2673 | Steps: 4 | Val loss: 2.1464 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 13:45:33 (running for 00:02:10.71)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  1.638 |      0.202 |                   14 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.327 |      0.368 |                   14 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.797 |      0.032 |                   15 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 16.724 |      0.009 |                   13 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.05970149253731343
[2m[36m(func pid=79093)[0m top5: 0.5149253731343284
[2m[36m(func pid=79093)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=79093)[0m f1_macro: 0.03151548244715947
[2m[36m(func pid=79093)[0m f1_weighted: 0.008755366570329026
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.205]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.37453358208955223
[2m[36m(func pid=78294)[0m top5: 0.8624067164179104
[2m[36m(func pid=78294)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=78294)[0m f1_macro: 0.23667399849196874
[2m[36m(func pid=78294)[0m f1_weighted: 0.36745091832206983
[2m[36m(func pid=78294)[0m f1_per_class: [0.209, 0.385, 0.267, 0.356, 0.121, 0.148, 0.574, 0.0, 0.164, 0.143]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.41744402985074625
[2m[36m(func pid=78677)[0m top5: 0.9169776119402985
[2m[36m(func pid=78677)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=78677)[0m f1_macro: 0.3495265179741069
[2m[36m(func pid=78677)[0m f1_weighted: 0.4264713498492857
[2m[36m(func pid=78677)[0m f1_per_class: [0.341, 0.511, 0.545, 0.428, 0.107, 0.016, 0.605, 0.205, 0.249, 0.488]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.9659 | Steps: 4 | Val loss: 27841.3906 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 8.8850 | Steps: 4 | Val loss: 11443.8994 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.3807 | Steps: 4 | Val loss: 1.7687 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=79525)[0m top1: 0.024720149253731342
[2m[36m(func pid=79525)[0m top5: 0.49673507462686567
[2m[36m(func pid=79525)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=79525)[0m f1_macro: 0.017382435065387378
[2m[36m(func pid=79525)[0m f1_weighted: 0.027781501630655837
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.161, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.1981 | Steps: 4 | Val loss: 2.7748 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:45:39 (running for 00:02:16.10)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  1.483 |      0.237 |                   15 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.267 |      0.35  |                   15 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  8.885 |      0.025 |                   16 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.966 |      0.017 |                   14 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.41511194029850745
[2m[36m(func pid=78294)[0m top5: 0.8931902985074627
[2m[36m(func pid=78294)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=78294)[0m f1_macro: 0.2715700709560388
[2m[36m(func pid=78294)[0m f1_weighted: 0.4045865022086268
[2m[36m(func pid=78294)[0m f1_per_class: [0.267, 0.437, 0.364, 0.434, 0.13, 0.144, 0.589, 0.0, 0.17, 0.182]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.060167910447761194
[2m[36m(func pid=79093)[0m top5: 0.5149253731343284
[2m[36m(func pid=79093)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=79093)[0m f1_macro: 0.02547966661947072
[2m[36m(func pid=79093)[0m f1_weighted: 0.010446770247776349
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.023, 0.0, 0.11, 0.0, 0.121]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.3358208955223881
[2m[36m(func pid=78677)[0m top5: 0.8959888059701493
[2m[36m(func pid=78677)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=78677)[0m f1_macro: 0.33061667970523734
[2m[36m(func pid=78677)[0m f1_weighted: 0.3474291924859914
[2m[36m(func pid=78677)[0m f1_per_class: [0.369, 0.502, 0.667, 0.313, 0.086, 0.0, 0.449, 0.262, 0.218, 0.439]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 17.0749 | Steps: 4 | Val loss: 11682.6660 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 6.8824 | Steps: 4 | Val loss: 28826.6758 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.1789 | Steps: 4 | Val loss: 1.7200 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=79525)[0m top1: 0.060167910447761194
[2m[36m(func pid=79525)[0m top5: 0.43796641791044777
[2m[36m(func pid=79525)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=79525)[0m f1_macro: 0.02864633167988549
[2m[36m(func pid=79525)[0m f1_weighted: 0.0464264963942151
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.269, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5866 | Steps: 4 | Val loss: 3.3939 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 13:45:44 (running for 00:02:21.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  1.381 |      0.272 |                   16 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.198 |      0.331 |                   16 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.882 |      0.069 |                   17 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 17.075 |      0.029 |                   15 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.300839552238806
[2m[36m(func pid=79093)[0m top5: 0.5149253731343284
[2m[36m(func pid=79093)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=79093)[0m f1_macro: 0.06926673027122801
[2m[36m(func pid=79093)[0m f1_weighted: 0.14301556819961797
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.023, 0.463, 0.0, 0.0, 0.207]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4351679104477612
[2m[36m(func pid=78294)[0m top5: 0.9095149253731343
[2m[36m(func pid=78294)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=78294)[0m f1_macro: 0.3068594523189434
[2m[36m(func pid=78294)[0m f1_weighted: 0.42532319647446887
[2m[36m(func pid=78294)[0m f1_per_class: [0.336, 0.494, 0.429, 0.476, 0.149, 0.18, 0.556, 0.031, 0.182, 0.235]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 8.6687 | Steps: 4 | Val loss: 10311.3447 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=78677)[0m top1: 0.29197761194029853
[2m[36m(func pid=78677)[0m top5: 0.8586753731343284
[2m[36m(func pid=78677)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=78677)[0m f1_macro: 0.3114464568208333
[2m[36m(func pid=78677)[0m f1_weighted: 0.28386669365426875
[2m[36m(func pid=78677)[0m f1_per_class: [0.396, 0.544, 0.621, 0.208, 0.08, 0.0, 0.308, 0.249, 0.249, 0.462]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 10.1736 | Steps: 4 | Val loss: 22012.5488 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.3287 | Steps: 4 | Val loss: 1.7092 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=79525)[0m top1: 0.03731343283582089
[2m[36m(func pid=79525)[0m top5: 0.36847014925373134
[2m[36m(func pid=79525)[0m f1_micro: 0.03731343283582089
[2m[36m(func pid=79525)[0m f1_macro: 0.02386192477433353
[2m[36m(func pid=79525)[0m f1_weighted: 0.03756698165215568
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.218, 0.021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2540 | Steps: 4 | Val loss: 3.7362 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:45:50 (running for 00:02:26.90)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  1.179 |      0.307 |                   17 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.587 |      0.311 |                   17 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 10.174 |      0.055 |                   18 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  8.669 |      0.024 |                   16 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.28078358208955223
[2m[36m(func pid=79093)[0m top5: 0.5256529850746269
[2m[36m(func pid=79093)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=79093)[0m f1_macro: 0.05519768625547139
[2m[36m(func pid=79093)[0m f1_weighted: 0.15056363740719164
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.037, 0.49, 0.0, 0.0, 0.025]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.42630597014925375
[2m[36m(func pid=78294)[0m top5: 0.902518656716418
[2m[36m(func pid=78294)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=78294)[0m f1_macro: 0.33696425593644347
[2m[36m(func pid=78294)[0m f1_weighted: 0.4149141930748185
[2m[36m(func pid=78294)[0m f1_per_class: [0.453, 0.52, 0.48, 0.498, 0.178, 0.144, 0.469, 0.1, 0.222, 0.305]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 4.8644 | Steps: 4 | Val loss: 6453.5376 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=78677)[0m top1: 0.28404850746268656
[2m[36m(func pid=78677)[0m top5: 0.8722014925373134
[2m[36m(func pid=78677)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=78677)[0m f1_macro: 0.3269504627924584
[2m[36m(func pid=78677)[0m f1_weighted: 0.2871120166057659
[2m[36m(func pid=78677)[0m f1_per_class: [0.583, 0.583, 0.625, 0.322, 0.088, 0.0, 0.186, 0.213, 0.242, 0.429]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 5.5874 | Steps: 4 | Val loss: 14385.8711 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.9885 | Steps: 4 | Val loss: 1.6675 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=79525)[0m top1: 0.07369402985074627
[2m[36m(func pid=79525)[0m top5: 0.31343283582089554
[2m[36m(func pid=79525)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=79525)[0m f1_macro: 0.028034373390160443
[2m[36m(func pid=79525)[0m f1_weighted: 0.039976546313962504
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.23, 0.035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7891 | Steps: 4 | Val loss: 3.9189 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:45:55 (running for 00:02:32.20)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  1.329 |      0.337 |                   18 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.254 |      0.327 |                   18 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.587 |      0.017 |                   19 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.864 |      0.028 |                   17 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.058768656716417914
[2m[36m(func pid=79093)[0m top5: 0.5666977611940298
[2m[36m(func pid=79093)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=79093)[0m f1_macro: 0.01705759759292142
[2m[36m(func pid=79093)[0m f1_weighted: 0.007316004794529509
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.01]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.43843283582089554
[2m[36m(func pid=78294)[0m top5: 0.9127798507462687
[2m[36m(func pid=78294)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=78294)[0m f1_macro: 0.36182885598219794
[2m[36m(func pid=78294)[0m f1_weighted: 0.43161367990776967
[2m[36m(func pid=78294)[0m f1_per_class: [0.532, 0.532, 0.558, 0.532, 0.151, 0.148, 0.452, 0.253, 0.212, 0.248]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 41.4671 | Steps: 4 | Val loss: 5549.1665 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=78677)[0m top1: 0.3045708955223881
[2m[36m(func pid=78677)[0m top5: 0.8507462686567164
[2m[36m(func pid=78677)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=78677)[0m f1_macro: 0.3050436694027483
[2m[36m(func pid=78677)[0m f1_weighted: 0.3019670460726063
[2m[36m(func pid=78677)[0m f1_per_class: [0.579, 0.621, 0.371, 0.463, 0.09, 0.008, 0.087, 0.211, 0.23, 0.39]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 6.2764 | Steps: 4 | Val loss: 4935.4536 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=79525)[0m top1: 0.07042910447761194
[2m[36m(func pid=79525)[0m top5: 0.2989738805970149
[2m[36m(func pid=79525)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=79525)[0m f1_macro: 0.024746598478059142
[2m[36m(func pid=79525)[0m f1_weighted: 0.0371037946630246
[2m[36m(func pid=79525)[0m f1_per_class: [0.01, 0.214, 0.024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8069 | Steps: 4 | Val loss: 1.6296 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4558 | Steps: 4 | Val loss: 3.6477 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:46:00 (running for 00:02:37.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.988 |      0.362 |                   19 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.789 |      0.305 |                   19 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.276 |      0.019 |                   20 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 41.467 |      0.025 |                   18 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.0625
[2m[36m(func pid=79093)[0m top5: 0.6464552238805971
[2m[36m(func pid=79093)[0m f1_micro: 0.0625
[2m[36m(func pid=79093)[0m f1_macro: 0.019273227006012982
[2m[36m(func pid=79093)[0m f1_weighted: 0.01376500494656764
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.032, 0.02, 0.0, 0.0, 0.0, 0.0, 0.141, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.449160447761194
[2m[36m(func pid=78294)[0m top5: 0.9197761194029851
[2m[36m(func pid=78294)[0m f1_micro: 0.449160447761194
[2m[36m(func pid=78294)[0m f1_macro: 0.38297305939251003
[2m[36m(func pid=78294)[0m f1_weighted: 0.44708363076054397
[2m[36m(func pid=78294)[0m f1_per_class: [0.606, 0.53, 0.545, 0.55, 0.184, 0.179, 0.45, 0.377, 0.209, 0.202]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.9695 | Steps: 4 | Val loss: 2088.8257 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=78677)[0m top1: 0.33908582089552236
[2m[36m(func pid=78677)[0m top5: 0.8717350746268657
[2m[36m(func pid=78677)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=78677)[0m f1_macro: 0.2684873796026209
[2m[36m(func pid=78677)[0m f1_weighted: 0.3200948639274103
[2m[36m(func pid=78677)[0m f1_per_class: [0.222, 0.574, 0.202, 0.598, 0.108, 0.008, 0.068, 0.25, 0.227, 0.429]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 5.7795 | Steps: 4 | Val loss: 1589.0156 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=79525)[0m top1: 0.10167910447761194
[2m[36m(func pid=79525)[0m top5: 0.3111007462686567
[2m[36m(func pid=79525)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=79525)[0m f1_macro: 0.02808199310278145
[2m[36m(func pid=79525)[0m f1_weighted: 0.043391868252446474
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.251, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.7306 | Steps: 4 | Val loss: 1.6426 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0968 | Steps: 4 | Val loss: 3.3792 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:46:06 (running for 00:02:43.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.807 |      0.383 |                   20 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.456 |      0.268 |                   20 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.78  |      0.025 |                   21 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.969 |      0.028 |                   19 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.06576492537313433
[2m[36m(func pid=79093)[0m top5: 0.6343283582089553
[2m[36m(func pid=79093)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=79093)[0m f1_macro: 0.025429262070330676
[2m[36m(func pid=79093)[0m f1_weighted: 0.020195155197281215
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.015, 0.035, 0.0, 0.0, 0.0, 0.173, 0.0, 0.031]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.41651119402985076
[2m[36m(func pid=78294)[0m top5: 0.9081156716417911
[2m[36m(func pid=78294)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=78294)[0m f1_macro: 0.36490203203536264
[2m[36m(func pid=78294)[0m f1_weighted: 0.4116738981690159
[2m[36m(func pid=78294)[0m f1_per_class: [0.617, 0.483, 0.545, 0.559, 0.205, 0.155, 0.361, 0.365, 0.209, 0.15]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.9478 | Steps: 4 | Val loss: 1255.0323 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=78677)[0m top1: 0.3582089552238806
[2m[36m(func pid=78677)[0m top5: 0.8875932835820896
[2m[36m(func pid=78677)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=78677)[0m f1_macro: 0.2999199574028716
[2m[36m(func pid=78677)[0m f1_weighted: 0.3472183286378903
[2m[36m(func pid=78677)[0m f1_per_class: [0.455, 0.505, 0.195, 0.607, 0.122, 0.023, 0.163, 0.268, 0.242, 0.419]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 5.9342 | Steps: 4 | Val loss: 1482.1113 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=79525)[0m top1: 0.11100746268656717
[2m[36m(func pid=79525)[0m top5: 0.279384328358209
[2m[36m(func pid=79525)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=79525)[0m f1_macro: 0.033223195135029734
[2m[36m(func pid=79525)[0m f1_weighted: 0.048137143002949204
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.268, 0.033, 0.0, 0.0, 0.0, 0.0, 0.031, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6737 | Steps: 4 | Val loss: 1.6694 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5726 | Steps: 4 | Val loss: 2.6511 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:46:11 (running for 00:02:48.82)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.731 |      0.365 |                   21 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.097 |      0.3   |                   21 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.934 |      0.11  |                   22 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.948 |      0.033 |                   20 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.3829291044776119
[2m[36m(func pid=79093)[0m top5: 0.6403917910447762
[2m[36m(func pid=79093)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=79093)[0m f1_macro: 0.10978685911370026
[2m[36m(func pid=79093)[0m f1_weighted: 0.29649272170146573
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.067, 0.453, 0.0, 0.0, 0.567, 0.011, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 24.1652 | Steps: 4 | Val loss: 1262.1533 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=78294)[0m top1: 0.3810634328358209
[2m[36m(func pid=78294)[0m top5: 0.9006529850746269
[2m[36m(func pid=78294)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=78294)[0m f1_macro: 0.3394841701131995
[2m[36m(func pid=78294)[0m f1_weighted: 0.3690439907130334
[2m[36m(func pid=78294)[0m f1_per_class: [0.615, 0.457, 0.558, 0.554, 0.157, 0.089, 0.268, 0.345, 0.212, 0.139]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.4291044776119403
[2m[36m(func pid=78677)[0m top5: 0.9053171641791045
[2m[36m(func pid=78677)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=78677)[0m f1_macro: 0.3550796610048191
[2m[36m(func pid=78677)[0m f1_weighted: 0.44185675333995106
[2m[36m(func pid=78677)[0m f1_per_class: [0.5, 0.566, 0.333, 0.576, 0.139, 0.095, 0.433, 0.29, 0.308, 0.311]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 9.2214 | Steps: 4 | Val loss: 3218.7183 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=79525)[0m top1: 0.03311567164179104
[2m[36m(func pid=79525)[0m top5: 0.24813432835820895
[2m[36m(func pid=79525)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=79525)[0m f1_macro: 0.027425540901274626
[2m[36m(func pid=79525)[0m f1_weighted: 0.030698960408894947
[2m[36m(func pid=79525)[0m f1_per_class: [0.012, 0.154, 0.031, 0.0, 0.0, 0.0, 0.0, 0.061, 0.0, 0.016]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8533 | Steps: 4 | Val loss: 1.6874 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0528 | Steps: 4 | Val loss: 2.4686 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:46:17 (running for 00:02:54.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.674 |      0.339 |                   22 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.573 |      0.355 |                   22 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  9.221 |      0.083 |                   23 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 24.165 |      0.027 |                   21 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.32136194029850745
[2m[36m(func pid=79093)[0m top5: 0.6623134328358209
[2m[36m(func pid=79093)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=79093)[0m f1_macro: 0.0832783612099074
[2m[36m(func pid=79093)[0m f1_weighted: 0.21581793624802906
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.094, 0.262, 0.0, 0.0, 0.477, 0.0, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 6.9506 | Steps: 4 | Val loss: 625.6395 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=78294)[0m top1: 0.34794776119402987
[2m[36m(func pid=78294)[0m top5: 0.9029850746268657
[2m[36m(func pid=78294)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=78294)[0m f1_macro: 0.31499867239438517
[2m[36m(func pid=78294)[0m f1_weighted: 0.32932267545483057
[2m[36m(func pid=78294)[0m f1_per_class: [0.592, 0.393, 0.571, 0.577, 0.154, 0.055, 0.174, 0.304, 0.204, 0.126]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.4612873134328358
[2m[36m(func pid=78677)[0m top5: 0.902518656716418
[2m[36m(func pid=78677)[0m f1_micro: 0.4612873134328358
[2m[36m(func pid=78677)[0m f1_macro: 0.36721910551663256
[2m[36m(func pid=78677)[0m f1_weighted: 0.4795261709086607
[2m[36m(func pid=78677)[0m f1_per_class: [0.467, 0.545, 0.324, 0.565, 0.139, 0.213, 0.548, 0.23, 0.328, 0.314]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 12.8623 | Steps: 4 | Val loss: 3471.4512 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=79525)[0m top1: 0.03544776119402985
[2m[36m(func pid=79525)[0m top5: 0.539179104477612
[2m[36m(func pid=79525)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=79525)[0m f1_macro: 0.043446768064098265
[2m[36m(func pid=79525)[0m f1_weighted: 0.02218081503271915
[2m[36m(func pid=79525)[0m f1_per_class: [0.033, 0.005, 0.041, 0.0, 0.0, 0.0, 0.0, 0.351, 0.0, 0.005]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.9657 | Steps: 4 | Val loss: 1.7084 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1736 | Steps: 4 | Val loss: 2.5096 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:46:22 (running for 00:02:59.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.853 |      0.315 |                   23 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.053 |      0.367 |                   23 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 12.862 |      0.018 |                   24 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  6.951 |      0.043 |                   22 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.013526119402985074
[2m[36m(func pid=79093)[0m top5: 0.6417910447761194
[2m[36m(func pid=79093)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=79093)[0m f1_macro: 0.018441979266620917
[2m[36m(func pid=79093)[0m f1_weighted: 0.003199851130981706
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.121, 0.0, 0.015, 0.0, 0.003, 0.02, 0.0, 0.025]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.3353544776119403
[2m[36m(func pid=78294)[0m top5: 0.9034514925373134
[2m[36m(func pid=78294)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=78294)[0m f1_macro: 0.3099820279360294
[2m[36m(func pid=78294)[0m f1_weighted: 0.30401260460731605
[2m[36m(func pid=78294)[0m f1_per_class: [0.58, 0.375, 0.6, 0.59, 0.19, 0.029, 0.096, 0.296, 0.22, 0.123]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 6.7491 | Steps: 4 | Val loss: 465.4424 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=78677)[0m top1: 0.47574626865671643
[2m[36m(func pid=78677)[0m top5: 0.8987873134328358
[2m[36m(func pid=78677)[0m f1_micro: 0.47574626865671643
[2m[36m(func pid=78677)[0m f1_macro: 0.35732192250415346
[2m[36m(func pid=78677)[0m f1_weighted: 0.4875724957725206
[2m[36m(func pid=78677)[0m f1_per_class: [0.443, 0.538, 0.269, 0.558, 0.154, 0.261, 0.583, 0.169, 0.327, 0.271]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 6.5352 | Steps: 4 | Val loss: 3248.8821 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=79525)[0m top1: 0.06343283582089553
[2m[36m(func pid=79525)[0m top5: 0.5214552238805971
[2m[36m(func pid=79525)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=79525)[0m f1_macro: 0.03288697517058878
[2m[36m(func pid=79525)[0m f1_weighted: 0.018841804542578
[2m[36m(func pid=79525)[0m f1_per_class: [0.046, 0.053, 0.052, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.038]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4723 | Steps: 4 | Val loss: 1.7473 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0926 | Steps: 4 | Val loss: 2.6591 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:46:28 (running for 00:03:05.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.966 |      0.31  |                   24 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.174 |      0.357 |                   24 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.535 |      0.055 |                   25 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  6.749 |      0.033 |                   23 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.11240671641791045
[2m[36m(func pid=79093)[0m top5: 0.6413246268656716
[2m[36m(func pid=79093)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=79093)[0m f1_macro: 0.05475063003687645
[2m[36m(func pid=79093)[0m f1_weighted: 0.0828571896616831
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.093, 0.259, 0.007, 0.0, 0.0, 0.169, 0.0, 0.02]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.32649253731343286
[2m[36m(func pid=78294)[0m top5: 0.8899253731343284
[2m[36m(func pid=78294)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=78294)[0m f1_macro: 0.30609840076991435
[2m[36m(func pid=78294)[0m f1_weighted: 0.29212610109177223
[2m[36m(func pid=78294)[0m f1_per_class: [0.593, 0.34, 0.6, 0.58, 0.185, 0.036, 0.08, 0.308, 0.227, 0.112]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 4.3700 | Steps: 4 | Val loss: 195.4004 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=78677)[0m top1: 0.44776119402985076
[2m[36m(func pid=78677)[0m top5: 0.8955223880597015
[2m[36m(func pid=78677)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=78677)[0m f1_macro: 0.3364230534378815
[2m[36m(func pid=78677)[0m f1_weighted: 0.46284630205429417
[2m[36m(func pid=78677)[0m f1_per_class: [0.317, 0.539, 0.231, 0.508, 0.197, 0.282, 0.549, 0.188, 0.262, 0.291]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 4.8486 | Steps: 4 | Val loss: 947.6008 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5098 | Steps: 4 | Val loss: 1.7570 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=79525)[0m top1: 0.025652985074626867
[2m[36m(func pid=79525)[0m top5: 0.5298507462686567
[2m[36m(func pid=79525)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=79525)[0m f1_macro: 0.027108528284520633
[2m[36m(func pid=79525)[0m f1_weighted: 0.020010748040526216
[2m[36m(func pid=79525)[0m f1_per_class: [0.079, 0.103, 0.075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4452 | Steps: 4 | Val loss: 2.7403 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 13:46:33 (running for 00:03:10.69)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.472 |      0.306 |                   25 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.093 |      0.336 |                   25 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  4.849 |      0.054 |                   26 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.37  |      0.027 |                   24 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.10774253731343283
[2m[36m(func pid=79093)[0m top5: 0.7280783582089553
[2m[36m(func pid=79093)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=79093)[0m f1_macro: 0.05422853119334562
[2m[36m(func pid=79093)[0m f1_weighted: 0.07379359077586604
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.052, 0.039, 0.188, 0.0, 0.0, 0.0, 0.2, 0.0, 0.063]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.3204291044776119
[2m[36m(func pid=78294)[0m top5: 0.8959888059701493
[2m[36m(func pid=78294)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=78294)[0m f1_macro: 0.3030767019778031
[2m[36m(func pid=78294)[0m f1_weighted: 0.2803291049338304
[2m[36m(func pid=78294)[0m f1_per_class: [0.593, 0.338, 0.6, 0.588, 0.174, 0.029, 0.039, 0.279, 0.251, 0.139]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.9862 | Steps: 4 | Val loss: 60.0734 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=78677)[0m top1: 0.42350746268656714
[2m[36m(func pid=78677)[0m top5: 0.8936567164179104
[2m[36m(func pid=78677)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=78677)[0m f1_macro: 0.3274181428516336
[2m[36m(func pid=78677)[0m f1_weighted: 0.44168535077333165
[2m[36m(func pid=78677)[0m f1_per_class: [0.364, 0.549, 0.118, 0.445, 0.229, 0.244, 0.524, 0.335, 0.199, 0.267]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 8.1749 | Steps: 4 | Val loss: 37121.2773 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=79525)[0m top1: 0.024253731343283583
[2m[36m(func pid=79525)[0m top5: 0.5480410447761194
[2m[36m(func pid=79525)[0m f1_micro: 0.024253731343283583
[2m[36m(func pid=79525)[0m f1_macro: 0.026665097802058085
[2m[36m(func pid=79525)[0m f1_weighted: 0.021335033263467338
[2m[36m(func pid=79525)[0m f1_per_class: [0.045, 0.097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.107, 0.018]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5416 | Steps: 4 | Val loss: 1.7895 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1266 | Steps: 4 | Val loss: 3.0991 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:46:39 (running for 00:03:16.12)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.51  |      0.303 |                   26 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.445 |      0.327 |                   26 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  8.175 |      0.014 |                   27 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.986 |      0.027 |                   25 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.060167910447761194
[2m[36m(func pid=79093)[0m top5: 0.5177238805970149
[2m[36m(func pid=79093)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=79093)[0m f1_macro: 0.01363270090103289
[2m[36m(func pid=79093)[0m f1_weighted: 0.010939995921790213
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.3111007462686567
[2m[36m(func pid=78294)[0m top5: 0.8936567164179104
[2m[36m(func pid=78294)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=78294)[0m f1_macro: 0.29565375255173576
[2m[36m(func pid=78294)[0m f1_weighted: 0.26982368908250093
[2m[36m(func pid=78294)[0m f1_per_class: [0.587, 0.273, 0.6, 0.597, 0.168, 0.049, 0.028, 0.271, 0.25, 0.134]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 5.1735 | Steps: 4 | Val loss: 97.5574 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=78677)[0m top1: 0.3712686567164179
[2m[36m(func pid=78677)[0m top5: 0.8810634328358209
[2m[36m(func pid=78677)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=78677)[0m f1_macro: 0.3030193130497788
[2m[36m(func pid=78677)[0m f1_weighted: 0.3896005887907396
[2m[36m(func pid=78677)[0m f1_per_class: [0.29, 0.554, 0.122, 0.368, 0.228, 0.213, 0.432, 0.39, 0.128, 0.305]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 10.3817 | Steps: 4 | Val loss: 36290.9102 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4947 | Steps: 4 | Val loss: 1.8244 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=79525)[0m top1: 0.015391791044776119
[2m[36m(func pid=79525)[0m top5: 0.5223880597014925
[2m[36m(func pid=79525)[0m f1_micro: 0.015391791044776119
[2m[36m(func pid=79525)[0m f1_macro: 0.013232494058646304
[2m[36m(func pid=79525)[0m f1_weighted: 0.000979173003226164
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.085, 0.0, 0.021, 0.0, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0462 | Steps: 4 | Val loss: 3.3360 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:46:44 (running for 00:03:21.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.542 |      0.296 |                   27 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.127 |      0.303 |                   27 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 10.382 |      0.011 |                   28 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.173 |      0.013 |                   26 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.05830223880597015
[2m[36m(func pid=79093)[0m top5: 0.5177238805970149
[2m[36m(func pid=79093)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=79093)[0m f1_macro: 0.011499444650129581
[2m[36m(func pid=79093)[0m f1_weighted: 0.00726848679593517
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.30830223880597013
[2m[36m(func pid=78294)[0m top5: 0.8899253731343284
[2m[36m(func pid=78294)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=78294)[0m f1_macro: 0.2949249189386051
[2m[36m(func pid=78294)[0m f1_weighted: 0.2743249531424492
[2m[36m(func pid=78294)[0m f1_per_class: [0.577, 0.316, 0.545, 0.579, 0.147, 0.087, 0.022, 0.27, 0.259, 0.147]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7478 | Steps: 4 | Val loss: 71.3388 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=78677)[0m top1: 0.3414179104477612
[2m[36m(func pid=78677)[0m top5: 0.8684701492537313
[2m[36m(func pid=78677)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=78677)[0m f1_macro: 0.287656813020416
[2m[36m(func pid=78677)[0m f1_weighted: 0.3654576005098738
[2m[36m(func pid=78677)[0m f1_per_class: [0.264, 0.541, 0.115, 0.373, 0.21, 0.235, 0.352, 0.369, 0.124, 0.294]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5530 | Steps: 4 | Val loss: 1.8404 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 6.6382 | Steps: 4 | Val loss: 21705.9824 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=79525)[0m top1: 0.014925373134328358
[2m[36m(func pid=79525)[0m top5: 0.5111940298507462
[2m[36m(func pid=79525)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=79525)[0m f1_macro: 0.016271818934626498
[2m[36m(func pid=79525)[0m f1_weighted: 0.0012213204553369911
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.107, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.037]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0772 | Steps: 4 | Val loss: 3.8226 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:46:49 (running for 00:03:26.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.553 |      0.303 |                   29 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.046 |      0.288 |                   28 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 10.382 |      0.011 |                   28 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.748 |      0.016 |                   27 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.292910447761194
[2m[36m(func pid=78294)[0m top5: 0.8899253731343284
[2m[36m(func pid=78294)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=78294)[0m f1_macro: 0.30316702526760403
[2m[36m(func pid=78294)[0m f1_weighted: 0.27029548080358595
[2m[36m(func pid=78294)[0m f1_per_class: [0.53, 0.35, 0.686, 0.537, 0.145, 0.073, 0.033, 0.261, 0.269, 0.148]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.05783582089552239
[2m[36m(func pid=79093)[0m top5: 0.5149253731343284
[2m[36m(func pid=79093)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=79093)[0m f1_macro: 0.010939567710630791
[2m[36m(func pid=79093)[0m f1_weighted: 0.006326988787864822
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 4.7771 | Steps: 4 | Val loss: 100.9468 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=78677)[0m top1: 0.3138992537313433
[2m[36m(func pid=78677)[0m top5: 0.8418843283582089
[2m[36m(func pid=78677)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=78677)[0m f1_macro: 0.2686285279809192
[2m[36m(func pid=78677)[0m f1_weighted: 0.33989685804641895
[2m[36m(func pid=78677)[0m f1_per_class: [0.227, 0.504, 0.107, 0.336, 0.217, 0.317, 0.313, 0.275, 0.111, 0.278]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4455 | Steps: 4 | Val loss: 1.8138 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 10.4186 | Steps: 4 | Val loss: 38387.0469 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=79525)[0m top1: 0.013992537313432836
[2m[36m(func pid=79525)[0m top5: 0.5223880597014925
[2m[36m(func pid=79525)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=79525)[0m f1_macro: 0.013471329280320899
[2m[36m(func pid=79525)[0m f1_weighted: 0.0009864475170896455
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.091, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6018 | Steps: 4 | Val loss: 4.4474 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:46:55 (running for 00:03:32.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.446 |      0.326 |                   30 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.077 |      0.269 |                   29 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.638 |      0.011 |                   29 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.777 |      0.013 |                   28 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.3101679104477612
[2m[36m(func pid=78294)[0m top5: 0.8875932835820896
[2m[36m(func pid=78294)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=78294)[0m f1_macro: 0.3262947016162064
[2m[36m(func pid=78294)[0m f1_weighted: 0.2930018589195761
[2m[36m(func pid=78294)[0m f1_per_class: [0.511, 0.434, 0.688, 0.495, 0.158, 0.208, 0.045, 0.276, 0.273, 0.175]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.2980410447761194
[2m[36m(func pid=79093)[0m top5: 0.5163246268656716
[2m[36m(func pid=79093)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=79093)[0m f1_macro: 0.04612053410321184
[2m[36m(func pid=79093)[0m f1_weighted: 0.13745812169753902
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.461, 0.0, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.4473 | Steps: 4 | Val loss: 84.2068 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=78677)[0m top1: 0.2677238805970149
[2m[36m(func pid=78677)[0m top5: 0.8138992537313433
[2m[36m(func pid=78677)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=78677)[0m f1_macro: 0.23545954127327554
[2m[36m(func pid=78677)[0m f1_weighted: 0.2856157852383066
[2m[36m(func pid=78677)[0m f1_per_class: [0.155, 0.481, 0.122, 0.335, 0.206, 0.285, 0.183, 0.169, 0.097, 0.32]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 7.1091 | Steps: 4 | Val loss: 18092.7363 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4597 | Steps: 4 | Val loss: 1.8180 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=79525)[0m top1: 0.013526119402985074
[2m[36m(func pid=79525)[0m top5: 0.5289179104477612
[2m[36m(func pid=79525)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=79525)[0m f1_macro: 0.013083059410962605
[2m[36m(func pid=79525)[0m f1_weighted: 0.0009126283825230171
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.096, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.017]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1440 | Steps: 4 | Val loss: 4.9116 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:47:00 (running for 00:03:37.81)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.46  |      0.322 |                   31 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.602 |      0.235 |                   30 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 10.419 |      0.046 |                   30 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.447 |      0.013 |                   29 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.31529850746268656
[2m[36m(func pid=78294)[0m top5: 0.8722014925373134
[2m[36m(func pid=78294)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=78294)[0m f1_macro: 0.32239479502633084
[2m[36m(func pid=78294)[0m f1_weighted: 0.2969640780876899
[2m[36m(func pid=78294)[0m f1_per_class: [0.473, 0.466, 0.632, 0.481, 0.151, 0.214, 0.054, 0.288, 0.245, 0.22]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.013526119402985074
[2m[36m(func pid=79093)[0m top5: 0.5419776119402985
[2m[36m(func pid=79093)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=79093)[0m f1_macro: 0.004693206278569183
[2m[36m(func pid=79093)[0m f1_weighted: 0.007996455147172692
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0, 0.021]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 4.6999 | Steps: 4 | Val loss: 80.7353 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=78677)[0m top1: 0.25279850746268656
[2m[36m(func pid=78677)[0m top5: 0.7915111940298507
[2m[36m(func pid=78677)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=78677)[0m f1_macro: 0.22836928812891455
[2m[36m(func pid=78677)[0m f1_weighted: 0.29437465484486997
[2m[36m(func pid=78677)[0m f1_per_class: [0.113, 0.389, 0.136, 0.286, 0.19, 0.229, 0.314, 0.292, 0.126, 0.209]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 5.2536 | Steps: 4 | Val loss: 9473.0889 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2129 | Steps: 4 | Val loss: 1.7906 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=79525)[0m top1: 0.029850746268656716
[2m[36m(func pid=79525)[0m top5: 0.4039179104477612
[2m[36m(func pid=79525)[0m f1_micro: 0.029850746268656716
[2m[36m(func pid=79525)[0m f1_macro: 0.018419040257135654
[2m[36m(func pid=79525)[0m f1_weighted: 0.0036101809491890973
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.005, 0.102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054, 0.023]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.1490 | Steps: 4 | Val loss: 5.0639 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=79093)[0m top1: 0.05783582089552239
[2m[36m(func pid=79093)[0m top5: 0.6063432835820896
[2m[36m(func pid=79093)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=79093)[0m f1_macro: 0.012627291242362526
[2m[36m(func pid=79093)[0m f1_weighted: 0.007303097546888774
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:47:06 (running for 00:03:43.20)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.46  |      0.322 |                   31 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.144 |      0.228 |                   31 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.254 |      0.013 |                   32 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.7   |      0.018 |                   30 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.32322761194029853
[2m[36m(func pid=78294)[0m top5: 0.8861940298507462
[2m[36m(func pid=78294)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=78294)[0m f1_macro: 0.3221490788022829
[2m[36m(func pid=78294)[0m f1_weighted: 0.3128942181292813
[2m[36m(func pid=78294)[0m f1_per_class: [0.452, 0.464, 0.571, 0.486, 0.169, 0.234, 0.1, 0.294, 0.22, 0.232]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8128 | Steps: 4 | Val loss: 88.1027 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=78677)[0m top1: 0.25466417910447764
[2m[36m(func pid=78677)[0m top5: 0.7784514925373134
[2m[36m(func pid=78677)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=78677)[0m f1_macro: 0.22730561393963308
[2m[36m(func pid=78677)[0m f1_weighted: 0.2963725575614087
[2m[36m(func pid=78677)[0m f1_per_class: [0.119, 0.339, 0.18, 0.273, 0.197, 0.177, 0.38, 0.278, 0.162, 0.167]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 8.7774 | Steps: 4 | Val loss: 6483.4883 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3416 | Steps: 4 | Val loss: 1.7708 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=79525)[0m top1: 0.03125
[2m[36m(func pid=79525)[0m top5: 0.39365671641791045
[2m[36m(func pid=79525)[0m f1_micro: 0.03125
[2m[36m(func pid=79525)[0m f1_macro: 0.02276009866115733
[2m[36m(func pid=79525)[0m f1_weighted: 0.004441465921508029
[2m[36m(func pid=79525)[0m f1_per_class: [0.04, 0.005, 0.108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.055, 0.02]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0704 | Steps: 4 | Val loss: 5.2279 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 13:47:11 (running for 00:03:48.71)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.342 |      0.329 |                   33 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.149 |      0.227 |                   32 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.254 |      0.013 |                   32 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.813 |      0.023 |                   31 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.3353544776119403
[2m[36m(func pid=78294)[0m top5: 0.8894589552238806
[2m[36m(func pid=78294)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=78294)[0m f1_macro: 0.3292856645768699
[2m[36m(func pid=78294)[0m f1_weighted: 0.3309026608385059
[2m[36m(func pid=78294)[0m f1_per_class: [0.321, 0.506, 0.545, 0.448, 0.162, 0.256, 0.159, 0.354, 0.201, 0.34]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m top1: 0.06343283582089553
[2m[36m(func pid=79093)[0m top5: 0.6301305970149254
[2m[36m(func pid=79093)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=79093)[0m f1_macro: 0.016297016013004727
[2m[36m(func pid=79093)[0m f1_weighted: 0.0175386171963479
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.126, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.3829 | Steps: 4 | Val loss: 53.1692 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=78677)[0m top1: 0.25046641791044777
[2m[36m(func pid=78677)[0m top5: 0.7653917910447762
[2m[36m(func pid=78677)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=78677)[0m f1_macro: 0.2197202372091002
[2m[36m(func pid=78677)[0m f1_weighted: 0.2902251575348686
[2m[36m(func pid=78677)[0m f1_per_class: [0.121, 0.256, 0.186, 0.285, 0.176, 0.146, 0.401, 0.325, 0.157, 0.145]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.7235 | Steps: 4 | Val loss: 3036.9048 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2364 | Steps: 4 | Val loss: 1.7295 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=79525)[0m top1: 0.03311567164179104
[2m[36m(func pid=79525)[0m top5: 0.4048507462686567
[2m[36m(func pid=79525)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=79525)[0m f1_macro: 0.032614865390687184
[2m[36m(func pid=79525)[0m f1_weighted: 0.00569016012544864
[2m[36m(func pid=79525)[0m f1_per_class: [0.093, 0.005, 0.173, 0.0, 0.0, 0.0, 0.0, 0.0, 0.055, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1572 | Steps: 4 | Val loss: 4.6610 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=79093)[0m top1: 0.06763059701492537
[2m[36m(func pid=79093)[0m top5: 0.6665111940298507
[2m[36m(func pid=79093)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=79093)[0m f1_macro: 0.022695387709552012
[2m[36m(func pid=79093)[0m f1_weighted: 0.025865871220586666
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.076, 0.0, 0.0, 0.0, 0.0, 0.017, 0.134, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:47:17 (running for 00:03:54.14)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.342 |      0.329 |                   33 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.07  |      0.22  |                   33 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.723 |      0.023 |                   34 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.383 |      0.033 |                   32 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.35027985074626866
[2m[36m(func pid=78294)[0m top5: 0.8997201492537313
[2m[36m(func pid=78294)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=78294)[0m f1_macro: 0.33814662532675377
[2m[36m(func pid=78294)[0m f1_weighted: 0.35606024721431984
[2m[36m(func pid=78294)[0m f1_per_class: [0.402, 0.519, 0.5, 0.455, 0.149, 0.246, 0.229, 0.352, 0.218, 0.312]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.9105 | Steps: 4 | Val loss: 74.2508 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=78677)[0m top1: 0.2905783582089552
[2m[36m(func pid=78677)[0m top5: 0.8092350746268657
[2m[36m(func pid=78677)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=78677)[0m f1_macro: 0.2352322055030319
[2m[36m(func pid=78677)[0m f1_weighted: 0.3188336460558969
[2m[36m(func pid=78677)[0m f1_per_class: [0.189, 0.258, 0.149, 0.405, 0.196, 0.165, 0.372, 0.318, 0.161, 0.139]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3464 | Steps: 4 | Val loss: 1.7030 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 18.3101 | Steps: 4 | Val loss: 1610.0828 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=79525)[0m top1: 0.03264925373134328
[2m[36m(func pid=79525)[0m top5: 0.659981343283582
[2m[36m(func pid=79525)[0m f1_micro: 0.03264925373134328
[2m[36m(func pid=79525)[0m f1_macro: 0.029839619151722778
[2m[36m(func pid=79525)[0m f1_weighted: 0.0061877192887120885
[2m[36m(func pid=79525)[0m f1_per_class: [0.074, 0.011, 0.145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.055, 0.015]
[2m[36m(func pid=79525)[0m 
== Status ==
Current time: 2024-01-07 13:47:22 (running for 00:03:59.51)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.346 |      0.343 |                   35 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.157 |      0.235 |                   34 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.723 |      0.023 |                   34 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.91  |      0.03  |                   33 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.3675373134328358
[2m[36m(func pid=78294)[0m top5: 0.9090485074626866
[2m[36m(func pid=78294)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=78294)[0m f1_macro: 0.343083753512478
[2m[36m(func pid=78294)[0m f1_weighted: 0.38194073533792977
[2m[36m(func pid=78294)[0m f1_per_class: [0.325, 0.537, 0.453, 0.45, 0.165, 0.268, 0.306, 0.348, 0.216, 0.364]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4659 | Steps: 4 | Val loss: 4.7423 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=79093)[0m top1: 0.06436567164179105
[2m[36m(func pid=79093)[0m top5: 0.6930970149253731
[2m[36m(func pid=79093)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=79093)[0m f1_macro: 0.021167566986160576
[2m[36m(func pid=79093)[0m f1_weighted: 0.020562061230257875
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.061, 0.0, 0.0, 0.0, 0.0, 0.005, 0.145, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.6115 | Steps: 4 | Val loss: 36.8443 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=78677)[0m top1: 0.28777985074626866
[2m[36m(func pid=78677)[0m top5: 0.8250932835820896
[2m[36m(func pid=78677)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=78677)[0m f1_macro: 0.232484234730298
[2m[36m(func pid=78677)[0m f1_weighted: 0.30699199011934314
[2m[36m(func pid=78677)[0m f1_per_class: [0.212, 0.227, 0.146, 0.443, 0.182, 0.188, 0.306, 0.303, 0.18, 0.137]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 8.9320 | Steps: 4 | Val loss: 1266.5323 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2157 | Steps: 4 | Val loss: 1.7094 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=79525)[0m top1: 0.017723880597014924
[2m[36m(func pid=79525)[0m top5: 0.42117537313432835
[2m[36m(func pid=79525)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=79525)[0m f1_macro: 0.024036545813463185
[2m[36m(func pid=79525)[0m f1_weighted: 0.001924378568252913
[2m[36m(func pid=79525)[0m f1_per_class: [0.025, 0.0, 0.197, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m top1: 0.2751865671641791
[2m[36m(func pid=79093)[0m top5: 0.6805037313432836
[2m[36m(func pid=79093)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=79093)[0m f1_macro: 0.055860748626179915
[2m[36m(func pid=79093)[0m f1_weighted: 0.1606905392733713
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.046, 0.0, 0.0, 0.0, 0.0, 0.513, 0.0, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:47:27 (running for 00:04:04.86)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.346 |      0.343 |                   35 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.466 |      0.232 |                   35 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  8.932 |      0.056 |                   36 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.611 |      0.024 |                   34 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.37033582089552236
[2m[36m(func pid=78294)[0m top5: 0.9006529850746269
[2m[36m(func pid=78294)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=78294)[0m f1_macro: 0.3469164638608775
[2m[36m(func pid=78294)[0m f1_weighted: 0.3883474597449791
[2m[36m(func pid=78294)[0m f1_per_class: [0.343, 0.545, 0.471, 0.442, 0.152, 0.242, 0.335, 0.364, 0.228, 0.348]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.2538 | Steps: 4 | Val loss: 4.4949 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.9794 | Steps: 4 | Val loss: 27.1309 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=78677)[0m top1: 0.3306902985074627
[2m[36m(func pid=78677)[0m top5: 0.8502798507462687
[2m[36m(func pid=78677)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=78677)[0m f1_macro: 0.25926053492780554
[2m[36m(func pid=78677)[0m f1_weighted: 0.369698580424253
[2m[36m(func pid=78677)[0m f1_per_class: [0.239, 0.37, 0.157, 0.418, 0.194, 0.257, 0.45, 0.174, 0.213, 0.12]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 10.2727 | Steps: 4 | Val loss: 731.6165 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=79525)[0m top1: 0.017723880597014924
[2m[36m(func pid=79525)[0m top5: 0.6865671641791045
[2m[36m(func pid=79525)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=79525)[0m f1_macro: 0.02349733615073501
[2m[36m(func pid=79525)[0m f1_weighted: 0.0019337337126085772
[2m[36m(func pid=79525)[0m f1_per_class: [0.025, 0.0, 0.183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2061 | Steps: 4 | Val loss: 1.7558 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:47:33 (running for 00:04:10.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.216 |      0.347 |                   36 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.254 |      0.259 |                   36 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 10.273 |      0.058 |                   37 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.979 |      0.023 |                   35 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.2621268656716418
[2m[36m(func pid=79093)[0m top5: 0.7117537313432836
[2m[36m(func pid=79093)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=79093)[0m f1_macro: 0.05776031196669405
[2m[36m(func pid=79093)[0m f1_weighted: 0.16003073287311026
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.032, 0.016, 0.53, 0.0, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.36800373134328357
[2m[36m(func pid=78294)[0m top5: 0.8894589552238806
[2m[36m(func pid=78294)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=78294)[0m f1_macro: 0.345483687508838
[2m[36m(func pid=78294)[0m f1_weighted: 0.38893531335554343
[2m[36m(func pid=78294)[0m f1_per_class: [0.284, 0.539, 0.49, 0.382, 0.145, 0.239, 0.396, 0.381, 0.246, 0.353]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0460 | Steps: 4 | Val loss: 5.2727 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7704 | Steps: 4 | Val loss: 15.3196 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=78677)[0m top1: 0.29384328358208955
[2m[36m(func pid=78677)[0m top5: 0.8148320895522388
[2m[36m(func pid=78677)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=78677)[0m f1_macro: 0.24286228885631314
[2m[36m(func pid=78677)[0m f1_weighted: 0.3071808901183335
[2m[36m(func pid=78677)[0m f1_per_class: [0.214, 0.399, 0.415, 0.171, 0.199, 0.252, 0.489, 0.014, 0.188, 0.088]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 4.6053 | Steps: 4 | Val loss: 559.5287 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=79525)[0m top1: 0.016791044776119403
[2m[36m(func pid=79525)[0m top5: 0.6399253731343284
[2m[36m(func pid=79525)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=79525)[0m f1_macro: 0.02395852522434801
[2m[36m(func pid=79525)[0m f1_weighted: 0.0015835059288677276
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3483 | Steps: 4 | Val loss: 1.7816 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:47:38 (running for 00:04:15.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.206 |      0.345 |                   37 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.046 |      0.243 |                   37 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  4.605 |      0.032 |                   38 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.77  |      0.024 |                   36 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.06389925373134328
[2m[36m(func pid=79093)[0m top5: 0.7350746268656716
[2m[36m(func pid=79093)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=79093)[0m f1_macro: 0.0315465347040816
[2m[36m(func pid=79093)[0m f1_weighted: 0.021008558113482408
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.053, 0.096, 0.0, 0.167, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.35494402985074625
[2m[36m(func pid=78294)[0m top5: 0.8857276119402985
[2m[36m(func pid=78294)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=78294)[0m f1_macro: 0.3421093743001378
[2m[36m(func pid=78294)[0m f1_weighted: 0.3806270330008593
[2m[36m(func pid=78294)[0m f1_per_class: [0.252, 0.5, 0.522, 0.346, 0.162, 0.258, 0.422, 0.373, 0.22, 0.366]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.9849 | Steps: 4 | Val loss: 12.9521 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.2039 | Steps: 4 | Val loss: 5.6531 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.7037 | Steps: 4 | Val loss: 636.3388 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=78677)[0m top1: 0.2947761194029851
[2m[36m(func pid=78677)[0m top5: 0.777518656716418
[2m[36m(func pid=78677)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=78677)[0m f1_macro: 0.27733429244084773
[2m[36m(func pid=78677)[0m f1_weighted: 0.30459802114495926
[2m[36m(func pid=78677)[0m f1_per_class: [0.159, 0.449, 0.489, 0.074, 0.175, 0.137, 0.496, 0.449, 0.258, 0.086]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m top1: 0.017257462686567165
[2m[36m(func pid=79525)[0m top5: 0.6450559701492538
[2m[36m(func pid=79525)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=79525)[0m f1_macro: 0.02985256345988645
[2m[36m(func pid=79525)[0m f1_weighted: 0.0024853716644273475
[2m[36m(func pid=79525)[0m f1_per_class: [0.038, 0.0, 0.238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1670 | Steps: 4 | Val loss: 1.7796 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:47:44 (running for 00:04:21.17)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.348 |      0.342 |                   38 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.204 |      0.277 |                   38 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.704 |      0.017 |                   39 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.985 |      0.03  |                   37 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.05783582089552239
[2m[36m(func pid=79093)[0m top5: 0.6753731343283582
[2m[36m(func pid=79093)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=79093)[0m f1_macro: 0.01730094378691776
[2m[36m(func pid=79093)[0m f1_weighted: 0.010464976741220334
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.165, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.3614738805970149
[2m[36m(func pid=78294)[0m top5: 0.8880597014925373
[2m[36m(func pid=78294)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=78294)[0m f1_macro: 0.34004568473312985
[2m[36m(func pid=78294)[0m f1_weighted: 0.38860086692071955
[2m[36m(func pid=78294)[0m f1_per_class: [0.234, 0.504, 0.5, 0.371, 0.16, 0.265, 0.422, 0.381, 0.224, 0.341]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.4457 | Steps: 4 | Val loss: 6.4915 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 13.5950 | Steps: 4 | Val loss: 21.6402 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 5.6152 | Steps: 4 | Val loss: 598.2174 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=78677)[0m top1: 0.2681902985074627
[2m[36m(func pid=78677)[0m top5: 0.6902985074626866
[2m[36m(func pid=78677)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=78677)[0m f1_macro: 0.201797606652585
[2m[36m(func pid=78677)[0m f1_weighted: 0.2557419662111072
[2m[36m(func pid=78677)[0m f1_per_class: [0.216, 0.422, 0.148, 0.022, 0.128, 0.024, 0.473, 0.35, 0.182, 0.053]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m top1: 0.06343283582089553
[2m[36m(func pid=79525)[0m top5: 0.636660447761194
[2m[36m(func pid=79525)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=79525)[0m f1_macro: 0.03104574616576685
[2m[36m(func pid=79525)[0m f1_weighted: 0.009996909528879069
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.011, 0.166, 0.0, 0.0, 0.0, 0.0, 0.121, 0.0, 0.013]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1784 | Steps: 4 | Val loss: 1.7663 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:47:49 (running for 00:04:26.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.167 |      0.34  |                   39 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.446 |      0.202 |                   39 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.615 |      0.017 |                   40 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 13.595 |      0.031 |                   38 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.05690298507462686
[2m[36m(func pid=79093)[0m top5: 0.6739738805970149
[2m[36m(func pid=79093)[0m f1_micro: 0.05690298507462686
[2m[36m(func pid=79093)[0m f1_macro: 0.01735419630156472
[2m[36m(func pid=79093)[0m f1_weighted: 0.010036941890830342
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.174, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.36986940298507465
[2m[36m(func pid=78294)[0m top5: 0.8931902985074627
[2m[36m(func pid=78294)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=78294)[0m f1_macro: 0.3445180592332682
[2m[36m(func pid=78294)[0m f1_weighted: 0.39782897050283855
[2m[36m(func pid=78294)[0m f1_per_class: [0.279, 0.507, 0.444, 0.397, 0.159, 0.271, 0.418, 0.401, 0.22, 0.348]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 4.2733 | Steps: 4 | Val loss: 8.2078 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.5637 | Steps: 4 | Val loss: 5.4760 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 6.6229 | Steps: 4 | Val loss: 608.2438 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=79525)[0m top1: 0.06576492537313433
[2m[36m(func pid=79525)[0m top5: 0.8041044776119403
[2m[36m(func pid=79525)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=79525)[0m f1_macro: 0.05715446415470969
[2m[36m(func pid=79525)[0m f1_weighted: 0.013642560147665907
[2m[36m(func pid=79525)[0m f1_per_class: [0.08, 0.016, 0.27, 0.0, 0.0, 0.0, 0.0, 0.113, 0.0, 0.093]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1933 | Steps: 4 | Val loss: 1.7719 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=78677)[0m top1: 0.33722014925373134
[2m[36m(func pid=78677)[0m top5: 0.7910447761194029
[2m[36m(func pid=78677)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=78677)[0m f1_macro: 0.22149221247174739
[2m[36m(func pid=78677)[0m f1_weighted: 0.30170246619946234
[2m[36m(func pid=78677)[0m f1_per_class: [0.305, 0.437, 0.09, 0.052, 0.118, 0.039, 0.609, 0.161, 0.197, 0.207]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:47:55 (running for 00:04:31.95)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.178 |      0.345 |                   40 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.564 |      0.221 |                   40 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.623 |      0.022 |                   41 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.273 |      0.057 |                   39 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.05830223880597015
[2m[36m(func pid=79093)[0m top5: 0.6735074626865671
[2m[36m(func pid=79093)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=79093)[0m f1_macro: 0.021758462675535845
[2m[36m(func pid=79093)[0m f1_weighted: 0.012251974605906166
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.177, 0.032, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.37173507462686567
[2m[36m(func pid=78294)[0m top5: 0.8917910447761194
[2m[36m(func pid=78294)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=78294)[0m f1_macro: 0.3462099319059838
[2m[36m(func pid=78294)[0m f1_weighted: 0.40455828090774554
[2m[36m(func pid=78294)[0m f1_per_class: [0.267, 0.499, 0.462, 0.378, 0.161, 0.278, 0.465, 0.383, 0.207, 0.361]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.3947 | Steps: 4 | Val loss: 7.9165 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.2815 | Steps: 4 | Val loss: 5.5260 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 6.2553 | Steps: 4 | Val loss: 493.9224 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1739 | Steps: 4 | Val loss: 1.7091 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=78677)[0m top1: 0.283115671641791
[2m[36m(func pid=78677)[0m top5: 0.8549440298507462
[2m[36m(func pid=78677)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=78677)[0m f1_macro: 0.232884587287769
[2m[36m(func pid=78677)[0m f1_weighted: 0.27116172030134267
[2m[36m(func pid=78677)[0m f1_per_class: [0.335, 0.447, 0.159, 0.175, 0.09, 0.335, 0.289, 0.062, 0.166, 0.269]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m top1: 0.0648320895522388
[2m[36m(func pid=79525)[0m top5: 0.8027052238805971
[2m[36m(func pid=79525)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=79525)[0m f1_macro: 0.05192224000974792
[2m[36m(func pid=79525)[0m f1_weighted: 0.012292426444724638
[2m[36m(func pid=79525)[0m f1_per_class: [0.085, 0.011, 0.255, 0.0, 0.0, 0.0, 0.0, 0.113, 0.0, 0.056]
[2m[36m(func pid=79525)[0m 
== Status ==
Current time: 2024-01-07 13:48:00 (running for 00:04:37.51)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.193 |      0.346 |                   41 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.281 |      0.233 |                   41 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  6.255 |      0.029 |                   42 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.395 |      0.052 |                   40 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.06296641791044776
[2m[36m(func pid=79093)[0m top5: 0.6697761194029851
[2m[36m(func pid=79093)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=79093)[0m f1_macro: 0.028834023854133285
[2m[36m(func pid=79093)[0m f1_weighted: 0.01764527173948882
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.002, 0.01, 0.0, 0.008, 0.0, 0.206, 0.062, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.3983208955223881
[2m[36m(func pid=78294)[0m top5: 0.9029850746268657
[2m[36m(func pid=78294)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=78294)[0m f1_macro: 0.36498614401134055
[2m[36m(func pid=78294)[0m f1_weighted: 0.43270850990309034
[2m[36m(func pid=78294)[0m f1_per_class: [0.264, 0.511, 0.533, 0.42, 0.174, 0.284, 0.51, 0.379, 0.214, 0.36]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6723 | Steps: 4 | Val loss: 11.1617 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8995 | Steps: 4 | Val loss: 5.3541 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 10.9036 | Steps: 4 | Val loss: 406.0720 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=79525)[0m top1: 0.06529850746268656
[2m[36m(func pid=79525)[0m top5: 0.7980410447761194
[2m[36m(func pid=79525)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=79525)[0m f1_macro: 0.043139989533552345
[2m[36m(func pid=79525)[0m f1_weighted: 0.0132601587417292
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.026, 0.222, 0.0, 0.0, 0.0, 0.0, 0.114, 0.0, 0.069]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m top1: 0.27611940298507465
[2m[36m(func pid=78677)[0m top5: 0.8987873134328358
[2m[36m(func pid=78677)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=78677)[0m f1_macro: 0.2697280421284349
[2m[36m(func pid=78677)[0m f1_weighted: 0.27452499007487924
[2m[36m(func pid=78677)[0m f1_per_class: [0.251, 0.409, 0.545, 0.268, 0.2, 0.369, 0.229, 0.027, 0.128, 0.27]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1806 | Steps: 4 | Val loss: 1.7025 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 13:48:06 (running for 00:04:43.00)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.174 |      0.365 |                   42 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.9   |      0.27  |                   42 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 10.904 |      0.028 |                   43 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.672 |      0.043 |                   41 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.03871268656716418
[2m[36m(func pid=79093)[0m top5: 0.6590485074626866
[2m[36m(func pid=79093)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=79093)[0m f1_macro: 0.027610192402160914
[2m[36m(func pid=79093)[0m f1_weighted: 0.03718815245746404
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.003, 0.072, 0.0, 0.121, 0.0, 0.019, 0.061, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.3983208955223881
[2m[36m(func pid=78294)[0m top5: 0.9067164179104478
[2m[36m(func pid=78294)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=78294)[0m f1_macro: 0.35967167479591716
[2m[36m(func pid=78294)[0m f1_weighted: 0.4323686029331751
[2m[36m(func pid=78294)[0m f1_per_class: [0.288, 0.511, 0.48, 0.454, 0.172, 0.287, 0.478, 0.375, 0.212, 0.34]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.6814 | Steps: 4 | Val loss: 11.1476 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.2394 | Steps: 4 | Val loss: 6.3983 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 5.6275 | Steps: 4 | Val loss: 415.9981 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=79525)[0m top1: 0.17397388059701493
[2m[36m(func pid=79525)[0m top5: 0.8083022388059702
[2m[36m(func pid=79525)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=79525)[0m f1_macro: 0.05594634444564319
[2m[36m(func pid=79525)[0m f1_weighted: 0.052972222848429125
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.299, 0.261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m top1: 0.2630597014925373
[2m[36m(func pid=78677)[0m top5: 0.871268656716418
[2m[36m(func pid=78677)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=78677)[0m f1_macro: 0.27078316891160814
[2m[36m(func pid=78677)[0m f1_weighted: 0.29327061127801457
[2m[36m(func pid=78677)[0m f1_per_class: [0.154, 0.408, 0.471, 0.276, 0.375, 0.213, 0.338, 0.096, 0.11, 0.268]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1109 | Steps: 4 | Val loss: 1.6512 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:48:11 (running for 00:04:48.44)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.181 |      0.36  |                   43 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.239 |      0.271 |                   43 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.627 |      0.065 |                   44 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.681 |      0.056 |                   42 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.08022388059701492
[2m[36m(func pid=79093)[0m top5: 0.6702425373134329
[2m[36m(func pid=79093)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=79093)[0m f1_macro: 0.06531792704543075
[2m[36m(func pid=79093)[0m f1_weighted: 0.10038024055298944
[2m[36m(func pid=79093)[0m f1_per_class: [0.018, 0.0, 0.0, 0.219, 0.005, 0.303, 0.0, 0.023, 0.085, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.40951492537313433
[2m[36m(func pid=78294)[0m top5: 0.9127798507462687
[2m[36m(func pid=78294)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=78294)[0m f1_macro: 0.35933349204148346
[2m[36m(func pid=78294)[0m f1_weighted: 0.44256785921435104
[2m[36m(func pid=78294)[0m f1_per_class: [0.313, 0.49, 0.436, 0.52, 0.156, 0.273, 0.462, 0.394, 0.23, 0.32]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 3.3149 | Steps: 4 | Val loss: 9.7090 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2668 | Steps: 4 | Val loss: 6.7152 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 4.7018 | Steps: 4 | Val loss: 307.6554 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=79525)[0m top1: 0.17397388059701493
[2m[36m(func pid=79525)[0m top5: 0.867070895522388
[2m[36m(func pid=79525)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=79525)[0m f1_macro: 0.057107438016528934
[2m[36m(func pid=79525)[0m f1_weighted: 0.05300164980880721
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.298, 0.273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1250 | Steps: 4 | Val loss: 1.5955 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=78677)[0m top1: 0.27238805970149255
[2m[36m(func pid=78677)[0m top5: 0.8409514925373134
[2m[36m(func pid=78677)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=78677)[0m f1_macro: 0.19216065704458457
[2m[36m(func pid=78677)[0m f1_weighted: 0.2938781387265577
[2m[36m(func pid=78677)[0m f1_per_class: [0.105, 0.454, 0.0, 0.222, 0.263, 0.077, 0.432, 0.111, 0.133, 0.125]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:48:16 (running for 00:04:53.84)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.111 |      0.359 |                   44 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  2.267 |      0.192 |                   44 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  4.702 |      0.135 |                   45 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.315 |      0.057 |                   43 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.23880597014925373
[2m[36m(func pid=79093)[0m top5: 0.6791044776119403
[2m[36m(func pid=79093)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=79093)[0m f1_macro: 0.13541248715214052
[2m[36m(func pid=79093)[0m f1_weighted: 0.19458605603231124
[2m[36m(func pid=79093)[0m f1_per_class: [0.077, 0.005, 0.048, 0.45, 0.0, 0.4, 0.0, 0.329, 0.044, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.42350746268656714
[2m[36m(func pid=78294)[0m top5: 0.9272388059701493
[2m[36m(func pid=78294)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=78294)[0m f1_macro: 0.3797436687725006
[2m[36m(func pid=78294)[0m f1_weighted: 0.45273576458229214
[2m[36m(func pid=78294)[0m f1_per_class: [0.405, 0.527, 0.511, 0.542, 0.165, 0.299, 0.445, 0.36, 0.197, 0.348]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 4.8270 | Steps: 4 | Val loss: 12.1258 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.6750 | Steps: 4 | Val loss: 7.7376 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 11.1505 | Steps: 4 | Val loss: 372.5185 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=79525)[0m top1: 0.011194029850746268
[2m[36m(func pid=79525)[0m top5: 0.8638059701492538
[2m[36m(func pid=79525)[0m f1_micro: 0.01119402985074627
[2m[36m(func pid=79525)[0m f1_macro: 0.016260617056559363
[2m[36m(func pid=79525)[0m f1_weighted: 0.0057793739353922706
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.024, 0.095, 0.0, 0.015, 0.0, 0.0, 0.0, 0.028, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m top1: 0.1730410447761194
[2m[36m(func pid=78677)[0m top5: 0.6506529850746269
[2m[36m(func pid=78677)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=78677)[0m f1_macro: 0.13383872323740242
[2m[36m(func pid=78677)[0m f1_weighted: 0.166256926318091
[2m[36m(func pid=78677)[0m f1_per_class: [0.119, 0.372, 0.0, 0.192, 0.262, 0.055, 0.11, 0.0, 0.112, 0.116]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0605 | Steps: 4 | Val loss: 1.5894 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:48:22 (running for 00:04:59.32)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.125 |      0.38  |                   45 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.675 |      0.134 |                   45 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      | 11.15  |      0.139 |                   46 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.827 |      0.016 |                   44 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.24347014925373134
[2m[36m(func pid=79093)[0m top5: 0.7000932835820896
[2m[36m(func pid=79093)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=79093)[0m f1_macro: 0.13921649059102809
[2m[36m(func pid=79093)[0m f1_weighted: 0.1999872977566621
[2m[36m(func pid=79093)[0m f1_per_class: [0.059, 0.125, 0.098, 0.402, 0.0, 0.427, 0.0, 0.26, 0.022, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.427705223880597
[2m[36m(func pid=78294)[0m top5: 0.925839552238806
[2m[36m(func pid=78294)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=78294)[0m f1_macro: 0.3777467443067001
[2m[36m(func pid=78294)[0m f1_weighted: 0.4616964510590488
[2m[36m(func pid=78294)[0m f1_per_class: [0.389, 0.498, 0.522, 0.541, 0.162, 0.3, 0.495, 0.355, 0.201, 0.314]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 5.9530 | Steps: 4 | Val loss: 7.0839 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7869 | Steps: 4 | Val loss: 8.1633 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.9543 | Steps: 4 | Val loss: 284.8297 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=79525)[0m top1: 0.010727611940298507
[2m[36m(func pid=79525)[0m top5: 0.8703358208955224
[2m[36m(func pid=79525)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=79525)[0m f1_macro: 0.055612019799767445
[2m[36m(func pid=79525)[0m f1_weighted: 0.0046537534353237155
[2m[36m(func pid=79525)[0m f1_per_class: [0.039, 0.0, 0.476, 0.0, 0.015, 0.0, 0.0, 0.0, 0.026, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1151 | Steps: 4 | Val loss: 1.5843 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=78677)[0m top1: 0.16837686567164178
[2m[36m(func pid=78677)[0m top5: 0.511660447761194
[2m[36m(func pid=78677)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=78677)[0m f1_macro: 0.1215975451128319
[2m[36m(func pid=78677)[0m f1_weighted: 0.16303002814574796
[2m[36m(func pid=78677)[0m f1_per_class: [0.163, 0.39, 0.047, 0.246, 0.106, 0.016, 0.054, 0.0, 0.128, 0.067]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:48:27 (running for 00:05:04.71)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.061 |      0.378 |                   46 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.787 |      0.122 |                   46 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.954 |      0.147 |                   47 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.953 |      0.056 |                   45 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.2593283582089552
[2m[36m(func pid=79093)[0m top5: 0.6898320895522388
[2m[36m(func pid=79093)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=79093)[0m f1_macro: 0.14734326904232184
[2m[36m(func pid=79093)[0m f1_weighted: 0.20142004151084092
[2m[36m(func pid=79093)[0m f1_per_class: [0.067, 0.377, 0.08, 0.254, 0.0, 0.416, 0.0, 0.278, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.44076492537313433
[2m[36m(func pid=78294)[0m top5: 0.9221082089552238
[2m[36m(func pid=78294)[0m f1_micro: 0.44076492537313433
[2m[36m(func pid=78294)[0m f1_macro: 0.38358956560012586
[2m[36m(func pid=78294)[0m f1_weighted: 0.4714143108042167
[2m[36m(func pid=78294)[0m f1_per_class: [0.398, 0.526, 0.511, 0.551, 0.164, 0.304, 0.501, 0.343, 0.216, 0.324]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 5.2338 | Steps: 4 | Val loss: 6.7044 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.0339 | Steps: 4 | Val loss: 7.0236 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 9.8862 | Steps: 4 | Val loss: 203.5336 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=79525)[0m top1: 0.11893656716417911
[2m[36m(func pid=79525)[0m top5: 0.6124067164179104
[2m[36m(func pid=79525)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=79525)[0m f1_macro: 0.08105524117683172
[2m[36m(func pid=79525)[0m f1_weighted: 0.028640237226833086
[2m[36m(func pid=79525)[0m f1_per_class: [0.071, 0.0, 0.529, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1315 | Steps: 4 | Val loss: 1.5289 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=78677)[0m top1: 0.18703358208955223
[2m[36m(func pid=78677)[0m top5: 0.5722947761194029
[2m[36m(func pid=78677)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=78677)[0m f1_macro: 0.1368631801295009
[2m[36m(func pid=78677)[0m f1_weighted: 0.21986204525148695
[2m[36m(func pid=78677)[0m f1_per_class: [0.157, 0.388, 0.021, 0.408, 0.099, 0.021, 0.094, 0.0, 0.117, 0.063]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:48:33 (running for 00:05:10.20)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.115 |      0.384 |                   47 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.034 |      0.137 |                   47 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  9.886 |      0.117 |                   48 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.234 |      0.081 |                   46 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.240205223880597
[2m[36m(func pid=79093)[0m top5: 0.6865671641791045
[2m[36m(func pid=79093)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=79093)[0m f1_macro: 0.11709406857456273
[2m[36m(func pid=79093)[0m f1_weighted: 0.1340810744526529
[2m[36m(func pid=79093)[0m f1_per_class: [0.079, 0.384, 0.0, 0.019, 0.0, 0.375, 0.0, 0.314, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4566231343283582
[2m[36m(func pid=78294)[0m top5: 0.9305037313432836
[2m[36m(func pid=78294)[0m f1_micro: 0.4566231343283582
[2m[36m(func pid=78294)[0m f1_macro: 0.3875890677382853
[2m[36m(func pid=78294)[0m f1_weighted: 0.4836183420220163
[2m[36m(func pid=78294)[0m f1_per_class: [0.417, 0.521, 0.471, 0.558, 0.206, 0.312, 0.535, 0.344, 0.201, 0.31]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.3494 | Steps: 4 | Val loss: 9.5163 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.5199 | Steps: 4 | Val loss: 5.7792 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 9.9591 | Steps: 4 | Val loss: 167.1479 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=79525)[0m top1: 0.11940298507462686
[2m[36m(func pid=79525)[0m top5: 0.7355410447761194
[2m[36m(func pid=79525)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=79525)[0m f1_macro: 0.0758286937140929
[2m[36m(func pid=79525)[0m f1_weighted: 0.027816315642014228
[2m[36m(func pid=79525)[0m f1_per_class: [0.037, 0.0, 0.512, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0999 | Steps: 4 | Val loss: 1.5243 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=78677)[0m top1: 0.16930970149253732
[2m[36m(func pid=78677)[0m top5: 0.6590485074626866
[2m[36m(func pid=78677)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=78677)[0m f1_macro: 0.14477300589601788
[2m[36m(func pid=78677)[0m f1_weighted: 0.20975655040623645
[2m[36m(func pid=78677)[0m f1_per_class: [0.181, 0.343, 0.038, 0.33, 0.063, 0.069, 0.108, 0.197, 0.055, 0.063]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:48:38 (running for 00:05:15.76)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.132 |      0.388 |                   48 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  2.52  |      0.145 |                   48 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  9.959 |      0.126 |                   49 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.349 |      0.076 |                   47 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.2439365671641791
[2m[36m(func pid=79093)[0m top5: 0.6721082089552238
[2m[36m(func pid=79093)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=79093)[0m f1_macro: 0.12628567433087917
[2m[36m(func pid=79093)[0m f1_weighted: 0.14792912921989676
[2m[36m(func pid=79093)[0m f1_per_class: [0.102, 0.375, 0.0, 0.074, 0.0, 0.345, 0.0, 0.367, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.8649 | Steps: 4 | Val loss: 4.5090 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=78294)[0m top1: 0.46175373134328357
[2m[36m(func pid=78294)[0m top5: 0.9281716417910447
[2m[36m(func pid=78294)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=78294)[0m f1_macro: 0.3885978082368589
[2m[36m(func pid=78294)[0m f1_weighted: 0.48606433803451843
[2m[36m(func pid=78294)[0m f1_per_class: [0.432, 0.544, 0.436, 0.569, 0.198, 0.31, 0.519, 0.341, 0.214, 0.321]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.3330 | Steps: 4 | Val loss: 7.0795 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 5.7142 | Steps: 4 | Val loss: 92.7992 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=79525)[0m top1: 0.11893656716417911
[2m[36m(func pid=79525)[0m top5: 0.7374067164179104
[2m[36m(func pid=79525)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=79525)[0m f1_macro: 0.08442440764691404
[2m[36m(func pid=79525)[0m f1_weighted: 0.029442838815572334
[2m[36m(func pid=79525)[0m f1_per_class: [0.077, 0.0, 0.533, 0.0, 0.0, 0.208, 0.0, 0.0, 0.026, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4062 | Steps: 4 | Val loss: 1.5522 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=78677)[0m top1: 0.16184701492537312
[2m[36m(func pid=78677)[0m top5: 0.6623134328358209
[2m[36m(func pid=78677)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=78677)[0m f1_macro: 0.14840418143331124
[2m[36m(func pid=78677)[0m f1_weighted: 0.20323478727187022
[2m[36m(func pid=78677)[0m f1_per_class: [0.174, 0.294, 0.027, 0.203, 0.081, 0.181, 0.186, 0.239, 0.035, 0.067]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:48:44 (running for 00:05:21.21)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.1   |      0.389 |                   49 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.333 |      0.148 |                   49 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.714 |      0.128 |                   50 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.865 |      0.084 |                   48 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.23087686567164178
[2m[36m(func pid=79093)[0m top5: 0.6357276119402985
[2m[36m(func pid=79093)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=79093)[0m f1_macro: 0.12791363397796726
[2m[36m(func pid=79093)[0m f1_weighted: 0.14502202272210798
[2m[36m(func pid=79093)[0m f1_per_class: [0.077, 0.387, 0.0, 0.046, 0.0, 0.344, 0.0, 0.425, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.8094 | Steps: 4 | Val loss: 4.3286 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=78294)[0m top1: 0.45802238805970147
[2m[36m(func pid=78294)[0m top5: 0.9309701492537313
[2m[36m(func pid=78294)[0m f1_micro: 0.45802238805970147
[2m[36m(func pid=78294)[0m f1_macro: 0.37900640045359957
[2m[36m(func pid=78294)[0m f1_weighted: 0.4828212276654807
[2m[36m(func pid=78294)[0m f1_per_class: [0.452, 0.524, 0.407, 0.578, 0.193, 0.323, 0.517, 0.292, 0.207, 0.298]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2077 | Steps: 4 | Val loss: 11.3064 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 5.5436 | Steps: 4 | Val loss: 59.3883 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=79525)[0m top1: 0.03684701492537314
[2m[36m(func pid=79525)[0m top5: 0.7388059701492538
[2m[36m(func pid=79525)[0m f1_micro: 0.03684701492537314
[2m[36m(func pid=79525)[0m f1_macro: 0.06928057964789103
[2m[36m(func pid=79525)[0m f1_weighted: 0.007887628280394774
[2m[36m(func pid=79525)[0m f1_per_class: [0.075, 0.0, 0.552, 0.003, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0512 | Steps: 4 | Val loss: 1.4953 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=78677)[0m top1: 0.13013059701492538
[2m[36m(func pid=78677)[0m top5: 0.644589552238806
[2m[36m(func pid=78677)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=78677)[0m f1_macro: 0.12524772161875308
[2m[36m(func pid=78677)[0m f1_weighted: 0.14900970297080773
[2m[36m(func pid=78677)[0m f1_per_class: [0.165, 0.305, 0.0, 0.102, 0.24, 0.062, 0.161, 0.089, 0.062, 0.067]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:48:49 (running for 00:05:26.59)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.406 |      0.379 |                   50 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.208 |      0.125 |                   50 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.544 |      0.068 |                   51 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.809 |      0.069 |                   49 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.18563432835820895
[2m[36m(func pid=79093)[0m top5: 0.7117537313432836
[2m[36m(func pid=79093)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=79093)[0m f1_macro: 0.06822496983787307
[2m[36m(func pid=79093)[0m f1_weighted: 0.09843426678856386
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.354, 0.0, 0.0, 0.0, 0.328, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.46222014925373134
[2m[36m(func pid=78294)[0m top5: 0.9365671641791045
[2m[36m(func pid=78294)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=78294)[0m f1_macro: 0.3881407054389239
[2m[36m(func pid=78294)[0m f1_weighted: 0.48596086357151685
[2m[36m(func pid=78294)[0m f1_per_class: [0.519, 0.526, 0.4, 0.586, 0.202, 0.333, 0.511, 0.286, 0.206, 0.313]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 10.5159 | Steps: 4 | Val loss: 9.4838 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7959 | Steps: 4 | Val loss: 13.3355 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.1224 | Steps: 4 | Val loss: 29.7905 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=79525)[0m top1: 0.03731343283582089
[2m[36m(func pid=79525)[0m top5: 0.746268656716418
[2m[36m(func pid=79525)[0m f1_micro: 0.03731343283582089
[2m[36m(func pid=79525)[0m f1_macro: 0.06661499817173018
[2m[36m(func pid=79525)[0m f1_weighted: 0.007231131181740936
[2m[36m(func pid=79525)[0m f1_per_class: [0.103, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0333 | Steps: 4 | Val loss: 1.4887 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=78677)[0m top1: 0.15858208955223882
[2m[36m(func pid=78677)[0m top5: 0.6665111940298507
[2m[36m(func pid=78677)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=78677)[0m f1_macro: 0.197033551238318
[2m[36m(func pid=78677)[0m f1_weighted: 0.15742715163734117
[2m[36m(func pid=78677)[0m f1_per_class: [0.578, 0.388, 0.069, 0.123, 0.375, 0.008, 0.089, 0.165, 0.105, 0.07]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:48:55 (running for 00:05:32.18)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.051 |      0.388 |                   51 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.796 |      0.197 |                   51 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.122 |      0.114 |                   52 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      | 10.516 |      0.067 |                   50 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.2248134328358209
[2m[36m(func pid=79093)[0m top5: 0.7901119402985075
[2m[36m(func pid=79093)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=79093)[0m f1_macro: 0.11411903005909854
[2m[36m(func pid=79093)[0m f1_weighted: 0.12942626148630848
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.374, 0.0, 0.0, 0.0, 0.339, 0.006, 0.421, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.46875
[2m[36m(func pid=78294)[0m top5: 0.9426305970149254
[2m[36m(func pid=78294)[0m f1_micro: 0.46875
[2m[36m(func pid=78294)[0m f1_macro: 0.4090751985452396
[2m[36m(func pid=78294)[0m f1_weighted: 0.49129797984347884
[2m[36m(func pid=78294)[0m f1_per_class: [0.593, 0.534, 0.429, 0.595, 0.2, 0.339, 0.491, 0.365, 0.218, 0.327]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.7525 | Steps: 4 | Val loss: 5.4651 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.3102 | Steps: 4 | Val loss: 8.5441 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 5.1065 | Steps: 4 | Val loss: 27.9006 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=79525)[0m top1: 0.03684701492537314
[2m[36m(func pid=79525)[0m top5: 0.4626865671641791
[2m[36m(func pid=79525)[0m f1_micro: 0.03684701492537314
[2m[36m(func pid=79525)[0m f1_macro: 0.07061589557033168
[2m[36m(func pid=79525)[0m f1_weighted: 0.007874622346709496
[2m[36m(func pid=79525)[0m f1_per_class: [0.069, 0.0, 0.571, 0.003, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0697 | Steps: 4 | Val loss: 1.4418 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=78677)[0m top1: 0.2733208955223881
[2m[36m(func pid=78677)[0m top5: 0.777518656716418
[2m[36m(func pid=78677)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=78677)[0m f1_macro: 0.21245404991008576
[2m[36m(func pid=78677)[0m f1_weighted: 0.24260575300626513
[2m[36m(func pid=78677)[0m f1_per_class: [0.0, 0.443, 0.333, 0.438, 0.308, 0.03, 0.05, 0.27, 0.135, 0.117]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:49:00 (running for 00:05:37.60)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.033 |      0.409 |                   52 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.31  |      0.212 |                   52 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.107 |      0.142 |                   53 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.752 |      0.071 |                   51 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.29151119402985076
[2m[36m(func pid=79093)[0m top5: 0.8134328358208955
[2m[36m(func pid=79093)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=79093)[0m f1_macro: 0.1421471961842599
[2m[36m(func pid=79093)[0m f1_weighted: 0.21368136448702504
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.385, 0.0, 0.0, 0.0, 0.363, 0.279, 0.395, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.5800 | Steps: 4 | Val loss: 4.3529 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=78294)[0m top1: 0.4771455223880597
[2m[36m(func pid=78294)[0m top5: 0.9463619402985075
[2m[36m(func pid=78294)[0m f1_micro: 0.4771455223880597
[2m[36m(func pid=78294)[0m f1_macro: 0.41258696648074117
[2m[36m(func pid=78294)[0m f1_weighted: 0.49643675934194886
[2m[36m(func pid=78294)[0m f1_per_class: [0.642, 0.558, 0.393, 0.597, 0.197, 0.318, 0.496, 0.371, 0.223, 0.33]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8257 | Steps: 4 | Val loss: 9.0982 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.9723 | Steps: 4 | Val loss: 13.9381 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=79525)[0m top1: 0.021921641791044777
[2m[36m(func pid=79525)[0m top5: 0.40718283582089554
[2m[36m(func pid=79525)[0m f1_micro: 0.021921641791044777
[2m[36m(func pid=79525)[0m f1_macro: 0.06540285299405837
[2m[36m(func pid=79525)[0m f1_weighted: 0.0051853597307858134
[2m[36m(func pid=79525)[0m f1_per_class: [0.035, 0.0, 0.593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0517 | Steps: 4 | Val loss: 1.3960 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=78677)[0m top1: 0.2989738805970149
[2m[36m(func pid=78677)[0m top5: 0.8134328358208955
[2m[36m(func pid=78677)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=78677)[0m f1_macro: 0.21588956782845084
[2m[36m(func pid=78677)[0m f1_weighted: 0.2590738252171569
[2m[36m(func pid=78677)[0m f1_per_class: [0.0, 0.422, 0.207, 0.534, 0.245, 0.057, 0.012, 0.236, 0.248, 0.197]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:49:06 (running for 00:05:43.01)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.07  |      0.413 |                   53 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.826 |      0.216 |                   53 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.972 |      0.195 |                   54 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.58  |      0.065 |                   52 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.2635261194029851
[2m[36m(func pid=79093)[0m top5: 0.7877798507462687
[2m[36m(func pid=79093)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=79093)[0m f1_macro: 0.19508569218554164
[2m[36m(func pid=79093)[0m f1_weighted: 0.21499198452952067
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.399, 0.625, 0.0, 0.0, 0.156, 0.336, 0.419, 0.0, 0.015]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 4.6513 | Steps: 4 | Val loss: 7.5051 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=78294)[0m top1: 0.4822761194029851
[2m[36m(func pid=78294)[0m top5: 0.9533582089552238
[2m[36m(func pid=78294)[0m f1_micro: 0.4822761194029851
[2m[36m(func pid=78294)[0m f1_macro: 0.4240633162640591
[2m[36m(func pid=78294)[0m f1_weighted: 0.4973333306442862
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.571, 0.4, 0.604, 0.232, 0.326, 0.478, 0.368, 0.216, 0.366]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.1125 | Steps: 4 | Val loss: 8.1309 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 5.0329 | Steps: 4 | Val loss: 19.7637 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=79525)[0m top1: 0.022388059701492536
[2m[36m(func pid=79525)[0m top5: 0.6688432835820896
[2m[36m(func pid=79525)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=79525)[0m f1_macro: 0.06168064964780834
[2m[36m(func pid=79525)[0m f1_weighted: 0.004262819890408428
[2m[36m(func pid=79525)[0m f1_per_class: [0.036, 0.0, 0.581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0658 | Steps: 4 | Val loss: 1.3914 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=78677)[0m top1: 0.2789179104477612
[2m[36m(func pid=78677)[0m top5: 0.8134328358208955
[2m[36m(func pid=78677)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=78677)[0m f1_macro: 0.20161118073896533
[2m[36m(func pid=78677)[0m f1_weighted: 0.24078129172103968
[2m[36m(func pid=78677)[0m f1_per_class: [0.125, 0.41, 0.205, 0.473, 0.082, 0.029, 0.021, 0.246, 0.23, 0.195]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:49:11 (running for 00:05:48.62)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.052 |      0.424 |                   54 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.113 |      0.202 |                   54 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  5.033 |      0.15  |                   55 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.651 |      0.062 |                   53 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.22901119402985073
[2m[36m(func pid=79093)[0m top5: 0.7532649253731343
[2m[36m(func pid=79093)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=79093)[0m f1_macro: 0.15041318922287228
[2m[36m(func pid=79093)[0m f1_weighted: 0.1811821195203813
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.381, 0.393, 0.0, 0.0, 0.038, 0.29, 0.383, 0.0, 0.019]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.48600746268656714
[2m[36m(func pid=78294)[0m top5: 0.9556902985074627
[2m[36m(func pid=78294)[0m f1_micro: 0.48600746268656714
[2m[36m(func pid=78294)[0m f1_macro: 0.4388743599830233
[2m[36m(func pid=78294)[0m f1_weighted: 0.5026969172592598
[2m[36m(func pid=78294)[0m f1_per_class: [0.714, 0.589, 0.5, 0.604, 0.239, 0.337, 0.478, 0.366, 0.222, 0.34]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 4.9732 | Steps: 4 | Val loss: 7.5593 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3043 | Steps: 4 | Val loss: 8.1241 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4210 | Steps: 4 | Val loss: 14.2742 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=79525)[0m top1: 0.017723880597014924
[2m[36m(func pid=79525)[0m top5: 0.6702425373134329
[2m[36m(func pid=79525)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=79525)[0m f1_macro: 0.07425515657514084
[2m[36m(func pid=79525)[0m f1_weighted: 0.007236995680160078
[2m[36m(func pid=79525)[0m f1_per_class: [0.131, 0.0, 0.562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.023]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0388 | Steps: 4 | Val loss: 1.4091 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=78677)[0m top1: 0.23227611940298507
[2m[36m(func pid=78677)[0m top5: 0.7798507462686567
[2m[36m(func pid=78677)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=78677)[0m f1_macro: 0.1682496499499209
[2m[36m(func pid=78677)[0m f1_weighted: 0.1886470856997336
[2m[36m(func pid=78677)[0m f1_per_class: [0.151, 0.401, 0.159, 0.286, 0.105, 0.0, 0.048, 0.24, 0.155, 0.137]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:49:17 (running for 00:05:54.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.066 |      0.439 |                   55 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.304 |      0.168 |                   55 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.421 |      0.128 |                   56 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.973 |      0.074 |                   54 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.16837686567164178
[2m[36m(func pid=79093)[0m top5: 0.7635261194029851
[2m[36m(func pid=79093)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=79093)[0m f1_macro: 0.12826414191239868
[2m[36m(func pid=79093)[0m f1_weighted: 0.12772291580093945
[2m[36m(func pid=79093)[0m f1_per_class: [0.016, 0.343, 0.392, 0.0, 0.0, 0.0, 0.149, 0.367, 0.0, 0.014]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4780783582089552
[2m[36m(func pid=78294)[0m top5: 0.9533582089552238
[2m[36m(func pid=78294)[0m f1_micro: 0.4780783582089552
[2m[36m(func pid=78294)[0m f1_macro: 0.43212433466829836
[2m[36m(func pid=78294)[0m f1_weighted: 0.4937275461847142
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.576, 0.48, 0.606, 0.222, 0.297, 0.461, 0.424, 0.214, 0.362]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.7727 | Steps: 4 | Val loss: 4.5719 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9319 | Steps: 4 | Val loss: 10.1748 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79525)[0m top1: 0.06296641791044776
[2m[36m(func pid=79525)[0m top5: 0.3805970149253731
[2m[36m(func pid=79525)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=79525)[0m f1_macro: 0.08031938846947079
[2m[36m(func pid=79525)[0m f1_weighted: 0.012383659443349498
[2m[36m(func pid=79525)[0m f1_per_class: [0.074, 0.0, 0.593, 0.0, 0.0, 0.0, 0.0, 0.111, 0.025, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.5516 | Steps: 4 | Val loss: 15.8020 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1135 | Steps: 4 | Val loss: 1.4192 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=78677)[0m top1: 0.2140858208955224
[2m[36m(func pid=78677)[0m top5: 0.7159514925373134
[2m[36m(func pid=78677)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=78677)[0m f1_macro: 0.197027991035721
[2m[36m(func pid=78677)[0m f1_weighted: 0.17843957558244958
[2m[36m(func pid=78677)[0m f1_per_class: [0.356, 0.37, 0.462, 0.104, 0.118, 0.008, 0.195, 0.22, 0.071, 0.067]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:49:22 (running for 00:05:59.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.039 |      0.432 |                   56 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.932 |      0.197 |                   56 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.552 |      0.106 |                   57 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.773 |      0.08  |                   55 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.14458955223880596
[2m[36m(func pid=79093)[0m top5: 0.7490671641791045
[2m[36m(func pid=79093)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=79093)[0m f1_macro: 0.1062939720657228
[2m[36m(func pid=79093)[0m f1_weighted: 0.10524021492443547
[2m[36m(func pid=79093)[0m f1_per_class: [0.039, 0.308, 0.25, 0.0, 0.0, 0.0, 0.099, 0.348, 0.0, 0.018]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.47947761194029853
[2m[36m(func pid=78294)[0m top5: 0.9584888059701493
[2m[36m(func pid=78294)[0m f1_micro: 0.47947761194029853
[2m[36m(func pid=78294)[0m f1_macro: 0.43743359970312057
[2m[36m(func pid=78294)[0m f1_weighted: 0.4926294926444153
[2m[36m(func pid=78294)[0m f1_per_class: [0.707, 0.593, 0.453, 0.62, 0.222, 0.283, 0.437, 0.414, 0.209, 0.435]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6573 | Steps: 4 | Val loss: 3.5878 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5717 | Steps: 4 | Val loss: 13.2237 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.7363 | Steps: 4 | Val loss: 10.7279 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=79525)[0m top1: 0.06203358208955224
[2m[36m(func pid=79525)[0m top5: 0.38013059701492535
[2m[36m(func pid=79525)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=79525)[0m f1_macro: 0.0686334460741728
[2m[36m(func pid=79525)[0m f1_weighted: 0.01099241477748025
[2m[36m(func pid=79525)[0m f1_per_class: [0.075, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0575 | Steps: 4 | Val loss: 1.4307 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=78677)[0m top1: 0.2019589552238806
[2m[36m(func pid=78677)[0m top5: 0.6870335820895522
[2m[36m(func pid=78677)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=78677)[0m f1_macro: 0.17137795540042153
[2m[36m(func pid=78677)[0m f1_weighted: 0.16509644203215912
[2m[36m(func pid=78677)[0m f1_per_class: [0.352, 0.343, 0.222, 0.082, 0.215, 0.008, 0.198, 0.185, 0.051, 0.058]
[2m[36m(func pid=78677)[0m 
== Status ==
Current time: 2024-01-07 13:49:27 (running for 00:06:04.38)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.113 |      0.437 |                   57 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.572 |      0.171 |                   57 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.552 |      0.106 |                   57 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.657 |      0.069 |                   56 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.12919776119402984
[2m[36m(func pid=79093)[0m top5: 0.7257462686567164
[2m[36m(func pid=79093)[0m f1_micro: 0.12919776119402984
[2m[36m(func pid=79093)[0m f1_macro: 0.0989528137120271
[2m[36m(func pid=79093)[0m f1_weighted: 0.1045756608040648
[2m[36m(func pid=79093)[0m f1_per_class: [0.032, 0.279, 0.2, 0.0, 0.0, 0.0, 0.112, 0.366, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.47341417910447764
[2m[36m(func pid=78294)[0m top5: 0.9566231343283582
[2m[36m(func pid=78294)[0m f1_micro: 0.47341417910447764
[2m[36m(func pid=78294)[0m f1_macro: 0.4277404524132744
[2m[36m(func pid=78294)[0m f1_weighted: 0.48480976154964067
[2m[36m(func pid=78294)[0m f1_per_class: [0.673, 0.596, 0.453, 0.603, 0.23, 0.282, 0.432, 0.405, 0.211, 0.393]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.9642 | Steps: 4 | Val loss: 2.6529 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7154 | Steps: 4 | Val loss: 12.7022 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7871 | Steps: 4 | Val loss: 9.4505 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=79525)[0m top1: 0.06203358208955224
[2m[36m(func pid=79525)[0m top5: 0.5345149253731343
[2m[36m(func pid=79525)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=79525)[0m f1_macro: 0.07235629441086011
[2m[36m(func pid=79525)[0m f1_weighted: 0.010708908725541874
[2m[36m(func pid=79525)[0m f1_per_class: [0.042, 0.0, 0.571, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0279 | Steps: 4 | Val loss: 1.4332 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:49:33 (running for 00:06:09.94)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.058 |      0.428 |                   58 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.715 |      0.165 |                   58 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.736 |      0.099 |                   58 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.964 |      0.072 |                   57 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78677)[0m top1: 0.2103544776119403
[2m[36m(func pid=78677)[0m top5: 0.7238805970149254
[2m[36m(func pid=78677)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=78677)[0m f1_macro: 0.16544567887747438
[2m[36m(func pid=78677)[0m f1_weighted: 0.17509957347438435
[2m[36m(func pid=78677)[0m f1_per_class: [0.481, 0.325, 0.0, 0.139, 0.282, 0.008, 0.194, 0.139, 0.026, 0.06]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m top1: 0.11147388059701492
[2m[36m(func pid=79093)[0m top5: 0.6749067164179104
[2m[36m(func pid=79093)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=79093)[0m f1_macro: 0.0868807749168197
[2m[36m(func pid=79093)[0m f1_weighted: 0.10308195718149112
[2m[36m(func pid=79093)[0m f1_per_class: [0.032, 0.161, 0.13, 0.0, 0.0, 0.0, 0.176, 0.369, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6470 | Steps: 4 | Val loss: 2.5821 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=78294)[0m top1: 0.46921641791044777
[2m[36m(func pid=78294)[0m top5: 0.9575559701492538
[2m[36m(func pid=78294)[0m f1_micro: 0.46921641791044777
[2m[36m(func pid=78294)[0m f1_macro: 0.43261502733687973
[2m[36m(func pid=78294)[0m f1_weighted: 0.47700368152208644
[2m[36m(func pid=78294)[0m f1_per_class: [0.686, 0.602, 0.48, 0.598, 0.232, 0.297, 0.397, 0.404, 0.235, 0.395]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6998 | Steps: 4 | Val loss: 11.5045 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=79525)[0m top1: 0.06203358208955224
[2m[36m(func pid=79525)[0m top5: 0.5335820895522388
[2m[36m(func pid=79525)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=79525)[0m f1_macro: 0.06253470667263769
[2m[36m(func pid=79525)[0m f1_weighted: 0.009541661375166264
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.514, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 4.4662 | Steps: 4 | Val loss: 4.6006 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0533 | Steps: 4 | Val loss: 1.4250 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:49:38 (running for 00:06:15.55)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.028 |      0.433 |                   59 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.7   |      0.193 |                   59 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.787 |      0.087 |                   59 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.647 |      0.063 |                   58 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78677)[0m top1: 0.24300373134328357
[2m[36m(func pid=78677)[0m top5: 0.7397388059701493
[2m[36m(func pid=78677)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=78677)[0m f1_macro: 0.19300257703936413
[2m[36m(func pid=78677)[0m f1_weighted: 0.21132687448393317
[2m[36m(func pid=78677)[0m f1_per_class: [0.516, 0.318, 0.133, 0.276, 0.268, 0.008, 0.194, 0.081, 0.052, 0.084]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m top1: 0.11380597014925373
[2m[36m(func pid=79093)[0m top5: 0.6352611940298507
[2m[36m(func pid=79093)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=79093)[0m f1_macro: 0.08213197696648207
[2m[36m(func pid=79093)[0m f1_weighted: 0.11964144937635143
[2m[36m(func pid=79093)[0m f1_per_class: [0.032, 0.045, 0.095, 0.0, 0.0, 0.0, 0.304, 0.344, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 5.5262 | Steps: 4 | Val loss: 2.4241 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=78294)[0m top1: 0.47527985074626866
[2m[36m(func pid=78294)[0m top5: 0.9566231343283582
[2m[36m(func pid=78294)[0m f1_micro: 0.47527985074626866
[2m[36m(func pid=78294)[0m f1_macro: 0.43709605881161473
[2m[36m(func pid=78294)[0m f1_weighted: 0.48373821896561686
[2m[36m(func pid=78294)[0m f1_per_class: [0.742, 0.621, 0.462, 0.598, 0.234, 0.295, 0.404, 0.412, 0.239, 0.364]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.2384 | Steps: 4 | Val loss: 10.0008 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=79525)[0m top1: 0.17490671641791045
[2m[36m(func pid=79525)[0m top5: 0.8185634328358209
[2m[36m(func pid=79525)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=79525)[0m f1_macro: 0.08533144716336534
[2m[36m(func pid=79525)[0m f1_weighted: 0.05484581990778003
[2m[36m(func pid=79525)[0m f1_per_class: [0.042, 0.296, 0.516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.0233 | Steps: 4 | Val loss: 3.4826 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.1796 | Steps: 4 | Val loss: 1.4502 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:49:44 (running for 00:06:21.01)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.053 |      0.437 |                   60 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.238 |      0.259 |                   60 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  4.466 |      0.082 |                   60 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.526 |      0.085 |                   59 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78677)[0m top1: 0.35261194029850745
[2m[36m(func pid=78677)[0m top5: 0.7714552238805971
[2m[36m(func pid=78677)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=78677)[0m f1_macro: 0.2588594956295789
[2m[36m(func pid=78677)[0m f1_weighted: 0.33357382481157827
[2m[36m(func pid=78677)[0m f1_per_class: [0.447, 0.408, 0.333, 0.414, 0.226, 0.016, 0.406, 0.142, 0.076, 0.121]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m top1: 0.13199626865671643
[2m[36m(func pid=79093)[0m top5: 0.6417910447761194
[2m[36m(func pid=79093)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=79093)[0m f1_macro: 0.09688006669867003
[2m[36m(func pid=79093)[0m f1_weighted: 0.14883923667871604
[2m[36m(func pid=79093)[0m f1_per_class: [0.035, 0.118, 0.112, 0.0, 0.0, 0.0, 0.359, 0.344, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 6.0630 | Steps: 4 | Val loss: 2.2811 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=78294)[0m top1: 0.466884328358209
[2m[36m(func pid=78294)[0m top5: 0.9556902985074627
[2m[36m(func pid=78294)[0m f1_micro: 0.46688432835820903
[2m[36m(func pid=78294)[0m f1_macro: 0.42925190143632186
[2m[36m(func pid=78294)[0m f1_weighted: 0.47147188290786685
[2m[36m(func pid=78294)[0m f1_per_class: [0.716, 0.614, 0.444, 0.599, 0.255, 0.314, 0.36, 0.412, 0.258, 0.32]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6993 | Steps: 4 | Val loss: 9.5540 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=79525)[0m top1: 0.17444029850746268
[2m[36m(func pid=79525)[0m top5: 0.8731343283582089
[2m[36m(func pid=79525)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=79525)[0m f1_macro: 0.08885289956131
[2m[36m(func pid=79525)[0m f1_weighted: 0.05582527857447422
[2m[36m(func pid=79525)[0m f1_per_class: [0.04, 0.295, 0.545, 0.0, 0.0, 0.008, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.8759 | Steps: 4 | Val loss: 4.3251 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2082 | Steps: 4 | Val loss: 1.5350 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:49:49 (running for 00:06:26.69)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.18  |      0.429 |                   61 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.699 |      0.275 |                   61 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.023 |      0.097 |                   61 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  6.063 |      0.089 |                   60 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78677)[0m top1: 0.3969216417910448
[2m[36m(func pid=78677)[0m top5: 0.7756529850746269
[2m[36m(func pid=78677)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=78677)[0m f1_macro: 0.2745404612839075
[2m[36m(func pid=78677)[0m f1_weighted: 0.3778281977390686
[2m[36m(func pid=78677)[0m f1_per_class: [0.302, 0.499, 0.258, 0.418, 0.24, 0.092, 0.466, 0.181, 0.118, 0.172]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m top1: 0.12360074626865672
[2m[36m(func pid=79093)[0m top5: 0.6730410447761194
[2m[36m(func pid=79093)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=79093)[0m f1_macro: 0.09100487263697009
[2m[36m(func pid=79093)[0m f1_weighted: 0.1256278830233577
[2m[36m(func pid=79093)[0m f1_per_class: [0.037, 0.193, 0.075, 0.0, 0.0, 0.0, 0.234, 0.371, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.8624 | Steps: 4 | Val loss: 4.0194 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=78294)[0m top1: 0.4505597014925373
[2m[36m(func pid=78294)[0m top5: 0.9477611940298507
[2m[36m(func pid=78294)[0m f1_micro: 0.4505597014925373
[2m[36m(func pid=78294)[0m f1_macro: 0.41791204988180625
[2m[36m(func pid=78294)[0m f1_weighted: 0.45290816283060154
[2m[36m(func pid=78294)[0m f1_per_class: [0.694, 0.614, 0.444, 0.561, 0.241, 0.315, 0.336, 0.416, 0.245, 0.313]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.7297 | Steps: 4 | Val loss: 5.0543 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7479 | Steps: 4 | Val loss: 8.9721 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=79525)[0m top1: 0.3003731343283582
[2m[36m(func pid=79525)[0m top5: 0.8773320895522388
[2m[36m(func pid=79525)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=79525)[0m f1_macro: 0.11022706163442067
[2m[36m(func pid=79525)[0m f1_weighted: 0.14470980161552946
[2m[36m(func pid=79525)[0m f1_per_class: [0.071, 0.0, 0.538, 0.003, 0.0, 0.0, 0.464, 0.0, 0.025, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.1210 | Steps: 4 | Val loss: 1.5345 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:49:55 (running for 00:06:32.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.208 |      0.418 |                   62 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.699 |      0.275 |                   61 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.73  |      0.063 |                   63 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.862 |      0.11  |                   61 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.09514925373134328
[2m[36m(func pid=79093)[0m top5: 0.6949626865671642
[2m[36m(func pid=79093)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=79093)[0m f1_macro: 0.06311969600003343
[2m[36m(func pid=79093)[0m f1_weighted: 0.05678142348306921
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.062, 0.0, 0.0, 0.0, 0.106, 0.386, 0.077, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.4099813432835821
[2m[36m(func pid=78677)[0m top5: 0.7770522388059702
[2m[36m(func pid=78677)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=78677)[0m f1_macro: 0.2788061775964913
[2m[36m(func pid=78677)[0m f1_weighted: 0.404926452516938
[2m[36m(func pid=78677)[0m f1_per_class: [0.233, 0.527, 0.175, 0.517, 0.232, 0.098, 0.429, 0.33, 0.065, 0.182]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.9967 | Steps: 4 | Val loss: 3.7554 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=78294)[0m top1: 0.44822761194029853
[2m[36m(func pid=78294)[0m top5: 0.945429104477612
[2m[36m(func pid=78294)[0m f1_micro: 0.44822761194029853
[2m[36m(func pid=78294)[0m f1_macro: 0.41157620485713836
[2m[36m(func pid=78294)[0m f1_weighted: 0.45143264372242825
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.604, 0.444, 0.557, 0.215, 0.302, 0.348, 0.41, 0.253, 0.303]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.5842 | Steps: 4 | Val loss: 4.6498 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0708 | Steps: 4 | Val loss: 8.9715 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=79525)[0m top1: 0.3003731343283582
[2m[36m(func pid=79525)[0m top5: 0.875
[2m[36m(func pid=79525)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=79525)[0m f1_macro: 0.10740358072842855
[2m[36m(func pid=79525)[0m f1_weighted: 0.1436851845854607
[2m[36m(func pid=79525)[0m f1_per_class: [0.069, 0.0, 0.538, 0.003, 0.0, 0.0, 0.463, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1508 | Steps: 4 | Val loss: 1.5222 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:50:00 (running for 00:06:37.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.121 |      0.412 |                   63 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.748 |      0.279 |                   62 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.584 |      0.061 |                   64 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.997 |      0.107 |                   62 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.08442164179104478
[2m[36m(func pid=79093)[0m top5: 0.6968283582089553
[2m[36m(func pid=79093)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=79093)[0m f1_macro: 0.061480217055276266
[2m[36m(func pid=79093)[0m f1_weighted: 0.041499693248153925
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.057, 0.0, 0.0, 0.0, 0.045, 0.434, 0.078, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.373134328358209
[2m[36m(func pid=78677)[0m top5: 0.7980410447761194
[2m[36m(func pid=78677)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=78677)[0m f1_macro: 0.2718500413311703
[2m[36m(func pid=78677)[0m f1_weighted: 0.3776849402511064
[2m[36m(func pid=78677)[0m f1_per_class: [0.175, 0.459, 0.22, 0.506, 0.205, 0.201, 0.344, 0.362, 0.081, 0.165]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.9636 | Steps: 4 | Val loss: 2.4705 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=78294)[0m top1: 0.45382462686567165
[2m[36m(func pid=78294)[0m top5: 0.9575559701492538
[2m[36m(func pid=78294)[0m f1_micro: 0.45382462686567165
[2m[36m(func pid=78294)[0m f1_macro: 0.4052408010087471
[2m[36m(func pid=78294)[0m f1_weighted: 0.45561957315080776
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.601, 0.433, 0.62, 0.208, 0.272, 0.325, 0.38, 0.244, 0.29]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.5267 | Steps: 4 | Val loss: 4.1247 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=79525)[0m top1: 0.29990671641791045
[2m[36m(func pid=79525)[0m top5: 0.8726679104477612
[2m[36m(func pid=79525)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=79525)[0m f1_macro: 0.10566206971940786
[2m[36m(func pid=79525)[0m f1_weighted: 0.14239701612971783
[2m[36m(func pid=79525)[0m f1_per_class: [0.073, 0.0, 0.522, 0.0, 0.0, 0.0, 0.462, 0.0, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9746 | Steps: 4 | Val loss: 7.7672 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0121 | Steps: 4 | Val loss: 1.5057 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:50:06 (running for 00:06:42.97)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.151 |      0.405 |                   64 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.071 |      0.272 |                   63 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.527 |      0.063 |                   65 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.964 |      0.106 |                   63 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.07975746268656717
[2m[36m(func pid=79093)[0m top5: 0.7364738805970149
[2m[36m(func pid=79093)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=79093)[0m f1_macro: 0.06257892122923944
[2m[36m(func pid=79093)[0m f1_weighted: 0.03897645694034564
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.065, 0.0, 0.0, 0.0, 0.034, 0.449, 0.078, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.36240671641791045
[2m[36m(func pid=78677)[0m top5: 0.8311567164179104
[2m[36m(func pid=78677)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=78677)[0m f1_macro: 0.26604402428251483
[2m[36m(func pid=78677)[0m f1_weighted: 0.3835644664498887
[2m[36m(func pid=78677)[0m f1_per_class: [0.091, 0.479, 0.102, 0.462, 0.174, 0.174, 0.397, 0.377, 0.178, 0.227]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 4.7384 | Steps: 4 | Val loss: 3.7122 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=78294)[0m top1: 0.457089552238806
[2m[36m(func pid=78294)[0m top5: 0.9542910447761194
[2m[36m(func pid=78294)[0m f1_micro: 0.457089552238806
[2m[36m(func pid=78294)[0m f1_macro: 0.4115186147692941
[2m[36m(func pid=78294)[0m f1_weighted: 0.4600114755248986
[2m[36m(func pid=78294)[0m f1_per_class: [0.673, 0.6, 0.464, 0.626, 0.239, 0.276, 0.332, 0.386, 0.231, 0.288]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.5852 | Steps: 4 | Val loss: 3.2746 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=79525)[0m top1: 0.30223880597014924
[2m[36m(func pid=79525)[0m top5: 0.7369402985074627
[2m[36m(func pid=79525)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=79525)[0m f1_macro: 0.11988728203541792
[2m[36m(func pid=79525)[0m f1_weighted: 0.14611018949729848
[2m[36m(func pid=79525)[0m f1_per_class: [0.105, 0.0, 0.56, 0.0, 0.0, 0.0, 0.464, 0.0, 0.07, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8210 | Steps: 4 | Val loss: 7.0523 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0178 | Steps: 4 | Val loss: 1.5279 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:50:11 (running for 00:06:48.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.012 |      0.412 |                   65 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.975 |      0.266 |                   64 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.585 |      0.099 |                   66 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.738 |      0.12  |                   64 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.14598880597014927
[2m[36m(func pid=79093)[0m top5: 0.7756529850746269
[2m[36m(func pid=79093)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=79093)[0m f1_macro: 0.09855570611585317
[2m[36m(func pid=79093)[0m f1_weighted: 0.12974039805229579
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.0, 0.066, 0.336, 0.0, 0.0, 0.019, 0.47, 0.084, 0.011]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.3423507462686567
[2m[36m(func pid=78677)[0m top5: 0.8572761194029851
[2m[36m(func pid=78677)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=78677)[0m f1_macro: 0.23603006899889983
[2m[36m(func pid=78677)[0m f1_weighted: 0.37214163279773854
[2m[36m(func pid=78677)[0m f1_per_class: [0.029, 0.485, 0.063, 0.353, 0.129, 0.095, 0.522, 0.238, 0.143, 0.304]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 4.0261 | Steps: 4 | Val loss: 4.0878 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=78294)[0m top1: 0.4519589552238806
[2m[36m(func pid=78294)[0m top5: 0.9580223880597015
[2m[36m(func pid=78294)[0m f1_micro: 0.4519589552238806
[2m[36m(func pid=78294)[0m f1_macro: 0.4150664406114134
[2m[36m(func pid=78294)[0m f1_weighted: 0.45529197259425735
[2m[36m(func pid=78294)[0m f1_per_class: [0.694, 0.605, 0.456, 0.606, 0.226, 0.281, 0.323, 0.401, 0.248, 0.31]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 4.2282 | Steps: 4 | Val loss: 3.2768 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=79525)[0m top1: 0.30130597014925375
[2m[36m(func pid=79525)[0m top5: 0.7378731343283582
[2m[36m(func pid=79525)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=79525)[0m f1_macro: 0.10796230596707235
[2m[36m(func pid=79525)[0m f1_weighted: 0.14389408692333716
[2m[36m(func pid=79525)[0m f1_per_class: [0.039, 0.0, 0.533, 0.0, 0.0, 0.0, 0.465, 0.0, 0.043, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6155 | Steps: 4 | Val loss: 8.1774 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1493 | Steps: 4 | Val loss: 1.6052 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:50:16 (running for 00:06:53.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.018 |      0.415 |                   66 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  0.821 |      0.236 |                   65 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  4.228 |      0.108 |                   67 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.026 |      0.108 |                   65 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.1875
[2m[36m(func pid=79093)[0m top5: 0.777518656716418
[2m[36m(func pid=79093)[0m f1_micro: 0.1875
[2m[36m(func pid=79093)[0m f1_macro: 0.10752109388577771
[2m[36m(func pid=79093)[0m f1_weighted: 0.17838908978118811
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.011, 0.093, 0.45, 0.0, 0.006, 0.088, 0.389, 0.024, 0.014]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.29244402985074625
[2m[36m(func pid=78677)[0m top5: 0.8166977611940298
[2m[36m(func pid=78677)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=78677)[0m f1_macro: 0.19667549875701362
[2m[36m(func pid=78677)[0m f1_weighted: 0.31698965795537515
[2m[36m(func pid=78677)[0m f1_per_class: [0.0, 0.449, 0.065, 0.224, 0.081, 0.047, 0.512, 0.189, 0.121, 0.278]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m top1: 0.43843283582089554
[2m[36m(func pid=78294)[0m top5: 0.9584888059701493
[2m[36m(func pid=78294)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=78294)[0m f1_macro: 0.4070665352990398
[2m[36m(func pid=78294)[0m f1_weighted: 0.4414258688316166
[2m[36m(func pid=78294)[0m f1_per_class: [0.702, 0.594, 0.464, 0.611, 0.213, 0.283, 0.281, 0.384, 0.248, 0.29]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.7642 | Steps: 4 | Val loss: 4.2981 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.5436 | Steps: 4 | Val loss: 3.7560 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=79525)[0m top1: 0.037779850746268655
[2m[36m(func pid=79525)[0m top5: 0.738339552238806
[2m[36m(func pid=79525)[0m f1_micro: 0.037779850746268655
[2m[36m(func pid=79525)[0m f1_macro: 0.06942012275576369
[2m[36m(func pid=79525)[0m f1_weighted: 0.006830576566831922
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.005, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.9420 | Steps: 4 | Val loss: 9.2624 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0763 | Steps: 4 | Val loss: 1.6250 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:50:22 (running for 00:06:58.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.149 |      0.407 |                   67 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.616 |      0.197 |                   66 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.544 |      0.095 |                   68 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.764 |      0.069 |                   66 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.18889925373134328
[2m[36m(func pid=79093)[0m top5: 0.7751865671641791
[2m[36m(func pid=79093)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=79093)[0m f1_macro: 0.09515977806849633
[2m[36m(func pid=79093)[0m f1_weighted: 0.19704871446800137
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.005, 0.098, 0.468, 0.0, 0.024, 0.177, 0.164, 0.0, 0.017]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.27238805970149255
[2m[36m(func pid=78677)[0m top5: 0.7765858208955224
[2m[36m(func pid=78677)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=78677)[0m f1_macro: 0.23468329615973738
[2m[36m(func pid=78677)[0m f1_weighted: 0.2870145058165865
[2m[36m(func pid=78677)[0m f1_per_class: [0.036, 0.417, 0.444, 0.186, 0.083, 0.183, 0.392, 0.253, 0.124, 0.227]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m top1: 0.439365671641791
[2m[36m(func pid=78294)[0m top5: 0.9575559701492538
[2m[36m(func pid=78294)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=78294)[0m f1_macro: 0.403211476799718
[2m[36m(func pid=78294)[0m f1_weighted: 0.4326605325421618
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.606, 0.456, 0.622, 0.204, 0.264, 0.243, 0.378, 0.256, 0.322]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.7081 | Steps: 4 | Val loss: 5.1377 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1975 | Steps: 4 | Val loss: 3.4447 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=79525)[0m top1: 0.037779850746268655
[2m[36m(func pid=79525)[0m top5: 0.4542910447761194
[2m[36m(func pid=79525)[0m f1_micro: 0.037779850746268655
[2m[36m(func pid=79525)[0m f1_macro: 0.0662094121816398
[2m[36m(func pid=79525)[0m f1_weighted: 0.006360785567411041
[2m[36m(func pid=79525)[0m f1_per_class: [0.043, 0.0, 0.556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2537 | Steps: 4 | Val loss: 1.5880 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.9660 | Steps: 4 | Val loss: 15.8612 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:50:27 (running for 00:07:04.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.076 |      0.403 |                   68 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  2.942 |      0.235 |                   67 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.197 |      0.105 |                   69 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.708 |      0.066 |                   67 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.20055970149253732
[2m[36m(func pid=79093)[0m top5: 0.7681902985074627
[2m[36m(func pid=79093)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=79093)[0m f1_macro: 0.10523773022394374
[2m[36m(func pid=79093)[0m f1_weighted: 0.22846342151653007
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.02, 0.104, 0.447, 0.0, 0.015, 0.3, 0.149, 0.0, 0.018]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78677)[0m top1: 0.2873134328358209
[2m[36m(func pid=78677)[0m top5: 0.757929104477612
[2m[36m(func pid=78677)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=78677)[0m f1_macro: 0.18813912324635387
[2m[36m(func pid=78677)[0m f1_weighted: 0.2764840874799995
[2m[36m(func pid=78677)[0m f1_per_class: [0.0, 0.376, 0.0, 0.159, 0.133, 0.316, 0.376, 0.193, 0.158, 0.171]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4454291044776119
[2m[36m(func pid=78294)[0m top5: 0.9538246268656716
[2m[36m(func pid=78294)[0m f1_micro: 0.4454291044776119
[2m[36m(func pid=78294)[0m f1_macro: 0.41047636529395853
[2m[36m(func pid=78294)[0m f1_weighted: 0.436705955191685
[2m[36m(func pid=78294)[0m f1_per_class: [0.694, 0.613, 0.456, 0.617, 0.228, 0.279, 0.247, 0.387, 0.27, 0.314]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6959 | Steps: 4 | Val loss: 3.7540 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 4.6826 | Steps: 4 | Val loss: 3.4913 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=79525)[0m top1: 0.037779850746268655
[2m[36m(func pid=79525)[0m top5: 0.5018656716417911
[2m[36m(func pid=79525)[0m f1_micro: 0.037779850746268655
[2m[36m(func pid=79525)[0m f1_macro: 0.07570296613140734
[2m[36m(func pid=79525)[0m f1_weighted: 0.007453299992569234
[2m[36m(func pid=79525)[0m f1_per_class: [0.077, 0.0, 0.615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.065, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0357 | Steps: 4 | Val loss: 1.6316 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.6041 | Steps: 4 | Val loss: 21.0115 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:50:32 (running for 00:07:09.68)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.254 |      0.41  |                   69 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.966 |      0.188 |                   68 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  4.683 |      0.111 |                   70 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  2.696 |      0.076 |                   68 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.19636194029850745
[2m[36m(func pid=79093)[0m top5: 0.7845149253731343
[2m[36m(func pid=79093)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=79093)[0m f1_macro: 0.11149615731180547
[2m[36m(func pid=79093)[0m f1_weighted: 0.24882718446789526
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.173, 0.063, 0.364, 0.0, 0.015, 0.363, 0.121, 0.0, 0.016]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.43703358208955223
[2m[36m(func pid=78294)[0m top5: 0.9510261194029851
[2m[36m(func pid=78294)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=78294)[0m f1_macro: 0.3996751030205129
[2m[36m(func pid=78294)[0m f1_weighted: 0.4297846725868573
[2m[36m(func pid=78294)[0m f1_per_class: [0.667, 0.615, 0.444, 0.614, 0.212, 0.264, 0.237, 0.379, 0.259, 0.306]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 4.1068 | Steps: 4 | Val loss: 3.9120 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=78677)[0m top1: 0.29757462686567165
[2m[36m(func pid=78677)[0m top5: 0.7416044776119403
[2m[36m(func pid=78677)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=78677)[0m f1_macro: 0.21636955068876046
[2m[36m(func pid=78677)[0m f1_weighted: 0.2813100708559669
[2m[36m(func pid=78677)[0m f1_per_class: [0.163, 0.363, 0.0, 0.206, 0.077, 0.363, 0.291, 0.381, 0.162, 0.158]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.4450 | Steps: 4 | Val loss: 2.5153 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=79525)[0m top1: 0.03871268656716418
[2m[36m(func pid=79525)[0m top5: 0.5009328358208955
[2m[36m(func pid=79525)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=79525)[0m f1_macro: 0.08627607134205688
[2m[36m(func pid=79525)[0m f1_weighted: 0.008540371318752718
[2m[36m(func pid=79525)[0m f1_per_class: [0.109, 0.0, 0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0203 | Steps: 4 | Val loss: 1.6326 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.0598 | Steps: 4 | Val loss: 19.9888 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:50:38 (running for 00:07:15.01)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.036 |      0.4   |                   70 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.604 |      0.216 |                   69 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.445 |      0.113 |                   71 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.107 |      0.086 |                   69 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.1837686567164179
[2m[36m(func pid=79093)[0m top5: 0.7919776119402985
[2m[36m(func pid=79093)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=79093)[0m f1_macro: 0.11257122499428171
[2m[36m(func pid=79093)[0m f1_weighted: 0.24185446847357966
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.213, 0.042, 0.255, 0.0, 0.0, 0.411, 0.198, 0.0, 0.008]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.44636194029850745
[2m[36m(func pid=78294)[0m top5: 0.9472947761194029
[2m[36m(func pid=78294)[0m f1_micro: 0.44636194029850745
[2m[36m(func pid=78294)[0m f1_macro: 0.4136898721958442
[2m[36m(func pid=78294)[0m f1_weighted: 0.4351035796449039
[2m[36m(func pid=78294)[0m f1_per_class: [0.66, 0.628, 0.511, 0.616, 0.193, 0.269, 0.234, 0.409, 0.27, 0.347]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.3218283582089552
[2m[36m(func pid=78677)[0m top5: 0.7653917910447762
[2m[36m(func pid=78677)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=78677)[0m f1_macro: 0.2259058861974279
[2m[36m(func pid=78677)[0m f1_weighted: 0.30185694884812087
[2m[36m(func pid=78677)[0m f1_per_class: [0.042, 0.37, 0.0, 0.297, 0.114, 0.407, 0.261, 0.41, 0.078, 0.281]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 4.0717 | Steps: 4 | Val loss: 3.4615 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.0782 | Steps: 4 | Val loss: 2.3488 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=79525)[0m top1: 0.06389925373134328
[2m[36m(func pid=79525)[0m top5: 0.23507462686567165
[2m[36m(func pid=79525)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=79525)[0m f1_macro: 0.08360406716213079
[2m[36m(func pid=79525)[0m f1_weighted: 0.014135362748805079
[2m[36m(func pid=79525)[0m f1_per_class: [0.038, 0.0, 0.615, 0.003, 0.0, 0.0, 0.0, 0.111, 0.068, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0350 | Steps: 4 | Val loss: 1.6345 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.4552 | Steps: 4 | Val loss: 11.8192 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=79093)[0m top1: 0.21921641791044777
[2m[36m(func pid=79093)[0m top5: 0.7910447761194029
[2m[36m(func pid=79093)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=79093)[0m f1_macro: 0.1461088282804355
[2m[36m(func pid=79093)[0m f1_weighted: 0.2708122481838711
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.234, 0.048, 0.231, 0.032, 0.0, 0.472, 0.429, 0.0, 0.015]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:50:43 (running for 00:07:20.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.02  |      0.414 |                   71 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  2.06  |      0.226 |                   70 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.078 |      0.146 |                   72 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  4.072 |      0.084 |                   70 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.43470149253731344
[2m[36m(func pid=78294)[0m top5: 0.9458955223880597
[2m[36m(func pid=78294)[0m f1_micro: 0.43470149253731344
[2m[36m(func pid=78294)[0m f1_macro: 0.3940009159842831
[2m[36m(func pid=78294)[0m f1_weighted: 0.433000107231472
[2m[36m(func pid=78294)[0m f1_per_class: [0.619, 0.615, 0.444, 0.602, 0.172, 0.243, 0.269, 0.377, 0.268, 0.33]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.4767 | Steps: 4 | Val loss: 3.3279 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=78677)[0m top1: 0.3199626865671642
[2m[36m(func pid=78677)[0m top5: 0.7831156716417911
[2m[36m(func pid=78677)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=78677)[0m f1_macro: 0.20536332202778107
[2m[36m(func pid=78677)[0m f1_weighted: 0.28330564194117097
[2m[36m(func pid=78677)[0m f1_per_class: [0.0, 0.399, 0.0, 0.407, 0.122, 0.287, 0.143, 0.328, 0.083, 0.286]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1832 | Steps: 4 | Val loss: 2.4083 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=79525)[0m top1: 0.0625
[2m[36m(func pid=79525)[0m top5: 0.23367537313432835
[2m[36m(func pid=79525)[0m f1_micro: 0.0625
[2m[36m(func pid=79525)[0m f1_macro: 0.0725775391578152
[2m[36m(func pid=79525)[0m f1_weighted: 0.012621765421982954
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.583, 0.007, 0.0, 0.0, 0.0, 0.111, 0.025, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0943 | Steps: 4 | Val loss: 1.6479 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.1904 | Steps: 4 | Val loss: 9.1746 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:50:49 (running for 00:07:25.90)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.035 |      0.394 |                   72 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.455 |      0.205 |                   71 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.183 |      0.135 |                   73 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.477 |      0.073 |                   71 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.19169776119402984
[2m[36m(func pid=79093)[0m top5: 0.7770522388059702
[2m[36m(func pid=79093)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=79093)[0m f1_macro: 0.13491204599311443
[2m[36m(func pid=79093)[0m f1_weighted: 0.23149072750127705
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.249, 0.034, 0.168, 0.039, 0.0, 0.381, 0.478, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4300373134328358
[2m[36m(func pid=78294)[0m top5: 0.9514925373134329
[2m[36m(func pid=78294)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=78294)[0m f1_macro: 0.39927457222195334
[2m[36m(func pid=78294)[0m f1_weighted: 0.4346110400270081
[2m[36m(func pid=78294)[0m f1_per_class: [0.667, 0.604, 0.462, 0.593, 0.176, 0.254, 0.286, 0.357, 0.258, 0.336]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 5.4122 | Steps: 4 | Val loss: 2.8484 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=78677)[0m top1: 0.3260261194029851
[2m[36m(func pid=78677)[0m top5: 0.7770522388059702
[2m[36m(func pid=78677)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=78677)[0m f1_macro: 0.2814185705750925
[2m[36m(func pid=78677)[0m f1_weighted: 0.30349360293552674
[2m[36m(func pid=78677)[0m f1_per_class: [0.29, 0.408, 0.667, 0.533, 0.176, 0.179, 0.119, 0.228, 0.075, 0.138]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.7856 | Steps: 4 | Val loss: 2.6469 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=79525)[0m top1: 0.0625
[2m[36m(func pid=79525)[0m top5: 0.23367537313432835
[2m[36m(func pid=79525)[0m f1_micro: 0.0625
[2m[36m(func pid=79525)[0m f1_macro: 0.08267240751646913
[2m[36m(func pid=79525)[0m f1_weighted: 0.012787806382114925
[2m[36m(func pid=79525)[0m f1_per_class: [0.075, 0.0, 0.636, 0.003, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0286 | Steps: 4 | Val loss: 1.6246 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.8841 | Steps: 4 | Val loss: 8.6460 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=79093)[0m top1: 0.16837686567164178
[2m[36m(func pid=79093)[0m top5: 0.7518656716417911
[2m[36m(func pid=79093)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=79093)[0m f1_macro: 0.12313358513138502
[2m[36m(func pid=79093)[0m f1_weighted: 0.20724064465611378
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.184, 0.026, 0.198, 0.034, 0.0, 0.309, 0.48, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:50:54 (running for 00:07:31.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.094 |      0.399 |                   73 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  3.19  |      0.281 |                   72 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.786 |      0.123 |                   74 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.412 |      0.083 |                   72 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.43470149253731344
[2m[36m(func pid=78294)[0m top5: 0.9500932835820896
[2m[36m(func pid=78294)[0m f1_micro: 0.43470149253731344
[2m[36m(func pid=78294)[0m f1_macro: 0.40476951391483273
[2m[36m(func pid=78294)[0m f1_weighted: 0.4374187959922009
[2m[36m(func pid=78294)[0m f1_per_class: [0.66, 0.612, 0.522, 0.601, 0.157, 0.242, 0.288, 0.351, 0.256, 0.358]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 3.0960 | Steps: 4 | Val loss: 2.9905 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=78677)[0m top1: 0.2873134328358209
[2m[36m(func pid=78677)[0m top5: 0.6958955223880597
[2m[36m(func pid=78677)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=78677)[0m f1_macro: 0.21218637339834232
[2m[36m(func pid=78677)[0m f1_weighted: 0.3050383031546023
[2m[36m(func pid=78677)[0m f1_per_class: [0.271, 0.332, 0.16, 0.471, 0.164, 0.111, 0.272, 0.216, 0.051, 0.075]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6729 | Steps: 4 | Val loss: 2.7075 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=79525)[0m top1: 0.06436567164179105
[2m[36m(func pid=79525)[0m top5: 0.27611940298507465
[2m[36m(func pid=79525)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=79525)[0m f1_macro: 0.0750480028523671
[2m[36m(func pid=79525)[0m f1_weighted: 0.012371319669435143
[2m[36m(func pid=79525)[0m f1_per_class: [0.122, 0.0, 0.476, 0.0, 0.0, 0.0, 0.0, 0.113, 0.0, 0.039]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1274 | Steps: 4 | Val loss: 1.5459 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.8370 | Steps: 4 | Val loss: 9.9133 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:50:59 (running for 00:07:36.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.118
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.029 |      0.405 |                   74 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.884 |      0.212 |                   73 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.673 |      0.118 |                   75 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  3.096 |      0.075 |                   73 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.1609141791044776
[2m[36m(func pid=79093)[0m top5: 0.7364738805970149
[2m[36m(func pid=79093)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=79093)[0m f1_macro: 0.11802381391002892
[2m[36m(func pid=79093)[0m f1_weighted: 0.20168290685637744
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.108, 0.027, 0.238, 0.03, 0.0, 0.297, 0.48, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.45615671641791045
[2m[36m(func pid=78294)[0m top5: 0.9500932835820896
[2m[36m(func pid=78294)[0m f1_micro: 0.45615671641791045
[2m[36m(func pid=78294)[0m f1_macro: 0.41021581972880955
[2m[36m(func pid=78294)[0m f1_weighted: 0.4645054848599289
[2m[36m(func pid=78294)[0m f1_per_class: [0.654, 0.61, 0.453, 0.599, 0.194, 0.249, 0.377, 0.362, 0.279, 0.324]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 6.3296 | Steps: 4 | Val loss: 3.7198 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=78677)[0m top1: 0.25093283582089554
[2m[36m(func pid=78677)[0m top5: 0.6697761194029851
[2m[36m(func pid=78677)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=78677)[0m f1_macro: 0.18190636116952752
[2m[36m(func pid=78677)[0m f1_weighted: 0.2867419325596751
[2m[36m(func pid=78677)[0m f1_per_class: [0.149, 0.304, 0.0, 0.321, 0.179, 0.128, 0.368, 0.221, 0.081, 0.068]
[2m[36m(func pid=78677)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 2.3262 | Steps: 4 | Val loss: 2.4902 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=79525)[0m top1: 0.013526119402985074
[2m[36m(func pid=79525)[0m top5: 0.2756529850746269
[2m[36m(func pid=79525)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=79525)[0m f1_macro: 0.04445563311867608
[2m[36m(func pid=79525)[0m f1_weighted: 0.0029169785455057306
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.0, 0.393, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.036]
[2m[36m(func pid=79525)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0179 | Steps: 4 | Val loss: 1.5221 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=78677)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.2567 | Steps: 4 | Val loss: 12.7287 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=79093)[0m top1: 0.1599813432835821
[2m[36m(func pid=79093)[0m top5: 0.7360074626865671
[2m[36m(func pid=79093)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=79093)[0m f1_macro: 0.12108119158718064
[2m[36m(func pid=79093)[0m f1_weighted: 0.2066112392601739
[2m[36m(func pid=79093)[0m f1_per_class: [0.043, 0.043, 0.025, 0.27, 0.028, 0.0, 0.318, 0.483, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:51:05 (running for 00:07:42.24)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.33699999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING  | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.127 |      0.41  |                   75 |
| train_5806f_00001 | RUNNING  | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.837 |      0.182 |                   74 |
| train_5806f_00002 | RUNNING  | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.326 |      0.121 |                   76 |
| train_5806f_00003 | RUNNING  | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  6.33  |      0.044 |                   74 |
| train_5806f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79525)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 5.2363 | Steps: 4 | Val loss: 4.4506 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=78294)[0m top1: 0.46548507462686567
[2m[36m(func pid=78294)[0m top5: 0.9528917910447762
[2m[36m(func pid=78294)[0m f1_micro: 0.4654850746268657
[2m[36m(func pid=78294)[0m f1_macro: 0.4132192584052106
[2m[36m(func pid=78294)[0m f1_weighted: 0.4805455529271267
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.606, 0.421, 0.607, 0.199, 0.271, 0.419, 0.356, 0.265, 0.308]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=78677)[0m top1: 0.18423507462686567
[2m[36m(func pid=78677)[0m top5: 0.5611007462686567
[2m[36m(func pid=78677)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=78677)[0m f1_macro: 0.159087329096141
[2m[36m(func pid=78677)[0m f1_weighted: 0.21182140355240622
[2m[36m(func pid=78677)[0m f1_per_class: [0.108, 0.25, 0.0, 0.148, 0.142, 0.126, 0.299, 0.263, 0.114, 0.14]
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 2.7946 | Steps: 4 | Val loss: 2.4167 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=79525)[0m top1: 0.17490671641791045
[2m[36m(func pid=79525)[0m top5: 0.37826492537313433
[2m[36m(func pid=79525)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=79525)[0m f1_macro: 0.07086269291498151
[2m[36m(func pid=79525)[0m f1_weighted: 0.053824106873968075
[2m[36m(func pid=79525)[0m f1_per_class: [0.0, 0.297, 0.369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043]
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0148 | Steps: 4 | Val loss: 1.4681 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=79093)[0m top1: 0.17583955223880596
[2m[36m(func pid=79093)[0m top5: 0.7388059701492538
[2m[36m(func pid=79093)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=79093)[0m f1_macro: 0.12355175992040154
[2m[36m(func pid=79093)[0m f1_weighted: 0.22424639544056318
[2m[36m(func pid=79093)[0m f1_per_class: [0.036, 0.04, 0.028, 0.315, 0.029, 0.0, 0.345, 0.443, 0.0, 0.0]
[2m[36m(func pid=78294)[0m top1: 0.48880597014925375
[2m[36m(func pid=78294)[0m top5: 0.9524253731343284
[2m[36m(func pid=78294)[0m f1_micro: 0.48880597014925375
[2m[36m(func pid=78294)[0m f1_macro: 0.4225311946750345
[2m[36m(func pid=78294)[0m f1_weighted: 0.504267341603138
[2m[36m(func pid=78294)[0m f1_per_class: [0.66, 0.609, 0.414, 0.615, 0.21, 0.287, 0.485, 0.355, 0.267, 0.325]
[2m[36m(func pid=97361)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97361)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=97361)[0m Configuration completed!
[2m[36m(func pid=97361)[0m New optimizer parameters:
[2m[36m(func pid=97361)[0m SGD (
[2m[36m(func pid=97361)[0m Parameter Group 0
[2m[36m(func pid=97361)[0m     dampening: 0
[2m[36m(func pid=97361)[0m     differentiable: False
[2m[36m(func pid=97361)[0m     foreach: None
[2m[36m(func pid=97361)[0m     lr: 0.0001
[2m[36m(func pid=97361)[0m     maximize: False
[2m[36m(func pid=97361)[0m     momentum: 0.9
[2m[36m(func pid=97361)[0m     nesterov: False
[2m[36m(func pid=97361)[0m     weight_decay: 0
[2m[36m(func pid=97361)[0m )
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:51:10 (running for 00:07:47.55)
Memory usage on this node: 20.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.018 |      0.413 |                   76 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.326 |      0.121 |                   76 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=97446)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97446)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=97446)[0m Configuration completed!
[2m[36m(func pid=97446)[0m New optimizer parameters:
[2m[36m(func pid=97446)[0m SGD (
[2m[36m(func pid=97446)[0m Parameter Group 0
[2m[36m(func pid=97446)[0m     dampening: 0
[2m[36m(func pid=97446)[0m     differentiable: False
[2m[36m(func pid=97446)[0m     foreach: None
[2m[36m(func pid=97446)[0m     lr: 0.001
[2m[36m(func pid=97446)[0m     maximize: False
[2m[36m(func pid=97446)[0m     momentum: 0.9
[2m[36m(func pid=97446)[0m     nesterov: False
[2m[36m(func pid=97446)[0m     weight_decay: 0
[2m[36m(func pid=97446)[0m )
[2m[36m(func pid=97446)[0m 
== Status ==
Current time: 2024-01-07 13:51:19 (running for 00:07:55.98)
Memory usage on this node: 23.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.018 |      0.413 |                   76 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.795 |      0.124 |                   77 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.2581 | Steps: 4 | Val loss: 2.5246 | Batch size: 32 | lr: 0.0001 | Duration: 5.23s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 2.6333 | Steps: 4 | Val loss: 2.3564 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0250 | Steps: 4 | Val loss: 1.4468 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=97361)[0m top1: 0.0648320895522388
[2m[36m(func pid=97361)[0m top5: 0.490205223880597
[2m[36m(func pid=97361)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=97361)[0m f1_macro: 0.03765402997002582
[2m[36m(func pid=97361)[0m f1_weighted: 0.037365134942735495
[2m[36m(func pid=97361)[0m f1_per_class: [0.079, 0.01, 0.0, 0.088, 0.0, 0.019, 0.0, 0.1, 0.023, 0.057]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0240 | Steps: 4 | Val loss: 2.4404 | Batch size: 32 | lr: 0.001 | Duration: 4.53s
== Status ==
Current time: 2024-01-07 13:51:24 (running for 00:08:01.35)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.015 |      0.423 |                   77 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.633 |      0.152 |                   78 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  3.258 |      0.038 |                    1 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |        |            |                      |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.22061567164179105
[2m[36m(func pid=79093)[0m top5: 0.7467350746268657
[2m[36m(func pid=79093)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=79093)[0m f1_macro: 0.15212365970719324
[2m[36m(func pid=79093)[0m f1_weighted: 0.27713515892644824
[2m[36m(func pid=79093)[0m f1_per_class: [0.122, 0.086, 0.022, 0.342, 0.033, 0.115, 0.435, 0.366, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.48600746268656714
[2m[36m(func pid=78294)[0m top5: 0.9561567164179104
[2m[36m(func pid=78294)[0m f1_micro: 0.48600746268656714
[2m[36m(func pid=78294)[0m f1_macro: 0.42530228112527596
[2m[36m(func pid=78294)[0m f1_weighted: 0.501832723586601
[2m[36m(func pid=78294)[0m f1_per_class: [0.66, 0.591, 0.453, 0.635, 0.224, 0.295, 0.466, 0.344, 0.26, 0.324]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1125 | Steps: 4 | Val loss: 2.5364 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=97446)[0m top1: 0.07929104477611941
[2m[36m(func pid=97446)[0m top5: 0.4906716417910448
[2m[36m(func pid=97446)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=97446)[0m f1_macro: 0.05724575772636712
[2m[36m(func pid=97446)[0m f1_weighted: 0.06601250462797098
[2m[36m(func pid=97446)[0m f1_per_class: [0.155, 0.064, 0.0, 0.149, 0.0, 0.021, 0.0, 0.097, 0.057, 0.029]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 2.5983 | Steps: 4 | Val loss: 2.3328 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.1884 | Steps: 4 | Val loss: 1.4593 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=97361)[0m top1: 0.06623134328358209
[2m[36m(func pid=97361)[0m top5: 0.48367537313432835
[2m[36m(func pid=97361)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=97361)[0m f1_macro: 0.03488056931264878
[2m[36m(func pid=97361)[0m f1_weighted: 0.044890762483011236
[2m[36m(func pid=97361)[0m f1_per_class: [0.051, 0.046, 0.0, 0.099, 0.0, 0.019, 0.0, 0.099, 0.0, 0.036]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7851 | Steps: 4 | Val loss: 2.3122 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:51:29 (running for 00:08:06.71)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.025 |      0.425 |                   78 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.598 |      0.141 |                   79 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  3.112 |      0.035 |                    2 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  3.024 |      0.057 |                    1 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.25046641791044777
[2m[36m(func pid=79093)[0m top5: 0.7770522388059702
[2m[36m(func pid=79093)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=79093)[0m f1_macro: 0.1410621184479976
[2m[36m(func pid=79093)[0m f1_weighted: 0.2794632070445819
[2m[36m(func pid=79093)[0m f1_per_class: [0.125, 0.082, 0.014, 0.417, 0.044, 0.293, 0.364, 0.073, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.49347014925373134
[2m[36m(func pid=78294)[0m top5: 0.9528917910447762
[2m[36m(func pid=78294)[0m f1_micro: 0.49347014925373134
[2m[36m(func pid=78294)[0m f1_macro: 0.4234128519144151
[2m[36m(func pid=78294)[0m f1_weighted: 0.5119173737737838
[2m[36m(func pid=78294)[0m f1_per_class: [0.667, 0.586, 0.414, 0.625, 0.204, 0.292, 0.515, 0.343, 0.258, 0.331]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9523 | Steps: 4 | Val loss: 2.5324 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=97446)[0m top1: 0.15345149253731344
[2m[36m(func pid=97446)[0m top5: 0.5513059701492538
[2m[36m(func pid=97446)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=97446)[0m f1_macro: 0.09860205604635061
[2m[36m(func pid=97446)[0m f1_weighted: 0.1439230101556601
[2m[36m(func pid=97446)[0m f1_per_class: [0.121, 0.154, 0.087, 0.347, 0.007, 0.022, 0.015, 0.139, 0.058, 0.036]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 2.9261 | Steps: 4 | Val loss: 2.4498 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0119 | Steps: 4 | Val loss: 1.4910 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=97361)[0m top1: 0.06856343283582089
[2m[36m(func pid=97361)[0m top5: 0.47574626865671643
[2m[36m(func pid=97361)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=97361)[0m f1_macro: 0.03797672352144191
[2m[36m(func pid=97361)[0m f1_weighted: 0.05799409016004999
[2m[36m(func pid=97361)[0m f1_per_class: [0.044, 0.076, 0.0, 0.135, 0.0, 0.006, 0.0, 0.089, 0.0, 0.029]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5066 | Steps: 4 | Val loss: 2.1506 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 13:51:35 (running for 00:08:11.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.188 |      0.423 |                   79 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.926 |      0.181 |                   80 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.952 |      0.038 |                    3 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  2.785 |      0.099 |                    2 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.3558768656716418
[2m[36m(func pid=79093)[0m top5: 0.7835820895522388
[2m[36m(func pid=79093)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=79093)[0m f1_macro: 0.1808736327307236
[2m[36m(func pid=79093)[0m f1_weighted: 0.35186112155367516
[2m[36m(func pid=79093)[0m f1_per_class: [0.09, 0.103, 0.048, 0.483, 0.057, 0.41, 0.481, 0.09, 0.047, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.47201492537313433
[2m[36m(func pid=78294)[0m top5: 0.9524253731343284
[2m[36m(func pid=78294)[0m f1_micro: 0.47201492537313433
[2m[36m(func pid=78294)[0m f1_macro: 0.41504134756221206
[2m[36m(func pid=78294)[0m f1_weighted: 0.49054408555907225
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.58, 0.462, 0.63, 0.191, 0.277, 0.454, 0.312, 0.253, 0.312]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8447 | Steps: 4 | Val loss: 2.4969 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=97446)[0m top1: 0.228544776119403
[2m[36m(func pid=97446)[0m top5: 0.6888992537313433
[2m[36m(func pid=97446)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=97446)[0m f1_macro: 0.14578987363556517
[2m[36m(func pid=97446)[0m f1_weighted: 0.21951375246257582
[2m[36m(func pid=97446)[0m f1_per_class: [0.168, 0.144, 0.235, 0.438, 0.018, 0.018, 0.184, 0.146, 0.052, 0.056]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 2.1105 | Steps: 4 | Val loss: 2.2307 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.1487 | Steps: 4 | Val loss: 1.5013 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=97361)[0m top1: 0.0914179104477612
[2m[36m(func pid=97361)[0m top5: 0.488339552238806
[2m[36m(func pid=97361)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=97361)[0m f1_macro: 0.05279297614236762
[2m[36m(func pid=97361)[0m f1_weighted: 0.08518893260672863
[2m[36m(func pid=97361)[0m f1_per_class: [0.047, 0.119, 0.0, 0.197, 0.0, 0.018, 0.0, 0.098, 0.024, 0.026]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.1555 | Steps: 4 | Val loss: 2.0307 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=79093)[0m top1: 0.37406716417910446
[2m[36m(func pid=79093)[0m top5: 0.789179104477612
[2m[36m(func pid=79093)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=79093)[0m f1_macro: 0.2026763997894967
[2m[36m(func pid=79093)[0m f1_weighted: 0.35560619535854554
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.119, 0.11, 0.498, 0.032, 0.406, 0.421, 0.367, 0.074, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:51:40 (running for 00:08:17.29)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.012 |      0.415 |                   80 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.11  |      0.203 |                   81 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.845 |      0.053 |                    4 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  2.507 |      0.146 |                    3 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.46828358208955223
[2m[36m(func pid=78294)[0m top5: 0.9556902985074627
[2m[36m(func pid=78294)[0m f1_micro: 0.46828358208955223
[2m[36m(func pid=78294)[0m f1_macro: 0.41424443640692
[2m[36m(func pid=78294)[0m f1_weighted: 0.4906154548586463
[2m[36m(func pid=78294)[0m f1_per_class: [0.717, 0.546, 0.429, 0.636, 0.176, 0.295, 0.46, 0.31, 0.256, 0.319]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97446)[0m top1: 0.32882462686567165
[2m[36m(func pid=97446)[0m top5: 0.7495335820895522
[2m[36m(func pid=97446)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=97446)[0m f1_macro: 0.18112247021621503
[2m[36m(func pid=97446)[0m f1_weighted: 0.3049027677788707
[2m[36m(func pid=97446)[0m f1_per_class: [0.295, 0.041, 0.138, 0.487, 0.038, 0.014, 0.479, 0.101, 0.098, 0.12]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8684 | Steps: 4 | Val loss: 2.4605 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 3.4994 | Steps: 4 | Val loss: 2.2440 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0786 | Steps: 4 | Val loss: 1.5187 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=97361)[0m top1: 0.10727611940298508
[2m[36m(func pid=97361)[0m top5: 0.5102611940298507
[2m[36m(func pid=97361)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=97361)[0m f1_macro: 0.05967337165406853
[2m[36m(func pid=97361)[0m f1_weighted: 0.09854412756025624
[2m[36m(func pid=97361)[0m f1_per_class: [0.05, 0.121, 0.0, 0.24, 0.0, 0.022, 0.0, 0.107, 0.02, 0.037]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8946 | Steps: 4 | Val loss: 2.0631 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:51:45 (running for 00:08:22.56)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.149 |      0.414 |                   81 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.499 |      0.201 |                   82 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.868 |      0.06  |                    5 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  2.155 |      0.181 |                    4 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.3283582089552239
[2m[36m(func pid=79093)[0m top5: 0.7919776119402985
[2m[36m(func pid=79093)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=79093)[0m f1_macro: 0.20052398813889413
[2m[36m(func pid=79093)[0m f1_weighted: 0.32833523159878614
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.316, 0.094, 0.405, 0.0, 0.407, 0.293, 0.416, 0.075, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.46968283582089554
[2m[36m(func pid=78294)[0m top5: 0.9528917910447762
[2m[36m(func pid=78294)[0m f1_micro: 0.46968283582089554
[2m[36m(func pid=78294)[0m f1_macro: 0.41411801669283455
[2m[36m(func pid=78294)[0m f1_weighted: 0.4883767131842061
[2m[36m(func pid=78294)[0m f1_per_class: [0.716, 0.548, 0.429, 0.637, 0.156, 0.279, 0.456, 0.312, 0.24, 0.37]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7205 | Steps: 4 | Val loss: 2.4327 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=97446)[0m top1: 0.25279850746268656
[2m[36m(func pid=97446)[0m top5: 0.7672574626865671
[2m[36m(func pid=97446)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=97446)[0m f1_macro: 0.182734829201821
[2m[36m(func pid=97446)[0m f1_weighted: 0.2725694207566804
[2m[36m(func pid=97446)[0m f1_per_class: [0.301, 0.03, 0.131, 0.333, 0.133, 0.015, 0.498, 0.219, 0.087, 0.081]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 3.2559 | Steps: 4 | Val loss: 2.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0151 | Steps: 4 | Val loss: 1.5366 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=97361)[0m top1: 0.11940298507462686
[2m[36m(func pid=97361)[0m top5: 0.5107276119402985
[2m[36m(func pid=97361)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=97361)[0m f1_macro: 0.06948222669145691
[2m[36m(func pid=97361)[0m f1_weighted: 0.11066951871625688
[2m[36m(func pid=97361)[0m f1_per_class: [0.052, 0.151, 0.0, 0.25, 0.0, 0.046, 0.0, 0.115, 0.05, 0.032]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4666 | Steps: 4 | Val loss: 2.0941 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 13:51:50 (running for 00:08:27.83)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.079 |      0.414 |                   82 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.256 |      0.187 |                   83 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.72  |      0.069 |                    6 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  1.895 |      0.183 |                    5 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.29197761194029853
[2m[36m(func pid=79093)[0m top5: 0.777518656716418
[2m[36m(func pid=79093)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=79093)[0m f1_macro: 0.18725877917396438
[2m[36m(func pid=79093)[0m f1_weighted: 0.27750733140599854
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.387, 0.084, 0.224, 0.0, 0.379, 0.252, 0.46, 0.087, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.466884328358209
[2m[36m(func pid=78294)[0m top5: 0.9477611940298507
[2m[36m(func pid=78294)[0m f1_micro: 0.46688432835820903
[2m[36m(func pid=78294)[0m f1_macro: 0.40703813989038473
[2m[36m(func pid=78294)[0m f1_weighted: 0.488975136315457
[2m[36m(func pid=78294)[0m f1_per_class: [0.687, 0.555, 0.421, 0.628, 0.14, 0.291, 0.461, 0.304, 0.253, 0.33]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7727 | Steps: 4 | Val loss: 2.4182 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=97446)[0m top1: 0.20708955223880596
[2m[36m(func pid=97446)[0m top5: 0.7611940298507462
[2m[36m(func pid=97446)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=97446)[0m f1_macro: 0.17748211452766646
[2m[36m(func pid=97446)[0m f1_weighted: 0.2232011444207693
[2m[36m(func pid=97446)[0m f1_per_class: [0.256, 0.227, 0.124, 0.265, 0.107, 0.05, 0.259, 0.275, 0.113, 0.1]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 2.2268 | Steps: 4 | Val loss: 2.1749 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0270 | Steps: 4 | Val loss: 1.5502 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=97361)[0m top1: 0.11940298507462686
[2m[36m(func pid=97361)[0m top5: 0.5177238805970149
[2m[36m(func pid=97361)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=97361)[0m f1_macro: 0.07407250397863102
[2m[36m(func pid=97361)[0m f1_weighted: 0.11141851452431287
[2m[36m(func pid=97361)[0m f1_per_class: [0.062, 0.167, 0.0, 0.236, 0.0, 0.05, 0.0, 0.124, 0.068, 0.035]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.0740 | Steps: 4 | Val loss: 2.1453 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:51:56 (running for 00:08:33.06)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.015 |      0.407 |                   83 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.227 |      0.2   |                   84 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.773 |      0.074 |                    7 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  1.467 |      0.177 |                    6 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.30223880597014924
[2m[36m(func pid=79093)[0m top5: 0.7789179104477612
[2m[36m(func pid=79093)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=79093)[0m f1_macro: 0.20017098927942514
[2m[36m(func pid=79093)[0m f1_weighted: 0.32128482318433454
[2m[36m(func pid=79093)[0m f1_per_class: [0.0, 0.275, 0.077, 0.394, 0.0, 0.338, 0.308, 0.522, 0.087, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4612873134328358
[2m[36m(func pid=78294)[0m top5: 0.9519589552238806
[2m[36m(func pid=78294)[0m f1_micro: 0.4612873134328358
[2m[36m(func pid=78294)[0m f1_macro: 0.40233110157247093
[2m[36m(func pid=78294)[0m f1_weighted: 0.4849255865867068
[2m[36m(func pid=78294)[0m f1_per_class: [0.68, 0.518, 0.421, 0.632, 0.12, 0.279, 0.472, 0.296, 0.258, 0.348]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6756 | Steps: 4 | Val loss: 2.3938 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=97446)[0m top1: 0.19776119402985073
[2m[36m(func pid=97446)[0m top5: 0.7201492537313433
[2m[36m(func pid=97446)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=97446)[0m f1_macro: 0.17297566721943847
[2m[36m(func pid=97446)[0m f1_weighted: 0.18496259123823522
[2m[36m(func pid=97446)[0m f1_per_class: [0.153, 0.345, 0.244, 0.278, 0.058, 0.032, 0.054, 0.294, 0.144, 0.128]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 2.2110 | Steps: 4 | Val loss: 2.0727 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0084 | Steps: 4 | Val loss: 1.5010 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=97361)[0m top1: 0.11893656716417911
[2m[36m(func pid=97361)[0m top5: 0.5242537313432836
[2m[36m(func pid=97361)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=97361)[0m f1_macro: 0.08753713550257977
[2m[36m(func pid=97361)[0m f1_weighted: 0.11177528884416943
[2m[36m(func pid=97361)[0m f1_per_class: [0.06, 0.169, 0.1, 0.228, 0.0, 0.055, 0.0, 0.138, 0.077, 0.05]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.0217 | Steps: 4 | Val loss: 2.1228 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:52:01 (running for 00:08:38.37)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.027 |      0.402 |                   84 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.211 |      0.2   |                   85 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.676 |      0.088 |                    8 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  1.074 |      0.173 |                    7 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.34281716417910446
[2m[36m(func pid=79093)[0m top5: 0.7971082089552238
[2m[36m(func pid=79093)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=79093)[0m f1_macro: 0.19958267956068415
[2m[36m(func pid=79093)[0m f1_weighted: 0.333400578317783
[2m[36m(func pid=79093)[0m f1_per_class: [0.036, 0.005, 0.107, 0.503, 0.0, 0.372, 0.393, 0.485, 0.095, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.478544776119403
[2m[36m(func pid=78294)[0m top5: 0.9556902985074627
[2m[36m(func pid=78294)[0m f1_micro: 0.478544776119403
[2m[36m(func pid=78294)[0m f1_macro: 0.4135210359379887
[2m[36m(func pid=78294)[0m f1_weighted: 0.4990235371874221
[2m[36m(func pid=78294)[0m f1_per_class: [0.694, 0.522, 0.462, 0.638, 0.15, 0.306, 0.502, 0.288, 0.245, 0.33]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5498 | Steps: 4 | Val loss: 2.3819 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=97446)[0m top1: 0.2140858208955224
[2m[36m(func pid=97446)[0m top5: 0.7355410447761194
[2m[36m(func pid=97446)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=97446)[0m f1_macro: 0.2128818915589834
[2m[36m(func pid=97446)[0m f1_weighted: 0.18960742924905638
[2m[36m(func pid=97446)[0m f1_per_class: [0.2, 0.414, 0.512, 0.298, 0.056, 0.019, 0.006, 0.314, 0.084, 0.226]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 3.3564 | Steps: 4 | Val loss: 2.1743 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0139 | Steps: 4 | Val loss: 1.5770 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=97361)[0m top1: 0.11800373134328358
[2m[36m(func pid=97361)[0m top5: 0.5335820895522388
[2m[36m(func pid=97361)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=97361)[0m f1_macro: 0.0874077713202222
[2m[36m(func pid=97361)[0m f1_weighted: 0.11322312617917361
[2m[36m(func pid=97361)[0m f1_per_class: [0.067, 0.171, 0.074, 0.218, 0.0, 0.072, 0.006, 0.147, 0.068, 0.053]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0053 | Steps: 4 | Val loss: 1.9540 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 13:52:06 (running for 00:08:43.88)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.008 |      0.414 |                   85 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  3.356 |      0.166 |                   86 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.55  |      0.087 |                    9 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  1.022 |      0.213 |                    8 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.34048507462686567
[2m[36m(func pid=79093)[0m top5: 0.7994402985074627
[2m[36m(func pid=79093)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=79093)[0m f1_macro: 0.1657729667597162
[2m[36m(func pid=79093)[0m f1_weighted: 0.3279999831106486
[2m[36m(func pid=79093)[0m f1_per_class: [0.11, 0.0, 0.105, 0.471, 0.0, 0.401, 0.487, 0.0, 0.084, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4608208955223881
[2m[36m(func pid=78294)[0m top5: 0.9472947761194029
[2m[36m(func pid=78294)[0m f1_micro: 0.4608208955223881
[2m[36m(func pid=78294)[0m f1_macro: 0.40027498362275804
[2m[36m(func pid=78294)[0m f1_weighted: 0.47761496244317053
[2m[36m(func pid=78294)[0m f1_per_class: [0.642, 0.496, 0.453, 0.631, 0.142, 0.313, 0.448, 0.299, 0.273, 0.306]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5264 | Steps: 4 | Val loss: 2.3604 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=97446)[0m top1: 0.283115671641791
[2m[36m(func pid=97446)[0m top5: 0.8083022388059702
[2m[36m(func pid=97446)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=97446)[0m f1_macro: 0.25846636627069486
[2m[36m(func pid=97446)[0m f1_weighted: 0.2623134981442423
[2m[36m(func pid=97446)[0m f1_per_class: [0.359, 0.499, 0.449, 0.39, 0.055, 0.065, 0.077, 0.333, 0.151, 0.206]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 2.0539 | Steps: 4 | Val loss: 2.3429 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0095 | Steps: 4 | Val loss: 1.5162 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=97361)[0m top1: 0.1142723880597015
[2m[36m(func pid=97361)[0m top5: 0.5396455223880597
[2m[36m(func pid=97361)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=97361)[0m f1_macro: 0.09241840530219889
[2m[36m(func pid=97361)[0m f1_weighted: 0.11222504224944913
[2m[36m(func pid=97361)[0m f1_per_class: [0.085, 0.166, 0.131, 0.196, 0.0, 0.087, 0.018, 0.153, 0.056, 0.032]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6497 | Steps: 4 | Val loss: 1.7029 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=79093)[0m top1: 0.3605410447761194
[2m[36m(func pid=79093)[0m top5: 0.7952425373134329
[2m[36m(func pid=79093)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=79093)[0m f1_macro: 0.17864735835246795
[2m[36m(func pid=79093)[0m f1_weighted: 0.34184455151418874
[2m[36m(func pid=79093)[0m f1_per_class: [0.105, 0.0, 0.179, 0.484, 0.0, 0.421, 0.512, 0.0, 0.085, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:52:12 (running for 00:08:49.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.014 |      0.4   |                   86 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.054 |      0.179 |                   87 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.526 |      0.092 |                   10 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  1.005 |      0.258 |                    9 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.47294776119402987
[2m[36m(func pid=78294)[0m top5: 0.9524253731343284
[2m[36m(func pid=78294)[0m f1_micro: 0.47294776119402987
[2m[36m(func pid=78294)[0m f1_macro: 0.4123667656986901
[2m[36m(func pid=78294)[0m f1_weighted: 0.49344167461915367
[2m[36m(func pid=78294)[0m f1_per_class: [0.642, 0.489, 0.522, 0.633, 0.182, 0.336, 0.502, 0.249, 0.264, 0.304]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5348 | Steps: 4 | Val loss: 2.3633 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=97446)[0m top1: 0.3917910447761194
[2m[36m(func pid=97446)[0m top5: 0.9048507462686567
[2m[36m(func pid=97446)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=97446)[0m f1_macro: 0.34216334761942785
[2m[36m(func pid=97446)[0m f1_weighted: 0.3907257938183661
[2m[36m(func pid=97446)[0m f1_per_class: [0.589, 0.49, 0.512, 0.548, 0.083, 0.11, 0.323, 0.341, 0.196, 0.229]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 2.6938 | Steps: 4 | Val loss: 2.3495 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0097 | Steps: 4 | Val loss: 1.5188 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=97361)[0m top1: 0.10867537313432836
[2m[36m(func pid=97361)[0m top5: 0.5317164179104478
[2m[36m(func pid=97361)[0m f1_micro: 0.10867537313432836
[2m[36m(func pid=97361)[0m f1_macro: 0.0960958382881404
[2m[36m(func pid=97361)[0m f1_weighted: 0.10592305917296163
[2m[36m(func pid=97361)[0m f1_per_class: [0.094, 0.147, 0.154, 0.176, 0.0, 0.08, 0.024, 0.152, 0.091, 0.043]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5534 | Steps: 4 | Val loss: 1.6685 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:52:17 (running for 00:08:54.67)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.01  |      0.412 |                   87 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.694 |      0.184 |                   88 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.535 |      0.096 |                   11 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.65  |      0.342 |                   10 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.35027985074626866
[2m[36m(func pid=79093)[0m top5: 0.7840485074626866
[2m[36m(func pid=79093)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=79093)[0m f1_macro: 0.18373403264954077
[2m[36m(func pid=79093)[0m f1_weighted: 0.33500299811458345
[2m[36m(func pid=79093)[0m f1_per_class: [0.094, 0.0, 0.276, 0.448, 0.0, 0.41, 0.526, 0.0, 0.083, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.46781716417910446
[2m[36m(func pid=78294)[0m top5: 0.9533582089552238
[2m[36m(func pid=78294)[0m f1_micro: 0.46781716417910446
[2m[36m(func pid=78294)[0m f1_macro: 0.40725378354108904
[2m[36m(func pid=78294)[0m f1_weighted: 0.4846426555936237
[2m[36m(func pid=78294)[0m f1_per_class: [0.667, 0.472, 0.5, 0.641, 0.178, 0.33, 0.483, 0.221, 0.258, 0.323]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.3252 | Steps: 4 | Val loss: 2.3572 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=97446)[0m top1: 0.38759328358208955
[2m[36m(func pid=97446)[0m top5: 0.9207089552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=97446)[0m f1_macro: 0.33395166946457583
[2m[36m(func pid=97446)[0m f1_weighted: 0.3841414806303201
[2m[36m(func pid=97446)[0m f1_per_class: [0.522, 0.36, 0.468, 0.567, 0.13, 0.123, 0.35, 0.399, 0.153, 0.268]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 2.3866 | Steps: 4 | Val loss: 2.1619 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.1422 | Steps: 4 | Val loss: 1.6536 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=97361)[0m top1: 0.10727611940298508
[2m[36m(func pid=97361)[0m top5: 0.527518656716418
[2m[36m(func pid=97361)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=97361)[0m f1_macro: 0.09822603102818131
[2m[36m(func pid=97361)[0m f1_weighted: 0.10804988546321836
[2m[36m(func pid=97361)[0m f1_per_class: [0.108, 0.145, 0.164, 0.166, 0.0, 0.091, 0.039, 0.152, 0.078, 0.04]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5893 | Steps: 4 | Val loss: 1.6293 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 13:52:23 (running for 00:09:00.25)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.01  |      0.407 |                   88 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.387 |      0.186 |                   89 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.325 |      0.098 |                   12 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.553 |      0.334 |                   11 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.30923507462686567
[2m[36m(func pid=79093)[0m top5: 0.7602611940298507
[2m[36m(func pid=79093)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=79093)[0m f1_macro: 0.1859898664191862
[2m[36m(func pid=79093)[0m f1_weighted: 0.2980723084890672
[2m[36m(func pid=79093)[0m f1_per_class: [0.105, 0.0, 0.386, 0.323, 0.0, 0.381, 0.515, 0.06, 0.09, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.43703358208955223
[2m[36m(func pid=78294)[0m top5: 0.9500932835820896
[2m[36m(func pid=78294)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=78294)[0m f1_macro: 0.38542727689883827
[2m[36m(func pid=78294)[0m f1_weighted: 0.44959739983882924
[2m[36m(func pid=78294)[0m f1_per_class: [0.607, 0.475, 0.48, 0.633, 0.139, 0.31, 0.383, 0.217, 0.262, 0.349]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.3724 | Steps: 4 | Val loss: 2.3507 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=97446)[0m top1: 0.3941231343283582
[2m[36m(func pid=97446)[0m top5: 0.9333022388059702
[2m[36m(func pid=97446)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=97446)[0m f1_macro: 0.33724792656198993
[2m[36m(func pid=97446)[0m f1_weighted: 0.3894035235480411
[2m[36m(func pid=97446)[0m f1_per_class: [0.492, 0.303, 0.478, 0.579, 0.151, 0.181, 0.365, 0.415, 0.153, 0.255]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 2.7042 | Steps: 4 | Val loss: 2.0964 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.1030 | Steps: 4 | Val loss: 1.6812 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=97361)[0m top1: 0.1044776119402985
[2m[36m(func pid=97361)[0m top5: 0.5293843283582089
[2m[36m(func pid=97361)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=97361)[0m f1_macro: 0.09501997348125918
[2m[36m(func pid=97361)[0m f1_weighted: 0.10654227167507727
[2m[36m(func pid=97361)[0m f1_per_class: [0.096, 0.128, 0.148, 0.164, 0.0, 0.088, 0.044, 0.158, 0.089, 0.034]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4412 | Steps: 4 | Val loss: 1.5240 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=79093)[0m top1: 0.24067164179104478
[2m[36m(func pid=79093)[0m top5: 0.7448694029850746
[2m[36m(func pid=79093)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=79093)[0m f1_macro: 0.19047970928009542
[2m[36m(func pid=79093)[0m f1_weighted: 0.20961285636357677
[2m[36m(func pid=79093)[0m f1_per_class: [0.085, 0.0, 0.426, 0.016, 0.0, 0.354, 0.432, 0.484, 0.107, 0.0]
== Status ==
Current time: 2024-01-07 13:52:28 (running for 00:09:05.68)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.142 |      0.385 |                   89 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.704 |      0.19  |                   90 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.372 |      0.095 |                   13 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.589 |      0.337 |                   12 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4351679104477612
[2m[36m(func pid=78294)[0m top5: 0.9496268656716418
[2m[36m(func pid=78294)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=78294)[0m f1_macro: 0.37862048587937014
[2m[36m(func pid=78294)[0m f1_weighted: 0.45541819251136206
[2m[36m(func pid=78294)[0m f1_per_class: [0.636, 0.486, 0.436, 0.63, 0.128, 0.299, 0.417, 0.139, 0.272, 0.344]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.3814 | Steps: 4 | Val loss: 2.3420 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=97446)[0m top1: 0.447294776119403
[2m[36m(func pid=97446)[0m top5: 0.9500932835820896
[2m[36m(func pid=97446)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=97446)[0m f1_macro: 0.38259569263795135
[2m[36m(func pid=97446)[0m f1_weighted: 0.4524638030349903
[2m[36m(func pid=97446)[0m f1_per_class: [0.595, 0.552, 0.449, 0.578, 0.146, 0.214, 0.414, 0.386, 0.199, 0.293]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 2.0649 | Steps: 4 | Val loss: 2.0998 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=97361)[0m top1: 0.10914179104477612
[2m[36m(func pid=97361)[0m top5: 0.5415111940298507
[2m[36m(func pid=97361)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=97361)[0m f1_macro: 0.09570066455218894
[2m[36m(func pid=97361)[0m f1_weighted: 0.11193201578502154
[2m[36m(func pid=97361)[0m f1_per_class: [0.112, 0.14, 0.11, 0.175, 0.011, 0.084, 0.047, 0.16, 0.084, 0.034]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0566 | Steps: 4 | Val loss: 1.6663 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5199 | Steps: 4 | Val loss: 1.4893 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:52:34 (running for 00:09:11.01)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.103 |      0.379 |                   90 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.065 |      0.182 |                   91 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.381 |      0.096 |                   14 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.441 |      0.383 |                   13 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.23087686567164178
[2m[36m(func pid=79093)[0m top5: 0.7625932835820896
[2m[36m(func pid=79093)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=79093)[0m f1_macro: 0.18212074509887793
[2m[36m(func pid=79093)[0m f1_weighted: 0.20391353895094408
[2m[36m(func pid=79093)[0m f1_per_class: [0.085, 0.0, 0.392, 0.103, 0.0, 0.369, 0.331, 0.499, 0.043, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4430970149253731
[2m[36m(func pid=78294)[0m top5: 0.9510261194029851
[2m[36m(func pid=78294)[0m f1_micro: 0.4430970149253731
[2m[36m(func pid=78294)[0m f1_macro: 0.3758614784412694
[2m[36m(func pid=78294)[0m f1_weighted: 0.45830693643102344
[2m[36m(func pid=78294)[0m f1_per_class: [0.624, 0.471, 0.436, 0.635, 0.135, 0.304, 0.434, 0.129, 0.243, 0.348]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3174 | Steps: 4 | Val loss: 2.3177 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=97446)[0m top1: 0.4505597014925373
[2m[36m(func pid=97446)[0m top5: 0.9500932835820896
[2m[36m(func pid=97446)[0m f1_micro: 0.4505597014925373
[2m[36m(func pid=97446)[0m f1_macro: 0.40519652026175634
[2m[36m(func pid=97446)[0m f1_weighted: 0.45579362959932196
[2m[36m(func pid=97446)[0m f1_per_class: [0.619, 0.596, 0.458, 0.559, 0.184, 0.293, 0.377, 0.392, 0.24, 0.333]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 2.3758 | Steps: 4 | Val loss: 2.0753 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=97361)[0m top1: 0.11613805970149253
[2m[36m(func pid=97361)[0m top5: 0.5643656716417911
[2m[36m(func pid=97361)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=97361)[0m f1_macro: 0.10286938508081896
[2m[36m(func pid=97361)[0m f1_weighted: 0.12288176684838249
[2m[36m(func pid=97361)[0m f1_per_class: [0.118, 0.17, 0.124, 0.182, 0.011, 0.081, 0.063, 0.145, 0.076, 0.059]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.1297 | Steps: 4 | Val loss: 1.7061 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3390 | Steps: 4 | Val loss: 1.5243 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 13:52:39 (running for 00:09:16.53)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.057 |      0.376 |                   91 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.376 |      0.197 |                   92 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.317 |      0.103 |                   15 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.52  |      0.405 |                   14 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.23460820895522388
[2m[36m(func pid=79093)[0m top5: 0.7742537313432836
[2m[36m(func pid=79093)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=79093)[0m f1_macro: 0.19714769027840356
[2m[36m(func pid=79093)[0m f1_weighted: 0.22121315408327363
[2m[36m(func pid=79093)[0m f1_per_class: [0.071, 0.0, 0.333, 0.141, 0.122, 0.398, 0.334, 0.548, 0.024, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.42630597014925375
[2m[36m(func pid=78294)[0m top5: 0.9472947761194029
[2m[36m(func pid=78294)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=78294)[0m f1_macro: 0.3692093886819056
[2m[36m(func pid=78294)[0m f1_weighted: 0.4446696068213138
[2m[36m(func pid=78294)[0m f1_per_class: [0.618, 0.422, 0.453, 0.622, 0.124, 0.293, 0.432, 0.128, 0.252, 0.348]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.2515 | Steps: 4 | Val loss: 2.2943 | Batch size: 32 | lr: 0.0001 | Duration: 3.29s
[2m[36m(func pid=97446)[0m top1: 0.4435634328358209
[2m[36m(func pid=97446)[0m top5: 0.9370335820895522
[2m[36m(func pid=97446)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=97446)[0m f1_macro: 0.4004269932259237
[2m[36m(func pid=97446)[0m f1_weighted: 0.43249439182017857
[2m[36m(func pid=97446)[0m f1_per_class: [0.643, 0.597, 0.353, 0.546, 0.198, 0.303, 0.302, 0.411, 0.224, 0.429]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 2.3868 | Steps: 4 | Val loss: 2.1524 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0288 | Steps: 4 | Val loss: 1.6968 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=97361)[0m top1: 0.12406716417910447
[2m[36m(func pid=97361)[0m top5: 0.582089552238806
[2m[36m(func pid=97361)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=97361)[0m f1_macro: 0.11025784443514255
[2m[36m(func pid=97361)[0m f1_weighted: 0.1325082823789667
[2m[36m(func pid=97361)[0m f1_per_class: [0.123, 0.204, 0.133, 0.171, 0.009, 0.075, 0.084, 0.164, 0.074, 0.065]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3082 | Steps: 4 | Val loss: 1.5332 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=79093)[0m top1: 0.21082089552238806
[2m[36m(func pid=79093)[0m top5: 0.7751865671641791
[2m[36m(func pid=79093)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=79093)[0m f1_macro: 0.182725401068521
[2m[36m(func pid=79093)[0m f1_weighted: 0.2038257312589862
[2m[36m(func pid=79093)[0m f1_per_class: [0.07, 0.0, 0.377, 0.11, 0.057, 0.402, 0.317, 0.494, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
== Status ==
Current time: 2024-01-07 13:52:45 (running for 00:09:21.91)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.13  |      0.369 |                   92 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.387 |      0.183 |                   93 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.251 |      0.11  |                   16 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.339 |      0.4   |                   15 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=78294)[0m top1: 0.4281716417910448
[2m[36m(func pid=78294)[0m top5: 0.9444962686567164
[2m[36m(func pid=78294)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=78294)[0m f1_macro: 0.3727116262438773
[2m[36m(func pid=78294)[0m f1_weighted: 0.4462427596779665
[2m[36m(func pid=78294)[0m f1_per_class: [0.667, 0.442, 0.436, 0.626, 0.131, 0.298, 0.419, 0.126, 0.245, 0.337]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2255 | Steps: 4 | Val loss: 2.2775 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=97446)[0m top1: 0.4505597014925373
[2m[36m(func pid=97446)[0m top5: 0.9361007462686567
[2m[36m(func pid=97446)[0m f1_micro: 0.4505597014925373
[2m[36m(func pid=97446)[0m f1_macro: 0.3911527560820419
[2m[36m(func pid=97446)[0m f1_weighted: 0.4432548926727597
[2m[36m(func pid=97446)[0m f1_per_class: [0.588, 0.598, 0.312, 0.549, 0.187, 0.291, 0.345, 0.397, 0.25, 0.394]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 2.4332 | Steps: 4 | Val loss: 2.1189 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3140 | Steps: 4 | Val loss: 1.7339 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=97361)[0m top1: 0.12779850746268656
[2m[36m(func pid=97361)[0m top5: 0.5993470149253731
[2m[36m(func pid=97361)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=97361)[0m f1_macro: 0.11703278166774811
[2m[36m(func pid=97361)[0m f1_weighted: 0.13691685614387775
[2m[36m(func pid=97361)[0m f1_per_class: [0.14, 0.179, 0.145, 0.165, 0.027, 0.078, 0.114, 0.171, 0.08, 0.071]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2555 | Steps: 4 | Val loss: 1.4939 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:52:50 (running for 00:09:27.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.029 |      0.373 |                   93 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.433 |      0.177 |                   94 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.226 |      0.117 |                   17 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.308 |      0.391 |                   16 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.18190298507462688
[2m[36m(func pid=79093)[0m top5: 0.7751865671641791
[2m[36m(func pid=79093)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=79093)[0m f1_macro: 0.17675861967554926
[2m[36m(func pid=79093)[0m f1_weighted: 0.19635720819814947
[2m[36m(func pid=79093)[0m f1_per_class: [0.069, 0.0, 0.343, 0.089, 0.054, 0.295, 0.34, 0.55, 0.028, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.43097014925373134
[2m[36m(func pid=78294)[0m top5: 0.9398320895522388
[2m[36m(func pid=78294)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=78294)[0m f1_macro: 0.38150411140307694
[2m[36m(func pid=78294)[0m f1_weighted: 0.45335856240788036
[2m[36m(func pid=78294)[0m f1_per_class: [0.66, 0.45, 0.545, 0.624, 0.123, 0.281, 0.445, 0.12, 0.26, 0.305]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.1289 | Steps: 4 | Val loss: 2.2701 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=97446)[0m top1: 0.4566231343283582
[2m[36m(func pid=97446)[0m top5: 0.9398320895522388
[2m[36m(func pid=97446)[0m f1_micro: 0.4566231343283582
[2m[36m(func pid=97446)[0m f1_macro: 0.38578143951938154
[2m[36m(func pid=97446)[0m f1_weighted: 0.4464725030921706
[2m[36m(func pid=97446)[0m f1_per_class: [0.559, 0.608, 0.353, 0.579, 0.196, 0.297, 0.329, 0.388, 0.206, 0.344]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 2.4509 | Steps: 4 | Val loss: 2.1518 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=97361)[0m top1: 0.1357276119402985
[2m[36m(func pid=97361)[0m top5: 0.6012126865671642
[2m[36m(func pid=97361)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=97361)[0m f1_macro: 0.12163742687267547
[2m[36m(func pid=97361)[0m f1_weighted: 0.1497605520509626
[2m[36m(func pid=97361)[0m f1_per_class: [0.141, 0.181, 0.145, 0.169, 0.039, 0.071, 0.151, 0.188, 0.084, 0.047]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0501 | Steps: 4 | Val loss: 1.6679 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2239 | Steps: 4 | Val loss: 1.5388 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:52:55 (running for 00:09:32.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.314 |      0.382 |                   94 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.451 |      0.147 |                   95 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.129 |      0.122 |                   18 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.255 |      0.386 |                   17 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.1571828358208955
[2m[36m(func pid=79093)[0m top5: 0.769589552238806
[2m[36m(func pid=79093)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=79093)[0m f1_macro: 0.1470372938613695
[2m[36m(func pid=79093)[0m f1_weighted: 0.17873066033891913
[2m[36m(func pid=79093)[0m f1_per_class: [0.072, 0.0, 0.267, 0.095, 0.041, 0.126, 0.35, 0.519, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.44029850746268656
[2m[36m(func pid=78294)[0m top5: 0.9449626865671642
[2m[36m(func pid=78294)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=78294)[0m f1_macro: 0.3869873206043891
[2m[36m(func pid=78294)[0m f1_weighted: 0.46437939454875504
[2m[36m(func pid=78294)[0m f1_per_class: [0.66, 0.467, 0.558, 0.619, 0.136, 0.295, 0.474, 0.114, 0.256, 0.291]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.9962 | Steps: 4 | Val loss: 2.2446 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=97446)[0m top1: 0.439365671641791
[2m[36m(func pid=97446)[0m top5: 0.9319029850746269
[2m[36m(func pid=97446)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=97446)[0m f1_macro: 0.37775822300996204
[2m[36m(func pid=97446)[0m f1_weighted: 0.42252542011796596
[2m[36m(func pid=97446)[0m f1_per_class: [0.571, 0.605, 0.387, 0.584, 0.171, 0.298, 0.25, 0.369, 0.178, 0.364]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 2.4785 | Steps: 4 | Val loss: 2.1843 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=97361)[0m top1: 0.14738805970149255
[2m[36m(func pid=97361)[0m top5: 0.6259328358208955
[2m[36m(func pid=97361)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=97361)[0m f1_macro: 0.12885233374844796
[2m[36m(func pid=97361)[0m f1_weighted: 0.16170633451150576
[2m[36m(func pid=97361)[0m f1_per_class: [0.166, 0.161, 0.131, 0.221, 0.047, 0.088, 0.146, 0.182, 0.087, 0.059]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0143 | Steps: 4 | Val loss: 1.6725 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6132 | Steps: 4 | Val loss: 1.5821 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=79093)[0m top1: 0.1515858208955224
[2m[36m(func pid=79093)[0m top5: 0.7453358208955224
== Status ==
Current time: 2024-01-07 13:53:01 (running for 00:09:38.21)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.05  |      0.387 |                   95 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.478 |      0.132 |                   96 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  1.996 |      0.129 |                   19 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.224 |      0.378 |                   18 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=79093)[0m f1_micro: 0.1515858208955224

[2m[36m(func pid=79093)[0m f1_macro: 0.1324449564791744
[2m[36m(func pid=79093)[0m f1_weighted: 0.17994861391266215
[2m[36m(func pid=79093)[0m f1_per_class: [0.081, 0.0, 0.197, 0.156, 0.036, 0.039, 0.34, 0.475, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.43097014925373134
[2m[36m(func pid=78294)[0m top5: 0.9416977611940298
[2m[36m(func pid=78294)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=78294)[0m f1_macro: 0.3768056511841125
[2m[36m(func pid=78294)[0m f1_weighted: 0.45564232741060556
[2m[36m(func pid=78294)[0m f1_per_class: [0.618, 0.421, 0.522, 0.622, 0.117, 0.292, 0.463, 0.158, 0.284, 0.271]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4193097014925373
[2m[36m(func pid=97446)[0m top5: 0.9291044776119403
[2m[36m(func pid=97446)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=97446)[0m f1_macro: 0.3681312597561486
[2m[36m(func pid=97446)[0m f1_weighted: 0.4001572890445862
[2m[36m(func pid=97446)[0m f1_per_class: [0.547, 0.606, 0.393, 0.581, 0.179, 0.307, 0.173, 0.372, 0.211, 0.312]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.0247 | Steps: 4 | Val loss: 2.2223 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 2.3792 | Steps: 4 | Val loss: 2.2533 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=97361)[0m top1: 0.16044776119402984
[2m[36m(func pid=97361)[0m top5: 0.6455223880597015
[2m[36m(func pid=97361)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=97361)[0m f1_macro: 0.1362785704946098
[2m[36m(func pid=97361)[0m f1_weighted: 0.17496371396740545
[2m[36m(func pid=97361)[0m f1_per_class: [0.163, 0.18, 0.125, 0.249, 0.047, 0.073, 0.153, 0.213, 0.089, 0.071]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0401 | Steps: 4 | Val loss: 1.6582 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1813 | Steps: 4 | Val loss: 1.7109 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 13:53:06 (running for 00:09:43.74)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.014 |      0.377 |                   96 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.379 |      0.117 |                   97 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  2.025 |      0.136 |                   20 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.613 |      0.368 |                   19 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.166044776119403
[2m[36m(func pid=79093)[0m top5: 0.7285447761194029
[2m[36m(func pid=79093)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=79093)[0m f1_macro: 0.11737099727766406
[2m[36m(func pid=79093)[0m f1_weighted: 0.19958434039831915
[2m[36m(func pid=79093)[0m f1_per_class: [0.094, 0.0, 0.182, 0.27, 0.033, 0.034, 0.352, 0.208, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.4388992537313433
[2m[36m(func pid=78294)[0m top5: 0.9393656716417911
[2m[36m(func pid=78294)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=78294)[0m f1_macro: 0.38602377716233915
[2m[36m(func pid=78294)[0m f1_weighted: 0.46656093119450054
[2m[36m(func pid=78294)[0m f1_per_class: [0.667, 0.448, 0.522, 0.613, 0.12, 0.294, 0.487, 0.167, 0.282, 0.26]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97446)[0m top1: 0.37826492537313433
[2m[36m(func pid=97446)[0m top5: 0.8950559701492538
[2m[36m(func pid=97446)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=97446)[0m f1_macro: 0.3484270061337481
[2m[36m(func pid=97446)[0m f1_weighted: 0.36613394635164576
[2m[36m(func pid=97446)[0m f1_per_class: [0.544, 0.574, 0.324, 0.53, 0.166, 0.276, 0.134, 0.384, 0.228, 0.324]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.9362 | Steps: 4 | Val loss: 2.1952 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 2.2884 | Steps: 4 | Val loss: 2.2631 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=97361)[0m top1: 0.16744402985074627
[2m[36m(func pid=97361)[0m top5: 0.6697761194029851
[2m[36m(func pid=97361)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=97361)[0m f1_macro: 0.14024141921623526
[2m[36m(func pid=97361)[0m f1_weighted: 0.1840198170850534
[2m[36m(func pid=97361)[0m f1_per_class: [0.167, 0.15, 0.126, 0.288, 0.043, 0.074, 0.163, 0.215, 0.085, 0.091]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0161 | Steps: 4 | Val loss: 1.6663 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2575 | Steps: 4 | Val loss: 1.8164 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 13:53:12 (running for 00:09:49.03)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.04  |      0.386 |                   97 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.288 |      0.12  |                   98 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  1.936 |      0.14  |                   21 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.181 |      0.348 |                   20 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.1599813432835821
[2m[36m(func pid=79093)[0m top5: 0.7000932835820896
[2m[36m(func pid=79093)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=79093)[0m f1_macro: 0.11967560425031012
[2m[36m(func pid=79093)[0m f1_weighted: 0.18940960863645814
[2m[36m(func pid=79093)[0m f1_per_class: [0.083, 0.0, 0.279, 0.344, 0.03, 0.028, 0.256, 0.176, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.44402985074626866
[2m[36m(func pid=78294)[0m top5: 0.933768656716418
[2m[36m(func pid=78294)[0m f1_micro: 0.44402985074626866
[2m[36m(func pid=78294)[0m f1_macro: 0.38557896831576166
[2m[36m(func pid=78294)[0m f1_weighted: 0.47546519749963617
[2m[36m(func pid=78294)[0m f1_per_class: [0.667, 0.466, 0.511, 0.599, 0.107, 0.283, 0.527, 0.155, 0.268, 0.272]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97446)[0m top1: 0.341884328358209
[2m[36m(func pid=97446)[0m top5: 0.867070895522388
[2m[36m(func pid=97446)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=97446)[0m f1_macro: 0.3201867391178858
[2m[36m(func pid=97446)[0m f1_weighted: 0.3402553727966996
[2m[36m(func pid=97446)[0m f1_per_class: [0.561, 0.508, 0.242, 0.523, 0.151, 0.245, 0.115, 0.369, 0.176, 0.313]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8172 | Steps: 4 | Val loss: 2.1903 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 2.4856 | Steps: 4 | Val loss: 2.2900 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=97361)[0m top1: 0.17350746268656717
[2m[36m(func pid=97361)[0m top5: 0.6744402985074627
[2m[36m(func pid=97361)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=97361)[0m f1_macro: 0.14800683594051414
[2m[36m(func pid=97361)[0m f1_weighted: 0.19118018640423282
[2m[36m(func pid=97361)[0m f1_per_class: [0.171, 0.126, 0.152, 0.285, 0.045, 0.09, 0.193, 0.225, 0.097, 0.097]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0065 | Steps: 4 | Val loss: 1.6709 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1334 | Steps: 4 | Val loss: 1.8451 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:53:17 (running for 00:09:54.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.016 |      0.386 |                   98 |
| train_5806f_00002 | RUNNING    | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.486 |      0.116 |                   99 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  1.817 |      0.148 |                   22 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.257 |      0.32  |                   21 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.16744402985074627
[2m[36m(func pid=79093)[0m top5: 0.6828358208955224
[2m[36m(func pid=79093)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=79093)[0m f1_macro: 0.11625091511875496
[2m[36m(func pid=79093)[0m f1_weighted: 0.1874958527581597
[2m[36m(func pid=79093)[0m f1_per_class: [0.093, 0.0, 0.224, 0.41, 0.029, 0.027, 0.186, 0.194, 0.0, 0.0]
[2m[36m(func pid=79093)[0m 
[2m[36m(func pid=78294)[0m top1: 0.43703358208955223
[2m[36m(func pid=78294)[0m top5: 0.9333022388059702
[2m[36m(func pid=78294)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=78294)[0m f1_macro: 0.3870583184913753
[2m[36m(func pid=78294)[0m f1_weighted: 0.47263450405827895
[2m[36m(func pid=78294)[0m f1_per_class: [0.625, 0.492, 0.5, 0.585, 0.111, 0.283, 0.505, 0.222, 0.271, 0.275]
[2m[36m(func pid=78294)[0m 
[2m[36m(func pid=97446)[0m top1: 0.3246268656716418
[2m[36m(func pid=97446)[0m top5: 0.863339552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=97446)[0m f1_macro: 0.32465169683633055
[2m[36m(func pid=97446)[0m f1_weighted: 0.3331708042062542
[2m[36m(func pid=97446)[0m f1_per_class: [0.591, 0.439, 0.226, 0.521, 0.197, 0.265, 0.124, 0.371, 0.137, 0.377]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.8198 | Steps: 4 | Val loss: 2.1839 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=79093)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 2.9270 | Steps: 4 | Val loss: 2.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=97361)[0m top1: 0.17444029850746268
[2m[36m(func pid=97361)[0m top5: 0.6828358208955224
[2m[36m(func pid=97361)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=97361)[0m f1_macro: 0.15041546155700897
[2m[36m(func pid=97361)[0m f1_weighted: 0.19380715329815343
[2m[36m(func pid=97361)[0m f1_per_class: [0.195, 0.135, 0.165, 0.293, 0.048, 0.096, 0.188, 0.223, 0.088, 0.074]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=78294)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0181 | Steps: 4 | Val loss: 1.6808 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4460 | Steps: 4 | Val loss: 1.7602 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 13:53:22 (running for 00:09:59.51)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.22175
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 3 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | RUNNING    | 192.168.7.53:78294 | 0.0001 |       0.99 |         0      |  0.007 |      0.387 |                   99 |
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361 | 0.0001 |       0.9  |         0      |  1.82  |      0.15  |                   23 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446 | 0.001  |       0.9  |         0      |  0.133 |      0.325 |                   22 |
| train_5806f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677 | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093 | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525 | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79093)[0m top1: 0.15438432835820895
[2m[36m(func pid=79093)[0m top5: 0.6814365671641791
[2m[36m(func pid=79093)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=79093)[0m f1_macro: 0.11151311136178466
[2m[36m(func pid=79093)[0m f1_weighted: 0.16826195987536818
[2m[36m(func pid=79093)[0m f1_per_class: [0.094, 0.0, 0.198, 0.397, 0.028, 0.026, 0.123, 0.247, 0.0, 0.0]
[2m[36m(func pid=78294)[0m top1: 0.4361007462686567
[2m[36m(func pid=78294)[0m top5: 0.9375
[2m[36m(func pid=78294)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=78294)[0m f1_macro: 0.38567536664796065
[2m[36m(func pid=78294)[0m f1_weighted: 0.4699988532006693
[2m[36m(func pid=78294)[0m f1_per_class: [0.598, 0.497, 0.49, 0.592, 0.108, 0.28, 0.488, 0.235, 0.252, 0.316]
[2m[36m(func pid=97446)[0m top1: 0.3362873134328358
[2m[36m(func pid=97446)[0m top5: 0.8941231343283582
[2m[36m(func pid=97446)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=97446)[0m f1_macro: 0.33816983935084177
[2m[36m(func pid=97446)[0m f1_weighted: 0.34361038899421165
[2m[36m(func pid=97446)[0m f1_per_class: [0.581, 0.416, 0.381, 0.53, 0.228, 0.259, 0.168, 0.346, 0.154, 0.32]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8889 | Steps: 4 | Val loss: 2.1942 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=97361)[0m top1: 0.17723880597014927
[2m[36m(func pid=97361)[0m top5: 0.6739738805970149
[2m[36m(func pid=97361)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=97361)[0m f1_macro: 0.156376257599367
[2m[36m(func pid=97361)[0m f1_weighted: 0.19716903424273613
[2m[36m(func pid=97361)[0m f1_per_class: [0.194, 0.135, 0.213, 0.317, 0.049, 0.081, 0.179, 0.23, 0.09, 0.076]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2268 | Steps: 4 | Val loss: 1.6727 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=97446)[0m top1: 0.3829291044776119
[2m[36m(func pid=97446)[0m top5: 0.9039179104477612
[2m[36m(func pid=97446)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=97446)[0m f1_macro: 0.3472166443768341
[2m[36m(func pid=97446)[0m f1_weighted: 0.3866450667106179
[2m[36m(func pid=97446)[0m f1_per_class: [0.58, 0.514, 0.289, 0.558, 0.188, 0.249, 0.231, 0.349, 0.185, 0.329]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.7839 | Steps: 4 | Val loss: 2.1829 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=103465)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103465)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=103465)[0m Configuration completed!
[2m[36m(func pid=103465)[0m New optimizer parameters:
[2m[36m(func pid=103465)[0m SGD (
[2m[36m(func pid=103465)[0m Parameter Group 0
[2m[36m(func pid=103465)[0m     dampening: 0
[2m[36m(func pid=103465)[0m     differentiable: False
[2m[36m(func pid=103465)[0m     foreach: None
[2m[36m(func pid=103465)[0m     lr: 0.01
[2m[36m(func pid=103465)[0m     maximize: False
[2m[36m(func pid=103465)[0m     momentum: 0.9
[2m[36m(func pid=103465)[0m     nesterov: False
[2m[36m(func pid=103465)[0m     weight_decay: 0
[2m[36m(func pid=103465)[0m )
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.18610074626865672
[2m[36m(func pid=97361)[0m top5: 0.6823694029850746
[2m[36m(func pid=97361)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=97361)[0m f1_macro: 0.16999513938671565
[2m[36m(func pid=97361)[0m f1_weighted: 0.2115146992650507
[2m[36m(func pid=97361)[0m f1_per_class: [0.224, 0.144, 0.25, 0.306, 0.049, 0.1, 0.22, 0.238, 0.097, 0.071]
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3308 | Steps: 4 | Val loss: 1.5900 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 13:53:29 (running for 00:10:06.86)
Memory usage on this node: 20.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.889 |      0.156 |                   24 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.227 |      0.347 |                   24 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103554)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=103554)[0m Configuration completed!
[2m[36m(func pid=103554)[0m New optimizer parameters:
[2m[36m(func pid=103554)[0m SGD (
[2m[36m(func pid=103554)[0m Parameter Group 0
[2m[36m(func pid=103554)[0m     dampening: 0
[2m[36m(func pid=103554)[0m     differentiable: False
[2m[36m(func pid=103554)[0m     foreach: None
[2m[36m(func pid=103554)[0m     lr: 0.1
[2m[36m(func pid=103554)[0m     maximize: False
[2m[36m(func pid=103554)[0m     momentum: 0.9
[2m[36m(func pid=103554)[0m     nesterov: False
[2m[36m(func pid=103554)[0m     weight_decay: 0
[2m[36m(func pid=103554)[0m )
[2m[36m(func pid=103554)[0m 
== Status ==
Current time: 2024-01-07 13:53:35 (running for 00:10:12.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.784 |      0.17  |                   25 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.331 |      0.385 |                   25 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |        |            |                      |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |        |            |                      |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4351679104477612
[2m[36m(func pid=97446)[0m top5: 0.9127798507462687
[2m[36m(func pid=97446)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=97446)[0m f1_macro: 0.38506998442521295
[2m[36m(func pid=97446)[0m f1_weighted: 0.42061449705606746
[2m[36m(func pid=97446)[0m f1_per_class: [0.646, 0.586, 0.312, 0.595, 0.186, 0.234, 0.254, 0.373, 0.25, 0.414]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1389 | Steps: 4 | Val loss: 2.4606 | Batch size: 32 | lr: 0.01 | Duration: 4.48s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.6490 | Steps: 4 | Val loss: 2.1374 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.4174 | Steps: 4 | Val loss: 85.0330 | Batch size: 32 | lr: 0.1 | Duration: 4.81s
[2m[36m(func pid=103465)[0m top1: 0.06529850746268656
[2m[36m(func pid=103465)[0m top5: 0.5041977611940298
[2m[36m(func pid=103465)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=103465)[0m f1_macro: 0.0643956042536188
[2m[36m(func pid=103465)[0m f1_weighted: 0.07031395596099754
[2m[36m(func pid=103465)[0m f1_per_class: [0.229, 0.0, 0.046, 0.0, 0.033, 0.076, 0.18, 0.0, 0.079, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1965 | Steps: 4 | Val loss: 1.5583 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=97361)[0m top1: 0.20708955223880596
[2m[36m(func pid=97361)[0m top5: 0.7238805970149254
[2m[36m(func pid=97361)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=97361)[0m f1_macro: 0.18047641088658084
[2m[36m(func pid=97361)[0m f1_weighted: 0.23438293897562687
[2m[36m(func pid=97361)[0m f1_per_class: [0.228, 0.17, 0.254, 0.327, 0.056, 0.101, 0.261, 0.242, 0.097, 0.069]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:53:40 (running for 00:10:17.28)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.649 |      0.18  |                   26 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.331 |      0.385 |                   25 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.139 |      0.064 |                    1 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  6.417 |      0.044 |                    1 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.1609141791044776
[2m[36m(func pid=103554)[0m top5: 0.43656716417910446
[2m[36m(func pid=103554)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=103554)[0m f1_macro: 0.04395450932697488
[2m[36m(func pid=103554)[0m f1_weighted: 0.11898845938178651
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.0, 0.426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4430970149253731
[2m[36m(func pid=97446)[0m top5: 0.9053171641791045
[2m[36m(func pid=97446)[0m f1_micro: 0.4430970149253731
[2m[36m(func pid=97446)[0m f1_macro: 0.40078213270024693
[2m[36m(func pid=97446)[0m f1_weighted: 0.421570981384052
[2m[36m(func pid=97446)[0m f1_per_class: [0.66, 0.606, 0.375, 0.565, 0.182, 0.286, 0.241, 0.406, 0.293, 0.393]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.8438 | Steps: 4 | Val loss: 2.2396 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.7806 | Steps: 4 | Val loss: 2.0999 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 25.5373 | Steps: 4 | Val loss: 1835871.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=103465)[0m top1: 0.2126865671641791
[2m[36m(func pid=103465)[0m top5: 0.6735074626865671
[2m[36m(func pid=103465)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=103465)[0m f1_macro: 0.13856195494217768
[2m[36m(func pid=103465)[0m f1_weighted: 0.22226841012628934
[2m[36m(func pid=103465)[0m f1_per_class: [0.312, 0.005, 0.095, 0.447, 0.047, 0.0, 0.286, 0.0, 0.094, 0.101]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1208 | Steps: 4 | Val loss: 1.5041 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=97361)[0m top1: 0.23041044776119404
[2m[36m(func pid=97361)[0m top5: 0.7546641791044776
[2m[36m(func pid=97361)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=97361)[0m f1_macro: 0.1912250058662078
[2m[36m(func pid=97361)[0m f1_weighted: 0.2586665787293312
[2m[36m(func pid=97361)[0m f1_per_class: [0.218, 0.195, 0.282, 0.35, 0.061, 0.087, 0.311, 0.244, 0.1, 0.064]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:53:45 (running for 00:10:22.70)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.781 |      0.191 |                   27 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.197 |      0.401 |                   26 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.844 |      0.139 |                    2 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 25.537 |      0.004 |                    2 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.020522388059701493
[2m[36m(func pid=103554)[0m top5: 0.5237873134328358
[2m[36m(func pid=103554)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=103554)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=103554)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=103554)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.458955223880597
[2m[36m(func pid=97446)[0m top5: 0.9230410447761194
[2m[36m(func pid=97446)[0m f1_micro: 0.458955223880597
[2m[36m(func pid=97446)[0m f1_macro: 0.4168878379848189
[2m[36m(func pid=97446)[0m f1_weighted: 0.4501839040263612
[2m[36m(func pid=97446)[0m f1_per_class: [0.674, 0.603, 0.429, 0.557, 0.171, 0.269, 0.36, 0.313, 0.368, 0.424]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.2716 | Steps: 4 | Val loss: 2.1743 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6766 | Steps: 4 | Val loss: 2.0915 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 26.3138 | Steps: 4 | Val loss: 383228896.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=103465)[0m top1: 0.30597014925373134
[2m[36m(func pid=103465)[0m top5: 0.792910447761194
[2m[36m(func pid=103465)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=103465)[0m f1_macro: 0.24920281651746828
[2m[36m(func pid=103465)[0m f1_weighted: 0.2525386206503709
[2m[36m(func pid=103465)[0m f1_per_class: [0.396, 0.441, 0.103, 0.467, 0.215, 0.074, 0.0, 0.326, 0.147, 0.323]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1916 | Steps: 4 | Val loss: 1.4929 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=97361)[0m top1: 0.23134328358208955
[2m[36m(func pid=97361)[0m top5: 0.7635261194029851
[2m[36m(func pid=97361)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=97361)[0m f1_macro: 0.194583848196681
[2m[36m(func pid=97361)[0m f1_weighted: 0.25738628133696473
[2m[36m(func pid=97361)[0m f1_per_class: [0.239, 0.169, 0.286, 0.342, 0.065, 0.085, 0.325, 0.254, 0.111, 0.07]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:53:51 (running for 00:10:27.95)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.677 |      0.195 |                   28 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.121 |      0.417 |                   27 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.272 |      0.249 |                    3 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 26.314 |      0.001 |                    3 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.007462686567164179
[2m[36m(func pid=103554)[0m top5: 0.5107276119402985
[2m[36m(func pid=103554)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=103554)[0m f1_macro: 0.0014814814814814816
[2m[36m(func pid=103554)[0m f1_weighted: 0.0001105583195135434
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.46361940298507465
[2m[36m(func pid=97446)[0m top5: 0.9300373134328358
[2m[36m(func pid=97446)[0m f1_micro: 0.46361940298507465
[2m[36m(func pid=97446)[0m f1_macro: 0.41590040771493386
[2m[36m(func pid=97446)[0m f1_weighted: 0.4628232537256466
[2m[36m(func pid=97446)[0m f1_per_class: [0.653, 0.604, 0.407, 0.553, 0.189, 0.297, 0.404, 0.288, 0.35, 0.415]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.2796 | Steps: 4 | Val loss: 2.0123 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.6431 | Steps: 4 | Val loss: 2.0702 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 10.3589 | Steps: 4 | Val loss: 15590302.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103465)[0m top1: 0.3833955223880597
[2m[36m(func pid=103465)[0m top5: 0.8740671641791045
[2m[36m(func pid=103465)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=103465)[0m f1_macro: 0.2964484403543898
[2m[36m(func pid=103465)[0m f1_weighted: 0.3929950159149808
[2m[36m(func pid=103465)[0m f1_per_class: [0.651, 0.614, 0.141, 0.42, 0.119, 0.152, 0.385, 0.281, 0.201, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.2462686567164179
[2m[36m(func pid=97361)[0m top5: 0.7770522388059702
[2m[36m(func pid=97361)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=97361)[0m f1_macro: 0.20207219643927612
[2m[36m(func pid=97361)[0m f1_weighted: 0.27684478281067193
[2m[36m(func pid=97361)[0m f1_per_class: [0.231, 0.23, 0.253, 0.332, 0.073, 0.09, 0.365, 0.25, 0.104, 0.094]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3194 | Steps: 4 | Val loss: 1.5817 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:53:56 (running for 00:10:33.22)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.643 |      0.202 |                   29 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.192 |      0.416 |                   28 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.28  |      0.296 |                    4 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 10.359 |      0.027 |                    4 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.06623134328358209
[2m[36m(func pid=103554)[0m top5: 0.542910447761194
[2m[36m(func pid=103554)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=103554)[0m f1_macro: 0.0270401624452004
[2m[36m(func pid=103554)[0m f1_weighted: 0.0443229319965678
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.257, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.42257462686567165
[2m[36m(func pid=97446)[0m top5: 0.9197761194029851
[2m[36m(func pid=97446)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=97446)[0m f1_macro: 0.38488673535967416
[2m[36m(func pid=97446)[0m f1_weighted: 0.41483486155828986
[2m[36m(func pid=97446)[0m f1_per_class: [0.646, 0.557, 0.449, 0.39, 0.154, 0.299, 0.432, 0.269, 0.327, 0.327]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9111 | Steps: 4 | Val loss: 3.0596 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.7361 | Steps: 4 | Val loss: 2.0720 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 8.9355 | Steps: 4 | Val loss: 336510.4062 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103465)[0m top1: 0.2737873134328358
[2m[36m(func pid=103465)[0m top5: 0.7481343283582089
[2m[36m(func pid=103465)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=103465)[0m f1_macro: 0.19888750741856465
[2m[36m(func pid=103465)[0m f1_weighted: 0.23373119712835105
[2m[36m(func pid=103465)[0m f1_per_class: [0.493, 0.248, 0.057, 0.064, 0.076, 0.0, 0.522, 0.0, 0.013, 0.516]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.24113805970149255
[2m[36m(func pid=97361)[0m top5: 0.777518656716418
[2m[36m(func pid=97361)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=97361)[0m f1_macro: 0.19913860840399827
[2m[36m(func pid=97361)[0m f1_weighted: 0.2726336722271342
[2m[36m(func pid=97361)[0m f1_per_class: [0.239, 0.256, 0.216, 0.3, 0.077, 0.091, 0.364, 0.255, 0.095, 0.098]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1070 | Steps: 4 | Val loss: 1.5845 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 13:54:01 (running for 00:10:38.42)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.736 |      0.199 |                   30 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.319 |      0.385 |                   29 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  0.911 |      0.199 |                    5 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  8.935 |      0.002 |                    5 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.0065298507462686565
[2m[36m(func pid=103554)[0m top5: 0.5097947761194029
[2m[36m(func pid=103554)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=103554)[0m f1_macro: 0.00174558007767306
[2m[36m(func pid=103554)[0m f1_weighted: 0.0010009621625022272
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.42024253731343286
[2m[36m(func pid=97446)[0m top5: 0.9174440298507462
[2m[36m(func pid=97446)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=97446)[0m f1_macro: 0.3937383314354176
[2m[36m(func pid=97446)[0m f1_weighted: 0.41965069087776413
[2m[36m(func pid=97446)[0m f1_per_class: [0.627, 0.561, 0.524, 0.411, 0.14, 0.258, 0.427, 0.345, 0.337, 0.308]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.5377 | Steps: 4 | Val loss: 6.8013 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.5992 | Steps: 4 | Val loss: 2.0589 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 24.0097 | Steps: 4 | Val loss: 13872.8418 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=103465)[0m top1: 0.07602611940298508
[2m[36m(func pid=103465)[0m top5: 0.7164179104477612
[2m[36m(func pid=103465)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=103465)[0m f1_macro: 0.07083461527401434
[2m[36m(func pid=103465)[0m f1_weighted: 0.059803466908602984
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.005, 0.0, 0.141, 0.286, 0.016, 0.009, 0.217, 0.0, 0.035]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1836 | Steps: 4 | Val loss: 1.6593 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=97361)[0m top1: 0.24860074626865672
[2m[36m(func pid=97361)[0m top5: 0.784981343283582
[2m[36m(func pid=97361)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=97361)[0m f1_macro: 0.20247467317828055
[2m[36m(func pid=97361)[0m f1_weighted: 0.27892594267726195
[2m[36m(func pid=97361)[0m f1_per_class: [0.243, 0.273, 0.192, 0.295, 0.083, 0.086, 0.379, 0.267, 0.098, 0.107]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:54:06 (running for 00:10:43.69)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.599 |      0.202 |                   31 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.107 |      0.394 |                   30 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.538 |      0.071 |                    6 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 24.01  |      0.021 |                    6 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.025186567164179104
[2m[36m(func pid=103554)[0m top5: 0.5536380597014925
[2m[36m(func pid=103554)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=103554)[0m f1_macro: 0.020751116687947966
[2m[36m(func pid=103554)[0m f1_weighted: 0.028641506810726462
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.159, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.40345149253731344
[2m[36m(func pid=97446)[0m top5: 0.8927238805970149
[2m[36m(func pid=97446)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=97446)[0m f1_macro: 0.38870672450283167
[2m[36m(func pid=97446)[0m f1_weighted: 0.40150505475403686
[2m[36m(func pid=97446)[0m f1_per_class: [0.647, 0.564, 0.585, 0.439, 0.108, 0.191, 0.359, 0.38, 0.289, 0.324]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.1210 | Steps: 4 | Val loss: 10.3288 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.5164 | Steps: 4 | Val loss: 2.0430 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 11.7715 | Steps: 4 | Val loss: 1664.6602 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=103465)[0m top1: 0.18936567164179105
[2m[36m(func pid=103465)[0m top5: 0.5951492537313433
[2m[36m(func pid=103465)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=103465)[0m f1_macro: 0.08654325834588578
[2m[36m(func pid=103465)[0m f1_weighted: 0.19662255205249585
[2m[36m(func pid=103465)[0m f1_per_class: [0.048, 0.237, 0.0, 0.538, 0.0, 0.043, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.1791 | Steps: 4 | Val loss: 1.6585 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=97361)[0m top1: 0.25699626865671643
[2m[36m(func pid=97361)[0m top5: 0.7901119402985075
[2m[36m(func pid=97361)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=97361)[0m f1_macro: 0.20447324078832702
[2m[36m(func pid=97361)[0m f1_weighted: 0.2870564858478111
[2m[36m(func pid=97361)[0m f1_per_class: [0.258, 0.283, 0.186, 0.294, 0.079, 0.078, 0.408, 0.247, 0.096, 0.116]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:54:12 (running for 00:10:48.94)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.516 |      0.204 |                   32 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.184 |      0.389 |                   31 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.121 |      0.087 |                    7 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 11.771 |      0.007 |                    7 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.025652985074626867
[2m[36m(func pid=103554)[0m top5: 0.4818097014925373
[2m[36m(func pid=103554)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=103554)[0m f1_macro: 0.007346183735643419
[2m[36m(func pid=103554)[0m f1_weighted: 0.0025049137023574032
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.071, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.40951492537313433
[2m[36m(func pid=97446)[0m top5: 0.9034514925373134
[2m[36m(func pid=97446)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=97446)[0m f1_macro: 0.37940608143825805
[2m[36m(func pid=97446)[0m f1_weighted: 0.4145567797954435
[2m[36m(func pid=97446)[0m f1_per_class: [0.634, 0.57, 0.55, 0.456, 0.094, 0.136, 0.417, 0.323, 0.308, 0.306]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.0214 | Steps: 4 | Val loss: 3.8311 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.6137 | Steps: 4 | Val loss: 2.0540 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 15.9333 | Steps: 4 | Val loss: 186.8194 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=103465)[0m top1: 0.09375
[2m[36m(func pid=103465)[0m top5: 0.6735074626865671
[2m[36m(func pid=103465)[0m f1_micro: 0.09375
[2m[36m(func pid=103465)[0m f1_macro: 0.0684295524115309
[2m[36m(func pid=103465)[0m f1_weighted: 0.09809533966920993
[2m[36m(func pid=103465)[0m f1_per_class: [0.126, 0.026, 0.0, 0.047, 0.0, 0.242, 0.159, 0.0, 0.083, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0933 | Steps: 4 | Val loss: 1.6128 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=97361)[0m top1: 0.2555970149253731
[2m[36m(func pid=97361)[0m top5: 0.7840485074626866
[2m[36m(func pid=97361)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=97361)[0m f1_macro: 0.20442650491039394
[2m[36m(func pid=97361)[0m f1_weighted: 0.2862919868911666
[2m[36m(func pid=97361)[0m f1_per_class: [0.296, 0.266, 0.173, 0.329, 0.08, 0.081, 0.382, 0.226, 0.112, 0.099]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:54:17 (running for 00:10:54.15)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.614 |      0.204 |                   33 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.179 |      0.379 |                   32 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.021 |      0.068 |                    8 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 15.933 |      0.014 |                    8 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.025652985074626867
[2m[36m(func pid=103554)[0m top5: 0.44263059701492535
[2m[36m(func pid=103554)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=103554)[0m f1_macro: 0.013615234802106097
[2m[36m(func pid=103554)[0m f1_weighted: 0.024884063661260256
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.079, 0.0, 0.034, 0.004]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4207089552238806
[2m[36m(func pid=97446)[0m top5: 0.9207089552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=97446)[0m f1_macro: 0.388185097595004
[2m[36m(func pid=97446)[0m f1_weighted: 0.43789918551120816
[2m[36m(func pid=97446)[0m f1_per_class: [0.615, 0.579, 0.571, 0.491, 0.088, 0.131, 0.462, 0.312, 0.287, 0.343]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.5367 | Steps: 4 | Val loss: 7.4915 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5709 | Steps: 4 | Val loss: 2.0541 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 8.4340 | Steps: 4 | Val loss: 1221.7878 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4119 | Steps: 4 | Val loss: 1.6340 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=103465)[0m top1: 0.10027985074626866
[2m[36m(func pid=103465)[0m top5: 0.48927238805970147
[2m[36m(func pid=103465)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=103465)[0m f1_macro: 0.06355703848753283
[2m[36m(func pid=103465)[0m f1_weighted: 0.08520214314428286
[2m[36m(func pid=103465)[0m f1_per_class: [0.12, 0.032, 0.0, 0.0, 0.154, 0.0, 0.249, 0.0, 0.044, 0.037]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.24860074626865672
[2m[36m(func pid=97361)[0m top5: 0.7765858208955224
[2m[36m(func pid=97361)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=97361)[0m f1_macro: 0.20509674523985014
[2m[36m(func pid=97361)[0m f1_weighted: 0.27572787390960873
[2m[36m(func pid=97361)[0m f1_per_class: [0.289, 0.252, 0.186, 0.326, 0.085, 0.067, 0.358, 0.245, 0.104, 0.137]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:54:22 (running for 00:10:59.45)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.571 |      0.205 |                   34 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.093 |      0.388 |                   33 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.537 |      0.064 |                    9 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  8.434 |      0.004 |                    9 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.012126865671641791
[2m[36m(func pid=103554)[0m top5: 0.507929104477612
[2m[36m(func pid=103554)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=103554)[0m f1_macro: 0.0036549862160381885
[2m[36m(func pid=103554)[0m f1_weighted: 0.0004837328252734004
[2m[36m(func pid=103554)[0m f1_per_class: [0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4193097014925373
[2m[36m(func pid=97446)[0m top5: 0.9193097014925373
[2m[36m(func pid=97446)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=97446)[0m f1_macro: 0.38870738497553575
[2m[36m(func pid=97446)[0m f1_weighted: 0.4368691088205467
[2m[36m(func pid=97446)[0m f1_per_class: [0.545, 0.59, 0.585, 0.511, 0.084, 0.128, 0.425, 0.387, 0.302, 0.33]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.7603 | Steps: 4 | Val loss: 2.0543 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1591 | Steps: 4 | Val loss: 8.2854 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 10.1309 | Steps: 4 | Val loss: 451.7924 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.1827 | Steps: 4 | Val loss: 1.6457 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=103465)[0m top1: 0.04477611940298507
[2m[36m(func pid=103465)[0m top5: 0.574160447761194
[2m[36m(func pid=103465)[0m f1_micro: 0.04477611940298508
[2m[36m(func pid=103465)[0m f1_macro: 0.034655293579204015
[2m[36m(func pid=103465)[0m f1_weighted: 0.03844499494475044
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.211, 0.037, 0.0, 0.04, 0.0, 0.003, 0.0, 0.0, 0.056]
[2m[36m(func pid=97361)[0m top1: 0.2355410447761194
[2m[36m(func pid=97361)[0m top5: 0.7821828358208955
[2m[36m(func pid=97361)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=97361)[0m f1_macro: 0.19899088028173584
[2m[36m(func pid=97361)[0m f1_weighted: 0.2611156539876247
[2m[36m(func pid=97361)[0m f1_per_class: [0.25, 0.266, 0.22, 0.286, 0.079, 0.086, 0.337, 0.228, 0.106, 0.132]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m 
== Status ==
Current time: 2024-01-07 13:54:27 (running for 00:11:04.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.76  |      0.199 |                   35 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.412 |      0.389 |                   34 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.159 |      0.035 |                   10 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 10.131 |      0.01  |                   10 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.017723880597014924
[2m[36m(func pid=103554)[0m top5: 0.46361940298507465
[2m[36m(func pid=103554)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=103554)[0m f1_macro: 0.010415789071613757
[2m[36m(func pid=103554)[0m f1_weighted: 0.013770176567397175
[2m[36m(func pid=103554)[0m f1_per_class: [0.012, 0.0, 0.047, 0.0, 0.0, 0.0, 0.044, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.39505597014925375
[2m[36m(func pid=97446)[0m top5: 0.9235074626865671
[2m[36m(func pid=97446)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=97446)[0m f1_macro: 0.36126441904714435
[2m[36m(func pid=97446)[0m f1_weighted: 0.40729749285493466
[2m[36m(func pid=97446)[0m f1_per_class: [0.547, 0.55, 0.522, 0.521, 0.09, 0.092, 0.363, 0.357, 0.301, 0.27]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3645 | Steps: 4 | Val loss: 9.6584 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.4894 | Steps: 4 | Val loss: 2.0033 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 6.9712 | Steps: 4 | Val loss: 425.2660 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.1009 | Steps: 4 | Val loss: 1.6708 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=103465)[0m top1: 0.04151119402985075
[2m[36m(func pid=103465)[0m top5: 0.4155783582089552
[2m[36m(func pid=103465)[0m f1_micro: 0.04151119402985075
[2m[36m(func pid=103465)[0m f1_macro: 0.03782714655509624
[2m[36m(func pid=103465)[0m f1_weighted: 0.045650343532953
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.144, 0.0, 0.003, 0.069, 0.0, 0.055, 0.0, 0.08, 0.026]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.261660447761194
[2m[36m(func pid=97361)[0m top5: 0.8129664179104478
[2m[36m(func pid=97361)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=97361)[0m f1_macro: 0.2144208508489382
[2m[36m(func pid=97361)[0m f1_weighted: 0.28347792204080824
[2m[36m(func pid=97361)[0m f1_per_class: [0.265, 0.242, 0.234, 0.39, 0.1, 0.095, 0.321, 0.231, 0.117, 0.149]
[2m[36m(func pid=97361)[0m 
== Status ==
Current time: 2024-01-07 13:54:32 (running for 00:11:09.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.489 |      0.214 |                   36 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.183 |      0.361 |                   35 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.364 |      0.038 |                   11 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  6.971 |      0.007 |                   11 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.013526119402985074
[2m[36m(func pid=103554)[0m top5: 0.4701492537313433
[2m[36m(func pid=103554)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=103554)[0m f1_macro: 0.007254814604807517
[2m[36m(func pid=103554)[0m f1_weighted: 0.0007211290704292272
[2m[36m(func pid=103554)[0m f1_per_class: [0.019, 0.0, 0.053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.39552238805970147
[2m[36m(func pid=97446)[0m top5: 0.9244402985074627
[2m[36m(func pid=97446)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=97446)[0m f1_macro: 0.3531236805871353
[2m[36m(func pid=97446)[0m f1_weighted: 0.41040191504516693
[2m[36m(func pid=97446)[0m f1_per_class: [0.542, 0.555, 0.471, 0.51, 0.099, 0.077, 0.393, 0.34, 0.284, 0.261]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.9703 | Steps: 4 | Val loss: 9.4170 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.3628 | Steps: 4 | Val loss: 1.9744 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 4.4325 | Steps: 4 | Val loss: 206.4663 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2395 | Steps: 4 | Val loss: 1.7068 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=103465)[0m top1: 0.07555970149253731
[2m[36m(func pid=103465)[0m top5: 0.5447761194029851
[2m[36m(func pid=103465)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=103465)[0m f1_macro: 0.045833281974390794
[2m[36m(func pid=103465)[0m f1_weighted: 0.07205296894534306
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.29, 0.0, 0.067, 0.0, 0.0, 0.003, 0.0, 0.06, 0.038]
[2m[36m(func pid=103465)[0m 
== Status ==
Current time: 2024-01-07 13:54:37 (running for 00:11:14.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.363 |      0.23  |                   37 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.101 |      0.353 |                   36 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.97  |      0.046 |                   12 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  6.971 |      0.007 |                   11 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m top1: 0.28125
[2m[36m(func pid=97361)[0m top5: 0.8274253731343284
[2m[36m(func pid=97361)[0m f1_micro: 0.28125
[2m[36m(func pid=97361)[0m f1_macro: 0.2298683033394319
[2m[36m(func pid=97361)[0m f1_weighted: 0.29771826313687394
[2m[36m(func pid=97361)[0m f1_per_class: [0.297, 0.26, 0.289, 0.443, 0.102, 0.09, 0.304, 0.237, 0.135, 0.14]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.04057835820895522
[2m[36m(func pid=103554)[0m top5: 0.5093283582089553
[2m[36m(func pid=103554)[0m f1_micro: 0.04057835820895522
[2m[36m(func pid=103554)[0m f1_macro: 0.021137969765863567
[2m[36m(func pid=103554)[0m f1_weighted: 0.04108606652933614
[2m[36m(func pid=103554)[0m f1_per_class: [0.018, 0.0, 0.057, 0.003, 0.0, 0.0, 0.132, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.37826492537313433
[2m[36m(func pid=97446)[0m top5: 0.9197761194029851
[2m[36m(func pid=97446)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=97446)[0m f1_macro: 0.3350331907755375
[2m[36m(func pid=97446)[0m f1_weighted: 0.3885938586155213
[2m[36m(func pid=97446)[0m f1_per_class: [0.461, 0.558, 0.429, 0.475, 0.137, 0.091, 0.361, 0.291, 0.281, 0.268]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.1128 | Steps: 4 | Val loss: 19.0599 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.4651 | Steps: 4 | Val loss: 1.9581 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 11.6866 | Steps: 4 | Val loss: 55.5292 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2136 | Steps: 4 | Val loss: 1.6861 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 13:54:43 (running for 00:11:19.90)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.363 |      0.23  |                   37 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.239 |      0.335 |                   37 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.113 |      0.107 |                   13 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  4.432 |      0.021 |                   12 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.08628731343283583
[2m[36m(func pid=103465)[0m top5: 0.6324626865671642
[2m[36m(func pid=103465)[0m f1_micro: 0.08628731343283583
[2m[36m(func pid=103465)[0m f1_macro: 0.10734623918711579
[2m[36m(func pid=103465)[0m f1_weighted: 0.07248088791736061
[2m[36m(func pid=103465)[0m f1_per_class: [0.517, 0.021, 0.009, 0.026, 0.0, 0.429, 0.0, 0.016, 0.027, 0.031]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.2989738805970149
[2m[36m(func pid=97361)[0m top5: 0.8330223880597015
[2m[36m(func pid=97361)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=97361)[0m f1_macro: 0.24993719048635782
[2m[36m(func pid=97361)[0m f1_weighted: 0.3207457555172931
[2m[36m(func pid=97361)[0m f1_per_class: [0.282, 0.284, 0.415, 0.455, 0.102, 0.105, 0.347, 0.254, 0.132, 0.123]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.09701492537313433
[2m[36m(func pid=103554)[0m top5: 0.582089552238806
[2m[36m(func pid=103554)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=103554)[0m f1_macro: 0.043336140468857864
[2m[36m(func pid=103554)[0m f1_weighted: 0.11661477815382916
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.0, 0.361, 0.019, 0.0, 0.053, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.3885261194029851
[2m[36m(func pid=97446)[0m top5: 0.9207089552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=97446)[0m f1_macro: 0.3433835798348104
[2m[36m(func pid=97446)[0m f1_weighted: 0.40134778298917345
[2m[36m(func pid=97446)[0m f1_per_class: [0.432, 0.562, 0.49, 0.508, 0.143, 0.079, 0.376, 0.292, 0.261, 0.291]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5831 | Steps: 4 | Val loss: 10.5522 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2685 | Steps: 4 | Val loss: 1.9653 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.8928 | Steps: 4 | Val loss: 6.9665 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1352 | Steps: 4 | Val loss: 1.6923 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 13:54:48 (running for 00:11:25.34)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.465 |      0.25  |                   38 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.214 |      0.343 |                   38 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.583 |      0.075 |                   14 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      | 11.687 |      0.043 |                   13 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.1044776119402985
[2m[36m(func pid=103465)[0m top5: 0.7933768656716418
[2m[36m(func pid=103465)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=103465)[0m f1_macro: 0.07535431460323498
[2m[36m(func pid=103465)[0m f1_weighted: 0.11877086890068718
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.049, 0.032, 0.0, 0.0, 0.114, 0.306, 0.031, 0.066, 0.154]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.29011194029850745
[2m[36m(func pid=97361)[0m top5: 0.8283582089552238
[2m[36m(func pid=97361)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=97361)[0m f1_macro: 0.24601761643643458
[2m[36m(func pid=97361)[0m f1_weighted: 0.30741826591441396
[2m[36m(func pid=97361)[0m f1_per_class: [0.275, 0.261, 0.44, 0.455, 0.103, 0.114, 0.311, 0.265, 0.122, 0.114]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.013992537313432836
[2m[36m(func pid=103554)[0m top5: 0.6609141791044776
[2m[36m(func pid=103554)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=103554)[0m f1_macro: 0.02481466112719231
[2m[36m(func pid=103554)[0m f1_weighted: 0.0032961091758812863
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.226, 0.0, 0.016, 0.0, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.38526119402985076
[2m[36m(func pid=97446)[0m top5: 0.9235074626865671
[2m[36m(func pid=97446)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=97446)[0m f1_macro: 0.35512735760241965
[2m[36m(func pid=97446)[0m f1_weighted: 0.40155431392945706
[2m[36m(func pid=97446)[0m f1_per_class: [0.466, 0.563, 0.533, 0.509, 0.124, 0.119, 0.355, 0.297, 0.255, 0.33]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.4148 | Steps: 4 | Val loss: 22.8943 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.3408 | Steps: 4 | Val loss: 1.9638 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.8370 | Steps: 4 | Val loss: 3.3243 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1098 | Steps: 4 | Val loss: 1.7011 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 13:54:53 (running for 00:11:30.68)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.269 |      0.246 |                   39 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.135 |      0.355 |                   39 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.415 |      0.075 |                   15 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.893 |      0.025 |                   14 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.14972014925373134
[2m[36m(func pid=103465)[0m top5: 0.6669776119402985
[2m[36m(func pid=103465)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=103465)[0m f1_macro: 0.07526414239844655
[2m[36m(func pid=103465)[0m f1_weighted: 0.15309858570912424
[2m[36m(func pid=103465)[0m f1_per_class: [0.044, 0.081, 0.0, 0.0, 0.024, 0.0, 0.457, 0.0, 0.0, 0.145]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.2891791044776119
[2m[36m(func pid=97361)[0m top5: 0.824160447761194
[2m[36m(func pid=97361)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=97361)[0m f1_macro: 0.24063891671904827
[2m[36m(func pid=97361)[0m f1_weighted: 0.2989538746418542
[2m[36m(func pid=97361)[0m f1_per_class: [0.267, 0.302, 0.373, 0.454, 0.125, 0.112, 0.263, 0.255, 0.131, 0.125]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.013059701492537313
[2m[36m(func pid=103554)[0m top5: 0.5783582089552238
[2m[36m(func pid=103554)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=103554)[0m f1_macro: 0.04059274717545757
[2m[36m(func pid=103554)[0m f1_weighted: 0.0042285242654007855
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.385, 0.0, 0.015, 0.0, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.37966417910447764
[2m[36m(func pid=97446)[0m top5: 0.9207089552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=97446)[0m f1_macro: 0.3540832679517655
[2m[36m(func pid=97446)[0m f1_weighted: 0.3992518133457038
[2m[36m(func pid=97446)[0m f1_per_class: [0.493, 0.528, 0.453, 0.525, 0.106, 0.171, 0.324, 0.342, 0.247, 0.352]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6822 | Steps: 4 | Val loss: 15.2080 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.3177 | Steps: 4 | Val loss: 1.9571 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 4.2688 | Steps: 4 | Val loss: 2.2620 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0839 | Steps: 4 | Val loss: 1.6920 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:54:59 (running for 00:11:35.95)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.341 |      0.241 |                   40 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.11  |      0.354 |                   40 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.682 |      0.076 |                   16 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  4.837 |      0.041 |                   15 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.1912313432835821
[2m[36m(func pid=103465)[0m top5: 0.777518656716418
[2m[36m(func pid=103465)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=103465)[0m f1_macro: 0.07584827041604478
[2m[36m(func pid=103465)[0m f1_weighted: 0.09844823828607835
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.4, 0.0, 0.01, 0.0, 0.0, 0.031, 0.292, 0.027, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.01166044776119403
[2m[36m(func pid=103554)[0m top5: 0.8185634328358209
[2m[36m(func pid=103554)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=103554)[0m f1_macro: 0.05242332826939661
[2m[36m(func pid=103554)[0m f1_weighted: 0.005898956494128556
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.5, 0.0, 0.015, 0.0, 0.009, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.2868470149253731
[2m[36m(func pid=97361)[0m top5: 0.8353544776119403
[2m[36m(func pid=97361)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=97361)[0m f1_macro: 0.24475595363942104
[2m[36m(func pid=97361)[0m f1_weighted: 0.2968301449539877
[2m[36m(func pid=97361)[0m f1_per_class: [0.303, 0.331, 0.364, 0.446, 0.124, 0.146, 0.233, 0.257, 0.109, 0.134]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.3726679104477612
[2m[36m(func pid=97446)[0m top5: 0.9183768656716418
[2m[36m(func pid=97446)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=97446)[0m f1_macro: 0.35991849328916586
[2m[36m(func pid=97446)[0m f1_weighted: 0.3988023865550395
[2m[36m(func pid=97446)[0m f1_per_class: [0.512, 0.516, 0.489, 0.503, 0.129, 0.21, 0.338, 0.328, 0.224, 0.35]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.2501 | Steps: 4 | Val loss: 32.7880 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 5.5415 | Steps: 4 | Val loss: 2.7949 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2196 | Steps: 4 | Val loss: 1.9540 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1978 | Steps: 4 | Val loss: 1.6864 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:55:04 (running for 00:11:41.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.318 |      0.245 |                   41 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.084 |      0.36  |                   41 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.25  |      0.109 |                   17 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  4.269 |      0.052 |                   16 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.1571828358208955
[2m[36m(func pid=103465)[0m top5: 0.6809701492537313
[2m[36m(func pid=103465)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=103465)[0m f1_macro: 0.10897054610861863
[2m[36m(func pid=103465)[0m f1_weighted: 0.12323914965334076
[2m[36m(func pid=103465)[0m f1_per_class: [0.08, 0.393, 0.224, 0.0, 0.0, 0.0, 0.124, 0.267, 0.0, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.0830223880597015
[2m[36m(func pid=103554)[0m top5: 0.6319962686567164
[2m[36m(func pid=103554)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=103554)[0m f1_macro: 0.06023263447430186
[2m[36m(func pid=103554)[0m f1_weighted: 0.04634315613814917
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.353, 0.135, 0.0, 0.0, 0.0, 0.115, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.28218283582089554
[2m[36m(func pid=97361)[0m top5: 0.8334888059701493
[2m[36m(func pid=97361)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=97361)[0m f1_macro: 0.2418005215110266
[2m[36m(func pid=97361)[0m f1_weighted: 0.2899825608761097
[2m[36m(func pid=97361)[0m f1_per_class: [0.29, 0.299, 0.369, 0.455, 0.124, 0.143, 0.219, 0.263, 0.126, 0.13]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.37826492537313433
[2m[36m(func pid=97446)[0m top5: 0.9155783582089553
[2m[36m(func pid=97446)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=97446)[0m f1_macro: 0.3665556750412547
[2m[36m(func pid=97446)[0m f1_weighted: 0.4101883307281617
[2m[36m(func pid=97446)[0m f1_per_class: [0.508, 0.414, 0.522, 0.545, 0.139, 0.227, 0.389, 0.333, 0.195, 0.394]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 4.7141 | Steps: 4 | Val loss: 3.7428 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.4791 | Steps: 4 | Val loss: 5.7264 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.3200 | Steps: 4 | Val loss: 1.9456 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1599 | Steps: 4 | Val loss: 1.7071 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 13:55:09 (running for 00:11:46.55)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.22  |      0.242 |                   42 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.198 |      0.367 |                   42 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.714 |      0.174 |                   18 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  5.542 |      0.06  |                   17 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.2294776119402985
[2m[36m(func pid=103465)[0m top5: 0.6721082089552238
[2m[36m(func pid=103465)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=103465)[0m f1_macro: 0.17390134570675433
[2m[36m(func pid=103465)[0m f1_weighted: 0.20305826664268728
[2m[36m(func pid=103465)[0m f1_per_class: [0.381, 0.328, 0.0, 0.0, 0.0, 0.397, 0.246, 0.314, 0.057, 0.017]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.06809701492537314
[2m[36m(func pid=103554)[0m top5: 0.6245335820895522
[2m[36m(func pid=103554)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=103554)[0m f1_macro: 0.03520023156117627
[2m[36m(func pid=103554)[0m f1_weighted: 0.01807835839647237
[2m[36m(func pid=103554)[0m f1_per_class: [0.107, 0.011, 0.088, 0.023, 0.0, 0.0, 0.0, 0.123, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.28777985074626866
[2m[36m(func pid=97361)[0m top5: 0.8423507462686567
[2m[36m(func pid=97361)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=97361)[0m f1_macro: 0.2522250322553923
[2m[36m(func pid=97361)[0m f1_weighted: 0.30426574897611697
[2m[36m(func pid=97361)[0m f1_per_class: [0.284, 0.322, 0.4, 0.436, 0.143, 0.176, 0.255, 0.284, 0.122, 0.099]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.3619402985074627
[2m[36m(func pid=97446)[0m top5: 0.9151119402985075
[2m[36m(func pid=97446)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=97446)[0m f1_macro: 0.35391417806736586
[2m[36m(func pid=97446)[0m f1_weighted: 0.3981756039791737
[2m[36m(func pid=97446)[0m f1_per_class: [0.541, 0.417, 0.458, 0.523, 0.125, 0.213, 0.373, 0.348, 0.181, 0.361]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 5.0006 | Steps: 4 | Val loss: 5.8290 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 5.4375 | Steps: 4 | Val loss: 5.4881 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1664 | Steps: 4 | Val loss: 1.9779 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0553 | Steps: 4 | Val loss: 1.6609 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=103465)[0m top1: 0.18470149253731344
[2m[36m(func pid=103465)[0m top5: 0.8428171641791045
[2m[36m(func pid=103465)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=103465)[0m f1_macro: 0.1086222747376685
[2m[36m(func pid=103465)[0m f1_weighted: 0.10167512231622403
[2m[36m(func pid=103465)[0m f1_per_class: [0.087, 0.45, 0.0, 0.0, 0.211, 0.0, 0.015, 0.226, 0.097, 0.0]
[2m[36m(func pid=103465)[0m 
== Status ==
Current time: 2024-01-07 13:55:15 (running for 00:11:51.95)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.32  |      0.252 |                   43 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.16  |      0.354 |                   43 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  5.001 |      0.109 |                   19 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.479 |      0.035 |                   18 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.06529850746268656
[2m[36m(func pid=103554)[0m top5: 0.394589552238806
[2m[36m(func pid=103554)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=103554)[0m f1_macro: 0.037061549341040556
[2m[36m(func pid=103554)[0m f1_weighted: 0.01318535047634642
[2m[36m(func pid=103554)[0m f1_per_class: [0.143, 0.016, 0.091, 0.0, 0.0, 0.0, 0.0, 0.121, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.2653917910447761
[2m[36m(func pid=97361)[0m top5: 0.8236940298507462
[2m[36m(func pid=97361)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=97361)[0m f1_macro: 0.2342664572288949
[2m[36m(func pid=97361)[0m f1_weighted: 0.2848512255230991
[2m[36m(func pid=97361)[0m f1_per_class: [0.26, 0.303, 0.369, 0.404, 0.114, 0.184, 0.235, 0.274, 0.11, 0.089]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.3917910447761194
[2m[36m(func pid=97446)[0m top5: 0.9207089552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=97446)[0m f1_macro: 0.3743668106914383
[2m[36m(func pid=97446)[0m f1_weighted: 0.41271582702052173
[2m[36m(func pid=97446)[0m f1_per_class: [0.579, 0.562, 0.478, 0.481, 0.137, 0.214, 0.368, 0.36, 0.209, 0.355]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 5.4294 | Steps: 4 | Val loss: 3.8817 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1717 | Steps: 4 | Val loss: 1.9765 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 5.1914 | Steps: 4 | Val loss: 5.7772 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0754 | Steps: 4 | Val loss: 1.7052 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 13:55:20 (running for 00:11:57.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.166 |      0.234 |                   44 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.055 |      0.374 |                   44 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  5.429 |      0.161 |                   20 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  5.438 |      0.037 |                   19 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.26492537313432835
[2m[36m(func pid=103465)[0m top5: 0.8530783582089553
[2m[36m(func pid=103465)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=103465)[0m f1_macro: 0.16055502259869778
[2m[36m(func pid=103465)[0m f1_weighted: 0.23878835117146094
[2m[36m(func pid=103465)[0m f1_per_class: [0.08, 0.525, 0.0, 0.39, 0.093, 0.169, 0.003, 0.229, 0.116, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.27005597014925375
[2m[36m(func pid=97361)[0m top5: 0.8227611940298507
[2m[36m(func pid=97361)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=97361)[0m f1_macro: 0.22675970958261157
[2m[36m(func pid=97361)[0m f1_weighted: 0.289705960706972
[2m[36m(func pid=97361)[0m f1_per_class: [0.258, 0.326, 0.245, 0.387, 0.128, 0.155, 0.263, 0.288, 0.115, 0.101]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.06902985074626866
[2m[36m(func pid=103554)[0m top5: 0.39225746268656714
[2m[36m(func pid=103554)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=103554)[0m f1_macro: 0.03967132964660175
[2m[36m(func pid=103554)[0m f1_weighted: 0.021751585590646067
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.077, 0.186, 0.0, 0.0, 0.0, 0.0, 0.118, 0.016, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.3810634328358209
[2m[36m(func pid=97446)[0m top5: 0.914179104477612
[2m[36m(func pid=97446)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=97446)[0m f1_macro: 0.3714263837716584
[2m[36m(func pid=97446)[0m f1_weighted: 0.38628915439579914
[2m[36m(func pid=97446)[0m f1_per_class: [0.596, 0.569, 0.471, 0.417, 0.151, 0.215, 0.33, 0.365, 0.226, 0.375]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 4.1693 | Steps: 4 | Val loss: 33.0058 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1751 | Steps: 4 | Val loss: 1.9375 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.6647 | Steps: 4 | Val loss: 5.7192 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0472 | Steps: 4 | Val loss: 1.6890 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=103465)[0m top1: 0.15858208955223882
[2m[36m(func pid=103465)[0m top5: 0.6305970149253731
[2m[36m(func pid=103465)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=103465)[0m f1_macro: 0.10052583491823039
[2m[36m(func pid=103465)[0m f1_weighted: 0.17307564002123127
[2m[36m(func pid=103465)[0m f1_per_class: [0.157, 0.453, 0.0, 0.314, 0.0, 0.0, 0.006, 0.0, 0.075, 0.0]
[2m[36m(func pid=103465)[0m 
== Status ==
Current time: 2024-01-07 13:55:25 (running for 00:12:02.59)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.172 |      0.227 |                   45 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.075 |      0.371 |                   45 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.169 |      0.101 |                   21 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  5.191 |      0.04  |                   20 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m top1: 0.29524253731343286
[2m[36m(func pid=97361)[0m top5: 0.8367537313432836
[2m[36m(func pid=97361)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=97361)[0m f1_macro: 0.24198754957763596
[2m[36m(func pid=97361)[0m f1_weighted: 0.3127766106294361
[2m[36m(func pid=97361)[0m f1_per_class: [0.281, 0.326, 0.255, 0.429, 0.132, 0.174, 0.291, 0.283, 0.131, 0.118]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.06623134328358209
[2m[36m(func pid=103554)[0m top5: 0.47574626865671643
[2m[36m(func pid=103554)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=103554)[0m f1_macro: 0.030493096646942802
[2m[36m(func pid=103554)[0m f1_weighted: 0.017404103741646797
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.031, 0.141, 0.0, 0.0, 0.0, 0.015, 0.118, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.3917910447761194
[2m[36m(func pid=97446)[0m top5: 0.9137126865671642
[2m[36m(func pid=97446)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=97446)[0m f1_macro: 0.3705115380902419
[2m[36m(func pid=97446)[0m f1_weighted: 0.3827880510284676
[2m[36m(func pid=97446)[0m f1_per_class: [0.606, 0.568, 0.429, 0.38, 0.129, 0.209, 0.349, 0.378, 0.265, 0.393]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.5750 | Steps: 4 | Val loss: 57.5068 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1222 | Steps: 4 | Val loss: 1.9393 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8142 | Steps: 4 | Val loss: 4.3657 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0808 | Steps: 4 | Val loss: 1.6121 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=103465)[0m top1: 0.05363805970149254
[2m[36m(func pid=103465)[0m top5: 0.6613805970149254
[2m[36m(func pid=103465)[0m f1_micro: 0.05363805970149254
[2m[36m(func pid=103465)[0m f1_macro: 0.0459584251889031
[2m[36m(func pid=103465)[0m f1_weighted: 0.07392625082731524
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.07, 0.0, 0.0, 0.0, 0.038, 0.164, 0.119, 0.047, 0.021]
[2m[36m(func pid=103465)[0m 
== Status ==
Current time: 2024-01-07 13:55:31 (running for 00:12:08.06)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.175 |      0.242 |                   46 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.047 |      0.371 |                   46 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.575 |      0.046 |                   22 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.665 |      0.03  |                   21 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m top1: 0.29617537313432835
[2m[36m(func pid=97361)[0m top5: 0.8376865671641791
[2m[36m(func pid=97361)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=97361)[0m f1_macro: 0.2344145343986669
[2m[36m(func pid=97361)[0m f1_weighted: 0.3141422009557301
[2m[36m(func pid=97361)[0m f1_per_class: [0.276, 0.284, 0.24, 0.449, 0.116, 0.139, 0.315, 0.296, 0.119, 0.111]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.06063432835820896
[2m[36m(func pid=103554)[0m top5: 0.47947761194029853
[2m[36m(func pid=103554)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=103554)[0m f1_macro: 0.02917066283858714
[2m[36m(func pid=103554)[0m f1_weighted: 0.011140266365615136
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.083, 0.007, 0.0, 0.0, 0.0, 0.115, 0.051, 0.035]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.41044776119402987
[2m[36m(func pid=97446)[0m top5: 0.9235074626865671
[2m[36m(func pid=97446)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=97446)[0m f1_macro: 0.38173412341106894
[2m[36m(func pid=97446)[0m f1_weighted: 0.40444284649558554
[2m[36m(func pid=97446)[0m f1_per_class: [0.587, 0.57, 0.407, 0.389, 0.135, 0.221, 0.402, 0.376, 0.314, 0.415]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.5648 | Steps: 4 | Val loss: 6.6145 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0143 | Steps: 4 | Val loss: 1.9339 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.7794 | Steps: 4 | Val loss: 3.0617 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0673 | Steps: 4 | Val loss: 1.5964 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 13:55:36 (running for 00:12:13.60)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.122 |      0.234 |                   47 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.081 |      0.382 |                   47 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.565 |      0.167 |                   23 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.814 |      0.029 |                   22 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.22201492537313433
[2m[36m(func pid=103465)[0m top5: 0.8059701492537313
[2m[36m(func pid=103465)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=103465)[0m f1_macro: 0.16675175309962553
[2m[36m(func pid=103465)[0m f1_weighted: 0.24699009234916836
[2m[36m(func pid=103465)[0m f1_per_class: [0.129, 0.288, 0.0, 0.272, 0.127, 0.064, 0.276, 0.459, 0.052, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.29990671641791045
[2m[36m(func pid=97361)[0m top5: 0.8423507462686567
[2m[36m(func pid=97361)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=97361)[0m f1_macro: 0.24524209786569454
[2m[36m(func pid=97361)[0m f1_weighted: 0.3131748654000806
[2m[36m(func pid=97361)[0m f1_per_class: [0.293, 0.302, 0.247, 0.461, 0.133, 0.171, 0.27, 0.309, 0.142, 0.124]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.06296641791044776
[2m[36m(func pid=103554)[0m top5: 0.4808768656716418
[2m[36m(func pid=103554)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=103554)[0m f1_macro: 0.0518430535321488
[2m[36m(func pid=103554)[0m f1_weighted: 0.014453221374542365
[2m[36m(func pid=103554)[0m f1_per_class: [0.041, 0.0, 0.333, 0.016, 0.0, 0.0, 0.0, 0.112, 0.016, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.41884328358208955
[2m[36m(func pid=97446)[0m top5: 0.9281716417910447
[2m[36m(func pid=97446)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=97446)[0m f1_macro: 0.390155249858528
[2m[36m(func pid=97446)[0m f1_weighted: 0.4108270030032761
[2m[36m(func pid=97446)[0m f1_per_class: [0.593, 0.569, 0.407, 0.354, 0.173, 0.243, 0.449, 0.365, 0.316, 0.433]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.0184 | Steps: 4 | Val loss: 1.9358 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.8847 | Steps: 4 | Val loss: 23.5955 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7197 | Steps: 4 | Val loss: 2.5699 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0543 | Steps: 4 | Val loss: 1.6259 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:55:42 (running for 00:12:19.17)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.014 |      0.245 |                   48 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.067 |      0.39  |                   48 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.885 |      0.084 |                   24 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.779 |      0.052 |                   23 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m top1: 0.2891791044776119
[2m[36m(func pid=97361)[0m top5: 0.8390858208955224
[2m[36m(func pid=97361)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=97361)[0m f1_macro: 0.23864604555181373
[2m[36m(func pid=97361)[0m f1_weighted: 0.307115466143319
[2m[36m(func pid=97361)[0m f1_per_class: [0.292, 0.299, 0.22, 0.442, 0.131, 0.176, 0.272, 0.284, 0.15, 0.119]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m top1: 0.2560634328358209
[2m[36m(func pid=103465)[0m top5: 0.6259328358208955
[2m[36m(func pid=103465)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=103465)[0m f1_macro: 0.08420031061365996
[2m[36m(func pid=103465)[0m f1_weighted: 0.1699659735645462
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.027, 0.0, 0.56, 0.188, 0.068, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.0228544776119403
[2m[36m(func pid=103554)[0m top5: 0.4864738805970149
[2m[36m(func pid=103554)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=103554)[0m f1_macro: 0.03355599503645106
[2m[36m(func pid=103554)[0m f1_weighted: 0.004391933060963216
[2m[36m(func pid=103554)[0m f1_per_class: [0.037, 0.0, 0.292, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4155783582089552
[2m[36m(func pid=97446)[0m top5: 0.9174440298507462
[2m[36m(func pid=97446)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=97446)[0m f1_macro: 0.3961931424733445
[2m[36m(func pid=97446)[0m f1_weighted: 0.40857262710292885
[2m[36m(func pid=97446)[0m f1_per_class: [0.584, 0.563, 0.48, 0.378, 0.145, 0.256, 0.413, 0.374, 0.332, 0.436]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0930 | Steps: 4 | Val loss: 1.9268 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 4.4114 | Steps: 4 | Val loss: 9.9190 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.0983 | Steps: 4 | Val loss: 2.3637 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2415 | Steps: 4 | Val loss: 1.6437 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=97361)[0m top1: 0.2873134328358209
[2m[36m(func pid=97361)[0m top5: 0.8488805970149254
[2m[36m(func pid=97361)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=97361)[0m f1_macro: 0.24245758005759727
[2m[36m(func pid=97361)[0m f1_weighted: 0.3082259925480434
[2m[36m(func pid=97361)[0m f1_per_class: [0.305, 0.281, 0.261, 0.435, 0.12, 0.166, 0.295, 0.281, 0.163, 0.118]
== Status ==
Current time: 2024-01-07 13:55:47 (running for 00:12:24.53)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.093 |      0.242 |                   50 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.054 |      0.396 |                   49 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.885 |      0.084 |                   24 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.72  |      0.034 |                   24 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m top1: 0.13852611940298507
[2m[36m(func pid=103465)[0m top5: 0.679570895522388
[2m[36m(func pid=103465)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=103465)[0m f1_macro: 0.11690971654502627
[2m[36m(func pid=103465)[0m f1_weighted: 0.09653547565040128
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.291, 0.0, 0.003, 0.198, 0.142, 0.0, 0.428, 0.08, 0.026]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.022388059701492536
[2m[36m(func pid=103554)[0m top5: 0.48740671641791045
[2m[36m(func pid=103554)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=103554)[0m f1_macro: 0.0363159275780635
[2m[36m(func pid=103554)[0m f1_weighted: 0.002763504701629611
[2m[36m(func pid=103554)[0m f1_per_class: [0.039, 0.0, 0.324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4193097014925373
[2m[36m(func pid=97446)[0m top5: 0.914179104477612
[2m[36m(func pid=97446)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=97446)[0m f1_macro: 0.3964027058906498
[2m[36m(func pid=97446)[0m f1_weighted: 0.41656773978697464
[2m[36m(func pid=97446)[0m f1_per_class: [0.629, 0.563, 0.511, 0.395, 0.134, 0.231, 0.434, 0.383, 0.3, 0.383]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.2659 | Steps: 4 | Val loss: 1.9089 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5382 | Steps: 4 | Val loss: 7.0242 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 5.0588 | Steps: 4 | Val loss: 2.3487 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.1406 | Steps: 4 | Val loss: 1.5307 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 13:55:53 (running for 00:12:30.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.266 |      0.244 |                   51 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.241 |      0.396 |                   50 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.411 |      0.117 |                   25 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.098 |      0.036 |                   25 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m top1: 0.2966417910447761
[2m[36m(func pid=97361)[0m top5: 0.8577425373134329
[2m[36m(func pid=97361)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=97361)[0m f1_macro: 0.24438911408460476
[2m[36m(func pid=97361)[0m f1_weighted: 0.31807649327055937
[2m[36m(func pid=97361)[0m f1_per_class: [0.299, 0.339, 0.255, 0.437, 0.103, 0.15, 0.301, 0.28, 0.15, 0.131]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m top1: 0.2887126865671642
[2m[36m(func pid=103465)[0m top5: 0.5597014925373134
[2m[36m(func pid=103465)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=103465)[0m f1_macro: 0.11484479208020079
[2m[36m(func pid=103465)[0m f1_weighted: 0.2145062576737773
[2m[36m(func pid=103465)[0m f1_per_class: [0.228, 0.432, 0.0, 0.003, 0.0, 0.0, 0.45, 0.0, 0.0, 0.035]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.016791044776119403
[2m[36m(func pid=103554)[0m top5: 0.38759328358208955
[2m[36m(func pid=103554)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=103554)[0m f1_macro: 0.02961250724408619
[2m[36m(func pid=103554)[0m f1_weighted: 0.005518418556026568
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.0, 0.259, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.45475746268656714
[2m[36m(func pid=97446)[0m top5: 0.929570895522388
[2m[36m(func pid=97446)[0m f1_micro: 0.45475746268656714
[2m[36m(func pid=97446)[0m f1_macro: 0.39065509905964746
[2m[36m(func pid=97446)[0m f1_weighted: 0.4599746949004439
[2m[36m(func pid=97446)[0m f1_per_class: [0.584, 0.571, 0.468, 0.46, 0.164, 0.188, 0.55, 0.327, 0.274, 0.32]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0356 | Steps: 4 | Val loss: 1.9017 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 3.3385 | Steps: 4 | Val loss: 7.6507 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 3.0615 | Steps: 4 | Val loss: 2.3439 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1636 | Steps: 4 | Val loss: 1.5636 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:55:58 (running for 00:12:35.36)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.036 |      0.257 |                   52 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.141 |      0.391 |                   51 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.538 |      0.115 |                   26 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  5.059 |      0.03  |                   26 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m top1: 0.30550373134328357
[2m[36m(func pid=97361)[0m top5: 0.8568097014925373
[2m[36m(func pid=97361)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=97361)[0m f1_macro: 0.2568667420956479
[2m[36m(func pid=97361)[0m f1_weighted: 0.32837418088449627
[2m[36m(func pid=97361)[0m f1_per_class: [0.341, 0.373, 0.286, 0.44, 0.101, 0.137, 0.309, 0.294, 0.17, 0.116]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m top1: 0.23880597014925373
[2m[36m(func pid=103465)[0m top5: 0.7397388059701493
[2m[36m(func pid=103465)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=103465)[0m f1_macro: 0.1179762634094651
[2m[36m(func pid=103465)[0m f1_weighted: 0.2310022587311387
[2m[36m(func pid=103465)[0m f1_per_class: [0.177, 0.317, 0.079, 0.016, 0.0, 0.0, 0.562, 0.0, 0.0, 0.028]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.017723880597014924
[2m[36m(func pid=103554)[0m top5: 0.6403917910447762
[2m[36m(func pid=103554)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=103554)[0m f1_macro: 0.013439361020011142
[2m[36m(func pid=103554)[0m f1_weighted: 0.01124105979192197
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.026, 0.062, 0.022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.449160447761194
[2m[36m(func pid=97446)[0m top5: 0.9239738805970149
[2m[36m(func pid=97446)[0m f1_micro: 0.449160447761194
[2m[36m(func pid=97446)[0m f1_macro: 0.38729871313573083
[2m[36m(func pid=97446)[0m f1_weighted: 0.44583793424612556
[2m[36m(func pid=97446)[0m f1_per_class: [0.561, 0.562, 0.524, 0.386, 0.151, 0.188, 0.577, 0.328, 0.295, 0.302]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.1400 | Steps: 4 | Val loss: 1.9262 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.1506 | Steps: 4 | Val loss: 35.5136 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.0337 | Steps: 4 | Val loss: 2.3570 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0848 | Steps: 4 | Val loss: 1.5285 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=97361)[0m top1: 0.28777985074626866
[2m[36m(func pid=97361)[0m top5: 0.8484141791044776
[2m[36m(func pid=97361)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=97361)[0m f1_macro: 0.25206156720426803
[2m[36m(func pid=97361)[0m f1_weighted: 0.31292259111280624
[2m[36m(func pid=97361)[0m f1_per_class: [0.346, 0.328, 0.348, 0.437, 0.093, 0.132, 0.29, 0.29, 0.163, 0.093]
== Status ==
Current time: 2024-01-07 13:56:03 (running for 00:12:40.64)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.14  |      0.252 |                   53 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.164 |      0.387 |                   52 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.338 |      0.118 |                   27 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.062 |      0.013 |                   27 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m top1: 0.28171641791044777
[2m[36m(func pid=103465)[0m top5: 0.7434701492537313
[2m[36m(func pid=103465)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=103465)[0m f1_macro: 0.14983219055159375
[2m[36m(func pid=103465)[0m f1_weighted: 0.2169834352835028
[2m[36m(func pid=103465)[0m f1_per_class: [0.082, 0.011, 0.23, 0.56, 0.09, 0.0, 0.112, 0.371, 0.0, 0.044]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.02005597014925373
[2m[36m(func pid=103554)[0m top5: 0.632929104477612
[2m[36m(func pid=103554)[0m f1_micro: 0.02005597014925373
[2m[36m(func pid=103554)[0m f1_macro: 0.009309661726924972
[2m[36m(func pid=103554)[0m f1_weighted: 0.015896494726741293
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.035, 0.0, 0.031, 0.0, 0.0, 0.003, 0.0, 0.0, 0.024]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4552238805970149
[2m[36m(func pid=97446)[0m top5: 0.9361007462686567
[2m[36m(func pid=97446)[0m f1_micro: 0.4552238805970149
[2m[36m(func pid=97446)[0m f1_macro: 0.40181071950452096
[2m[36m(func pid=97446)[0m f1_weighted: 0.4546845437718378
[2m[36m(func pid=97446)[0m f1_per_class: [0.593, 0.59, 0.478, 0.386, 0.133, 0.212, 0.565, 0.382, 0.321, 0.358]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0130 | Steps: 4 | Val loss: 1.9113 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.0020 | Steps: 4 | Val loss: 2.2828 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.0760 | Steps: 4 | Val loss: 88.1259 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2027 | Steps: 4 | Val loss: 1.5280 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 13:56:09 (running for 00:12:46.11)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.013 |      0.256 |                   54 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.085 |      0.402 |                   53 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.151 |      0.15  |                   28 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.034 |      0.009 |                   28 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97361)[0m top1: 0.2868470149253731
[2m[36m(func pid=97361)[0m top5: 0.8544776119402985
[2m[36m(func pid=97361)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=97361)[0m f1_macro: 0.2558314903988968
[2m[36m(func pid=97361)[0m f1_weighted: 0.3087612230071516
[2m[36m(func pid=97361)[0m f1_per_class: [0.368, 0.351, 0.348, 0.408, 0.099, 0.136, 0.283, 0.305, 0.157, 0.102]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.036380597014925374
[2m[36m(func pid=103554)[0m top5: 0.6371268656716418
[2m[36m(func pid=103554)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=103554)[0m f1_macro: 0.07633758512815003
[2m[36m(func pid=103554)[0m f1_weighted: 0.042687578187930154
[2m[36m(func pid=103554)[0m f1_per_class: [0.16, 0.085, 0.417, 0.075, 0.0, 0.0, 0.003, 0.0, 0.0, 0.023]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.16977611940298507
[2m[36m(func pid=103465)[0m top5: 0.6100746268656716
[2m[36m(func pid=103465)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=103465)[0m f1_macro: 0.1428961452663115
[2m[36m(func pid=103465)[0m f1_weighted: 0.10719988241851575
[2m[36m(func pid=103465)[0m f1_per_class: [0.127, 0.344, 0.208, 0.003, 0.079, 0.074, 0.015, 0.462, 0.073, 0.044]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4519589552238806
[2m[36m(func pid=97446)[0m top5: 0.9402985074626866
[2m[36m(func pid=97446)[0m f1_micro: 0.4519589552238806
[2m[36m(func pid=97446)[0m f1_macro: 0.4034469116251934
[2m[36m(func pid=97446)[0m f1_weighted: 0.46053691327711377
[2m[36m(func pid=97446)[0m f1_per_class: [0.576, 0.592, 0.524, 0.426, 0.129, 0.259, 0.533, 0.372, 0.311, 0.312]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9542 | Steps: 4 | Val loss: 1.9233 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 5.5757 | Steps: 4 | Val loss: 89.2208 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8010 | Steps: 4 | Val loss: 2.3907 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0753 | Steps: 4 | Val loss: 1.4568 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 13:56:14 (running for 00:12:51.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.013 |      0.256 |                   54 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.203 |      0.403 |                   54 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  5.576 |      0.125 |                   30 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.002 |      0.076 |                   29 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.14132462686567165
[2m[36m(func pid=103465)[0m top5: 0.46921641791044777
[2m[36m(func pid=103465)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=103465)[0m f1_macro: 0.12536306891839427
[2m[36m(func pid=103465)[0m f1_weighted: 0.10734639721408237
[2m[36m(func pid=103465)[0m f1_per_class: [0.15, 0.355, 0.256, 0.0, 0.069, 0.019, 0.076, 0.252, 0.035, 0.042]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.021921641791044777
[2m[36m(func pid=103554)[0m top5: 0.6273320895522388
[2m[36m(func pid=103554)[0m f1_micro: 0.021921641791044777
[2m[36m(func pid=103554)[0m f1_macro: 0.047322376383482245
[2m[36m(func pid=103554)[0m f1_weighted: 0.01439778073193428
[2m[36m(func pid=103554)[0m f1_per_class: [0.105, 0.059, 0.286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.271455223880597
[2m[36m(func pid=97361)[0m top5: 0.8526119402985075
[2m[36m(func pid=97361)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=97361)[0m f1_macro: 0.2517726174214762
[2m[36m(func pid=97361)[0m f1_weighted: 0.2935072695733578
[2m[36m(func pid=97361)[0m f1_per_class: [0.35, 0.32, 0.414, 0.401, 0.088, 0.119, 0.263, 0.306, 0.161, 0.096]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4626865671641791
[2m[36m(func pid=97446)[0m top5: 0.9496268656716418
[2m[36m(func pid=97446)[0m f1_micro: 0.4626865671641791
[2m[36m(func pid=97446)[0m f1_macro: 0.41282661269784143
[2m[36m(func pid=97446)[0m f1_weighted: 0.4793308647876378
[2m[36m(func pid=97446)[0m f1_per_class: [0.6, 0.601, 0.537, 0.546, 0.141, 0.277, 0.474, 0.349, 0.318, 0.286]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.0740 | Steps: 4 | Val loss: 1.9025 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 4.9750 | Steps: 4 | Val loss: 20.7458 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6368 | Steps: 4 | Val loss: 2.4795 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.1094 | Steps: 4 | Val loss: 1.4123 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:56:20 (running for 00:12:56.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.954 |      0.252 |                   55 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.075 |      0.413 |                   55 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.975 |      0.081 |                   31 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.801 |      0.047 |                   30 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.07602611940298508
[2m[36m(func pid=103465)[0m top5: 0.5555037313432836
[2m[36m(func pid=103465)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=103465)[0m f1_macro: 0.08063896794634753
[2m[36m(func pid=103465)[0m f1_weighted: 0.06836225164349977
[2m[36m(func pid=103465)[0m f1_per_class: [0.256, 0.0, 0.122, 0.0, 0.09, 0.065, 0.175, 0.0, 0.048, 0.05]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.29384328358208955
[2m[36m(func pid=97361)[0m top5: 0.8577425373134329
[2m[36m(func pid=97361)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=97361)[0m f1_macro: 0.27683077270497625
[2m[36m(func pid=97361)[0m f1_weighted: 0.32182870283542575
[2m[36m(func pid=97361)[0m f1_per_class: [0.345, 0.353, 0.478, 0.39, 0.124, 0.169, 0.326, 0.311, 0.172, 0.099]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.12406716417910447
[2m[36m(func pid=103554)[0m top5: 0.6194029850746269
[2m[36m(func pid=103554)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=103554)[0m f1_macro: 0.05313576930710288
[2m[36m(func pid=103554)[0m f1_weighted: 0.0380173812696317
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.069, 0.247, 0.0, 0.0, 0.216, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.46548507462686567
[2m[36m(func pid=97446)[0m top5: 0.9556902985074627
[2m[36m(func pid=97446)[0m f1_micro: 0.4654850746268657
[2m[36m(func pid=97446)[0m f1_macro: 0.4259661026650193
[2m[36m(func pid=97446)[0m f1_weighted: 0.47306528596601694
[2m[36m(func pid=97446)[0m f1_per_class: [0.617, 0.591, 0.512, 0.592, 0.159, 0.269, 0.397, 0.429, 0.34, 0.354]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 4.5805 | Steps: 4 | Val loss: 7.8300 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7796 | Steps: 4 | Val loss: 2.4713 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.0021 | Steps: 4 | Val loss: 1.9106 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1158 | Steps: 4 | Val loss: 1.4506 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:56:25 (running for 00:13:02.27)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.074 |      0.277 |                   56 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.109 |      0.426 |                   56 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.58  |      0.13  |                   32 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.637 |      0.053 |                   31 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.2644589552238806
[2m[36m(func pid=103465)[0m top5: 0.6613805970149254
[2m[36m(func pid=103465)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=103465)[0m f1_macro: 0.13015465696969347
[2m[36m(func pid=103465)[0m f1_weighted: 0.2016212368900417
[2m[36m(func pid=103465)[0m f1_per_class: [0.104, 0.0, 0.141, 0.053, 0.097, 0.0, 0.585, 0.07, 0.087, 0.165]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.12546641791044777
[2m[36m(func pid=103554)[0m top5: 0.6152052238805971
[2m[36m(func pid=103554)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=103554)[0m f1_macro: 0.05805410296491108
[2m[36m(func pid=103554)[0m f1_weighted: 0.040860157331043735
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.085, 0.281, 0.0, 0.0, 0.215, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.283115671641791
[2m[36m(func pid=97361)[0m top5: 0.8600746268656716
[2m[36m(func pid=97361)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=97361)[0m f1_macro: 0.26798233136397875
[2m[36m(func pid=97361)[0m f1_weighted: 0.30884645467776217
[2m[36m(func pid=97361)[0m f1_per_class: [0.356, 0.34, 0.453, 0.381, 0.123, 0.196, 0.294, 0.302, 0.141, 0.094]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4626865671641791
[2m[36m(func pid=97446)[0m top5: 0.9477611940298507
[2m[36m(func pid=97446)[0m f1_micro: 0.4626865671641791
[2m[36m(func pid=97446)[0m f1_macro: 0.40765990522368584
[2m[36m(func pid=97446)[0m f1_weighted: 0.46283371871792073
[2m[36m(func pid=97446)[0m f1_per_class: [0.571, 0.515, 0.533, 0.632, 0.175, 0.262, 0.38, 0.446, 0.286, 0.275]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.9152 | Steps: 4 | Val loss: 6.2661 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.7209 | Steps: 4 | Val loss: 2.5901 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0553 | Steps: 4 | Val loss: 1.8817 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.1520 | Steps: 4 | Val loss: 1.4987 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:56:30 (running for 00:13:07.67)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.002 |      0.268 |                   57 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.116 |      0.408 |                   57 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.915 |      0.171 |                   33 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.78  |      0.058 |                   32 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.3162313432835821
[2m[36m(func pid=103465)[0m top5: 0.769589552238806
[2m[36m(func pid=103465)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=103465)[0m f1_macro: 0.170952069380697
[2m[36m(func pid=103465)[0m f1_weighted: 0.288852591701239
[2m[36m(func pid=103465)[0m f1_per_class: [0.069, 0.427, 0.0, 0.559, 0.0, 0.414, 0.006, 0.114, 0.038, 0.082]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.020522388059701493
[2m[36m(func pid=103554)[0m top5: 0.6254664179104478
[2m[36m(func pid=103554)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=103554)[0m f1_macro: 0.04762197862499458
[2m[36m(func pid=103554)[0m f1_weighted: 0.016818169632571135
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.081, 0.36, 0.0, 0.016, 0.0, 0.0, 0.0, 0.02, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.30363805970149255
[2m[36m(func pid=97361)[0m top5: 0.8717350746268657
[2m[36m(func pid=97361)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=97361)[0m f1_macro: 0.2752989739652326
[2m[36m(func pid=97361)[0m f1_weighted: 0.32730273650888847
[2m[36m(func pid=97361)[0m f1_per_class: [0.356, 0.385, 0.4, 0.405, 0.159, 0.186, 0.31, 0.302, 0.148, 0.102]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4533582089552239
[2m[36m(func pid=97446)[0m top5: 0.9347014925373134
[2m[36m(func pid=97446)[0m f1_micro: 0.4533582089552239
[2m[36m(func pid=97446)[0m f1_macro: 0.4031471348411628
[2m[36m(func pid=97446)[0m f1_weighted: 0.45301027827478735
[2m[36m(func pid=97446)[0m f1_per_class: [0.544, 0.553, 0.533, 0.613, 0.161, 0.25, 0.348, 0.442, 0.313, 0.274]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.6049 | Steps: 4 | Val loss: 8.9321 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.9993 | Steps: 4 | Val loss: 1.8272 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7745 | Steps: 4 | Val loss: 2.3686 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0643 | Steps: 4 | Val loss: 1.4799 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 13:56:36 (running for 00:13:13.17)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.055 |      0.275 |                   58 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.152 |      0.403 |                   58 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.605 |      0.09  |                   34 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.721 |      0.048 |                   33 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.14039179104477612
[2m[36m(func pid=103465)[0m top5: 0.5699626865671642
[2m[36m(func pid=103465)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=103465)[0m f1_macro: 0.0903483294415249
[2m[36m(func pid=103465)[0m f1_weighted: 0.08873814392380983
[2m[36m(func pid=103465)[0m f1_per_class: [0.228, 0.417, 0.062, 0.0, 0.0, 0.043, 0.009, 0.0, 0.118, 0.027]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.34048507462686567
[2m[36m(func pid=97361)[0m top5: 0.8894589552238806
[2m[36m(func pid=97361)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=97361)[0m f1_macro: 0.29672958123406235
[2m[36m(func pid=97361)[0m f1_weighted: 0.36022622882360283
[2m[36m(func pid=97361)[0m f1_per_class: [0.41, 0.413, 0.387, 0.464, 0.145, 0.192, 0.339, 0.313, 0.156, 0.148]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103554)[0m top1: 0.024720149253731342
[2m[36m(func pid=103554)[0m top5: 0.5914179104477612
[2m[36m(func pid=103554)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=103554)[0m f1_macro: 0.051796110473937354
[2m[36m(func pid=103554)[0m f1_weighted: 0.02138750444338576
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.11, 0.392, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.45615671641791045
[2m[36m(func pid=97446)[0m top5: 0.9393656716417911
[2m[36m(func pid=97446)[0m f1_micro: 0.45615671641791045
[2m[36m(func pid=97446)[0m f1_macro: 0.39403376908538873
[2m[36m(func pid=97446)[0m f1_weighted: 0.45785574463504924
[2m[36m(func pid=97446)[0m f1_per_class: [0.504, 0.594, 0.49, 0.583, 0.148, 0.227, 0.389, 0.429, 0.25, 0.328]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3415 | Steps: 4 | Val loss: 5.1163 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.9091 | Steps: 4 | Val loss: 1.8246 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7398 | Steps: 4 | Val loss: 2.3849 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0963 | Steps: 4 | Val loss: 1.4624 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=103465)[0m top1: 0.23180970149253732
[2m[36m(func pid=103465)[0m top5: 0.7173507462686567
[2m[36m(func pid=103465)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=103465)[0m f1_macro: 0.20593942981310276
[2m[36m(func pid=103465)[0m f1_weighted: 0.252241677124246
[2m[36m(func pid=103465)[0m f1_per_class: [0.163, 0.086, 0.072, 0.07, 0.051, 0.361, 0.456, 0.535, 0.123, 0.143]
[2m[36m(func pid=103465)[0m 
== Status ==
Current time: 2024-01-07 13:56:41 (running for 00:13:18.67)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.999 |      0.297 |                   59 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.064 |      0.394 |                   59 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.341 |      0.206 |                   35 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.774 |      0.052 |                   34 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.020522388059701493
[2m[36m(func pid=103554)[0m top5: 0.5373134328358209
[2m[36m(func pid=103554)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=103554)[0m f1_macro: 0.031215289125182993
[2m[36m(func pid=103554)[0m f1_weighted: 0.014161607626732286
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.074, 0.222, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3460820895522388
[2m[36m(func pid=97361)[0m top5: 0.8899253731343284
[2m[36m(func pid=97361)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=97361)[0m f1_macro: 0.2977479577607186
[2m[36m(func pid=97361)[0m f1_weighted: 0.36438695381101915
[2m[36m(func pid=97361)[0m f1_per_class: [0.419, 0.39, 0.381, 0.508, 0.137, 0.196, 0.32, 0.323, 0.165, 0.137]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.45988805970149255
[2m[36m(func pid=97446)[0m top5: 0.9356343283582089
[2m[36m(func pid=97446)[0m f1_micro: 0.45988805970149255
[2m[36m(func pid=97446)[0m f1_macro: 0.40777715870357706
[2m[36m(func pid=97446)[0m f1_weighted: 0.46248720006826155
[2m[36m(func pid=97446)[0m f1_per_class: [0.56, 0.597, 0.511, 0.555, 0.143, 0.209, 0.425, 0.405, 0.349, 0.324]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 5.2251 | Steps: 4 | Val loss: 5.3530 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 3.1205 | Steps: 4 | Val loss: 2.3490 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8089 | Steps: 4 | Val loss: 1.8020 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0384 | Steps: 4 | Val loss: 1.4577 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:56:47 (running for 00:13:23.92)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.909 |      0.298 |                   60 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.096 |      0.408 |                   60 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  5.225 |      0.143 |                   36 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.74  |      0.031 |                   35 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.2513992537313433
[2m[36m(func pid=103465)[0m top5: 0.742070895522388
[2m[36m(func pid=103465)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=103465)[0m f1_macro: 0.1432830520408058
[2m[36m(func pid=103465)[0m f1_weighted: 0.23382908431018443
[2m[36m(func pid=103465)[0m f1_per_class: [0.202, 0.032, 0.0, 0.061, 0.042, 0.016, 0.617, 0.301, 0.085, 0.077]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.03125
[2m[36m(func pid=103554)[0m top5: 0.5382462686567164
[2m[36m(func pid=103554)[0m f1_micro: 0.03125
[2m[36m(func pid=103554)[0m f1_macro: 0.03211038071706452
[2m[36m(func pid=103554)[0m f1_weighted: 0.034398747812303426
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.01, 0.182, 0.113, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.36007462686567165
[2m[36m(func pid=97361)[0m top5: 0.8987873134328358
[2m[36m(func pid=97361)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=97361)[0m f1_macro: 0.30884761354602674
[2m[36m(func pid=97361)[0m f1_weighted: 0.37671070894926334
[2m[36m(func pid=97361)[0m f1_per_class: [0.406, 0.437, 0.358, 0.487, 0.161, 0.217, 0.342, 0.338, 0.171, 0.172]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4603544776119403
[2m[36m(func pid=97446)[0m top5: 0.9379664179104478
[2m[36m(func pid=97446)[0m f1_micro: 0.4603544776119403
[2m[36m(func pid=97446)[0m f1_macro: 0.40834268374095356
[2m[36m(func pid=97446)[0m f1_weighted: 0.46034572699176934
[2m[36m(func pid=97446)[0m f1_per_class: [0.443, 0.595, 0.611, 0.5, 0.187, 0.221, 0.48, 0.386, 0.268, 0.391]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.8298 | Steps: 4 | Val loss: 9.1423 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8971 | Steps: 4 | Val loss: 2.3585 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0407 | Steps: 4 | Val loss: 1.5224 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.8271 | Steps: 4 | Val loss: 1.7993 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=103465)[0m top1: 0.17957089552238806
[2m[36m(func pid=103465)[0m top5: 0.7639925373134329
[2m[36m(func pid=103465)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=103465)[0m f1_macro: 0.14196213735169672
[2m[36m(func pid=103465)[0m f1_weighted: 0.16032963979800616
[2m[36m(func pid=103465)[0m f1_per_class: [0.135, 0.358, 0.133, 0.227, 0.073, 0.124, 0.006, 0.21, 0.053, 0.099]
== Status ==
Current time: 2024-01-07 13:56:52 (running for 00:13:29.56)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.809 |      0.309 |                   61 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.038 |      0.408 |                   61 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.83  |      0.142 |                   37 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.12  |      0.032 |                   36 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.04664179104477612
[2m[36m(func pid=103554)[0m top5: 0.7915111940298507
[2m[36m(func pid=103554)[0m f1_micro: 0.04664179104477612
[2m[36m(func pid=103554)[0m f1_macro: 0.04856917257912528
[2m[36m(func pid=103554)[0m f1_weighted: 0.06303778709624629
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.097, 0.19, 0.159, 0.017, 0.0, 0.0, 0.0, 0.022, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.449160447761194
[2m[36m(func pid=97446)[0m top5: 0.9277052238805971
[2m[36m(func pid=97446)[0m f1_micro: 0.449160447761194
[2m[36m(func pid=97446)[0m f1_macro: 0.3932446087321996
[2m[36m(func pid=97446)[0m f1_weighted: 0.4428291401544373
[2m[36m(func pid=97446)[0m f1_per_class: [0.439, 0.564, 0.55, 0.414, 0.164, 0.213, 0.522, 0.396, 0.283, 0.387]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3605410447761194
[2m[36m(func pid=97361)[0m top5: 0.8973880597014925
[2m[36m(func pid=97361)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=97361)[0m f1_macro: 0.3100889550122382
[2m[36m(func pid=97361)[0m f1_weighted: 0.3740026629225048
[2m[36m(func pid=97361)[0m f1_per_class: [0.408, 0.441, 0.381, 0.503, 0.161, 0.209, 0.323, 0.298, 0.2, 0.177]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.9292 | Steps: 4 | Val loss: 5.4924 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.0861 | Steps: 4 | Val loss: 2.3135 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0850 | Steps: 4 | Val loss: 1.5555 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8337 | Steps: 4 | Val loss: 1.8124 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 13:56:58 (running for 00:13:35.00)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.827 |      0.31  |                   62 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.041 |      0.393 |                   62 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.929 |      0.139 |                   38 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.897 |      0.049 |                   37 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4454291044776119
[2m[36m(func pid=97446)[0m top5: 0.925839552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.4454291044776119
[2m[36m(func pid=97446)[0m f1_macro: 0.3934567122183771
[2m[36m(func pid=97446)[0m f1_weighted: 0.4376782874191766
[2m[36m(func pid=97446)[0m f1_per_class: [0.468, 0.558, 0.558, 0.396, 0.164, 0.203, 0.527, 0.383, 0.315, 0.364]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.22294776119402984
[2m[36m(func pid=103465)[0m top5: 0.7112873134328358
[2m[36m(func pid=103465)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=103465)[0m f1_macro: 0.13926220716169616
[2m[36m(func pid=103465)[0m f1_weighted: 0.1836269382837668
[2m[36m(func pid=103465)[0m f1_per_class: [0.108, 0.0, 0.0, 0.499, 0.213, 0.2, 0.009, 0.181, 0.113, 0.07]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.06902985074626866
[2m[36m(func pid=103554)[0m top5: 0.6907649253731343
[2m[36m(func pid=103554)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=103554)[0m f1_macro: 0.07731137954208327
[2m[36m(func pid=103554)[0m f1_weighted: 0.08775554486787364
[2m[36m(func pid=103554)[0m f1_per_class: [0.203, 0.021, 0.229, 0.278, 0.017, 0.0, 0.0, 0.0, 0.025, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3568097014925373
[2m[36m(func pid=97361)[0m top5: 0.8917910447761194
[2m[36m(func pid=97361)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=97361)[0m f1_macro: 0.3075951892169661
[2m[36m(func pid=97361)[0m f1_weighted: 0.36909359258095464
[2m[36m(func pid=97361)[0m f1_per_class: [0.433, 0.461, 0.358, 0.49, 0.145, 0.217, 0.303, 0.3, 0.187, 0.181]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0410 | Steps: 4 | Val loss: 1.5559 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.8593 | Steps: 4 | Val loss: 2.3108 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.6607 | Steps: 4 | Val loss: 3.4579 | Batch size: 32 | lr: 0.01 | Duration: 3.26s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8891 | Steps: 4 | Val loss: 1.8082 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:57:03 (running for 00:13:40.39)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.834 |      0.308 |                   63 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.085 |      0.393 |                   63 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.929 |      0.139 |                   38 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.859 |      0.091 |                   39 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.43330223880597013
[2m[36m(func pid=97446)[0m top5: 0.9286380597014925
[2m[36m(func pid=97446)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=97446)[0m f1_macro: 0.3852848568365924
[2m[36m(func pid=97446)[0m f1_weighted: 0.4272854359048312
[2m[36m(func pid=97446)[0m f1_per_class: [0.426, 0.561, 0.537, 0.366, 0.145, 0.216, 0.515, 0.394, 0.308, 0.386]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.2943097014925373
[2m[36m(func pid=103554)[0m top5: 0.6809701492537313
[2m[36m(func pid=103554)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=103554)[0m f1_macro: 0.09113968132689954
[2m[36m(func pid=103554)[0m f1_weighted: 0.22535766127715964
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.005, 0.14, 0.254, 0.0, 0.0, 0.512, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.23134328358208955
[2m[36m(func pid=103465)[0m top5: 0.7961753731343284
[2m[36m(func pid=103465)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=103465)[0m f1_macro: 0.1918775334730081
[2m[36m(func pid=103465)[0m f1_weighted: 0.23048573248191112
[2m[36m(func pid=103465)[0m f1_per_class: [0.336, 0.011, 0.311, 0.491, 0.091, 0.221, 0.155, 0.1, 0.118, 0.085]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3530783582089552
[2m[36m(func pid=97361)[0m top5: 0.8955223880597015
[2m[36m(func pid=97361)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=97361)[0m f1_macro: 0.31039168883741564
[2m[36m(func pid=97361)[0m f1_weighted: 0.37029238220397986
[2m[36m(func pid=97361)[0m f1_per_class: [0.374, 0.428, 0.453, 0.476, 0.149, 0.217, 0.34, 0.314, 0.178, 0.175]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0225 | Steps: 4 | Val loss: 1.6073 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.5077 | Steps: 4 | Val loss: 2.6346 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7657 | Steps: 4 | Val loss: 8.8252 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.9629 | Steps: 4 | Val loss: 1.8105 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 13:57:08 (running for 00:13:45.75)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.889 |      0.31  |                   64 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.022 |      0.382 |                   65 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.661 |      0.192 |                   39 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.859 |      0.091 |                   39 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.41651119402985076
[2m[36m(func pid=97446)[0m top5: 0.9244402985074627
[2m[36m(func pid=97446)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=97446)[0m f1_macro: 0.38191098729254835
[2m[36m(func pid=97446)[0m f1_weighted: 0.4118588412889029
[2m[36m(func pid=97446)[0m f1_per_class: [0.404, 0.549, 0.579, 0.341, 0.146, 0.24, 0.488, 0.378, 0.303, 0.391]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.27005597014925375
[2m[36m(func pid=103554)[0m top5: 0.7159514925373134
[2m[36m(func pid=103554)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=103554)[0m f1_macro: 0.06019625031231425
[2m[36m(func pid=103554)[0m f1_weighted: 0.14965967312075906
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.005, 0.09, 0.003, 0.0, 0.0, 0.492, 0.011, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.09841417910447761
[2m[36m(func pid=103465)[0m top5: 0.6543843283582089
[2m[36m(func pid=103465)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=103465)[0m f1_macro: 0.09231943996551192
[2m[36m(func pid=103465)[0m f1_weighted: 0.09939002313989728
[2m[36m(func pid=103465)[0m f1_per_class: [0.128, 0.39, 0.0, 0.073, 0.16, 0.018, 0.006, 0.0, 0.117, 0.031]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3568097014925373
[2m[36m(func pid=97361)[0m top5: 0.8875932835820896
[2m[36m(func pid=97361)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=97361)[0m f1_macro: 0.3038021877145624
[2m[36m(func pid=97361)[0m f1_weighted: 0.3742174459891676
[2m[36m(func pid=97361)[0m f1_per_class: [0.347, 0.432, 0.421, 0.484, 0.114, 0.178, 0.355, 0.346, 0.196, 0.167]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0272 | Steps: 4 | Val loss: 1.6304 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7109 | Steps: 4 | Val loss: 12.1352 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.8528 | Steps: 4 | Val loss: 2.7361 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1569 | Steps: 4 | Val loss: 1.8551 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:57:14 (running for 00:13:51.17)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.963 |      0.304 |                   65 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.027 |      0.381 |                   66 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.766 |      0.092 |                   40 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.508 |      0.06  |                   40 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.40951492537313433
[2m[36m(func pid=97446)[0m top5: 0.9202425373134329
[2m[36m(func pid=97446)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=97446)[0m f1_macro: 0.3812091294933345
[2m[36m(func pid=97446)[0m f1_weighted: 0.40518745363907094
[2m[36m(func pid=97446)[0m f1_per_class: [0.4, 0.555, 0.579, 0.361, 0.141, 0.243, 0.439, 0.389, 0.324, 0.382]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.09794776119402986
[2m[36m(func pid=103465)[0m top5: 0.6497201492537313
[2m[36m(func pid=103465)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=103465)[0m f1_macro: 0.0688149493761703
[2m[36m(func pid=103465)[0m f1_weighted: 0.08575684665434104
[2m[36m(func pid=103465)[0m f1_per_class: [0.114, 0.442, 0.0, 0.01, 0.0, 0.015, 0.0, 0.0, 0.072, 0.034]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.3050373134328358
[2m[36m(func pid=103554)[0m top5: 0.5522388059701493
[2m[36m(func pid=103554)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=103554)[0m f1_macro: 0.08709779863130379
[2m[36m(func pid=103554)[0m f1_weighted: 0.15811886955734808
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.081, 0.313, 0.0, 0.0, 0.0, 0.477, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3316231343283582
[2m[36m(func pid=97361)[0m top5: 0.8680037313432836
[2m[36m(func pid=97361)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=97361)[0m f1_macro: 0.28983249970208586
[2m[36m(func pid=97361)[0m f1_weighted: 0.3468247776073037
[2m[36m(func pid=97361)[0m f1_per_class: [0.324, 0.446, 0.414, 0.442, 0.094, 0.154, 0.304, 0.342, 0.205, 0.173]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0513 | Steps: 4 | Val loss: 1.5650 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4898 | Steps: 4 | Val loss: 4.8652 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.8723 | Steps: 4 | Val loss: 2.7860 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.8324 | Steps: 4 | Val loss: 1.8550 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:57:19 (running for 00:13:56.64)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  1.157 |      0.29  |                   66 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.051 |      0.388 |                   67 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.711 |      0.069 |                   41 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.853 |      0.087 |                   41 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.44029850746268656
[2m[36m(func pid=97446)[0m top5: 0.925839552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=97446)[0m f1_macro: 0.38836018866195643
[2m[36m(func pid=97446)[0m f1_weighted: 0.4496818974265236
[2m[36m(func pid=97446)[0m f1_per_class: [0.389, 0.597, 0.512, 0.48, 0.152, 0.224, 0.462, 0.393, 0.324, 0.351]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.14225746268656717
[2m[36m(func pid=103465)[0m top5: 0.773320895522388
[2m[36m(func pid=103465)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=103465)[0m f1_macro: 0.1393094091286934
[2m[36m(func pid=103465)[0m f1_weighted: 0.1527694773318907
[2m[36m(func pid=103465)[0m f1_per_class: [0.215, 0.437, 0.021, 0.007, 0.11, 0.031, 0.155, 0.319, 0.049, 0.049]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.3069029850746269
[2m[36m(func pid=103554)[0m top5: 0.6879664179104478
[2m[36m(func pid=103554)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=103554)[0m f1_macro: 0.07513373253493014
[2m[36m(func pid=103554)[0m f1_weighted: 0.1753845267886402
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.165, 0.095, 0.0, 0.0, 0.0, 0.491, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.322294776119403
[2m[36m(func pid=97361)[0m top5: 0.8745335820895522
[2m[36m(func pid=97361)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=97361)[0m f1_macro: 0.2866646724335603
[2m[36m(func pid=97361)[0m f1_weighted: 0.34277818647074604
[2m[36m(func pid=97361)[0m f1_per_class: [0.338, 0.449, 0.414, 0.416, 0.081, 0.156, 0.314, 0.334, 0.201, 0.163]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1540 | Steps: 4 | Val loss: 1.5425 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.1092 | Steps: 4 | Val loss: 4.2525 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 3.2001 | Steps: 4 | Val loss: 2.5942 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7705 | Steps: 4 | Val loss: 1.8278 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:57:25 (running for 00:14:02.21)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.832 |      0.287 |                   67 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.051 |      0.388 |                   67 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.109 |      0.136 |                   43 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.872 |      0.075 |                   42 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4458955223880597
[2m[36m(func pid=97446)[0m top5: 0.9253731343283582
[2m[36m(func pid=97446)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=97446)[0m f1_macro: 0.39216145866858393
[2m[36m(func pid=97446)[0m f1_weighted: 0.4508729761262311
[2m[36m(func pid=97446)[0m f1_per_class: [0.394, 0.581, 0.537, 0.477, 0.15, 0.255, 0.468, 0.386, 0.301, 0.372]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.1525186567164179
[2m[36m(func pid=103465)[0m top5: 0.7803171641791045
[2m[36m(func pid=103465)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=103465)[0m f1_macro: 0.13597311454918404
[2m[36m(func pid=103465)[0m f1_weighted: 0.17563731666306362
[2m[36m(func pid=103465)[0m f1_per_class: [0.348, 0.399, 0.006, 0.06, 0.048, 0.0, 0.242, 0.136, 0.069, 0.052]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.3003731343283582
[2m[36m(func pid=103554)[0m top5: 0.5293843283582089
[2m[36m(func pid=103554)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=103554)[0m f1_macro: 0.09803264169180183
[2m[36m(func pid=103554)[0m f1_weighted: 0.15800951735693186
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.061, 0.409, 0.0, 0.0, 0.0, 0.483, 0.01, 0.018, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.33675373134328357
[2m[36m(func pid=97361)[0m top5: 0.8913246268656716
[2m[36m(func pid=97361)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=97361)[0m f1_macro: 0.2973136641403356
[2m[36m(func pid=97361)[0m f1_weighted: 0.35652630455878886
[2m[36m(func pid=97361)[0m f1_per_class: [0.357, 0.441, 0.436, 0.46, 0.078, 0.159, 0.318, 0.348, 0.203, 0.173]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5818 | Steps: 4 | Val loss: 2.9225 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0353 | Steps: 4 | Val loss: 1.6034 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.8108 | Steps: 4 | Val loss: 2.7022 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7918 | Steps: 4 | Val loss: 1.8302 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:57:30 (running for 00:14:07.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.77  |      0.297 |                   68 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.035 |      0.381 |                   69 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.109 |      0.136 |                   43 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.2   |      0.098 |                   43 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4230410447761194
[2m[36m(func pid=97446)[0m top5: 0.9197761194029851
[2m[36m(func pid=97446)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=97446)[0m f1_macro: 0.38131524754836554
[2m[36m(func pid=97446)[0m f1_weighted: 0.42682245140581293
[2m[36m(func pid=97446)[0m f1_per_class: [0.363, 0.568, 0.579, 0.42, 0.129, 0.205, 0.47, 0.373, 0.311, 0.395]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.27238805970149255
[2m[36m(func pid=103465)[0m top5: 0.8470149253731343
[2m[36m(func pid=103465)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=103465)[0m f1_macro: 0.1664347007004978
[2m[36m(func pid=103465)[0m f1_weighted: 0.24652074184758474
[2m[36m(func pid=103465)[0m f1_per_class: [0.311, 0.516, 0.009, 0.01, 0.028, 0.0, 0.47, 0.043, 0.121, 0.156]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.061567164179104475
[2m[36m(func pid=103554)[0m top5: 0.5177238805970149
[2m[36m(func pid=103554)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=103554)[0m f1_macro: 0.05305551934276551
[2m[36m(func pid=103554)[0m f1_weighted: 0.010043418614424399
[2m[36m(func pid=103554)[0m f1_per_class: [0.071, 0.0, 0.348, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3353544776119403
[2m[36m(func pid=97361)[0m top5: 0.8917910447761194
[2m[36m(func pid=97361)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=97361)[0m f1_macro: 0.3050568323512298
[2m[36m(func pid=97361)[0m f1_weighted: 0.35504760137024444
[2m[36m(func pid=97361)[0m f1_per_class: [0.348, 0.443, 0.522, 0.465, 0.082, 0.159, 0.309, 0.34, 0.186, 0.198]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1934 | Steps: 4 | Val loss: 1.6745 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.5357 | Steps: 4 | Val loss: 2.0645 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.1446 | Steps: 4 | Val loss: 2.6605 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6772 | Steps: 4 | Val loss: 1.8379 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:57:35 (running for 00:14:12.85)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.792 |      0.305 |                   69 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.193 |      0.38  |                   70 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.582 |      0.166 |                   44 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.811 |      0.053 |                   44 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4137126865671642
[2m[36m(func pid=97446)[0m top5: 0.9085820895522388
[2m[36m(func pid=97446)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=97446)[0m f1_macro: 0.37973014953136397
[2m[36m(func pid=97446)[0m f1_weighted: 0.4198295710013358
[2m[36m(func pid=97446)[0m f1_per_class: [0.366, 0.572, 0.564, 0.437, 0.109, 0.181, 0.432, 0.397, 0.312, 0.427]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.3871268656716418
[2m[36m(func pid=103465)[0m top5: 0.8614738805970149
[2m[36m(func pid=103465)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=103465)[0m f1_macro: 0.2158039464321048
[2m[36m(func pid=103465)[0m f1_weighted: 0.35706253730013865
[2m[36m(func pid=103465)[0m f1_per_class: [0.243, 0.484, 0.0, 0.204, 0.0, 0.382, 0.545, 0.0, 0.116, 0.185]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.05970149253731343
[2m[36m(func pid=103554)[0m top5: 0.5223880597014925
[2m[36m(func pid=103554)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=103554)[0m f1_macro: 0.023932298517933874
[2m[36m(func pid=103554)[0m f1_weighted: 0.009503014393960217
[2m[36m(func pid=103554)[0m f1_per_class: [0.095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.032, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3362873134328358
[2m[36m(func pid=97361)[0m top5: 0.8889925373134329
[2m[36m(func pid=97361)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=97361)[0m f1_macro: 0.2974557421831058
[2m[36m(func pid=97361)[0m f1_weighted: 0.36121845584806916
[2m[36m(func pid=97361)[0m f1_per_class: [0.306, 0.433, 0.511, 0.458, 0.086, 0.161, 0.353, 0.299, 0.18, 0.188]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0693 | Steps: 4 | Val loss: 1.8005 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7942 | Steps: 4 | Val loss: 3.8024 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7213 | Steps: 4 | Val loss: 2.3557 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7943 | Steps: 4 | Val loss: 1.8331 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:57:41 (running for 00:14:18.26)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.677 |      0.297 |                   70 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.069 |      0.361 |                   71 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.536 |      0.216 |                   45 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.145 |      0.024 |                   45 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.386660447761194
[2m[36m(func pid=97446)[0m top5: 0.8815298507462687
[2m[36m(func pid=97446)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=97446)[0m f1_macro: 0.36123856786742015
[2m[36m(func pid=97446)[0m f1_weighted: 0.3716609745144449
[2m[36m(func pid=97446)[0m f1_per_class: [0.418, 0.573, 0.5, 0.426, 0.12, 0.169, 0.283, 0.404, 0.293, 0.427]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.39598880597014924
[2m[36m(func pid=103465)[0m top5: 0.8628731343283582
[2m[36m(func pid=103465)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=103465)[0m f1_macro: 0.2193621700882174
[2m[36m(func pid=103465)[0m f1_weighted: 0.36226064745246017
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.47, 0.121, 0.19, 0.0, 0.416, 0.576, 0.0, 0.157, 0.263]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.06203358208955224
[2m[36m(func pid=103554)[0m top5: 0.5265858208955224
[2m[36m(func pid=103554)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=103554)[0m f1_macro: 0.055408763644057765
[2m[36m(func pid=103554)[0m f1_weighted: 0.010087454216075814
[2m[36m(func pid=103554)[0m f1_per_class: [0.063, 0.0, 0.378, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.33955223880597013
[2m[36m(func pid=97361)[0m top5: 0.8894589552238806
[2m[36m(func pid=97361)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=97361)[0m f1_macro: 0.3009104120927484
[2m[36m(func pid=97361)[0m f1_weighted: 0.35870911756936946
[2m[36m(func pid=97361)[0m f1_per_class: [0.323, 0.428, 0.522, 0.482, 0.082, 0.14, 0.324, 0.332, 0.192, 0.185]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0616 | Steps: 4 | Val loss: 1.8570 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 5.6031 | Steps: 4 | Val loss: 4.5200 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7232 | Steps: 4 | Val loss: 2.3772 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7409 | Steps: 4 | Val loss: 1.7979 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:57:46 (running for 00:14:23.71)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.794 |      0.301 |                   71 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.062 |      0.36  |                   72 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.794 |      0.219 |                   46 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.721 |      0.055 |                   46 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.3689365671641791
[2m[36m(func pid=97446)[0m top5: 0.8843283582089553
[2m[36m(func pid=97446)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=97446)[0m f1_macro: 0.36000744273186347
[2m[36m(func pid=97446)[0m f1_weighted: 0.3541666132360306
[2m[36m(func pid=97446)[0m f1_per_class: [0.459, 0.581, 0.533, 0.393, 0.109, 0.152, 0.257, 0.346, 0.368, 0.4]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.025652985074626867
[2m[36m(func pid=103554)[0m top5: 0.5811567164179104
[2m[36m(func pid=103554)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=103554)[0m f1_macro: 0.05902334372437461
[2m[36m(func pid=103554)[0m f1_weighted: 0.02452674878532679
[2m[36m(func pid=103554)[0m f1_per_class: [0.115, 0.095, 0.34, 0.01, 0.016, 0.0, 0.0, 0.014, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.34095149253731344
[2m[36m(func pid=103465)[0m top5: 0.8292910447761194
[2m[36m(func pid=103465)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=103465)[0m f1_macro: 0.22877598286666814
[2m[36m(func pid=103465)[0m f1_weighted: 0.30186274901239996
[2m[36m(func pid=103465)[0m f1_per_class: [0.109, 0.472, 0.0, 0.164, 0.0, 0.351, 0.342, 0.373, 0.16, 0.316]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.35074626865671643
[2m[36m(func pid=97361)[0m top5: 0.8941231343283582
[2m[36m(func pid=97361)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=97361)[0m f1_macro: 0.30404198182252645
[2m[36m(func pid=97361)[0m f1_weighted: 0.36828171681124544
[2m[36m(func pid=97361)[0m f1_per_class: [0.341, 0.426, 0.471, 0.488, 0.094, 0.138, 0.353, 0.318, 0.196, 0.216]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0707 | Steps: 4 | Val loss: 1.7709 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.8919 | Steps: 4 | Val loss: 2.2651 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.0409 | Steps: 4 | Val loss: 3.9227 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6875 | Steps: 4 | Val loss: 1.7791 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:57:52 (running for 00:14:29.19)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.741 |      0.304 |                   72 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.071 |      0.375 |                   73 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  5.603 |      0.229 |                   47 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.723 |      0.059 |                   47 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.3969216417910448
[2m[36m(func pid=97446)[0m top5: 0.8875932835820896
[2m[36m(func pid=97446)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=97446)[0m f1_macro: 0.37460369301622565
[2m[36m(func pid=97446)[0m f1_weighted: 0.3823856200119122
[2m[36m(func pid=97446)[0m f1_per_class: [0.538, 0.596, 0.49, 0.493, 0.12, 0.169, 0.237, 0.356, 0.373, 0.373]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.17164179104477612
[2m[36m(func pid=103554)[0m top5: 0.5862873134328358
[2m[36m(func pid=103554)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=103554)[0m f1_macro: 0.08119101006401655
[2m[36m(func pid=103554)[0m f1_weighted: 0.056453014179252144
[2m[36m(func pid=103554)[0m f1_per_class: [0.116, 0.289, 0.4, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.2635261194029851
[2m[36m(func pid=103465)[0m top5: 0.8064365671641791
[2m[36m(func pid=103465)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=103465)[0m f1_macro: 0.16857464138605838
[2m[36m(func pid=103465)[0m f1_weighted: 0.1901672196037848
[2m[36m(func pid=103465)[0m f1_per_class: [0.181, 0.465, 0.0, 0.157, 0.05, 0.321, 0.021, 0.29, 0.0, 0.2]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3694029850746269
[2m[36m(func pid=97361)[0m top5: 0.9001865671641791
[2m[36m(func pid=97361)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=97361)[0m f1_macro: 0.3156111462756453
[2m[36m(func pid=97361)[0m f1_weighted: 0.3895031068594094
[2m[36m(func pid=97361)[0m f1_per_class: [0.354, 0.471, 0.444, 0.479, 0.09, 0.155, 0.394, 0.324, 0.234, 0.211]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0957 | Steps: 4 | Val loss: 1.7497 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.8952 | Steps: 4 | Val loss: 4.8067 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6799 | Steps: 4 | Val loss: 2.2983 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6981 | Steps: 4 | Val loss: 1.7538 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:57:57 (running for 00:14:34.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.22175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.688 |      0.316 |                   73 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.096 |      0.387 |                   74 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  4.041 |      0.169 |                   48 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.892 |      0.081 |                   48 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4085820895522388
[2m[36m(func pid=97446)[0m top5: 0.8852611940298507
[2m[36m(func pid=97446)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=97446)[0m f1_macro: 0.3873384811819362
[2m[36m(func pid=97446)[0m f1_weighted: 0.398074892385848
[2m[36m(func pid=97446)[0m f1_per_class: [0.519, 0.61, 0.5, 0.537, 0.122, 0.195, 0.224, 0.377, 0.397, 0.392]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.14598880597014927
[2m[36m(func pid=103465)[0m top5: 0.8199626865671642
[2m[36m(func pid=103465)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=103465)[0m f1_macro: 0.12237465112353185
[2m[36m(func pid=103465)[0m f1_weighted: 0.12056740586600229
[2m[36m(func pid=103465)[0m f1_per_class: [0.096, 0.298, 0.0, 0.044, 0.055, 0.245, 0.021, 0.324, 0.0, 0.141]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.16837686567164178
[2m[36m(func pid=103554)[0m top5: 0.5722947761194029
[2m[36m(func pid=103554)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=103554)[0m f1_macro: 0.05368825639505224
[2m[36m(func pid=103554)[0m f1_weighted: 0.053146579946330354
[2m[36m(func pid=103554)[0m f1_per_class: [0.137, 0.289, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97361)[0m top1: 0.37826492537313433
[2m[36m(func pid=97361)[0m top5: 0.9071828358208955
[2m[36m(func pid=97361)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=97361)[0m f1_macro: 0.33427788496081356
[2m[36m(func pid=97361)[0m f1_weighted: 0.39568297492851195
[2m[36m(func pid=97361)[0m f1_per_class: [0.444, 0.48, 0.533, 0.504, 0.094, 0.166, 0.378, 0.322, 0.204, 0.218]
[2m[36m(func pid=97361)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0282 | Steps: 4 | Val loss: 1.6661 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.8548 | Steps: 4 | Val loss: 2.8843 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.8084 | Steps: 4 | Val loss: 2.2613 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=97361)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6821 | Steps: 4 | Val loss: 1.7384 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:58:03 (running for 00:14:39.99)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.392
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00004 | RUNNING    | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.698 |      0.334 |                   74 |
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.028 |      0.392 |                   75 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.895 |      0.122 |                   49 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.68  |      0.054 |                   49 |
| train_5806f_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4295708955223881
[2m[36m(func pid=97446)[0m top5: 0.9039179104477612
[2m[36m(func pid=97446)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=97446)[0m f1_macro: 0.3924360574187088
[2m[36m(func pid=97446)[0m f1_weighted: 0.43173918256804344
[2m[36m(func pid=97446)[0m f1_per_class: [0.526, 0.631, 0.462, 0.576, 0.117, 0.202, 0.291, 0.371, 0.369, 0.379]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.05083955223880597
[2m[36m(func pid=103554)[0m top5: 0.5676305970149254
[2m[36m(func pid=103554)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=103554)[0m f1_macro: 0.06030307453245193
[2m[36m(func pid=103554)[0m f1_weighted: 0.039977402564845846
[2m[36m(func pid=103554)[0m f1_per_class: [0.017, 0.217, 0.368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.2621268656716418
[2m[36m(func pid=103465)[0m top5: 0.8348880597014925
[2m[36m(func pid=103465)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=103465)[0m f1_macro: 0.17986304452018378
[2m[36m(func pid=103465)[0m f1_weighted: 0.23652765282886978
[2m[36m(func pid=103465)[0m f1_per_class: [0.158, 0.475, 0.0, 0.159, 0.073, 0.255, 0.179, 0.399, 0.0, 0.101]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=97361)[0m top1: 0.3829291044776119
[2m[36m(func pid=97361)[0m top5: 0.9085820895522388
[2m[36m(func pid=97361)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=97361)[0m f1_macro: 0.3363061015729354
[2m[36m(func pid=97361)[0m f1_weighted: 0.39446277910494304
[2m[36m(func pid=97361)[0m f1_per_class: [0.489, 0.494, 0.5, 0.498, 0.103, 0.15, 0.373, 0.327, 0.208, 0.22]
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.9914 | Steps: 4 | Val loss: 2.3067 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0423 | Steps: 4 | Val loss: 1.7166 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.1147 | Steps: 4 | Val loss: 2.6353 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=103465)[0m top1: 0.324160447761194
[2m[36m(func pid=103465)[0m top5: 0.8125
[2m[36m(func pid=103465)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=103465)[0m f1_macro: 0.28462348668241244
[2m[36m(func pid=103465)[0m f1_weighted: 0.3038915036550777
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.453, 0.645, 0.047, 0.084, 0.309, 0.448, 0.554, 0.185, 0.122]
[2m[36m(func pid=103554)[0m top1: 0.0480410447761194
[2m[36m(func pid=103554)[0m top5: 0.6091417910447762
[2m[36m(func pid=103554)[0m f1_micro: 0.0480410447761194
[2m[36m(func pid=103554)[0m f1_macro: 0.05687443965289527
[2m[36m(func pid=103554)[0m f1_weighted: 0.03660286901790174
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.193, 0.348, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.4193097014925373
[2m[36m(func pid=97446)[0m top5: 0.8936567164179104
[2m[36m(func pid=97446)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=97446)[0m f1_macro: 0.3786606887176354
[2m[36m(func pid=97446)[0m f1_weighted: 0.4257358849366199
[2m[36m(func pid=97446)[0m f1_per_class: [0.507, 0.625, 0.462, 0.557, 0.101, 0.19, 0.302, 0.381, 0.343, 0.319]
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.2108 | Steps: 4 | Val loss: 2.2878 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:58:08 (running for 00:14:45.40)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.028 |      0.392 |                   75 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.855 |      0.18  |                   50 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.991 |      0.057 |                   51 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 13:58:13 (running for 00:14:50.45)
Memory usage on this node: 23.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.028 |      0.392 |                   75 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.115 |      0.285 |                   51 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.991 |      0.057 |                   51 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=116143)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=116143)[0m Configuration completed!
[2m[36m(func pid=116143)[0m New optimizer parameters:
[2m[36m(func pid=116143)[0m SGD (
[2m[36m(func pid=116143)[0m Parameter Group 0
[2m[36m(func pid=116143)[0m     dampening: 0
[2m[36m(func pid=116143)[0m     differentiable: False
[2m[36m(func pid=116143)[0m     foreach: None
[2m[36m(func pid=116143)[0m     lr: 0.0001
[2m[36m(func pid=116143)[0m     maximize: False
[2m[36m(func pid=116143)[0m     momentum: 0.99
[2m[36m(func pid=116143)[0m     nesterov: False
[2m[36m(func pid=116143)[0m     weight_decay: 0.0001
[2m[36m(func pid=116143)[0m )
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=103554)[0m top1: 0.037779850746268655
[2m[36m(func pid=103554)[0m top5: 0.48134328358208955
[2m[36m(func pid=103554)[0m f1_micro: 0.037779850746268655
[2m[36m(func pid=103554)[0m f1_macro: 0.05415917948132513
[2m[36m(func pid=103554)[0m f1_weighted: 0.03139879527358898
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.152, 0.356, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0593 | Steps: 4 | Val loss: 1.7165 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.4386 | Steps: 4 | Val loss: 3.2111 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7004 | Steps: 4 | Val loss: 2.2423 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0018 | Steps: 4 | Val loss: 2.5382 | Batch size: 32 | lr: 0.0001 | Duration: 4.92s
== Status ==
Current time: 2024-01-07 13:58:19 (running for 00:14:55.91)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.042 |      0.379 |                   76 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.439 |      0.225 |                   52 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  3.211 |      0.054 |                   52 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.41744402985074625
[2m[36m(func pid=97446)[0m top5: 0.9001865671641791
[2m[36m(func pid=97446)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=97446)[0m f1_macro: 0.3766096579334545
[2m[36m(func pid=97446)[0m f1_weighted: 0.4295779446934733
[2m[36m(func pid=97446)[0m f1_per_class: [0.529, 0.614, 0.49, 0.56, 0.096, 0.16, 0.335, 0.364, 0.326, 0.292]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.2947761194029851
[2m[36m(func pid=103465)[0m top5: 0.7751865671641791
[2m[36m(func pid=103465)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=103465)[0m f1_macro: 0.22532856594580894
[2m[36m(func pid=103465)[0m f1_weighted: 0.2770750751737791
[2m[36m(func pid=103465)[0m f1_per_class: [0.0, 0.412, 0.2, 0.0, 0.065, 0.269, 0.454, 0.533, 0.181, 0.14]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.13059701492537312
[2m[36m(func pid=103554)[0m top5: 0.6926305970149254
[2m[36m(func pid=103554)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=103554)[0m f1_macro: 0.07487822586050205
[2m[36m(func pid=103554)[0m f1_weighted: 0.060111404580451236
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.077, 0.364, 0.068, 0.0, 0.219, 0.0, 0.0, 0.022, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.06203358208955224
[2m[36m(func pid=116143)[0m top5: 0.4832089552238806
[2m[36m(func pid=116143)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=116143)[0m f1_macro: 0.03487923312159095
[2m[36m(func pid=116143)[0m f1_weighted: 0.03587815283484666
[2m[36m(func pid=116143)[0m f1_per_class: [0.078, 0.015, 0.0, 0.081, 0.0, 0.02, 0.0, 0.097, 0.024, 0.034]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.1620 | Steps: 4 | Val loss: 1.6775 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.9717 | Steps: 4 | Val loss: 3.2151 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.8910 | Steps: 4 | Val loss: 2.2171 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1106 | Steps: 4 | Val loss: 2.5520 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:58:24 (running for 00:15:01.20)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.162 |      0.392 |                   78 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.439 |      0.225 |                   52 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.7   |      0.075 |                   53 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  3.002 |      0.035 |                    1 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.43656716417910446
[2m[36m(func pid=97446)[0m top5: 0.9034514925373134
[2m[36m(func pid=97446)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=97446)[0m f1_macro: 0.39166784427096696
[2m[36m(func pid=97446)[0m f1_weighted: 0.4436409386172956
[2m[36m(func pid=97446)[0m f1_per_class: [0.576, 0.623, 0.48, 0.591, 0.1, 0.158, 0.341, 0.367, 0.345, 0.336]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.1609141791044776
[2m[36m(func pid=103465)[0m top5: 0.8073694029850746
[2m[36m(func pid=103465)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=103465)[0m f1_macro: 0.1525567776330159
[2m[36m(func pid=103465)[0m f1_weighted: 0.16316452758290997
[2m[36m(func pid=103465)[0m f1_per_class: [0.034, 0.294, 0.0, 0.007, 0.029, 0.238, 0.16, 0.462, 0.222, 0.08]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.15485074626865672
[2m[36m(func pid=103554)[0m top5: 0.6128731343283582
[2m[36m(func pid=103554)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=103554)[0m f1_macro: 0.08865330731563223
[2m[36m(func pid=103554)[0m f1_weighted: 0.09434664235348134
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.125, 0.375, 0.16, 0.0, 0.226, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.058768656716417914
[2m[36m(func pid=116143)[0m top5: 0.480410447761194
[2m[36m(func pid=116143)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=116143)[0m f1_macro: 0.03174577006342288
[2m[36m(func pid=116143)[0m f1_weighted: 0.03929757873119756
[2m[36m(func pid=116143)[0m f1_per_class: [0.051, 0.018, 0.0, 0.098, 0.0, 0.02, 0.0, 0.09, 0.0, 0.041]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0603 | Steps: 4 | Val loss: 1.5640 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.6413 | Steps: 4 | Val loss: 6.6352 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.8437 | Steps: 4 | Val loss: 2.2057 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9833 | Steps: 4 | Val loss: 2.5297 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 13:58:29 (running for 00:15:06.72)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.06  |      0.398 |                   79 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.972 |      0.153 |                   53 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.891 |      0.089 |                   54 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  3.111 |      0.032 |                    2 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.45382462686567165
[2m[36m(func pid=97446)[0m top5: 0.9277052238805971
[2m[36m(func pid=97446)[0m f1_micro: 0.45382462686567165
[2m[36m(func pid=97446)[0m f1_macro: 0.39772720614708285
[2m[36m(func pid=97446)[0m f1_weighted: 0.46566432431417076
[2m[36m(func pid=97446)[0m f1_per_class: [0.574, 0.575, 0.471, 0.611, 0.135, 0.227, 0.404, 0.336, 0.344, 0.302]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.16930970149253732
[2m[36m(func pid=103465)[0m top5: 0.7430037313432836
[2m[36m(func pid=103465)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=103465)[0m f1_macro: 0.1283801164805038
[2m[36m(func pid=103465)[0m f1_weighted: 0.1336566004931559
[2m[36m(func pid=103465)[0m f1_per_class: [0.197, 0.461, 0.0, 0.061, 0.027, 0.019, 0.022, 0.402, 0.0, 0.094]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.14085820895522388
[2m[36m(func pid=103554)[0m top5: 0.6291977611940298
[2m[36m(func pid=103554)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=103554)[0m f1_macro: 0.08316849924625078
[2m[36m(func pid=103554)[0m f1_weighted: 0.06446833861646423
[2m[36m(func pid=103554)[0m f1_per_class: [0.087, 0.147, 0.326, 0.026, 0.0, 0.246, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.07649253731343283
[2m[36m(func pid=116143)[0m top5: 0.4780783582089552
[2m[36m(func pid=116143)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=116143)[0m f1_macro: 0.04452859310988161
[2m[36m(func pid=116143)[0m f1_weighted: 0.06543560227588745
[2m[36m(func pid=116143)[0m f1_per_class: [0.043, 0.056, 0.0, 0.165, 0.0, 0.013, 0.0, 0.102, 0.024, 0.041]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3910 | Steps: 4 | Val loss: 1.5945 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.5046 | Steps: 4 | Val loss: 1.7377 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4692 | Steps: 4 | Val loss: 2.1687 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9031 | Steps: 4 | Val loss: 2.4819 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 13:58:35 (running for 00:15:12.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.391 |      0.384 |                   80 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.641 |      0.128 |                   54 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.844 |      0.083 |                   55 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.983 |      0.045 |                    3 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4361007462686567
[2m[36m(func pid=97446)[0m top5: 0.925839552238806
[2m[36m(func pid=97446)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=97446)[0m f1_macro: 0.38417412677739327
[2m[36m(func pid=97446)[0m f1_weighted: 0.4471085076047975
[2m[36m(func pid=97446)[0m f1_per_class: [0.569, 0.491, 0.471, 0.612, 0.127, 0.242, 0.39, 0.325, 0.315, 0.302]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.4458955223880597
[2m[36m(func pid=103465)[0m top5: 0.8931902985074627
[2m[36m(func pid=103465)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=103465)[0m f1_macro: 0.33774969102017927
[2m[36m(func pid=103465)[0m f1_weighted: 0.45045679264170907
[2m[36m(func pid=103465)[0m f1_per_class: [0.209, 0.519, 0.593, 0.475, 0.084, 0.222, 0.541, 0.541, 0.0, 0.194]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.13199626865671643
[2m[36m(func pid=103554)[0m top5: 0.6707089552238806
[2m[36m(func pid=103554)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=103554)[0m f1_macro: 0.0788049401548057
[2m[36m(func pid=103554)[0m f1_weighted: 0.0553595831326603
[2m[36m(func pid=103554)[0m f1_per_class: [0.071, 0.096, 0.326, 0.01, 0.0, 0.285, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.08675373134328358
[2m[36m(func pid=116143)[0m top5: 0.4939365671641791
[2m[36m(func pid=116143)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=116143)[0m f1_macro: 0.053404682532036694
[2m[36m(func pid=116143)[0m f1_weighted: 0.08460585753185296
[2m[36m(func pid=116143)[0m f1_per_class: [0.043, 0.068, 0.0, 0.221, 0.0, 0.024, 0.0, 0.106, 0.038, 0.034]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0324 | Steps: 4 | Val loss: 1.5785 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4740 | Steps: 4 | Val loss: 1.9408 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.7617 | Steps: 4 | Val loss: 2.1505 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8147 | Steps: 4 | Val loss: 2.4541 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=97446)[0m top1: 0.43703358208955223
[2m[36m(func pid=97446)[0m top5: 0.9319029850746269
[2m[36m(func pid=97446)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=97446)[0m f1_macro: 0.39497847223555727
[2m[36m(func pid=97446)[0m f1_weighted: 0.44115477119685537
[2m[36m(func pid=97446)[0m f1_per_class: [0.559, 0.578, 0.471, 0.594, 0.149, 0.229, 0.336, 0.318, 0.353, 0.364]
[2m[36m(func pid=97446)[0m 
== Status ==
Current time: 2024-01-07 13:58:40 (running for 00:15:17.68)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.032 |      0.395 |                   81 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.505 |      0.338 |                   55 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.469 |      0.079 |                   56 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.903 |      0.053 |                    4 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103465)[0m top1: 0.3101679104477612
[2m[36m(func pid=103465)[0m top5: 0.9043843283582089
[2m[36m(func pid=103465)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=103465)[0m f1_macro: 0.26368859233943054
[2m[36m(func pid=103465)[0m f1_weighted: 0.3409311171659448
[2m[36m(func pid=103465)[0m f1_per_class: [0.293, 0.311, 0.545, 0.433, 0.042, 0.128, 0.388, 0.419, 0.077, 0.0]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.12126865671641791
[2m[36m(func pid=103554)[0m top5: 0.7318097014925373
[2m[36m(func pid=103554)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=103554)[0m f1_macro: 0.06754142620693852
[2m[36m(func pid=103554)[0m f1_weighted: 0.04565274002467889
[2m[36m(func pid=103554)[0m f1_per_class: [0.064, 0.041, 0.258, 0.0, 0.0, 0.313, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.09888059701492537
[2m[36m(func pid=116143)[0m top5: 0.4976679104477612
[2m[36m(func pid=116143)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=116143)[0m f1_macro: 0.06450887120795416
[2m[36m(func pid=116143)[0m f1_weighted: 0.10160591679599199
[2m[36m(func pid=116143)[0m f1_per_class: [0.066, 0.12, 0.0, 0.243, 0.0, 0.04, 0.0, 0.085, 0.056, 0.034]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0295 | Steps: 4 | Val loss: 1.5972 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3702 | Steps: 4 | Val loss: 2.2917 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6775 | Steps: 4 | Val loss: 2.1719 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6782 | Steps: 4 | Val loss: 2.4115 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:58:45 (running for 00:15:22.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.029 |      0.392 |                   82 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.474 |      0.264 |                   56 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.762 |      0.068 |                   57 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.815 |      0.065 |                    5 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4221082089552239
[2m[36m(func pid=97446)[0m top5: 0.9356343283582089
[2m[36m(func pid=97446)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=97446)[0m f1_macro: 0.3924605969196325
[2m[36m(func pid=97446)[0m f1_weighted: 0.43567329856414877
[2m[36m(func pid=97446)[0m f1_per_class: [0.576, 0.588, 0.48, 0.568, 0.16, 0.217, 0.349, 0.286, 0.319, 0.383]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.22901119402985073
[2m[36m(func pid=103465)[0m top5: 0.8843283582089553
[2m[36m(func pid=103465)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=103465)[0m f1_macro: 0.22705896505686535
[2m[36m(func pid=103465)[0m f1_weighted: 0.23934956213347036
[2m[36m(func pid=103465)[0m f1_per_class: [0.275, 0.426, 0.343, 0.159, 0.046, 0.246, 0.202, 0.32, 0.187, 0.067]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.13199626865671643
[2m[36m(func pid=103554)[0m top5: 0.6968283582089553
[2m[36m(func pid=103554)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=103554)[0m f1_macro: 0.08408953560671953
[2m[36m(func pid=103554)[0m f1_weighted: 0.05278381053017813
[2m[36m(func pid=103554)[0m f1_per_class: [0.08, 0.112, 0.391, 0.0, 0.0, 0.258, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.1044776119402985
[2m[36m(func pid=116143)[0m top5: 0.503731343283582
[2m[36m(func pid=116143)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=116143)[0m f1_macro: 0.08283865151956896
[2m[36m(func pid=116143)[0m f1_weighted: 0.10936150114108843
[2m[36m(func pid=116143)[0m f1_per_class: [0.085, 0.145, 0.138, 0.24, 0.0, 0.065, 0.006, 0.06, 0.052, 0.037]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0309 | Steps: 4 | Val loss: 1.5784 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.7652 | Steps: 4 | Val loss: 4.1563 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7007 | Steps: 4 | Val loss: 2.3580 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6461 | Steps: 4 | Val loss: 2.3796 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:58:51 (running for 00:15:28.08)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.031 |      0.399 |                   83 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.37  |      0.227 |                   57 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.677 |      0.084 |                   58 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.678 |      0.083 |                    6 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4314365671641791
[2m[36m(func pid=97446)[0m top5: 0.9375
[2m[36m(func pid=97446)[0m f1_micro: 0.4314365671641791
[2m[36m(func pid=97446)[0m f1_macro: 0.39949095009006286
[2m[36m(func pid=97446)[0m f1_weighted: 0.4409856045840524
[2m[36m(func pid=97446)[0m f1_per_class: [0.583, 0.604, 0.5, 0.572, 0.187, 0.192, 0.359, 0.286, 0.338, 0.374]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.30363805970149255
[2m[36m(func pid=103465)[0m top5: 0.8488805970149254
[2m[36m(func pid=103465)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=103465)[0m f1_macro: 0.2350219982533775
[2m[36m(func pid=103465)[0m f1_weighted: 0.2769812175771244
[2m[36m(func pid=103465)[0m f1_per_class: [0.207, 0.516, 0.0, 0.029, 0.077, 0.36, 0.321, 0.527, 0.202, 0.111]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.134794776119403
[2m[36m(func pid=103554)[0m top5: 0.7080223880597015
[2m[36m(func pid=103554)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=103554)[0m f1_macro: 0.07509884106835277
[2m[36m(func pid=103554)[0m f1_weighted: 0.05812323648906759
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.131, 0.326, 0.023, 0.0, 0.224, 0.0, 0.0, 0.047, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.1021455223880597
[2m[36m(func pid=116143)[0m top5: 0.5149253731343284
[2m[36m(func pid=116143)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=116143)[0m f1_macro: 0.08670006366424834
[2m[36m(func pid=116143)[0m f1_weighted: 0.10769007333646455
[2m[36m(func pid=116143)[0m f1_per_class: [0.114, 0.147, 0.157, 0.217, 0.0, 0.071, 0.021, 0.024, 0.07, 0.046]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0499 | Steps: 4 | Val loss: 1.6170 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.5010 | Steps: 4 | Val loss: 3.9228 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6930 | Steps: 4 | Val loss: 2.3413 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3288 | Steps: 4 | Val loss: 2.3479 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=97446)[0m top1: 0.41744402985074625
[2m[36m(func pid=97446)[0m top5: 0.9365671641791045
[2m[36m(func pid=97446)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=97446)[0m f1_macro: 0.3974400801934464
[2m[36m(func pid=97446)[0m f1_weighted: 0.43173297116665116
[2m[36m(func pid=97446)[0m f1_per_class: [0.606, 0.603, 0.533, 0.545, 0.19, 0.199, 0.355, 0.27, 0.315, 0.358]
== Status ==
Current time: 2024-01-07 13:58:56 (running for 00:15:33.29)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.05  |      0.397 |                   84 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.765 |      0.235 |                   58 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.701 |      0.075 |                   59 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.646 |      0.087 |                    7 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.33255597014925375
[2m[36m(func pid=103465)[0m top5: 0.8722014925373134
[2m[36m(func pid=103465)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=103465)[0m f1_macro: 0.21465869056653303
[2m[36m(func pid=103465)[0m f1_weighted: 0.3004186194754349
[2m[36m(func pid=103465)[0m f1_per_class: [0.26, 0.509, 0.115, 0.134, 0.0, 0.367, 0.4, 0.016, 0.139, 0.205]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m top1: 0.1417910447761194
[2m[36m(func pid=103554)[0m top5: 0.7350746268656716
[2m[36m(func pid=103554)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=103554)[0m f1_macro: 0.0709090415841079
[2m[36m(func pid=103554)[0m f1_weighted: 0.062043855813338065
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.195, 0.267, 0.0, 0.0, 0.23, 0.0, 0.0, 0.018, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.09561567164179105
[2m[36m(func pid=116143)[0m top5: 0.5410447761194029
[2m[36m(func pid=116143)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=116143)[0m f1_macro: 0.08505537103144653
[2m[36m(func pid=116143)[0m f1_weighted: 0.10436732201395094
[2m[36m(func pid=116143)[0m f1_per_class: [0.108, 0.132, 0.147, 0.177, 0.007, 0.077, 0.052, 0.027, 0.079, 0.043]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.1544 | Steps: 4 | Val loss: 1.6929 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.4557 | Steps: 4 | Val loss: 2.2397 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.4101 | Steps: 4 | Val loss: 3.6581 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2649 | Steps: 4 | Val loss: 2.3034 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:59:01 (running for 00:15:38.73)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.154 |      0.396 |                   85 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.501 |      0.215 |                   59 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.693 |      0.071 |                   60 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.329 |      0.085 |                    8 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.40205223880597013
[2m[36m(func pid=97446)[0m top5: 0.929570895522388
[2m[36m(func pid=97446)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=97446)[0m f1_macro: 0.39636186161954357
[2m[36m(func pid=97446)[0m f1_weighted: 0.40849793184414185
[2m[36m(func pid=97446)[0m f1_per_class: [0.61, 0.61, 0.533, 0.503, 0.187, 0.203, 0.306, 0.272, 0.339, 0.4]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.17537313432835822
[2m[36m(func pid=103554)[0m top5: 0.7313432835820896
[2m[36m(func pid=103554)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=103554)[0m f1_macro: 0.08978603430429824
[2m[36m(func pid=103554)[0m f1_weighted: 0.08392700852881843
[2m[36m(func pid=103554)[0m f1_per_class: [0.033, 0.296, 0.286, 0.0, 0.0, 0.262, 0.0, 0.0, 0.021, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.31296641791044777
[2m[36m(func pid=103465)[0m top5: 0.8493470149253731
[2m[36m(func pid=103465)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=103465)[0m f1_macro: 0.21227019842518766
[2m[36m(func pid=103465)[0m f1_weighted: 0.2861850931866179
[2m[36m(func pid=103465)[0m f1_per_class: [0.233, 0.512, 0.0, 0.41, 0.0, 0.252, 0.08, 0.329, 0.178, 0.129]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.11333955223880597
[2m[36m(func pid=116143)[0m top5: 0.5970149253731343
[2m[36m(func pid=116143)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=116143)[0m f1_macro: 0.09922186178682899
[2m[36m(func pid=116143)[0m f1_weighted: 0.12428999447916655
[2m[36m(func pid=116143)[0m f1_per_class: [0.128, 0.1, 0.158, 0.151, 0.034, 0.121, 0.141, 0.029, 0.096, 0.034]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0222 | Steps: 4 | Val loss: 1.6165 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1531 | Steps: 4 | Val loss: 3.9861 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.4464 | Steps: 4 | Val loss: 2.2312 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0765 | Steps: 4 | Val loss: 2.2740 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 13:59:07 (running for 00:15:44.20)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.022 |      0.409 |                   86 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.41  |      0.212 |                   60 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.456 |      0.09  |                   61 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.265 |      0.099 |                    9 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4221082089552239
[2m[36m(func pid=97446)[0m top5: 0.9379664179104478
[2m[36m(func pid=97446)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=97446)[0m f1_macro: 0.40874808920150174
[2m[36m(func pid=97446)[0m f1_weighted: 0.4338034767071879
[2m[36m(func pid=97446)[0m f1_per_class: [0.587, 0.601, 0.615, 0.52, 0.171, 0.199, 0.377, 0.29, 0.358, 0.368]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.12220149253731344
[2m[36m(func pid=103554)[0m top5: 0.6982276119402985
[2m[36m(func pid=103554)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=103554)[0m f1_macro: 0.04495998811520704
[2m[36m(func pid=103554)[0m f1_weighted: 0.04080786069483056
[2m[36m(func pid=103554)[0m f1_per_class: [0.078, 0.044, 0.054, 0.0, 0.0, 0.274, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103465)[0m top1: 0.19449626865671643
[2m[36m(func pid=103465)[0m top5: 0.6898320895522388
[2m[36m(func pid=103465)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=103465)[0m f1_macro: 0.13046511414570608
[2m[36m(func pid=103465)[0m f1_weighted: 0.17678770392490092
[2m[36m(func pid=103465)[0m f1_per_class: [0.194, 0.488, 0.065, 0.269, 0.0, 0.039, 0.0, 0.072, 0.124, 0.053]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=116143)[0m top1: 0.11753731343283583
[2m[36m(func pid=116143)[0m top5: 0.6291977611940298
[2m[36m(func pid=116143)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=116143)[0m f1_macro: 0.1016418041735069
[2m[36m(func pid=116143)[0m f1_weighted: 0.12775400821924562
[2m[36m(func pid=116143)[0m f1_per_class: [0.149, 0.087, 0.13, 0.126, 0.041, 0.129, 0.182, 0.015, 0.088, 0.07]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0684 | Steps: 4 | Val loss: 1.5475 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3992 | Steps: 4 | Val loss: 2.2214 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.7497 | Steps: 4 | Val loss: 2.9706 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0876 | Steps: 4 | Val loss: 2.2395 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:59:12 (running for 00:15:49.57)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.068 |      0.42  |                   87 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.153 |      0.13  |                   61 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.446 |      0.045 |                   62 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.076 |      0.102 |                   10 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.44029850746268656
[2m[36m(func pid=97446)[0m top5: 0.9482276119402985
[2m[36m(func pid=97446)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=97446)[0m f1_macro: 0.41965392528985035
[2m[36m(func pid=97446)[0m f1_weighted: 0.453431560860192
[2m[36m(func pid=97446)[0m f1_per_class: [0.593, 0.608, 0.571, 0.516, 0.193, 0.191, 0.446, 0.286, 0.342, 0.451]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.21735074626865672
[2m[36m(func pid=103554)[0m top5: 0.7033582089552238
[2m[36m(func pid=103554)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=103554)[0m f1_macro: 0.0932714713902911
[2m[36m(func pid=103554)[0m f1_weighted: 0.15292985496298286
[2m[36m(func pid=103554)[0m f1_per_class: [0.12, 0.0, 0.086, 0.416, 0.014, 0.297, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.23833955223880596
[2m[36m(func pid=103465)[0m top5: 0.8055037313432836
[2m[36m(func pid=103465)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=103465)[0m f1_macro: 0.1937586270059551
[2m[36m(func pid=103465)[0m f1_weighted: 0.19877973164479062
[2m[36m(func pid=103465)[0m f1_per_class: [0.145, 0.482, 0.108, 0.092, 0.052, 0.233, 0.093, 0.457, 0.098, 0.178]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.13619402985074627
[2m[36m(func pid=116143)[0m top5: 0.6571828358208955
[2m[36m(func pid=116143)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=116143)[0m f1_macro: 0.11195142356291976
[2m[36m(func pid=116143)[0m f1_weighted: 0.1419990040091717
[2m[36m(func pid=116143)[0m f1_per_class: [0.17, 0.1, 0.111, 0.11, 0.053, 0.171, 0.221, 0.0, 0.085, 0.098]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0516 | Steps: 4 | Val loss: 1.5417 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.5437 | Steps: 4 | Val loss: 2.1973 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.4985 | Steps: 4 | Val loss: 2.7976 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.8222 | Steps: 4 | Val loss: 2.2017 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 13:59:18 (running for 00:15:54.97)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.052 |      0.421 |                   88 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.75  |      0.194 |                   62 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.399 |      0.093 |                   63 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  2.088 |      0.112 |                   11 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4412313432835821
[2m[36m(func pid=97446)[0m top5: 0.9496268656716418
[2m[36m(func pid=97446)[0m f1_micro: 0.4412313432835821
[2m[36m(func pid=97446)[0m f1_macro: 0.4210783161217372
[2m[36m(func pid=97446)[0m f1_weighted: 0.4536264514876797
[2m[36m(func pid=97446)[0m f1_per_class: [0.579, 0.6, 0.632, 0.49, 0.187, 0.196, 0.474, 0.291, 0.329, 0.433]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.18470149253731344
[2m[36m(func pid=103554)[0m top5: 0.6851679104477612
[2m[36m(func pid=103554)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=103554)[0m f1_macro: 0.11552388215507856
[2m[36m(func pid=103554)[0m f1_weighted: 0.14832202846206508
[2m[36m(func pid=103554)[0m f1_per_class: [0.087, 0.0, 0.34, 0.383, 0.018, 0.328, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.18563432835820895
[2m[36m(func pid=103465)[0m top5: 0.8236940298507462
[2m[36m(func pid=103465)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=103465)[0m f1_macro: 0.18363954300196283
[2m[36m(func pid=103465)[0m f1_weighted: 0.16827940693671112
[2m[36m(func pid=103465)[0m f1_per_class: [0.211, 0.351, 0.263, 0.041, 0.044, 0.0, 0.202, 0.369, 0.206, 0.15]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.15764925373134328
[2m[36m(func pid=116143)[0m top5: 0.6893656716417911
[2m[36m(func pid=116143)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=116143)[0m f1_macro: 0.12888300256855728
[2m[36m(func pid=116143)[0m f1_weighted: 0.16718908357152845
[2m[36m(func pid=116143)[0m f1_per_class: [0.211, 0.116, 0.096, 0.109, 0.049, 0.177, 0.29, 0.0, 0.086, 0.154]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0292 | Steps: 4 | Val loss: 1.5802 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.5186 | Steps: 4 | Val loss: 2.1011 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 3.1897 | Steps: 4 | Val loss: 2.8947 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.7752 | Steps: 4 | Val loss: 2.1447 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=97446)[0m top1: 0.425839552238806
[2m[36m(func pid=97446)[0m top5: 0.9458955223880597
[2m[36m(func pid=97446)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=97446)[0m f1_macro: 0.40727069798619847
[2m[36m(func pid=97446)[0m f1_weighted: 0.44101047185343073
[2m[36m(func pid=97446)[0m f1_per_class: [0.571, 0.587, 0.615, 0.5, 0.157, 0.142, 0.454, 0.288, 0.325, 0.433]
[2m[36m(func pid=97446)[0m 
== Status ==
Current time: 2024-01-07 13:59:23 (running for 00:16:00.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.029 |      0.407 |                   89 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.499 |      0.184 |                   63 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.544 |      0.116 |                   64 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.822 |      0.129 |                   12 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.3255597014925373
[2m[36m(func pid=103554)[0m top5: 0.7518656716417911
[2m[36m(func pid=103554)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=103554)[0m f1_macro: 0.14869866957482186
[2m[36m(func pid=103554)[0m f1_weighted: 0.30167279604297353
[2m[36m(func pid=103554)[0m f1_per_class: [0.085, 0.0, 0.333, 0.501, 0.038, 0.0, 0.529, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.19682835820895522
[2m[36m(func pid=103465)[0m top5: 0.8404850746268657
[2m[36m(func pid=103465)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=103465)[0m f1_macro: 0.18422292141098326
[2m[36m(func pid=103465)[0m f1_weighted: 0.19379399040326542
[2m[36m(func pid=103465)[0m f1_per_class: [0.185, 0.33, 0.118, 0.158, 0.046, 0.0, 0.183, 0.415, 0.221, 0.185]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.1875
[2m[36m(func pid=116143)[0m top5: 0.7112873134328358
[2m[36m(func pid=116143)[0m f1_micro: 0.1875
[2m[36m(func pid=116143)[0m f1_macro: 0.13990692139979324
[2m[36m(func pid=116143)[0m f1_weighted: 0.19519693132894353
[2m[36m(func pid=116143)[0m f1_per_class: [0.234, 0.138, 0.081, 0.096, 0.056, 0.165, 0.387, 0.0, 0.096, 0.146]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.1111 | Steps: 4 | Val loss: 1.5547 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.3030 | Steps: 4 | Val loss: 2.0524 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6788 | Steps: 4 | Val loss: 2.7554 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.5358 | Steps: 4 | Val loss: 2.0991 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:59:29 (running for 00:16:06.15)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.111 |      0.408 |                   90 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  3.19  |      0.184 |                   64 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.519 |      0.149 |                   65 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.775 |      0.14  |                   13 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4430970149253731
[2m[36m(func pid=97446)[0m top5: 0.9426305970149254
[2m[36m(func pid=97446)[0m f1_micro: 0.4430970149253731
[2m[36m(func pid=97446)[0m f1_macro: 0.4082746346760547
[2m[36m(func pid=97446)[0m f1_weighted: 0.456402795411156
[2m[36m(func pid=97446)[0m f1_per_class: [0.557, 0.585, 0.558, 0.5, 0.147, 0.162, 0.493, 0.326, 0.329, 0.424]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.36847014925373134
[2m[36m(func pid=103554)[0m top5: 0.7453358208955224
[2m[36m(func pid=103554)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=103554)[0m f1_macro: 0.14558257460191343
[2m[36m(func pid=103554)[0m f1_weighted: 0.31242977498598284
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.005, 0.333, 0.523, 0.046, 0.0, 0.548, 0.0, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.27472014925373134
[2m[36m(func pid=103465)[0m top5: 0.8530783582089553
[2m[36m(func pid=103465)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=103465)[0m f1_macro: 0.2513097050712298
[2m[36m(func pid=103465)[0m f1_weighted: 0.25074892844752744
[2m[36m(func pid=103465)[0m f1_per_class: [0.138, 0.441, 0.108, 0.256, 0.177, 0.393, 0.063, 0.445, 0.166, 0.327]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.2103544776119403
[2m[36m(func pid=116143)[0m top5: 0.7448694029850746
[2m[36m(func pid=116143)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=116143)[0m f1_macro: 0.1560731772777057
[2m[36m(func pid=116143)[0m f1_weighted: 0.21973043929735878
[2m[36m(func pid=116143)[0m f1_per_class: [0.328, 0.152, 0.058, 0.081, 0.053, 0.151, 0.467, 0.045, 0.09, 0.136]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0284 | Steps: 4 | Val loss: 1.6052 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6287 | Steps: 4 | Val loss: 2.0723 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9569 | Steps: 4 | Val loss: 2.7853 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.2623 | Steps: 4 | Val loss: 2.0411 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:59:34 (running for 00:16:11.49)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.028 |      0.407 |                   91 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.679 |      0.251 |                   65 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.303 |      0.146 |                   66 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.536 |      0.156 |                   14 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.44169776119402987
[2m[36m(func pid=97446)[0m top5: 0.9361007462686567
[2m[36m(func pid=97446)[0m f1_micro: 0.4416977611940298
[2m[36m(func pid=97446)[0m f1_macro: 0.4069953239828119
[2m[36m(func pid=97446)[0m f1_weighted: 0.4617484086037003
[2m[36m(func pid=97446)[0m f1_per_class: [0.541, 0.571, 0.615, 0.482, 0.127, 0.193, 0.533, 0.327, 0.26, 0.42]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.3064365671641791
[2m[36m(func pid=103554)[0m top5: 0.7691231343283582
[2m[36m(func pid=103554)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=103554)[0m f1_macro: 0.1255519811493307
[2m[36m(func pid=103554)[0m f1_weighted: 0.23154975732406813
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.368, 0.276, 0.0, 0.0, 0.0, 0.552, 0.0, 0.06, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.29244402985074625
[2m[36m(func pid=103465)[0m top5: 0.8913246268656716
[2m[36m(func pid=103465)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=103465)[0m f1_macro: 0.24287148901392341
[2m[36m(func pid=103465)[0m f1_weighted: 0.3363095314165769
[2m[36m(func pid=103465)[0m f1_per_class: [0.158, 0.402, 0.062, 0.328, 0.0, 0.279, 0.345, 0.487, 0.223, 0.144]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.23740671641791045
[2m[36m(func pid=116143)[0m top5: 0.7798507462686567
[2m[36m(func pid=116143)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=116143)[0m f1_macro: 0.17863554234798334
[2m[36m(func pid=116143)[0m f1_weighted: 0.2546910737423116
[2m[36m(func pid=116143)[0m f1_per_class: [0.362, 0.203, 0.05, 0.129, 0.066, 0.136, 0.507, 0.057, 0.106, 0.17]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0346 | Steps: 4 | Val loss: 1.6386 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.4179 | Steps: 4 | Val loss: 2.1417 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4916 | Steps: 4 | Val loss: 5.1287 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.3524 | Steps: 4 | Val loss: 1.9717 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 13:59:39 (running for 00:16:16.80)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.035 |      0.402 |                   92 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.957 |      0.243 |                   66 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.629 |      0.126 |                   67 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.262 |      0.179 |                   15 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.42117537313432835
[2m[36m(func pid=97446)[0m top5: 0.9300373134328358
[2m[36m(func pid=97446)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=97446)[0m f1_macro: 0.40208677095268214
[2m[36m(func pid=97446)[0m f1_weighted: 0.44298478568252575
[2m[36m(func pid=97446)[0m f1_per_class: [0.571, 0.566, 0.6, 0.488, 0.138, 0.18, 0.475, 0.312, 0.229, 0.462]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.31343283582089554
[2m[36m(func pid=103554)[0m top5: 0.7388059701492538
[2m[36m(func pid=103554)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=103554)[0m f1_macro: 0.12735551681482898
[2m[36m(func pid=103554)[0m f1_weighted: 0.2280172439227657
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.358, 0.304, 0.0, 0.0, 0.0, 0.545, 0.0, 0.066, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.23694029850746268
[2m[36m(func pid=103465)[0m top5: 0.8013059701492538
[2m[36m(func pid=103465)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=103465)[0m f1_macro: 0.19029889044106585
[2m[36m(func pid=103465)[0m f1_weighted: 0.2986405149560221
[2m[36m(func pid=103465)[0m f1_per_class: [0.216, 0.279, 0.096, 0.421, 0.0, 0.043, 0.317, 0.492, 0.0, 0.04]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.28218283582089554
[2m[36m(func pid=116143)[0m top5: 0.8138992537313433
[2m[36m(func pid=116143)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=116143)[0m f1_macro: 0.21294098445175552
[2m[36m(func pid=116143)[0m f1_weighted: 0.30337508069499924
[2m[36m(func pid=116143)[0m f1_per_class: [0.396, 0.316, 0.059, 0.181, 0.064, 0.139, 0.536, 0.129, 0.137, 0.172]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0462 | Steps: 4 | Val loss: 1.6097 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2898 | Steps: 4 | Val loss: 2.1281 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6586 | Steps: 4 | Val loss: 5.6911 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.3762 | Steps: 4 | Val loss: 1.8983 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 13:59:45 (running for 00:16:22.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.046 |      0.402 |                   93 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.492 |      0.19  |                   67 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.418 |      0.127 |                   68 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.352 |      0.213 |                   16 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.43097014925373134
[2m[36m(func pid=97446)[0m top5: 0.9286380597014925
[2m[36m(func pid=97446)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=97446)[0m f1_macro: 0.40179697481659227
[2m[36m(func pid=97446)[0m f1_weighted: 0.45749217552968097
[2m[36m(func pid=97446)[0m f1_per_class: [0.545, 0.529, 0.571, 0.502, 0.139, 0.188, 0.532, 0.311, 0.222, 0.479]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.3125
[2m[36m(func pid=103554)[0m top5: 0.7285447761194029
[2m[36m(func pid=103554)[0m f1_micro: 0.3125
[2m[36m(func pid=103554)[0m f1_macro: 0.13050338365397793
[2m[36m(func pid=103554)[0m f1_weighted: 0.2318445586737178
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.376, 0.324, 0.0, 0.0, 0.0, 0.548, 0.0, 0.057, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.20662313432835822
[2m[36m(func pid=103465)[0m top5: 0.8031716417910447
[2m[36m(func pid=103465)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=103465)[0m f1_macro: 0.1493269632328989
[2m[36m(func pid=103465)[0m f1_weighted: 0.24388298930630173
[2m[36m(func pid=103465)[0m f1_per_class: [0.171, 0.308, 0.124, 0.476, 0.086, 0.0, 0.148, 0.148, 0.0, 0.032]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.30830223880597013
[2m[36m(func pid=116143)[0m top5: 0.8582089552238806
[2m[36m(func pid=116143)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=116143)[0m f1_macro: 0.2445779315227961
[2m[36m(func pid=116143)[0m f1_weighted: 0.33815378551472874
[2m[36m(func pid=116143)[0m f1_per_class: [0.429, 0.363, 0.064, 0.234, 0.074, 0.155, 0.544, 0.233, 0.154, 0.197]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.5189 | Steps: 4 | Val loss: 2.1457 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0224 | Steps: 4 | Val loss: 1.6355 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.4929 | Steps: 4 | Val loss: 2.3140 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.1613 | Steps: 4 | Val loss: 1.8264 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:59:50 (running for 00:16:27.72)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.046 |      0.402 |                   93 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.659 |      0.149 |                   68 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.519 |      0.117 |                   70 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.376 |      0.245 |                   17 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4300373134328358
[2m[36m(func pid=97446)[0m top5: 0.9244402985074627
[2m[36m(func pid=97446)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=97446)[0m f1_macro: 0.40110181403626666
[2m[36m(func pid=97446)[0m f1_weighted: 0.4558326734737313
[2m[36m(func pid=97446)[0m f1_per_class: [0.528, 0.52, 0.632, 0.508, 0.133, 0.136, 0.547, 0.31, 0.213, 0.486]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.27238805970149255
[2m[36m(func pid=103554)[0m top5: 0.7294776119402985
[2m[36m(func pid=103554)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=103554)[0m f1_macro: 0.11728187062846909
[2m[36m(func pid=103554)[0m f1_weighted: 0.22481808628054745
[2m[36m(func pid=103554)[0m f1_per_class: [0.02, 0.338, 0.222, 0.0, 0.0, 0.0, 0.552, 0.0, 0.0, 0.041]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.32975746268656714
[2m[36m(func pid=103465)[0m top5: 0.8726679104477612
[2m[36m(func pid=103465)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=103465)[0m f1_macro: 0.24898320837894383
[2m[36m(func pid=103465)[0m f1_weighted: 0.3202242068594645
[2m[36m(func pid=103465)[0m f1_per_class: [0.267, 0.533, 0.156, 0.459, 0.116, 0.0, 0.19, 0.52, 0.173, 0.078]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.353544776119403
[2m[36m(func pid=116143)[0m top5: 0.8824626865671642
[2m[36m(func pid=116143)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=116143)[0m f1_macro: 0.2819464900675732
[2m[36m(func pid=116143)[0m f1_weighted: 0.384687278215802
[2m[36m(func pid=116143)[0m f1_per_class: [0.494, 0.462, 0.073, 0.297, 0.09, 0.215, 0.545, 0.281, 0.176, 0.188]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.8258 | Steps: 4 | Val loss: 2.1055 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0467 | Steps: 4 | Val loss: 1.6854 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.5395 | Steps: 4 | Val loss: 2.9299 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.1724 | Steps: 4 | Val loss: 1.7938 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:59:56 (running for 00:16:33.16)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.022 |      0.401 |                   94 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.493 |      0.249 |                   69 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.826 |      0.088 |                   71 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.161 |      0.282 |                   18 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.2178171641791045
[2m[36m(func pid=103554)[0m top5: 0.7448694029850746
[2m[36m(func pid=103554)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=103554)[0m f1_macro: 0.08842789752459491
[2m[36m(func pid=103554)[0m f1_weighted: 0.16946807579085255
[2m[36m(func pid=103554)[0m f1_per_class: [0.071, 0.005, 0.2, 0.0, 0.0, 0.0, 0.553, 0.0, 0.022, 0.034]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.42024253731343286
[2m[36m(func pid=97446)[0m top5: 0.9225746268656716
[2m[36m(func pid=97446)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=97446)[0m f1_macro: 0.39641316746240995
[2m[36m(func pid=97446)[0m f1_weighted: 0.44690487059634737
[2m[36m(func pid=97446)[0m f1_per_class: [0.529, 0.513, 0.632, 0.478, 0.135, 0.12, 0.551, 0.333, 0.204, 0.469]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.2513992537313433
[2m[36m(func pid=103465)[0m top5: 0.8922574626865671
[2m[36m(func pid=103465)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=103465)[0m f1_macro: 0.20091186897228605
[2m[36m(func pid=103465)[0m f1_weighted: 0.21905651196242268
[2m[36m(func pid=103465)[0m f1_per_class: [0.133, 0.455, 0.247, 0.327, 0.095, 0.173, 0.006, 0.274, 0.166, 0.133]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.3582089552238806
[2m[36m(func pid=116143)[0m top5: 0.898320895522388
[2m[36m(func pid=116143)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=116143)[0m f1_macro: 0.2954378085883359
[2m[36m(func pid=116143)[0m f1_weighted: 0.3961043880711603
[2m[36m(func pid=116143)[0m f1_per_class: [0.529, 0.491, 0.082, 0.377, 0.1, 0.186, 0.49, 0.334, 0.159, 0.208]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0206 | Steps: 4 | Val loss: 1.6331 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.4738 | Steps: 4 | Val loss: 1.9836 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.1693 | Steps: 4 | Val loss: 4.4708 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.0007 | Steps: 4 | Val loss: 1.7633 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:00:01 (running for 00:16:38.49)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.047 |      0.396 |                   95 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.539 |      0.201 |                   70 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.474 |      0.175 |                   72 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.172 |      0.295 |                   19 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103554)[0m top1: 0.36800373134328357
[2m[36m(func pid=103554)[0m top5: 0.7873134328358209
[2m[36m(func pid=103554)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=103554)[0m f1_macro: 0.17491357935957091
[2m[36m(func pid=103554)[0m f1_weighted: 0.32464996133491536
[2m[36m(func pid=103554)[0m f1_per_class: [0.025, 0.089, 0.25, 0.533, 0.0, 0.0, 0.461, 0.362, 0.0, 0.03]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=97446)[0m top1: 0.42490671641791045
[2m[36m(func pid=97446)[0m top5: 0.9272388059701493
[2m[36m(func pid=97446)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=97446)[0m f1_macro: 0.39619442604030997
[2m[36m(func pid=97446)[0m f1_weighted: 0.4531696835519392
[2m[36m(func pid=97446)[0m f1_per_class: [0.522, 0.516, 0.558, 0.483, 0.149, 0.187, 0.543, 0.331, 0.209, 0.466]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103465)[0m top1: 0.28125
[2m[36m(func pid=103465)[0m top5: 0.8442164179104478
[2m[36m(func pid=103465)[0m f1_micro: 0.28125
[2m[36m(func pid=103465)[0m f1_macro: 0.19867879085853296
[2m[36m(func pid=103465)[0m f1_weighted: 0.2822497485443131
[2m[36m(func pid=103465)[0m f1_per_class: [0.161, 0.348, 0.108, 0.362, 0.0, 0.333, 0.184, 0.379, 0.063, 0.049]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.36847014925373134
[2m[36m(func pid=116143)[0m top5: 0.9090485074626866
[2m[36m(func pid=116143)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=116143)[0m f1_macro: 0.296331878619399
[2m[36m(func pid=116143)[0m f1_weighted: 0.3998150738916741
[2m[36m(func pid=116143)[0m f1_per_class: [0.515, 0.502, 0.11, 0.427, 0.113, 0.152, 0.46, 0.352, 0.163, 0.169]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2463 | Steps: 4 | Val loss: 2.0014 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0100 | Steps: 4 | Val loss: 1.6009 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6092 | Steps: 4 | Val loss: 2.6011 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.7592 | Steps: 4 | Val loss: 1.7247 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 14:00:06 (running for 00:16:43.76)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.01  |      0.398 |                   97 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.169 |      0.199 |                   71 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.474 |      0.175 |                   72 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.001 |      0.296 |                   20 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4291044776119403
[2m[36m(func pid=97446)[0m top5: 0.9379664179104478
[2m[36m(func pid=97446)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=97446)[0m f1_macro: 0.3981231224823828
[2m[36m(func pid=97446)[0m f1_weighted: 0.45526825630945733
[2m[36m(func pid=97446)[0m f1_per_class: [0.55, 0.55, 0.533, 0.496, 0.142, 0.208, 0.506, 0.339, 0.226, 0.43]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.27472014925373134
[2m[36m(func pid=103554)[0m top5: 0.7943097014925373
[2m[36m(func pid=103554)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=103554)[0m f1_macro: 0.14219976732478967
[2m[36m(func pid=103554)[0m f1_weighted: 0.2203356929204937
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.427, 0.207, 0.119, 0.027, 0.0, 0.313, 0.329, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.36380597014925375
[2m[36m(func pid=103465)[0m top5: 0.8194962686567164
[2m[36m(func pid=103465)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=103465)[0m f1_macro: 0.2582667390445015
[2m[36m(func pid=103465)[0m f1_weighted: 0.3953214745186416
[2m[36m(func pid=103465)[0m f1_per_class: [0.171, 0.2, 0.269, 0.389, 0.0, 0.289, 0.611, 0.483, 0.11, 0.061]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.3675373134328358
[2m[36m(func pid=116143)[0m top5: 0.9090485074626866
[2m[36m(func pid=116143)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=116143)[0m f1_macro: 0.2962355660006159
[2m[36m(func pid=116143)[0m f1_weighted: 0.38160065996295023
[2m[36m(func pid=116143)[0m f1_per_class: [0.525, 0.514, 0.183, 0.475, 0.115, 0.112, 0.363, 0.326, 0.19, 0.16]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0228 | Steps: 4 | Val loss: 1.5879 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2876 | Steps: 4 | Val loss: 2.0511 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.7485 | Steps: 4 | Val loss: 2.1709 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7184 | Steps: 4 | Val loss: 1.6906 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:00:12 (running for 00:16:48.94)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.023 |      0.408 |                   98 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.609 |      0.258 |                   72 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.246 |      0.142 |                   73 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.759 |      0.296 |                   21 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=97446)[0m top1: 0.4361007462686567
[2m[36m(func pid=97446)[0m top5: 0.9398320895522388
[2m[36m(func pid=97446)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=97446)[0m f1_macro: 0.4080158271914649
[2m[36m(func pid=97446)[0m f1_weighted: 0.45814534144860297
[2m[36m(func pid=97446)[0m f1_per_class: [0.579, 0.572, 0.585, 0.508, 0.144, 0.18, 0.501, 0.316, 0.248, 0.448]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.09748134328358209
[2m[36m(func pid=103554)[0m top5: 0.7980410447761194
[2m[36m(func pid=103554)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=103554)[0m f1_macro: 0.08109758961354374
[2m[36m(func pid=103554)[0m f1_weighted: 0.070991742701422
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.141, 0.292, 0.0, 0.027, 0.0, 0.102, 0.249, 0.0, 0.0]
[2m[36m(func pid=103554)[0m 
[2m[36m(func pid=103465)[0m top1: 0.31156716417910446
[2m[36m(func pid=103465)[0m top5: 0.8498134328358209
[2m[36m(func pid=103465)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=103465)[0m f1_macro: 0.21305147187592527
[2m[36m(func pid=103465)[0m f1_weighted: 0.34695918767075207
[2m[36m(func pid=103465)[0m f1_per_class: [0.114, 0.363, 0.0, 0.353, 0.035, 0.194, 0.44, 0.432, 0.122, 0.077]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m top1: 0.3670708955223881
[2m[36m(func pid=116143)[0m top5: 0.9174440298507462
[2m[36m(func pid=116143)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=116143)[0m f1_macro: 0.3088344108412885
[2m[36m(func pid=116143)[0m f1_weighted: 0.3724650608175692
[2m[36m(func pid=116143)[0m f1_per_class: [0.569, 0.503, 0.3, 0.493, 0.119, 0.103, 0.321, 0.313, 0.198, 0.169]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0206 | Steps: 4 | Val loss: 1.6048 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=103554)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.4111 | Steps: 4 | Val loss: 2.1068 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.4180 | Steps: 4 | Val loss: 2.7221 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6821 | Steps: 4 | Val loss: 1.6636 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=97446)[0m top1: 0.43423507462686567
[2m[36m(func pid=97446)[0m top5: 0.9365671641791045
[2m[36m(func pid=97446)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=97446)[0m f1_macro: 0.413189956508268== Status ==
Current time: 2024-01-07 14:00:17 (running for 00:16:54.38)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00005 | RUNNING    | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.021 |      0.413 |                   99 |
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  1.749 |      0.213 |                   73 |
| train_5806f_00007 | RUNNING    | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.288 |      0.081 |                   74 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.718 |      0.309 |                   22 |
| train_5806f_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=97446)[0m f1_weighted: 0.45629911802267076
[2m[36m(func pid=97446)[0m f1_per_class: [0.569, 0.567, 0.615, 0.49, 0.146, 0.2, 0.505, 0.318, 0.256, 0.466]
[2m[36m(func pid=97446)[0m 
[2m[36m(func pid=103554)[0m top1: 0.08208955223880597
[2m[36m(func pid=103554)[0m top5: 0.7887126865671642
[2m[36m(func pid=103554)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=103554)[0m f1_macro: 0.06668335724645849
[2m[36m(func pid=103554)[0m f1_weighted: 0.05145663477202466
[2m[36m(func pid=103554)[0m f1_per_class: [0.0, 0.03, 0.25, 0.0, 0.024, 0.0, 0.098, 0.265, 0.0, 0.0]
[2m[36m(func pid=103465)[0m top1: 0.20335820895522388
[2m[36m(func pid=103465)[0m top5: 0.8582089552238806
[2m[36m(func pid=103465)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=103465)[0m f1_macro: 0.16649458258246647
[2m[36m(func pid=103465)[0m f1_weighted: 0.21722534618911918
[2m[36m(func pid=103465)[0m f1_per_class: [0.024, 0.397, 0.028, 0.26, 0.088, 0.111, 0.128, 0.311, 0.12, 0.197]
[2m[36m(func pid=116143)[0m top1: 0.36847014925373134
[2m[36m(func pid=116143)[0m top5: 0.9235074626865671
[2m[36m(func pid=116143)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=116143)[0m f1_macro: 0.3153539673569525
[2m[36m(func pid=116143)[0m f1_weighted: 0.3649754237563197
[2m[36m(func pid=116143)[0m f1_per_class: [0.585, 0.519, 0.369, 0.516, 0.129, 0.097, 0.266, 0.304, 0.195, 0.172]
[2m[36m(func pid=103465)[0m 
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=97446)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0155 | Steps: 4 | Val loss: 1.6110 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=103465)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.3366 | Steps: 4 | Val loss: 3.3190 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7224 | Steps: 4 | Val loss: 1.6350 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=97446)[0m top1: 0.42863805970149255
[2m[36m(func pid=97446)[0m top5: 0.9333022388059702
[2m[36m(func pid=97446)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=97446)[0m f1_macro: 0.40167398139597366
[2m[36m(func pid=97446)[0m f1_weighted: 0.45258616055521267
[2m[36m(func pid=97446)[0m f1_per_class: [0.533, 0.564, 0.571, 0.488, 0.17, 0.236, 0.49, 0.314, 0.236, 0.415]
[2m[36m(func pid=116143)[0m top1: 0.3628731343283582
[2m[36m(func pid=116143)[0m top5: 0.9253731343283582
[2m[36m(func pid=116143)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=116143)[0m f1_macro: 0.3184113665599062
[2m[36m(func pid=116143)[0m f1_weighted: 0.3477388422246297
[2m[36m(func pid=116143)[0m f1_per_class: [0.608, 0.51, 0.436, 0.539, 0.153, 0.09, 0.194, 0.288, 0.19, 0.175]
[2m[36m(func pid=103465)[0m top1: 0.1982276119402985
[2m[36m(func pid=103465)[0m top5: 0.8428171641791045
[2m[36m(func pid=103465)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=103465)[0m f1_macro: 0.17255580679332488
[2m[36m(func pid=103465)[0m f1_weighted: 0.2050445681016926
[2m[36m(func pid=103465)[0m f1_per_class: [0.138, 0.23, 0.026, 0.309, 0.137, 0.272, 0.082, 0.264, 0.061, 0.206]
== Status ==
Current time: 2024-01-07 14:00:22 (running for 00:16:59.79)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.364
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00006 | RUNNING    | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.418 |      0.166 |                   74 |
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.682 |      0.315 |                   23 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=121898)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=121898)[0m Configuration completed!
[2m[36m(func pid=121898)[0m New optimizer parameters:
[2m[36m(func pid=121898)[0m SGD (
[2m[36m(func pid=121898)[0m Parameter Group 0
[2m[36m(func pid=121898)[0m     dampening: 0
[2m[36m(func pid=121898)[0m     differentiable: False
[2m[36m(func pid=121898)[0m     foreach: None
[2m[36m(func pid=121898)[0m     lr: 0.001
[2m[36m(func pid=121898)[0m     maximize: False
[2m[36m(func pid=121898)[0m     momentum: 0.99
[2m[36m(func pid=121898)[0m     nesterov: False
[2m[36m(func pid=121898)[0m     weight_decay: 0.0001
[2m[36m(func pid=121898)[0m )
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6321 | Steps: 4 | Val loss: 1.6429 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1688 | Steps: 4 | Val loss: 2.4371 | Batch size: 32 | lr: 0.001 | Duration: 5.00s
[2m[36m(func pid=116143)[0m top1: 0.3582089552238806
[2m[36m(func pid=116143)[0m top5: 0.9197761194029851
[2m[36m(func pid=116143)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=116143)[0m f1_macro: 0.3094443879416401
[2m[36m(func pid=116143)[0m f1_weighted: 0.33107469070953066
[2m[36m(func pid=116143)[0m f1_per_class: [0.552, 0.525, 0.453, 0.535, 0.145, 0.089, 0.136, 0.293, 0.196, 0.169]
[2m[36m(func pid=121898)[0m top1: 0.08768656716417911
[2m[36m(func pid=121898)[0m top5: 0.49673507462686567
[2m[36m(func pid=121898)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=121898)[0m f1_macro: 0.06121556477656446
[2m[36m(func pid=121898)[0m f1_weighted: 0.07759510350386832
[2m[36m(func pid=121898)[0m f1_per_class: [0.138, 0.077, 0.0, 0.183, 0.0, 0.021, 0.0, 0.102, 0.056, 0.036]
[2m[36m(func pid=122441)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122441)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=122441)[0m Configuration completed!
[2m[36m(func pid=122441)[0m New optimizer parameters:
[2m[36m(func pid=122441)[0m SGD (
[2m[36m(func pid=122441)[0m Parameter Group 0
[2m[36m(func pid=122441)[0m     dampening: 0
[2m[36m(func pid=122441)[0m     differentiable: False
[2m[36m(func pid=122441)[0m     foreach: None
[2m[36m(func pid=122441)[0m     lr: 0.01
[2m[36m(func pid=122441)[0m     maximize: False
[2m[36m(func pid=122441)[0m     momentum: 0.99
[2m[36m(func pid=122441)[0m     nesterov: False
[2m[36m(func pid=122441)[0m     weight_decay: 0.0001
[2m[36m(func pid=122441)[0m )
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:00:28 (running for 00:17:05.02)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.722 |      0.318 |                   24 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=122462)[0m Dataloader to compute accuracy: val

[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122462)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=122462)[0m Configuration completed!
[2m[36m(func pid=122462)[0m New optimizer parameters:
[2m[36m(func pid=122462)[0m SGD (
[2m[36m(func pid=122462)[0m Parameter Group 0
[2m[36m(func pid=122462)[0m     dampening: 0
[2m[36m(func pid=122462)[0m     differentiable: False
[2m[36m(func pid=122462)[0m     foreach: None
[2m[36m(func pid=122462)[0m     lr: 0.1
[2m[36m(func pid=122462)[0m     maximize: False
[2m[36m(func pid=122462)[0m     momentum: 0.99
[2m[36m(func pid=122462)[0m     nesterov: False
[2m[36m(func pid=122462)[0m     weight_decay: 0.0001
[2m[36m(func pid=122462)[0m )
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:00:36 (running for 00:17:13.24)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.722 |      0.318 |                   24 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  3.169 |      0.061 |                    1 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6029 | Steps: 4 | Val loss: 1.6149 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7812 | Steps: 4 | Val loss: 2.3056 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1877 | Steps: 4 | Val loss: 2.3108 | Batch size: 32 | lr: 0.01 | Duration: 4.92s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.2391 | Steps: 4 | Val loss: 17.5741 | Batch size: 32 | lr: 0.1 | Duration: 4.48s
== Status ==
Current time: 2024-01-07 14:00:41 (running for 00:17:18.27)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.632 |      0.309 |                   25 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  3.169 |      0.061 |                    1 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3675373134328358
[2m[36m(func pid=116143)[0m top5: 0.925839552238806
[2m[36m(func pid=116143)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=116143)[0m f1_macro: 0.3277914924534912
[2m[36m(func pid=116143)[0m f1_weighted: 0.3363807161649049
[2m[36m(func pid=116143)[0m f1_per_class: [0.543, 0.527, 0.545, 0.547, 0.164, 0.139, 0.119, 0.3, 0.198, 0.195]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.14319029850746268
[2m[36m(func pid=121898)[0m top5: 0.5564365671641791
[2m[36m(func pid=121898)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=121898)[0m f1_macro: 0.1049878218697731
[2m[36m(func pid=121898)[0m f1_weighted: 0.14081141285127066
[2m[36m(func pid=121898)[0m f1_per_class: [0.138, 0.216, 0.137, 0.283, 0.008, 0.029, 0.027, 0.116, 0.079, 0.016]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.3111007462686567
[2m[36m(func pid=122441)[0m top5: 0.5513059701492538
[2m[36m(func pid=122441)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=122441)[0m f1_macro: 0.14315313616497882
[2m[36m(func pid=122441)[0m f1_weighted: 0.15392034235951185
[2m[36m(func pid=122441)[0m f1_per_class: [0.415, 0.0, 0.355, 0.0, 0.186, 0.0, 0.476, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m top1: 0.17210820895522388
[2m[36m(func pid=122462)[0m top5: 0.38619402985074625
[2m[36m(func pid=122462)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=122462)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=122462)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3759 | Steps: 4 | Val loss: 1.6371 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5424 | Steps: 4 | Val loss: 2.2299 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 21.7833 | Steps: 4 | Val loss: 304615328.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.1741 | Steps: 4 | Val loss: 3.1691 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 14:00:47 (running for 00:17:23.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.376 |      0.354 |                   27 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  2.781 |      0.105 |                    2 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.188 |      0.143 |                    1 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  6.239 |      0.029 |                    1 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3670708955223881
[2m[36m(func pid=116143)[0m top5: 0.9263059701492538
[2m[36m(func pid=116143)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=116143)[0m f1_macro: 0.3537247257058175
[2m[36m(func pid=116143)[0m f1_weighted: 0.34739074185226443
[2m[36m(func pid=116143)[0m f1_per_class: [0.53, 0.455, 0.686, 0.561, 0.204, 0.22, 0.142, 0.305, 0.263, 0.17]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.1515858208955224
[2m[36m(func pid=121898)[0m top5: 0.6529850746268657
[2m[36m(func pid=121898)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=121898)[0m f1_macro: 0.11174530865284674
[2m[36m(func pid=121898)[0m f1_weighted: 0.16668943526135246
[2m[36m(func pid=121898)[0m f1_per_class: [0.164, 0.272, 0.097, 0.167, 0.031, 0.096, 0.183, 0.0, 0.107, 0.0]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.11287313432835822
[2m[36m(func pid=122462)[0m top5: 0.5149253731343284
[2m[36m(func pid=122462)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=122462)[0m f1_macro: 0.02061328790459966
[2m[36m(func pid=122462)[0m f1_weighted: 0.023555296346207632
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.206, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.18796641791044777
[2m[36m(func pid=122441)[0m top5: 0.5102611940298507
[2m[36m(func pid=122441)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=122441)[0m f1_macro: 0.17575761189274697
[2m[36m(func pid=122441)[0m f1_weighted: 0.13205508208359332
[2m[36m(func pid=122441)[0m f1_per_class: [0.422, 0.505, 0.084, 0.051, 0.145, 0.024, 0.0, 0.211, 0.098, 0.217]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5153 | Steps: 4 | Val loss: 1.6760 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0999 | Steps: 4 | Val loss: 2.1592 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 24.4675 | Steps: 4 | Val loss: 3642727424.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.8538 | Steps: 4 | Val loss: 5.3495 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:00:52 (running for 00:17:29.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.515 |      0.349 |                   28 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  2.542 |      0.112 |                    3 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.174 |      0.176 |                    2 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 21.783 |      0.021 |                    2 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.36007462686567165
[2m[36m(func pid=116143)[0m top5: 0.9160447761194029
[2m[36m(func pid=116143)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=116143)[0m f1_macro: 0.3491132596491533
[2m[36m(func pid=116143)[0m f1_weighted: 0.3464844764057142
[2m[36m(func pid=116143)[0m f1_per_class: [0.476, 0.446, 0.667, 0.546, 0.206, 0.222, 0.159, 0.307, 0.287, 0.175]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.16511194029850745
[2m[36m(func pid=121898)[0m top5: 0.7313432835820896
[2m[36m(func pid=121898)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=121898)[0m f1_macro: 0.12936825320046547
[2m[36m(func pid=121898)[0m f1_weighted: 0.18190116126841077
[2m[36m(func pid=121898)[0m f1_per_class: [0.175, 0.258, 0.151, 0.101, 0.047, 0.149, 0.279, 0.0, 0.134, 0.0]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.006063432835820896
[2m[36m(func pid=122462)[0m top5: 0.5093283582089553
[2m[36m(func pid=122462)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=122462)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=122462)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.28404850746268656
[2m[36m(func pid=122441)[0m top5: 0.6259328358208955
[2m[36m(func pid=122441)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=122441)[0m f1_macro: 0.2685182968165517
[2m[36m(func pid=122441)[0m f1_weighted: 0.21762877896844193
[2m[36m(func pid=122441)[0m f1_per_class: [0.606, 0.507, 0.571, 0.301, 0.0, 0.106, 0.0, 0.164, 0.179, 0.25]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3400 | Steps: 4 | Val loss: 1.6793 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.4634 | Steps: 4 | Val loss: 1.9865 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 40.1354 | Steps: 4 | Val loss: 63478496.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.1401 | Steps: 4 | Val loss: 8.0240 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 14:00:58 (running for 00:17:34.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.34  |      0.352 |                   29 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  2.1   |      0.129 |                    4 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  1.854 |      0.269 |                    3 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 24.467 |      0.001 |                    3 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3614738805970149
[2m[36m(func pid=116143)[0m top5: 0.9137126865671642
[2m[36m(func pid=116143)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=116143)[0m f1_macro: 0.3524163133768202
[2m[36m(func pid=116143)[0m f1_weighted: 0.34557908746044047
[2m[36m(func pid=116143)[0m f1_per_class: [0.507, 0.407, 0.667, 0.55, 0.194, 0.254, 0.157, 0.34, 0.255, 0.193]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.2490671641791045
[2m[36m(func pid=121898)[0m top5: 0.832089552238806
[2m[36m(func pid=121898)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=121898)[0m f1_macro: 0.20615614289357662
[2m[36m(func pid=121898)[0m f1_weighted: 0.27432646639844704
[2m[36m(func pid=121898)[0m f1_per_class: [0.153, 0.24, 0.358, 0.196, 0.066, 0.141, 0.472, 0.168, 0.138, 0.129]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.2980410447761194
[2m[36m(func pid=122462)[0m top5: 0.5149253731343284
[2m[36m(func pid=122462)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=122462)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=122462)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.11520522388059702
[2m[36m(func pid=122441)[0m top5: 0.6352611940298507
[2m[36m(func pid=122441)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=122441)[0m f1_macro: 0.14326186695787038
[2m[36m(func pid=122441)[0m f1_weighted: 0.09504345501272715
[2m[36m(func pid=122441)[0m f1_per_class: [0.108, 0.062, 0.0, 0.185, 0.4, 0.0, 0.0, 0.387, 0.084, 0.207]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2963 | Steps: 4 | Val loss: 1.6664 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9991 | Steps: 4 | Val loss: 1.9091 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 33.4204 | Steps: 4 | Val loss: 9439492.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.2189 | Steps: 4 | Val loss: 21.0617 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:01:03 (running for 00:17:40.47)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.296 |      0.355 |                   30 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.463 |      0.206 |                    5 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  1.14  |      0.143 |                    4 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 40.135 |      0.046 |                    4 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3572761194029851
[2m[36m(func pid=116143)[0m top5: 0.9165111940298507
[2m[36m(func pid=116143)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=116143)[0m f1_macro: 0.3554920699746139
[2m[36m(func pid=116143)[0m f1_weighted: 0.3359369908064991
[2m[36m(func pid=116143)[0m f1_per_class: [0.496, 0.405, 0.71, 0.564, 0.244, 0.26, 0.113, 0.324, 0.244, 0.194]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.24067164179104478
[2m[36m(func pid=121898)[0m top5: 0.8666044776119403
[2m[36m(func pid=121898)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=121898)[0m f1_macro: 0.2421428785119895
[2m[36m(func pid=121898)[0m f1_weighted: 0.23636207693257918
[2m[36m(func pid=121898)[0m f1_per_class: [0.203, 0.353, 0.585, 0.391, 0.142, 0.043, 0.107, 0.228, 0.168, 0.202]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.2789179104477612
[2m[36m(func pid=122462)[0m top5: 0.7821828358208955
[2m[36m(func pid=122462)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=122462)[0m f1_macro: 0.04361779722830051
[2m[36m(func pid=122462)[0m f1_weighted: 0.12165784861251727
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.0, 0.436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.1310634328358209
[2m[36m(func pid=122441)[0m top5: 0.47201492537313433
[2m[36m(func pid=122441)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=122441)[0m f1_macro: 0.1540851734582565
[2m[36m(func pid=122441)[0m f1_weighted: 0.15335240414333276
[2m[36m(func pid=122441)[0m f1_per_class: [0.345, 0.286, 0.033, 0.313, 0.04, 0.0, 0.0, 0.031, 0.074, 0.419]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.0085 | Steps: 4 | Val loss: 1.6596 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8087 | Steps: 4 | Val loss: 1.8352 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 37.6938 | Steps: 4 | Val loss: 7799673.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.3596 | Steps: 4 | Val loss: 62.9708 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 14:01:09 (running for 00:17:45.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  1.008 |      0.366 |                   31 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.999 |      0.242 |                    6 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.219 |      0.154 |                    5 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 33.42  |      0.044 |                    5 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3521455223880597
[2m[36m(func pid=116143)[0m top5: 0.9249067164179104
[2m[36m(func pid=116143)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=116143)[0m f1_macro: 0.36597135476896175
[2m[36m(func pid=116143)[0m f1_weighted: 0.3317114947813081
[2m[36m(func pid=116143)[0m f1_per_class: [0.511, 0.431, 0.759, 0.547, 0.267, 0.296, 0.085, 0.316, 0.244, 0.205]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.3050373134328358
[2m[36m(func pid=121898)[0m top5: 0.8857276119402985
[2m[36m(func pid=121898)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=121898)[0m f1_macro: 0.290387129162928
[2m[36m(func pid=121898)[0m f1_weighted: 0.2691157128185851
[2m[36m(func pid=121898)[0m f1_per_class: [0.312, 0.474, 0.632, 0.516, 0.314, 0.031, 0.015, 0.246, 0.205, 0.159]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.17210820895522388
[2m[36m(func pid=122462)[0m top5: 0.5727611940298507
[2m[36m(func pid=122462)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=122462)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=122462)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.06110074626865672
[2m[36m(func pid=122441)[0m top5: 0.5083955223880597
[2m[36m(func pid=122441)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=122441)[0m f1_macro: 0.05413009730106275
[2m[36m(func pid=122441)[0m f1_weighted: 0.03254171371105893
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.175, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3316 | Steps: 4 | Val loss: 1.6284 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9415 | Steps: 4 | Val loss: 1.7486 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 39.0897 | Steps: 4 | Val loss: 1839096.2500 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 7.1187 | Steps: 4 | Val loss: 1013.8085 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 14:01:14 (running for 00:17:51.39)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.332 |      0.381 |                   32 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.809 |      0.29  |                    7 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.36  |      0.054 |                    6 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 37.694 |      0.029 |                    6 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.36427238805970147
[2m[36m(func pid=116143)[0m top5: 0.9351679104477612
[2m[36m(func pid=116143)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=116143)[0m f1_macro: 0.3811395094299494
[2m[36m(func pid=116143)[0m f1_weighted: 0.343923862311461
[2m[36m(func pid=116143)[0m f1_per_class: [0.556, 0.455, 0.759, 0.557, 0.256, 0.292, 0.1, 0.317, 0.229, 0.292]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.3414179104477612
[2m[36m(func pid=121898)[0m top5: 0.9001865671641791
[2m[36m(func pid=121898)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=121898)[0m f1_macro: 0.33268273062346865
[2m[36m(func pid=121898)[0m f1_weighted: 0.2962018772954872
[2m[36m(func pid=121898)[0m f1_per_class: [0.432, 0.462, 0.649, 0.549, 0.36, 0.141, 0.015, 0.288, 0.265, 0.166]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.020522388059701493
[2m[36m(func pid=122462)[0m top5: 0.5237873134328358
[2m[36m(func pid=122462)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=122462)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=122462)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=122462)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.2677238805970149
[2m[36m(func pid=122441)[0m top5: 0.5592350746268657
[2m[36m(func pid=122441)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=122441)[0m f1_macro: 0.05645893262974929
[2m[36m(func pid=122441)[0m f1_weighted: 0.1511296843463901
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3076 | Steps: 4 | Val loss: 1.6838 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4838 | Steps: 4 | Val loss: 1.7236 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 7.8243 | Steps: 4 | Val loss: 178795.4062 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 7.3486 | Steps: 4 | Val loss: 11162.5352 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:01:19 (running for 00:17:56.75)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.308 |      0.365 |                   33 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.941 |      0.333 |                    8 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.119 |      0.056 |                    7 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 39.09  |      0.004 |                    7 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.35074626865671643
[2m[36m(func pid=116143)[0m top5: 0.9193097014925373
[2m[36m(func pid=116143)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=116143)[0m f1_macro: 0.3651783169920554
[2m[36m(func pid=116143)[0m f1_weighted: 0.33325731515785983
[2m[36m(func pid=116143)[0m f1_per_class: [0.5, 0.453, 0.71, 0.53, 0.256, 0.274, 0.102, 0.32, 0.227, 0.28]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.3787313432835821
[2m[36m(func pid=121898)[0m top5: 0.9001865671641791
[2m[36m(func pid=121898)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=121898)[0m f1_macro: 0.3501906627786783
[2m[36m(func pid=121898)[0m f1_weighted: 0.33681119763300066
[2m[36m(func pid=121898)[0m f1_per_class: [0.617, 0.524, 0.414, 0.525, 0.27, 0.247, 0.078, 0.367, 0.234, 0.226]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.03311567164179104
[2m[36m(func pid=122462)[0m top5: 0.5149253731343284
[2m[36m(func pid=122462)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=122462)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=122462)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.006063432835820896
[2m[36m(func pid=122441)[0m top5: 0.39738805970149255
[2m[36m(func pid=122441)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=122441)[0m f1_macro: 0.0012566457225712904
[2m[36m(func pid=122441)[0m f1_weighted: 7.619586937232638e-05
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5190 | Steps: 4 | Val loss: 1.7066 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3483 | Steps: 4 | Val loss: 1.6744 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.9847 | Steps: 4 | Val loss: 48557.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 14:01:25 (running for 00:18:02.08)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.519 |      0.366 |                   34 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.484 |      0.35  |                    9 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.349 |      0.001 |                    8 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  7.824 |      0.006 |                    8 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.33908582089552236
[2m[36m(func pid=116143)[0m top5: 0.9202425373134329
[2m[36m(func pid=116143)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=116143)[0m f1_macro: 0.365764745027379
[2m[36m(func pid=116143)[0m f1_weighted: 0.3253612111848979
[2m[36m(func pid=116143)[0m f1_per_class: [0.53, 0.39, 0.733, 0.533, 0.301, 0.289, 0.102, 0.325, 0.208, 0.245]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 9.2863 | Steps: 4 | Val loss: 89237.4609 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=121898)[0m top1: 0.3871268656716418
[2m[36m(func pid=121898)[0m top5: 0.9342350746268657
[2m[36m(func pid=121898)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=121898)[0m f1_macro: 0.3542499451035414
[2m[36m(func pid=121898)[0m f1_weighted: 0.36075014829942315
[2m[36m(func pid=121898)[0m f1_per_class: [0.673, 0.508, 0.282, 0.532, 0.227, 0.276, 0.148, 0.384, 0.186, 0.327]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.03311567164179104
[2m[36m(func pid=122462)[0m top5: 0.5149253731343284
[2m[36m(func pid=122462)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=122462)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=122462)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.03311567164179104
[2m[36m(func pid=122441)[0m top5: 0.5149253731343284
[2m[36m(func pid=122441)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=122441)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=122441)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.1564 | Steps: 4 | Val loss: 1.7242 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2319 | Steps: 4 | Val loss: 1.7192 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 4.0656 | Steps: 4 | Val loss: 20397.8281 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 14:01:30 (running for 00:18:07.44)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.156 |      0.365 |                   35 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.348 |      0.354 |                   10 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  9.286 |      0.006 |                    9 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.985 |      0.006 |                    9 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.33955223880597013
[2m[36m(func pid=116143)[0m top5: 0.9174440298507462
[2m[36m(func pid=116143)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=116143)[0m f1_macro: 0.36498063244588824
[2m[36m(func pid=116143)[0m f1_weighted: 0.32223060265331316
[2m[36m(func pid=116143)[0m f1_per_class: [0.53, 0.39, 0.71, 0.535, 0.304, 0.304, 0.083, 0.33, 0.205, 0.259]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 7.3776 | Steps: 4 | Val loss: 29607.3945 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=121898)[0m top1: 0.3871268656716418
[2m[36m(func pid=121898)[0m top5: 0.9370335820895522
[2m[36m(func pid=121898)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=121898)[0m f1_macro: 0.35295634869740555
[2m[36m(func pid=121898)[0m f1_weighted: 0.3865652565945762
[2m[36m(func pid=121898)[0m f1_per_class: [0.629, 0.526, 0.253, 0.533, 0.217, 0.278, 0.238, 0.327, 0.166, 0.364]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.17210820895522388
[2m[36m(func pid=122462)[0m top5: 0.5727611940298507
[2m[36m(func pid=122462)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=122462)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=122462)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.027052238805970148
[2m[36m(func pid=122441)[0m top5: 0.34095149253731344
[2m[36m(func pid=122441)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=122441)[0m f1_macro: 0.008596542812485492
[2m[36m(func pid=122441)[0m f1_weighted: 0.002560177358768608
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.075, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2067 | Steps: 4 | Val loss: 1.7339 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1798 | Steps: 4 | Val loss: 1.7955 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.9497 | Steps: 4 | Val loss: 19159.2441 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:01:35 (running for 00:18:12.72)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.207 |      0.367 |                   36 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.232 |      0.353 |                   11 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.378 |      0.009 |                   10 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.066 |      0.029 |                   10 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.35074626865671643
[2m[36m(func pid=116143)[0m top5: 0.9015858208955224
[2m[36m(func pid=116143)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=116143)[0m f1_macro: 0.3671502586673646
[2m[36m(func pid=116143)[0m f1_weighted: 0.33798155501825766
[2m[36m(func pid=116143)[0m f1_per_class: [0.538, 0.448, 0.688, 0.52, 0.25, 0.305, 0.113, 0.358, 0.197, 0.255]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 6.1806 | Steps: 4 | Val loss: 163.9158 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=121898)[0m top1: 0.37966417910447764
[2m[36m(func pid=121898)[0m top5: 0.9333022388059702
[2m[36m(func pid=121898)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=121898)[0m f1_macro: 0.35365625784927507
[2m[36m(func pid=121898)[0m f1_weighted: 0.39461214193143623
[2m[36m(func pid=121898)[0m f1_per_class: [0.614, 0.527, 0.22, 0.496, 0.228, 0.268, 0.299, 0.351, 0.161, 0.372]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.025186567164179104
[2m[36m(func pid=122462)[0m top5: 0.5321828358208955
[2m[36m(func pid=122462)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=122462)[0m f1_macro: 0.02050937508475036
[2m[36m(func pid=122462)[0m f1_weighted: 0.009778163428279541
[2m[36m(func pid=122462)[0m f1_per_class: [0.041, 0.047, 0.0, 0.0, 0.118, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3602 | Steps: 4 | Val loss: 1.7673 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=122441)[0m top1: 0.09654850746268656
[2m[36m(func pid=122441)[0m top5: 0.3871268656716418
[2m[36m(func pid=122441)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=122441)[0m f1_macro: 0.037221250868361484
[2m[36m(func pid=122441)[0m f1_weighted: 0.05893839940046378
[2m[36m(func pid=122441)[0m f1_per_class: [0.033, 0.312, 0.0, 0.0, 0.013, 0.0, 0.015, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1357 | Steps: 4 | Val loss: 1.9522 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 4.7857 | Steps: 4 | Val loss: 23122.8379 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:01:41 (running for 00:18:18.03)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.36  |      0.37  |                   37 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.18  |      0.354 |                   12 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  6.181 |      0.037 |                   11 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.95  |      0.021 |                   11 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.35634328358208955
[2m[36m(func pid=116143)[0m top5: 0.8852611940298507
[2m[36m(func pid=116143)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=116143)[0m f1_macro: 0.369785552333668
[2m[36m(func pid=116143)[0m f1_weighted: 0.33756789863938447
[2m[36m(func pid=116143)[0m f1_per_class: [0.565, 0.468, 0.615, 0.528, 0.247, 0.296, 0.091, 0.375, 0.179, 0.333]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 6.7243 | Steps: 4 | Val loss: 603.5325 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=121898)[0m top1: 0.37406716417910446
[2m[36m(func pid=121898)[0m top5: 0.9160447761194029
[2m[36m(func pid=121898)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=121898)[0m f1_macro: 0.34140758603905336
[2m[36m(func pid=121898)[0m f1_weighted: 0.3927456713259771
[2m[36m(func pid=121898)[0m f1_per_class: [0.543, 0.556, 0.21, 0.455, 0.17, 0.212, 0.338, 0.37, 0.172, 0.39]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.0228544776119403
[2m[36m(func pid=122462)[0m top5: 0.5331156716417911
[2m[36m(func pid=122462)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=122462)[0m f1_macro: 0.035324006551018916
[2m[36m(func pid=122462)[0m f1_weighted: 0.0034000603562539134
[2m[36m(func pid=122462)[0m f1_per_class: [0.041, 0.0, 0.0, 0.0, 0.258, 0.0, 0.0, 0.0, 0.0, 0.054]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0937 | Steps: 4 | Val loss: 1.7473 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=122441)[0m top1: 0.20055970149253732
[2m[36m(func pid=122441)[0m top5: 0.5018656716417911
[2m[36m(func pid=122441)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=122441)[0m f1_macro: 0.059365706455772126
[2m[36m(func pid=122441)[0m f1_weighted: 0.12259230000147336
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.405, 0.0, 0.177, 0.0, 0.0, 0.012, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0741 | Steps: 4 | Val loss: 2.0500 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 9.2888 | Steps: 4 | Val loss: 22301.5566 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:01:46 (running for 00:18:23.44)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.094 |      0.379 |                   38 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.136 |      0.341 |                   13 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  6.724 |      0.059 |                   12 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.786 |      0.035 |                   12 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3670708955223881
[2m[36m(func pid=116143)[0m top5: 0.8936567164179104
[2m[36m(func pid=116143)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=116143)[0m f1_macro: 0.37920425778212896
[2m[36m(func pid=116143)[0m f1_weighted: 0.3505977889238682
[2m[36m(func pid=116143)[0m f1_per_class: [0.618, 0.481, 0.545, 0.53, 0.247, 0.311, 0.11, 0.413, 0.176, 0.361]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 7.6452 | Steps: 4 | Val loss: 22753.6484 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=121898)[0m top1: 0.36427238805970147
[2m[36m(func pid=121898)[0m top5: 0.9011194029850746
[2m[36m(func pid=121898)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=121898)[0m f1_macro: 0.34135370966599915
[2m[36m(func pid=121898)[0m f1_weighted: 0.3738951187656976
[2m[36m(func pid=121898)[0m f1_per_class: [0.42, 0.552, 0.292, 0.396, 0.173, 0.227, 0.321, 0.412, 0.191, 0.429]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.0228544776119403
[2m[36m(func pid=122462)[0m top5: 0.5345149253731343
[2m[36m(func pid=122462)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=122462)[0m f1_macro: 0.00894246161710547
[2m[36m(func pid=122462)[0m f1_weighted: 0.004718230275999576
[2m[36m(func pid=122462)[0m f1_per_class: [0.042, 0.0, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0, 0.019, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4001 | Steps: 4 | Val loss: 1.6931 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=122441)[0m top1: 0.27798507462686567
[2m[36m(func pid=122441)[0m top5: 0.5592350746268657
[2m[36m(func pid=122441)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=122441)[0m f1_macro: 0.048415602013162994
[2m[36m(func pid=122441)[0m f1_weighted: 0.12951273322701248
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2421 | Steps: 4 | Val loss: 2.1417 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 5.7452 | Steps: 4 | Val loss: 16520.0762 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:01:51 (running for 00:18:28.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.4   |      0.375 |                   39 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.074 |      0.341 |                   14 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.645 |      0.048 |                   13 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  9.289 |      0.009 |                   13 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3763992537313433
[2m[36m(func pid=116143)[0m top5: 0.9071828358208955
[2m[36m(func pid=116143)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=116143)[0m f1_macro: 0.37488651688675995
[2m[36m(func pid=116143)[0m f1_weighted: 0.35515012347475444
[2m[36m(func pid=116143)[0m f1_per_class: [0.602, 0.466, 0.471, 0.547, 0.289, 0.32, 0.115, 0.417, 0.185, 0.338]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 16.9717 | Steps: 4 | Val loss: 133.4835 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=121898)[0m top1: 0.36800373134328357
[2m[36m(func pid=121898)[0m top5: 0.8908582089552238
[2m[36m(func pid=121898)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=121898)[0m f1_macro: 0.3323274455512865
[2m[36m(func pid=121898)[0m f1_weighted: 0.37754247622032444
[2m[36m(func pid=121898)[0m f1_per_class: [0.385, 0.545, 0.342, 0.357, 0.151, 0.223, 0.381, 0.393, 0.236, 0.31]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.027052238805970148
[2m[36m(func pid=122462)[0m top5: 0.5373134328358209
[2m[36m(func pid=122462)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=122462)[0m f1_macro: 0.013815774732470726
[2m[36m(func pid=122462)[0m f1_weighted: 0.010589208842653363
[2m[36m(func pid=122462)[0m f1_per_class: [0.044, 0.0, 0.0, 0.0, 0.0, 0.081, 0.0, 0.0, 0.013, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1107 | Steps: 4 | Val loss: 1.6607 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=122441)[0m top1: 0.15764925373134328
[2m[36m(func pid=122441)[0m top5: 0.5186567164179104
[2m[36m(func pid=122441)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=122441)[0m f1_macro: 0.0693776921318604
[2m[36m(func pid=122441)[0m f1_weighted: 0.10258565825759147
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.29, 0.0, 0.151, 0.08, 0.0, 0.0, 0.173, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.1360 | Steps: 4 | Val loss: 1.9827 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 11.7285 | Steps: 4 | Val loss: 8880.2500 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:01:57 (running for 00:18:34.07)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.111 |      0.388 |                   40 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.242 |      0.332 |                   15 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 16.972 |      0.069 |                   14 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  5.745 |      0.014 |                   14 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.38759328358208955
[2m[36m(func pid=116143)[0m top5: 0.9174440298507462
[2m[36m(func pid=116143)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=116143)[0m f1_macro: 0.3881338073349626
[2m[36m(func pid=116143)[0m f1_weighted: 0.36695416615185783
[2m[36m(func pid=116143)[0m f1_per_class: [0.636, 0.479, 0.49, 0.553, 0.261, 0.322, 0.134, 0.432, 0.188, 0.388]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 10.7691 | Steps: 4 | Val loss: 23.5117 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=121898)[0m top1: 0.41511194029850745
[2m[36m(func pid=121898)[0m top5: 0.9221082089552238
[2m[36m(func pid=121898)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=121898)[0m f1_macro: 0.3640288911816123
[2m[36m(func pid=121898)[0m f1_weighted: 0.44010336742539324
[2m[36m(func pid=121898)[0m f1_per_class: [0.359, 0.571, 0.406, 0.461, 0.152, 0.262, 0.453, 0.43, 0.28, 0.265]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.033582089552238806
[2m[36m(func pid=122462)[0m top5: 0.5410447761194029
[2m[36m(func pid=122462)[0m f1_micro: 0.033582089552238806
[2m[36m(func pid=122462)[0m f1_macro: 0.018809468569364062
[2m[36m(func pid=122462)[0m f1_weighted: 0.01872089194228067
[2m[36m(func pid=122462)[0m f1_per_class: [0.048, 0.0, 0.0, 0.0, 0.0, 0.131, 0.009, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1151 | Steps: 4 | Val loss: 1.6498 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=122441)[0m top1: 0.012126865671641791
[2m[36m(func pid=122441)[0m top5: 0.30970149253731344
[2m[36m(func pid=122441)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=122441)[0m f1_macro: 0.008834180874829853
[2m[36m(func pid=122441)[0m f1_weighted: 0.018611214618558386
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.002, 0.063, 0.016, 0.007, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6422 | Steps: 4 | Val loss: 1.9024 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:02:02 (running for 00:18:39.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.115 |      0.38  |                   41 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.136 |      0.364 |                   16 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 10.769 |      0.009 |                   15 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 11.729 |      0.019 |                   15 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3941231343283582
[2m[36m(func pid=116143)[0m top5: 0.9146455223880597
[2m[36m(func pid=116143)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=116143)[0m f1_macro: 0.3803144240524231
[2m[36m(func pid=116143)[0m f1_weighted: 0.3782138992306537
[2m[36m(func pid=116143)[0m f1_per_class: [0.636, 0.47, 0.444, 0.553, 0.24, 0.323, 0.179, 0.428, 0.204, 0.325]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 5.0879 | Steps: 4 | Val loss: 5538.8818 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 8.8489 | Steps: 4 | Val loss: 1396.8302 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=121898)[0m top1: 0.46781716417910446
[2m[36m(func pid=121898)[0m top5: 0.925839552238806
[2m[36m(func pid=121898)[0m f1_micro: 0.46781716417910446
[2m[36m(func pid=121898)[0m f1_macro: 0.39422871166359974
[2m[36m(func pid=121898)[0m f1_weighted: 0.492918035281358
[2m[36m(func pid=121898)[0m f1_per_class: [0.453, 0.612, 0.406, 0.545, 0.177, 0.319, 0.502, 0.412, 0.308, 0.21]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.0625
[2m[36m(func pid=122462)[0m top5: 0.523320895522388
[2m[36m(func pid=122462)[0m f1_micro: 0.0625
[2m[36m(func pid=122462)[0m f1_macro: 0.035975545878836945
[2m[36m(func pid=122462)[0m f1_weighted: 0.041845755881083815
[2m[36m(func pid=122462)[0m f1_per_class: [0.056, 0.047, 0.0, 0.016, 0.0, 0.237, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1993 | Steps: 4 | Val loss: 1.6480 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=122441)[0m top1: 0.058768656716417914
[2m[36m(func pid=122441)[0m top5: 0.5055970149253731
[2m[36m(func pid=122441)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=122441)[0m f1_macro: 0.026761804606640394
[2m[36m(func pid=122441)[0m f1_weighted: 0.030287421480377854
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.122, 0.0, 0.0, 0.015, 0.06, 0.0, 0.0, 0.071, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0687 | Steps: 4 | Val loss: 1.6813 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 14:02:07 (running for 00:18:44.47)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.199 |      0.375 |                   42 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.642 |      0.394 |                   17 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  8.849 |      0.027 |                   16 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  5.088 |      0.036 |                   16 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.39925373134328357
[2m[36m(func pid=116143)[0m top5: 0.9169776119402985
[2m[36m(func pid=116143)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=116143)[0m f1_macro: 0.374942783674584
[2m[36m(func pid=116143)[0m f1_weighted: 0.382722918478993
[2m[36m(func pid=116143)[0m f1_per_class: [0.619, 0.484, 0.407, 0.555, 0.222, 0.306, 0.193, 0.427, 0.203, 0.333]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 6.1388 | Steps: 4 | Val loss: 3388.0425 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 8.5578 | Steps: 4 | Val loss: 2063.2893 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=121898)[0m top1: 0.5111940298507462
[2m[36m(func pid=121898)[0m top5: 0.9477611940298507
[2m[36m(func pid=121898)[0m f1_micro: 0.5111940298507462
[2m[36m(func pid=121898)[0m f1_macro: 0.4305142012778371
[2m[36m(func pid=121898)[0m f1_weighted: 0.5263417339398491
[2m[36m(func pid=121898)[0m f1_per_class: [0.646, 0.565, 0.473, 0.637, 0.21, 0.38, 0.53, 0.351, 0.291, 0.222]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0845 | Steps: 4 | Val loss: 1.6199 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=122462)[0m top1: 0.0732276119402985
[2m[36m(func pid=122462)[0m top5: 0.5051305970149254
[2m[36m(func pid=122462)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=122462)[0m f1_macro: 0.03950557189359845
[2m[36m(func pid=122462)[0m f1_weighted: 0.06836606432676105
[2m[36m(func pid=122462)[0m f1_per_class: [0.066, 0.101, 0.0, 0.026, 0.0, 0.023, 0.13, 0.0, 0.025, 0.025]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.020522388059701493
[2m[36m(func pid=122441)[0m top5: 0.5457089552238806
[2m[36m(func pid=122441)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=122441)[0m f1_macro: 0.013993218197312751
[2m[36m(func pid=122441)[0m f1_weighted: 0.014007051115545436
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.068, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.043, 0.023]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0468 | Steps: 4 | Val loss: 1.6746 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 14:02:12 (running for 00:18:49.74)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.084 |      0.383 |                   43 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.069 |      0.431 |                   18 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  8.558 |      0.014 |                   17 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  6.139 |      0.04  |                   17 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.41744402985074625
[2m[36m(func pid=116143)[0m top5: 0.9127798507462687
[2m[36m(func pid=116143)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=116143)[0m f1_macro: 0.3834942678903107
[2m[36m(func pid=116143)[0m f1_weighted: 0.4027303296029558
[2m[36m(func pid=116143)[0m f1_per_class: [0.598, 0.531, 0.4, 0.556, 0.229, 0.328, 0.22, 0.446, 0.226, 0.301]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 11.8269 | Steps: 4 | Val loss: 2874.8516 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 7.7794 | Steps: 4 | Val loss: 3015.1211 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=121898)[0m top1: 0.5279850746268657
[2m[36m(func pid=121898)[0m top5: 0.9561567164179104
[2m[36m(func pid=121898)[0m f1_micro: 0.5279850746268657
[2m[36m(func pid=121898)[0m f1_macro: 0.44760698534342974
[2m[36m(func pid=121898)[0m f1_weighted: 0.5297864815263057
[2m[36m(func pid=121898)[0m f1_per_class: [0.633, 0.572, 0.6, 0.656, 0.236, 0.382, 0.513, 0.374, 0.26, 0.248]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1366 | Steps: 4 | Val loss: 1.5881 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=122462)[0m top1: 0.09421641791044776
[2m[36m(func pid=122462)[0m top5: 0.5181902985074627
[2m[36m(func pid=122462)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=122462)[0m f1_macro: 0.045416344787786354
[2m[36m(func pid=122462)[0m f1_weighted: 0.08560428871113439
[2m[36m(func pid=122462)[0m f1_per_class: [0.087, 0.08, 0.0, 0.025, 0.0, 0.0, 0.206, 0.0, 0.057, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.10261194029850747
[2m[36m(func pid=122441)[0m top5: 0.4939365671641791
[2m[36m(func pid=122441)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=122441)[0m f1_macro: 0.04324282577711614
[2m[36m(func pid=122441)[0m f1_weighted: 0.1050083465472489
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.049, 0.005, 0.0, 0.0, 0.0, 0.317, 0.0, 0.061, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0800 | Steps: 4 | Val loss: 1.7462 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=116143)[0m top1: 0.4193097014925373
[2m[36m(func pid=116143)[0m top5: 0.9239738805970149
[2m[36m(func pid=116143)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=116143)[0m f1_macro: 0.38244584897392314
[2m[36m(func pid=116143)[0m f1_weighted: 0.4039408708334601
[2m[36m(func pid=116143)[0m f1_per_class: [0.66, 0.521, 0.387, 0.575, 0.189, 0.308, 0.219, 0.43, 0.228, 0.306]
[2m[36m(func pid=116143)[0m 
== Status ==
Current time: 2024-01-07 14:02:18 (running for 00:18:55.18)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.137 |      0.382 |                   44 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.047 |      0.448 |                   19 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.779 |      0.043 |                   18 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 11.827 |      0.045 |                   18 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 11.4463 | Steps: 4 | Val loss: 2698.8333 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 11.3485 | Steps: 4 | Val loss: 18581.1445 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=121898)[0m top1: 0.5200559701492538
[2m[36m(func pid=121898)[0m top5: 0.9598880597014925
[2m[36m(func pid=121898)[0m f1_micro: 0.5200559701492538
[2m[36m(func pid=121898)[0m f1_macro: 0.4528410088228422
[2m[36m(func pid=121898)[0m f1_weighted: 0.514702905951203
[2m[36m(func pid=121898)[0m f1_per_class: [0.595, 0.538, 0.686, 0.639, 0.295, 0.413, 0.489, 0.372, 0.226, 0.275]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1284 | Steps: 4 | Val loss: 1.6361 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=122462)[0m top1: 0.09188432835820895
[2m[36m(func pid=122462)[0m top5: 0.519589552238806
[2m[36m(func pid=122462)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=122462)[0m f1_macro: 0.04320155373684679
[2m[36m(func pid=122462)[0m f1_weighted: 0.08826927332722644
[2m[36m(func pid=122462)[0m f1_per_class: [0.064, 0.056, 0.0, 0.2, 0.0, 0.0, 0.067, 0.0, 0.044, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.16044776119402984
[2m[36m(func pid=122441)[0m top5: 0.49720149253731344
[2m[36m(func pid=122441)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=122441)[0m f1_macro: 0.042965430511732426
[2m[36m(func pid=122441)[0m f1_weighted: 0.06485253167256685
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.049, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0524 | Steps: 4 | Val loss: 1.8964 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:02:23 (running for 00:19:00.60)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.128 |      0.383 |                   45 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.08  |      0.453 |                   20 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 11.349 |      0.043 |                   19 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 11.446 |      0.043 |                   19 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4137126865671642
[2m[36m(func pid=116143)[0m top5: 0.9155783582089553
[2m[36m(func pid=116143)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=116143)[0m f1_macro: 0.3827114347654426
[2m[36m(func pid=116143)[0m f1_weighted: 0.3971202677688441
[2m[36m(func pid=116143)[0m f1_per_class: [0.68, 0.505, 0.387, 0.584, 0.178, 0.309, 0.193, 0.444, 0.224, 0.323]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 9.0931 | Steps: 4 | Val loss: 1086.7079 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 9.6810 | Steps: 4 | Val loss: 13024.9951 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=121898)[0m top1: 0.5097947761194029
[2m[36m(func pid=121898)[0m top5: 0.9594216417910447
[2m[36m(func pid=121898)[0m f1_micro: 0.5097947761194029
[2m[36m(func pid=121898)[0m f1_macro: 0.4553670045450815
[2m[36m(func pid=121898)[0m f1_weighted: 0.4960269170321006
[2m[36m(func pid=121898)[0m f1_per_class: [0.563, 0.549, 0.692, 0.618, 0.329, 0.415, 0.436, 0.394, 0.213, 0.345]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0809 | Steps: 4 | Val loss: 1.6767 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=122462)[0m top1: 0.09421641791044776
[2m[36m(func pid=122462)[0m top5: 0.5494402985074627
[2m[36m(func pid=122462)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=122462)[0m f1_macro: 0.04512729110861177
[2m[36m(func pid=122462)[0m f1_weighted: 0.09246353098761864
[2m[36m(func pid=122462)[0m f1_per_class: [0.072, 0.085, 0.0, 0.145, 0.0, 0.0, 0.12, 0.0, 0.0, 0.03]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.16044776119402984
[2m[36m(func pid=122441)[0m top5: 0.38759328358208955
[2m[36m(func pid=122441)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=122441)[0m f1_macro: 0.03871282230370758
[2m[36m(func pid=122441)[0m f1_weighted: 0.06276704128866754
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:02:28 (running for 00:19:05.80)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.081 |      0.377 |                   46 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.052 |      0.455 |                   21 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  9.681 |      0.039 |                   20 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  9.093 |      0.045 |                   20 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.40671641791044777
[2m[36m(func pid=116143)[0m top5: 0.9034514925373134
[2m[36m(func pid=116143)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=116143)[0m f1_macro: 0.3768773577485251
[2m[36m(func pid=116143)[0m f1_weighted: 0.3893539122309941
[2m[36m(func pid=116143)[0m f1_per_class: [0.631, 0.559, 0.407, 0.558, 0.168, 0.289, 0.171, 0.43, 0.255, 0.302]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0641 | Steps: 4 | Val loss: 2.0196 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 13.1246 | Steps: 4 | Val loss: 417.0135 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=121898)[0m top1: 0.5051305970149254
[2m[36m(func pid=121898)[0m top5: 0.9617537313432836
[2m[36m(func pid=121898)[0m f1_micro: 0.5051305970149254
[2m[36m(func pid=121898)[0m f1_macro: 0.4590711249761291
[2m[36m(func pid=121898)[0m f1_weighted: 0.4873015620282596
[2m[36m(func pid=121898)[0m f1_per_class: [0.492, 0.555, 0.8, 0.609, 0.333, 0.419, 0.414, 0.386, 0.196, 0.387]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 6.6355 | Steps: 4 | Val loss: 5804.0205 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0714 | Steps: 4 | Val loss: 1.6214 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=122462)[0m top1: 0.11893656716417911
[2m[36m(func pid=122462)[0m top5: 0.6487873134328358
[2m[36m(func pid=122462)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=122462)[0m f1_macro: 0.056144701084279304
[2m[36m(func pid=122462)[0m f1_weighted: 0.11337569249909164
[2m[36m(func pid=122462)[0m f1_per_class: [0.089, 0.154, 0.009, 0.136, 0.0, 0.0, 0.157, 0.0, 0.0, 0.017]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.27098880597014924
[2m[36m(func pid=122441)[0m top5: 0.5289179104477612
[2m[36m(func pid=122441)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=122441)[0m f1_macro: 0.052722323049001804
[2m[36m(func pid=122441)[0m f1_weighted: 0.14705200178779423
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:02:34 (running for 00:19:11.18)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.071 |      0.387 |                   47 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.064 |      0.459 |                   22 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  6.635 |      0.053 |                   21 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 13.125 |      0.056 |                   21 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.42350746268656714
[2m[36m(func pid=116143)[0m top5: 0.914179104477612
[2m[36m(func pid=116143)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=116143)[0m f1_macro: 0.38736623705397455
[2m[36m(func pid=116143)[0m f1_weighted: 0.4070564996212361
[2m[36m(func pid=116143)[0m f1_per_class: [0.602, 0.595, 0.4, 0.546, 0.183, 0.302, 0.214, 0.433, 0.276, 0.323]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2564 | Steps: 4 | Val loss: 2.0616 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 11.1540 | Steps: 4 | Val loss: 465.2649 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=121898)[0m top1: 0.49720149253731344
[2m[36m(func pid=121898)[0m top5: 0.9622201492537313
[2m[36m(func pid=121898)[0m f1_micro: 0.49720149253731344
[2m[36m(func pid=121898)[0m f1_macro: 0.45815774094176714
[2m[36m(func pid=121898)[0m f1_weighted: 0.4787364736560508
[2m[36m(func pid=121898)[0m f1_per_class: [0.467, 0.578, 0.774, 0.585, 0.368, 0.419, 0.389, 0.411, 0.218, 0.373]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 5.3154 | Steps: 4 | Val loss: 7185.7480 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0703 | Steps: 4 | Val loss: 1.5849 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=122462)[0m top1: 0.057369402985074626
[2m[36m(func pid=122462)[0m top5: 0.47761194029850745
[2m[36m(func pid=122462)[0m f1_micro: 0.057369402985074626
[2m[36m(func pid=122462)[0m f1_macro: 0.028720197523085737
[2m[36m(func pid=122462)[0m f1_weighted: 0.0315303778371012
[2m[36m(func pid=122462)[0m f1_per_class: [0.054, 0.142, 0.0, 0.0, 0.0, 0.0, 0.003, 0.089, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.2658582089552239
[2m[36m(func pid=122441)[0m top5: 0.5135261194029851
[2m[36m(func pid=122441)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=122441)[0m f1_macro: 0.05329593267882188
[2m[36m(func pid=122441)[0m f1_weighted: 0.1486519017814155
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:02:39 (running for 00:19:16.52)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.071 |      0.387 |                   47 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.256 |      0.458 |                   23 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  5.315 |      0.053 |                   22 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 | 11.154 |      0.029 |                   22 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4253731343283582
[2m[36m(func pid=116143)[0m top5: 0.9267723880597015
[2m[36m(func pid=116143)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=116143)[0m f1_macro: 0.39249845353068885
[2m[36m(func pid=116143)[0m f1_weighted: 0.41773259359719067
[2m[36m(func pid=116143)[0m f1_per_class: [0.618, 0.584, 0.387, 0.546, 0.186, 0.315, 0.248, 0.444, 0.274, 0.323]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2394 | Steps: 4 | Val loss: 2.1439 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 4.9280 | Steps: 4 | Val loss: 559.0681 | Batch size: 32 | lr: 0.1 | Duration: 3.28s
[2m[36m(func pid=121898)[0m top1: 0.4650186567164179
[2m[36m(func pid=121898)[0m top5: 0.9631529850746269
[2m[36m(func pid=121898)[0m f1_micro: 0.46501865671641784
[2m[36m(func pid=121898)[0m f1_macro: 0.4539717838603818
[2m[36m(func pid=121898)[0m f1_weighted: 0.45808869091673354
[2m[36m(func pid=121898)[0m f1_per_class: [0.583, 0.637, 0.667, 0.488, 0.346, 0.381, 0.378, 0.421, 0.28, 0.36]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.1092 | Steps: 4 | Val loss: 1.6128 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 9.9465 | Steps: 4 | Val loss: 5488.3828 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=122462)[0m top1: 0.03871268656716418
[2m[36m(func pid=122462)[0m top5: 0.30736940298507465
[2m[36m(func pid=122462)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=122462)[0m f1_macro: 0.019227974026414425
[2m[36m(func pid=122462)[0m f1_weighted: 0.016057390573540102
[2m[36m(func pid=122462)[0m f1_per_class: [0.049, 0.074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.069, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:02:45 (running for 00:19:21.92)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.109 |      0.389 |                   49 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.239 |      0.454 |                   24 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  5.315 |      0.053 |                   22 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.928 |      0.019 |                   23 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.42490671641791045
[2m[36m(func pid=116143)[0m top5: 0.9188432835820896
[2m[36m(func pid=116143)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=116143)[0m f1_macro: 0.3885371991787188
[2m[36m(func pid=116143)[0m f1_weighted: 0.4148940218634331
[2m[36m(func pid=116143)[0m f1_per_class: [0.586, 0.599, 0.393, 0.532, 0.174, 0.3, 0.251, 0.432, 0.302, 0.316]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m top1: 0.23647388059701493
[2m[36m(func pid=122441)[0m top5: 0.4920708955223881
[2m[36m(func pid=122441)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=122441)[0m f1_macro: 0.05544532950558817
[2m[36m(func pid=122441)[0m f1_weighted: 0.14562668885360674
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.518, 0.0, 0.0, 0.0, 0.0, 0.037, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0763 | Steps: 4 | Val loss: 2.7397 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 7.1417 | Steps: 4 | Val loss: 497.7628 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=121898)[0m top1: 0.4141791044776119
[2m[36m(func pid=121898)[0m top5: 0.9267723880597015
[2m[36m(func pid=121898)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=121898)[0m f1_macro: 0.42298489213365703
[2m[36m(func pid=121898)[0m f1_weighted: 0.40251199941183813
[2m[36m(func pid=121898)[0m f1_per_class: [0.553, 0.612, 0.632, 0.27, 0.304, 0.38, 0.415, 0.391, 0.315, 0.357]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1404 | Steps: 4 | Val loss: 1.6097 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 7.7503 | Steps: 4 | Val loss: 4482.5166 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=122462)[0m top1: 0.11753731343283583
[2m[36m(func pid=122462)[0m top5: 0.2966417910447761
[2m[36m(func pid=122462)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=122462)[0m f1_macro: 0.040836262144257535
[2m[36m(func pid=122462)[0m f1_weighted: 0.04787074427915292
[2m[36m(func pid=122462)[0m f1_per_class: [0.079, 0.254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.075, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.4314365671641791
[2m[36m(func pid=116143)[0m top5: 0.9207089552238806
[2m[36m(func pid=116143)[0m f1_micro: 0.4314365671641791
[2m[36m(func pid=116143)[0m f1_macro: 0.39776735410324227
[2m[36m(func pid=116143)[0m f1_weighted: 0.42371987155010454
[2m[36m(func pid=116143)[0m f1_per_class: [0.63, 0.605, 0.375, 0.525, 0.164, 0.294, 0.277, 0.432, 0.349, 0.327]
[2m[36m(func pid=116143)[0m 
== Status ==
Current time: 2024-01-07 14:02:50 (running for 00:19:27.49)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.14  |      0.398 |                   50 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.076 |      0.423 |                   25 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  9.947 |      0.055 |                   23 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  7.142 |      0.041 |                   24 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122441)[0m top1: 0.024253731343283583
[2m[36m(func pid=122441)[0m top5: 0.25652985074626866
[2m[36m(func pid=122441)[0m f1_micro: 0.024253731343283583
[2m[36m(func pid=122441)[0m f1_macro: 0.008907621780425857
[2m[36m(func pid=122441)[0m f1_weighted: 0.007636322354144849
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.011, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.065, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.6859 | Steps: 4 | Val loss: 2.7145 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 8.8642 | Steps: 4 | Val loss: 443.8725 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0473 | Steps: 4 | Val loss: 1.6272 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=121898)[0m top1: 0.43656716417910446
[2m[36m(func pid=121898)[0m top5: 0.9071828358208955
[2m[36m(func pid=121898)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=121898)[0m f1_macro: 0.40315682750931386
[2m[36m(func pid=121898)[0m f1_weighted: 0.43278633544316747
[2m[36m(func pid=121898)[0m f1_per_class: [0.543, 0.594, 0.49, 0.315, 0.22, 0.368, 0.506, 0.347, 0.291, 0.356]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 10.9195 | Steps: 4 | Val loss: 5487.6895 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=122462)[0m top1: 0.017257462686567165
[2m[36m(func pid=122462)[0m top5: 0.2896455223880597
[2m[36m(func pid=122462)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=122462)[0m f1_macro: 0.016304794942422256
[2m[36m(func pid=122462)[0m f1_weighted: 0.003393856050015073
[2m[36m(func pid=122462)[0m f1_per_class: [0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.08]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:02:56 (running for 00:19:33.03)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.047 |      0.39  |                   51 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.686 |      0.403 |                   26 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.75  |      0.009 |                   24 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  8.864 |      0.016 |                   25 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.425839552238806
[2m[36m(func pid=116143)[0m top5: 0.9207089552238806
[2m[36m(func pid=116143)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=116143)[0m f1_macro: 0.3902907922136202
[2m[36m(func pid=116143)[0m f1_weighted: 0.41684686192827736
[2m[36m(func pid=116143)[0m f1_per_class: [0.611, 0.6, 0.366, 0.55, 0.165, 0.297, 0.237, 0.432, 0.324, 0.32]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m top1: 0.1553171641791045
[2m[36m(func pid=122441)[0m top5: 0.5139925373134329
[2m[36m(func pid=122441)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=122441)[0m f1_macro: 0.05777101122027062
[2m[36m(func pid=122441)[0m f1_weighted: 0.08312597549468174
[2m[36m(func pid=122441)[0m f1_per_class: [0.004, 0.365, 0.0, 0.0, 0.0, 0.164, 0.0, 0.0, 0.044, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1485 | Steps: 4 | Val loss: 3.5490 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 6.7973 | Steps: 4 | Val loss: 335.5999 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1608 | Steps: 4 | Val loss: 1.6018 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=121898)[0m top1: 0.38759328358208955
[2m[36m(func pid=121898)[0m top5: 0.8661380597014925
[2m[36m(func pid=121898)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=121898)[0m f1_macro: 0.30543194880394015
[2m[36m(func pid=121898)[0m f1_weighted: 0.3803731511361573
[2m[36m(func pid=121898)[0m f1_per_class: [0.568, 0.247, 0.462, 0.56, 0.158, 0.054, 0.494, 0.044, 0.209, 0.258]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 10.8795 | Steps: 4 | Val loss: 5172.4482 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=122462)[0m top1: 0.016791044776119403
[2m[36m(func pid=122462)[0m top5: 0.283115671641791
[2m[36m(func pid=122462)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=122462)[0m f1_macro: 0.00914747554638309
[2m[36m(func pid=122462)[0m f1_weighted: 0.00283222136442979
[2m[36m(func pid=122462)[0m f1_per_class: [0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.076, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:03:01 (running for 00:19:38.24)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.161 |      0.394 |                   52 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.149 |      0.305 |                   27 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 10.919 |      0.058 |                   25 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  6.797 |      0.009 |                   26 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4319029850746269
[2m[36m(func pid=116143)[0m top5: 0.9244402985074627
[2m[36m(func pid=116143)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=116143)[0m f1_macro: 0.3937603862218168
[2m[36m(func pid=116143)[0m f1_weighted: 0.42684372571536056
[2m[36m(func pid=116143)[0m f1_per_class: [0.591, 0.602, 0.377, 0.548, 0.177, 0.304, 0.272, 0.419, 0.333, 0.315]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m top1: 0.2271455223880597
[2m[36m(func pid=122441)[0m top5: 0.5009328358208955
[2m[36m(func pid=122441)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=122441)[0m f1_macro: 0.08712343380415129
[2m[36m(func pid=122441)[0m f1_weighted: 0.16712401910573474
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.332, 0.0, 0.0, 0.0, 0.276, 0.263, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3235 | Steps: 4 | Val loss: 4.1867 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 4.9454 | Steps: 4 | Val loss: 196.1673 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3543 | Steps: 4 | Val loss: 1.6387 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=121898)[0m top1: 0.35634328358208955
[2m[36m(func pid=121898)[0m top5: 0.835820895522388
[2m[36m(func pid=121898)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=121898)[0m f1_macro: 0.28050080843873054
[2m[36m(func pid=121898)[0m f1_weighted: 0.34775446383611686
[2m[36m(func pid=121898)[0m f1_per_class: [0.574, 0.181, 0.4, 0.597, 0.198, 0.008, 0.418, 0.014, 0.141, 0.272]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 10.2242 | Steps: 4 | Val loss: 3432.4749 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=122462)[0m top1: 0.022388059701492536
[2m[36m(func pid=122462)[0m top5: 0.3894589552238806
[2m[36m(func pid=122462)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=122462)[0m f1_macro: 0.016717479591468836
[2m[36m(func pid=122462)[0m f1_weighted: 0.0159231379084992
[2m[36m(func pid=122462)[0m f1_per_class: [0.017, 0.076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.074, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:03:06 (running for 00:19:43.71)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.354 |      0.388 |                   53 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.323 |      0.281 |                   28 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 10.88  |      0.087 |                   26 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.945 |      0.017 |                   27 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.43236940298507465
[2m[36m(func pid=116143)[0m top5: 0.9160447761194029
[2m[36m(func pid=116143)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=116143)[0m f1_macro: 0.3881765023863123
[2m[36m(func pid=116143)[0m f1_weighted: 0.4244373757928585
[2m[36m(func pid=116143)[0m f1_per_class: [0.581, 0.608, 0.366, 0.55, 0.172, 0.294, 0.271, 0.373, 0.346, 0.321]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3708 | Steps: 4 | Val loss: 4.1269 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=122441)[0m top1: 0.09235074626865672
[2m[36m(func pid=122441)[0m top5: 0.3148320895522388
[2m[36m(func pid=122441)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=122441)[0m f1_macro: 0.040920752341428404
[2m[36m(func pid=122441)[0m f1_weighted: 0.06394393052749102
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.314, 0.0, 0.0, 0.005, 0.0, 0.026, 0.0, 0.064, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 4.1375 | Steps: 4 | Val loss: 95.8269 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0552 | Steps: 4 | Val loss: 1.6406 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=121898)[0m top1: 0.3605410447761194
[2m[36m(func pid=121898)[0m top5: 0.8512126865671642
[2m[36m(func pid=121898)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=121898)[0m f1_macro: 0.28914751091352264
[2m[36m(func pid=121898)[0m f1_weighted: 0.36230370759385205
[2m[36m(func pid=121898)[0m f1_per_class: [0.557, 0.31, 0.289, 0.607, 0.312, 0.0, 0.39, 0.012, 0.115, 0.3]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 7.8203 | Steps: 4 | Val loss: 4805.6450 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=122462)[0m top1: 0.029850746268656716
[2m[36m(func pid=122462)[0m top5: 0.4048507462686567
[2m[36m(func pid=122462)[0m f1_micro: 0.029850746268656716
[2m[36m(func pid=122462)[0m f1_macro: 0.01971442210179774
[2m[36m(func pid=122462)[0m f1_weighted: 0.026216675480752936
[2m[36m(func pid=122462)[0m f1_per_class: [0.022, 0.144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:03:12 (running for 00:19:49.15)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.055 |      0.385 |                   54 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.371 |      0.289 |                   29 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 10.224 |      0.041 |                   27 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.137 |      0.02  |                   28 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.42350746268656714
[2m[36m(func pid=116143)[0m top5: 0.9244402985074627
[2m[36m(func pid=116143)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=116143)[0m f1_macro: 0.3851635084945958
[2m[36m(func pid=116143)[0m f1_weighted: 0.4197314080315622
[2m[36m(func pid=116143)[0m f1_per_class: [0.581, 0.599, 0.356, 0.55, 0.149, 0.283, 0.26, 0.402, 0.335, 0.337]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1117 | Steps: 4 | Val loss: 4.4328 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=122441)[0m top1: 0.025652985074626867
[2m[36m(func pid=122441)[0m top5: 0.31902985074626866
[2m[36m(func pid=122441)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=122441)[0m f1_macro: 0.013459060955628571
[2m[36m(func pid=122441)[0m f1_weighted: 0.01454997501808747
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.023, 0.0, 0.04, 0.0, 0.071, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 5.0686 | Steps: 4 | Val loss: 129.2806 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.1042 | Steps: 4 | Val loss: 1.5207 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=121898)[0m top1: 0.29151119402985076
[2m[36m(func pid=121898)[0m top5: 0.8097014925373134
[2m[36m(func pid=121898)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=121898)[0m f1_macro: 0.23846804033785904
[2m[36m(func pid=121898)[0m f1_weighted: 0.3243416029962168
[2m[36m(func pid=121898)[0m f1_per_class: [0.444, 0.396, 0.111, 0.422, 0.256, 0.0, 0.404, 0.0, 0.104, 0.246]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 26.5340 | Steps: 4 | Val loss: 5140.2739 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=122462)[0m top1: 0.12033582089552239
[2m[36m(func pid=122462)[0m top5: 0.40205223880597013
[2m[36m(func pid=122462)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=122462)[0m f1_macro: 0.04467359401656755
[2m[36m(func pid=122462)[0m f1_weighted: 0.044151673871994146
[2m[36m(func pid=122462)[0m f1_per_class: [0.07, 0.062, 0.0, 0.007, 0.0, 0.246, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:03:17 (running for 00:19:54.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.104 |      0.411 |                   55 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.112 |      0.238 |                   30 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.82  |      0.013 |                   28 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  5.069 |      0.045 |                   29 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4528917910447761
[2m[36m(func pid=116143)[0m top5: 0.9430970149253731
[2m[36m(func pid=116143)[0m f1_micro: 0.4528917910447761
[2m[36m(func pid=116143)[0m f1_macro: 0.41097068882579
[2m[36m(func pid=116143)[0m f1_weighted: 0.457608823862391
[2m[36m(func pid=116143)[0m f1_per_class: [0.593, 0.6, 0.436, 0.561, 0.171, 0.312, 0.36, 0.417, 0.326, 0.333]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.9238 | Steps: 4 | Val loss: 4.6809 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=122441)[0m top1: 0.03311567164179104
[2m[36m(func pid=122441)[0m top5: 0.37966417910447764
[2m[36m(func pid=122441)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=122441)[0m f1_macro: 0.006454545454545454
[2m[36m(func pid=122441)[0m f1_weighted: 0.002137466078697422
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.065, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.4930 | Steps: 4 | Val loss: 128.3383 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.1282 | Steps: 4 | Val loss: 1.6072 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=121898)[0m top1: 0.2896455223880597
[2m[36m(func pid=121898)[0m top5: 0.7719216417910447
[2m[36m(func pid=121898)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=121898)[0m f1_macro: 0.24462140545465152
[2m[36m(func pid=121898)[0m f1_weighted: 0.2878567796076419
[2m[36m(func pid=121898)[0m f1_per_class: [0.4, 0.315, 0.324, 0.203, 0.296, 0.0, 0.526, 0.007, 0.147, 0.228]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 9.3900 | Steps: 4 | Val loss: 3902.2949 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=122462)[0m top1: 0.12126865671641791
[2m[36m(func pid=122462)[0m top5: 0.28451492537313433
[2m[36m(func pid=122462)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=122462)[0m f1_macro: 0.04492723406148839
[2m[36m(func pid=122462)[0m f1_weighted: 0.04113362758953179
[2m[36m(func pid=122462)[0m f1_per_class: [0.088, 0.01, 0.0, 0.023, 0.0, 0.24, 0.003, 0.0, 0.085, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:03:23 (running for 00:20:00.03)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.128 |      0.405 |                   56 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.924 |      0.245 |                   31 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 26.534 |      0.006 |                   29 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.493 |      0.045 |                   30 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4412313432835821
[2m[36m(func pid=116143)[0m top5: 0.9263059701492538
[2m[36m(func pid=116143)[0m f1_micro: 0.4412313432835821
[2m[36m(func pid=116143)[0m f1_macro: 0.40525477941388
[2m[36m(func pid=116143)[0m f1_weighted: 0.43359524512517367
[2m[36m(func pid=116143)[0m f1_per_class: [0.624, 0.608, 0.394, 0.545, 0.16, 0.282, 0.29, 0.456, 0.354, 0.34]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2826 | Steps: 4 | Val loss: 4.2700 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=122441)[0m top1: 0.05457089552238806
[2m[36m(func pid=122441)[0m top5: 0.5111940298507462
[2m[36m(func pid=122441)[0m f1_micro: 0.05457089552238806
[2m[36m(func pid=122441)[0m f1_macro: 0.02061646842583565
[2m[36m(func pid=122441)[0m f1_weighted: 0.009103477179981443
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.092, 0.114, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 4.6987 | Steps: 4 | Val loss: 112.5000 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0812 | Steps: 4 | Val loss: 1.5989 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=121898)[0m top1: 0.35774253731343286
[2m[36m(func pid=121898)[0m top5: 0.8208955223880597
[2m[36m(func pid=121898)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=121898)[0m f1_macro: 0.292513431896139
[2m[36m(func pid=121898)[0m f1_weighted: 0.2989062614495343
[2m[36m(func pid=121898)[0m f1_per_class: [0.537, 0.365, 0.571, 0.111, 0.286, 0.09, 0.56, 0.086, 0.119, 0.2]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 15.0822 | Steps: 4 | Val loss: 4728.4507 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=122462)[0m top1: 0.12033582089552239
[2m[36m(func pid=122462)[0m top5: 0.7140858208955224
[2m[36m(func pid=122462)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=122462)[0m f1_macro: 0.04236595091087342
[2m[36m(func pid=122462)[0m f1_weighted: 0.03564756099403972
[2m[36m(func pid=122462)[0m f1_per_class: [0.094, 0.0, 0.0, 0.01, 0.0, 0.241, 0.003, 0.0, 0.076, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:03:28 (running for 00:20:05.39)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.081 |      0.405 |                   57 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.283 |      0.293 |                   32 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  9.39  |      0.021 |                   30 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.699 |      0.042 |                   31 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.44216417910447764
[2m[36m(func pid=116143)[0m top5: 0.925839552238806
[2m[36m(func pid=116143)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=116143)[0m f1_macro: 0.4053394456066859
[2m[36m(func pid=116143)[0m f1_weighted: 0.43277668513585177
[2m[36m(func pid=116143)[0m f1_per_class: [0.636, 0.618, 0.377, 0.541, 0.157, 0.276, 0.285, 0.448, 0.386, 0.33]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6465 | Steps: 4 | Val loss: 4.4177 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=122441)[0m top1: 0.06996268656716417
[2m[36m(func pid=122441)[0m top5: 0.601679104477612
[2m[36m(func pid=122441)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=122441)[0m f1_macro: 0.025595705443779638
[2m[36m(func pid=122441)[0m f1_weighted: 0.014248560875939472
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.136, 0.111, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 6.0146 | Steps: 4 | Val loss: 79.7094 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0362 | Steps: 4 | Val loss: 1.5584 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=121898)[0m top1: 0.3656716417910448
[2m[36m(func pid=121898)[0m top5: 0.8451492537313433
[2m[36m(func pid=121898)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=121898)[0m f1_macro: 0.2958575586277594
[2m[36m(func pid=121898)[0m f1_weighted: 0.29814767626165267
[2m[36m(func pid=121898)[0m f1_per_class: [0.513, 0.313, 0.511, 0.129, 0.338, 0.13, 0.537, 0.234, 0.041, 0.214]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 28.8021 | Steps: 4 | Val loss: 2542.8296 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=122462)[0m top1: 0.13432835820895522
[2m[36m(func pid=122462)[0m top5: 0.7467350746268657
[2m[36m(func pid=122462)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=122462)[0m f1_macro: 0.054787881899262536
[2m[36m(func pid=122462)[0m f1_weighted: 0.06288565331246106
[2m[36m(func pid=122462)[0m f1_per_class: [0.107, 0.0, 0.0, 0.109, 0.0, 0.235, 0.006, 0.0, 0.024, 0.067]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:03:33 (running for 00:20:10.78)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.036 |      0.399 |                   58 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.647 |      0.296 |                   33 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 15.082 |      0.026 |                   31 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  6.015 |      0.055 |                   32 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.439365671641791
[2m[36m(func pid=116143)[0m top5: 0.9398320895522388
[2m[36m(func pid=116143)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=116143)[0m f1_macro: 0.39938591438591386
[2m[36m(func pid=116143)[0m f1_weighted: 0.4371670178152443
[2m[36m(func pid=116143)[0m f1_per_class: [0.624, 0.608, 0.366, 0.555, 0.143, 0.264, 0.308, 0.429, 0.318, 0.38]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5543 | Steps: 4 | Val loss: 4.2213 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=122441)[0m top1: 0.029850746268656716
[2m[36m(func pid=122441)[0m top5: 0.6105410447761194
[2m[36m(func pid=122441)[0m f1_micro: 0.029850746268656716
[2m[36m(func pid=122441)[0m f1_macro: 0.023179316271208304
[2m[36m(func pid=122441)[0m f1_weighted: 0.021725533808761376
[2m[36m(func pid=122441)[0m f1_per_class: [0.019, 0.0, 0.032, 0.016, 0.0, 0.0, 0.043, 0.0, 0.122, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 5.5474 | Steps: 4 | Val loss: 57.3295 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1600 | Steps: 4 | Val loss: 1.5349 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=121898)[0m top1: 0.3871268656716418
[2m[36m(func pid=121898)[0m top5: 0.8610074626865671
[2m[36m(func pid=121898)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=121898)[0m f1_macro: 0.29232597871619354
[2m[36m(func pid=121898)[0m f1_weighted: 0.34546891322728274
[2m[36m(func pid=121898)[0m f1_per_class: [0.277, 0.261, 0.558, 0.324, 0.226, 0.129, 0.547, 0.303, 0.033, 0.266]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.23041044776119404
[2m[36m(func pid=122462)[0m top5: 0.7541977611940298
[2m[36m(func pid=122462)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=122462)[0m f1_macro: 0.04999912447103459
[2m[36m(func pid=122462)[0m f1_weighted: 0.10938867061162887
[2m[36m(func pid=122462)[0m f1_per_class: [0.116, 0.0, 0.0, 0.384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 5.0057 | Steps: 4 | Val loss: 1041.7260 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:03:39 (running for 00:20:16.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.16  |      0.402 |                   59 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.554 |      0.292 |                   34 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 28.802 |      0.023 |                   32 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  5.547 |      0.05  |                   33 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4458955223880597
[2m[36m(func pid=116143)[0m top5: 0.9435634328358209
[2m[36m(func pid=116143)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=116143)[0m f1_macro: 0.4023206085349589
[2m[36m(func pid=116143)[0m f1_weighted: 0.4439017978314548
[2m[36m(func pid=116143)[0m f1_per_class: [0.602, 0.609, 0.366, 0.553, 0.16, 0.279, 0.327, 0.427, 0.333, 0.368]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4791 | Steps: 4 | Val loss: 4.5617 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=122441)[0m top1: 0.02751865671641791
[2m[36m(func pid=122441)[0m top5: 0.5340485074626866
[2m[36m(func pid=122441)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=122441)[0m f1_macro: 0.01752620358351362
[2m[36m(func pid=122441)[0m f1_weighted: 0.02295864336366255
[2m[36m(func pid=122441)[0m f1_per_class: [0.052, 0.0, 0.018, 0.074, 0.028, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.7035 | Steps: 4 | Val loss: 46.9741 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0246 | Steps: 4 | Val loss: 1.6151 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=121898)[0m top1: 0.36473880597014924
[2m[36m(func pid=121898)[0m top5: 0.8255597014925373
[2m[36m(func pid=121898)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=121898)[0m f1_macro: 0.26824791708820234
[2m[36m(func pid=121898)[0m f1_weighted: 0.33899809912663303
[2m[36m(func pid=121898)[0m f1_per_class: [0.341, 0.225, 0.632, 0.377, 0.158, 0.013, 0.57, 0.137, 0.051, 0.178]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:03:44 (running for 00:20:21.35)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.16  |      0.402 |                   59 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.479 |      0.268 |                   35 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  5.006 |      0.018 |                   33 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  6.704 |      0.052 |                   34 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.23787313432835822
[2m[36m(func pid=122462)[0m top5: 0.7667910447761194
[2m[36m(func pid=122462)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=122462)[0m f1_macro: 0.051585052719181915
[2m[36m(func pid=122462)[0m f1_weighted: 0.11275321572912023
[2m[36m(func pid=122462)[0m f1_per_class: [0.121, 0.0, 0.0, 0.392, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 4.4041 | Steps: 4 | Val loss: 856.1842 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=116143)[0m top1: 0.43283582089552236
[2m[36m(func pid=116143)[0m top5: 0.933768656716418
[2m[36m(func pid=116143)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=116143)[0m f1_macro: 0.3948132547616314
[2m[36m(func pid=116143)[0m f1_weighted: 0.43438581606533355
[2m[36m(func pid=116143)[0m f1_per_class: [0.607, 0.602, 0.333, 0.545, 0.132, 0.246, 0.317, 0.432, 0.333, 0.4]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5794 | Steps: 4 | Val loss: 5.2528 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=122441)[0m top1: 0.020522388059701493
[2m[36m(func pid=122441)[0m top5: 0.46408582089552236
[2m[36m(func pid=122441)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=122441)[0m f1_macro: 0.014624009849056527
[2m[36m(func pid=122441)[0m f1_weighted: 0.012906369837642557
[2m[36m(func pid=122441)[0m f1_per_class: [0.043, 0.028, 0.017, 0.024, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.9367 | Steps: 4 | Val loss: 33.4720 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4682 | Steps: 4 | Val loss: 1.5556 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=121898)[0m top1: 0.36800373134328357
[2m[36m(func pid=121898)[0m top5: 0.7877798507462687
[2m[36m(func pid=121898)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=121898)[0m f1_macro: 0.235097800906736
[2m[36m(func pid=121898)[0m f1_weighted: 0.34113776791666356
[2m[36m(func pid=121898)[0m f1_per_class: [0.248, 0.166, 0.462, 0.4, 0.159, 0.013, 0.621, 0.0, 0.095, 0.187]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:03:49 (running for 00:20:26.76)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.025 |      0.395 |                   60 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.579 |      0.235 |                   36 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  4.404 |      0.015 |                   34 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.937 |      0.058 |                   35 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.2294776119402985
[2m[36m(func pid=122462)[0m top5: 0.7565298507462687
[2m[36m(func pid=122462)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=122462)[0m f1_macro: 0.058393426161229
[2m[36m(func pid=122462)[0m f1_weighted: 0.11436716483756348
[2m[36m(func pid=122462)[0m f1_per_class: [0.103, 0.0, 0.0, 0.382, 0.0, 0.0, 0.009, 0.0, 0.089, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.44263059701492535
[2m[36m(func pid=116143)[0m top5: 0.941231343283582
[2m[36m(func pid=116143)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=116143)[0m f1_macro: 0.4052744481598321
[2m[36m(func pid=116143)[0m f1_weighted: 0.4519345056334518
[2m[36m(func pid=116143)[0m f1_per_class: [0.611, 0.594, 0.462, 0.541, 0.142, 0.25, 0.39, 0.412, 0.291, 0.36]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 13.7469 | Steps: 4 | Val loss: 756.9299 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.9513 | Steps: 4 | Val loss: 6.4173 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 5.5919 | Steps: 4 | Val loss: 29.8436 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0335 | Steps: 4 | Val loss: 1.6161 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=122441)[0m top1: 0.09701492537313433
[2m[36m(func pid=122441)[0m top5: 0.46455223880597013
[2m[36m(func pid=122441)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=122441)[0m f1_macro: 0.04103077207322328
[2m[36m(func pid=122441)[0m f1_weighted: 0.06461225602976044
[2m[36m(func pid=122441)[0m f1_per_class: [0.046, 0.324, 0.012, 0.028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m top1: 0.35401119402985076
[2m[36m(func pid=121898)[0m top5: 0.7355410447761194
[2m[36m(func pid=121898)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=121898)[0m f1_macro: 0.19746587623849315
[2m[36m(func pid=121898)[0m f1_weighted: 0.324256243290678
[2m[36m(func pid=121898)[0m f1_per_class: [0.193, 0.027, 0.25, 0.433, 0.138, 0.007, 0.621, 0.031, 0.066, 0.208]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:03:55 (running for 00:20:32.27)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.468 |      0.405 |                   61 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.951 |      0.197 |                   37 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 13.747 |      0.041 |                   35 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  5.592 |      0.081 |                   36 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.29850746268656714
[2m[36m(func pid=122462)[0m top5: 0.6403917910447762
[2m[36m(func pid=122462)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=122462)[0m f1_macro: 0.08112147936011226
[2m[36m(func pid=122462)[0m f1_weighted: 0.1872821825821789
[2m[36m(func pid=122462)[0m f1_per_class: [0.093, 0.0, 0.0, 0.129, 0.0, 0.0, 0.492, 0.0, 0.078, 0.019]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.43050373134328357
[2m[36m(func pid=116143)[0m top5: 0.9319029850746269
[2m[36m(func pid=116143)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=116143)[0m f1_macro: 0.40136756253107164
[2m[36m(func pid=116143)[0m f1_weighted: 0.4260157201619704
[2m[36m(func pid=116143)[0m f1_per_class: [0.596, 0.602, 0.413, 0.525, 0.136, 0.242, 0.308, 0.423, 0.329, 0.438]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 6.9169 | Steps: 4 | Val loss: 590.7585 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3955 | Steps: 4 | Val loss: 6.8789 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.8305 | Steps: 4 | Val loss: 22.7963 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=122441)[0m top1: 0.15065298507462688
[2m[36m(func pid=122441)[0m top5: 0.4780783582089552
[2m[36m(func pid=122441)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=122441)[0m f1_macro: 0.06618187260795676
[2m[36m(func pid=122441)[0m f1_weighted: 0.1023473426581275
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.351, 0.013, 0.047, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0205 | Steps: 4 | Val loss: 1.6012 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=121898)[0m top1: 0.39505597014925375
[2m[36m(func pid=121898)[0m top5: 0.742070895522388
[2m[36m(func pid=121898)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=121898)[0m f1_macro: 0.2576568393651859
[2m[36m(func pid=121898)[0m f1_weighted: 0.36696163972794077
[2m[36m(func pid=121898)[0m f1_per_class: [0.259, 0.021, 0.324, 0.555, 0.126, 0.052, 0.577, 0.276, 0.08, 0.306]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:04:00 (running for 00:20:37.60)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.033 |      0.401 |                   62 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.395 |      0.258 |                   38 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  6.917 |      0.066 |                   36 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.83  |      0.017 |                   37 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.017723880597014924
[2m[36m(func pid=122462)[0m top5: 0.644589552238806
[2m[36m(func pid=122462)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=122462)[0m f1_macro: 0.017458243752687907
[2m[36m(func pid=122462)[0m f1_weighted: 0.010245576715386884
[2m[36m(func pid=122462)[0m f1_per_class: [0.057, 0.0, 0.0, 0.016, 0.0, 0.0, 0.006, 0.0, 0.072, 0.023]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.43796641791044777
[2m[36m(func pid=116143)[0m top5: 0.9356343283582089
[2m[36m(func pid=116143)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=116143)[0m f1_macro: 0.40085141237780003
[2m[36m(func pid=116143)[0m f1_weighted: 0.4414834111097016
[2m[36m(func pid=116143)[0m f1_per_class: [0.618, 0.598, 0.371, 0.534, 0.129, 0.23, 0.358, 0.437, 0.316, 0.416]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 11.7754 | Steps: 4 | Val loss: 573.9201 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1960 | Steps: 4 | Val loss: 7.2673 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 6.3429 | Steps: 4 | Val loss: 29.2741 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0204 | Steps: 4 | Val loss: 1.6708 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=122441)[0m top1: 0.1599813432835821
[2m[36m(func pid=122441)[0m top5: 0.46828358208955223
[2m[36m(func pid=122441)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=122441)[0m f1_macro: 0.07453440990916148
[2m[36m(func pid=122441)[0m f1_weighted: 0.11352938224567934
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.347, 0.026, 0.073, 0.0, 0.291, 0.0, 0.0, 0.0, 0.009]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m top1: 0.4048507462686567
[2m[36m(func pid=121898)[0m top5: 0.78125
[2m[36m(func pid=121898)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=121898)[0m f1_macro: 0.28930632246792415
[2m[36m(func pid=121898)[0m f1_weighted: 0.36071152252486005
[2m[36m(func pid=121898)[0m f1_per_class: [0.514, 0.027, 0.358, 0.61, 0.103, 0.051, 0.477, 0.301, 0.088, 0.364]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:04:06 (running for 00:20:43.04)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.021 |      0.401 |                   63 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.196 |      0.289 |                   39 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 11.775 |      0.075 |                   37 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  6.343 |      0.017 |                   38 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.01585820895522388
[2m[36m(func pid=122462)[0m top5: 0.5802238805970149
[2m[36m(func pid=122462)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=122462)[0m f1_macro: 0.01723238655355433
[2m[36m(func pid=122462)[0m f1_weighted: 0.004880646897266471
[2m[36m(func pid=122462)[0m f1_per_class: [0.085, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058, 0.023]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.42257462686567165
[2m[36m(func pid=116143)[0m top5: 0.9309701492537313
[2m[36m(func pid=116143)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=116143)[0m f1_macro: 0.378872471274179
[2m[36m(func pid=116143)[0m f1_weighted: 0.43171176315394244
[2m[36m(func pid=116143)[0m f1_per_class: [0.576, 0.593, 0.317, 0.52, 0.113, 0.202, 0.36, 0.432, 0.303, 0.371]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 14.6649 | Steps: 4 | Val loss: 514.1992 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3666 | Steps: 4 | Val loss: 7.2708 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.5268 | Steps: 4 | Val loss: 24.3296 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0779 | Steps: 4 | Val loss: 1.6716 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=122441)[0m top1: 0.11753731343283583
[2m[36m(func pid=122441)[0m top5: 0.4207089552238806
[2m[36m(func pid=122441)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=122441)[0m f1_macro: 0.05774899748459741
[2m[36m(func pid=122441)[0m f1_weighted: 0.08149122543956588
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.19, 0.034, 0.051, 0.0, 0.299, 0.0, 0.0, 0.0, 0.003]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m top1: 0.36473880597014924
[2m[36m(func pid=121898)[0m top5: 0.7868470149253731
[2m[36m(func pid=121898)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=121898)[0m f1_macro: 0.25215664097989504
[2m[36m(func pid=121898)[0m f1_weighted: 0.3294224299492062
[2m[36m(func pid=121898)[0m f1_per_class: [0.424, 0.119, 0.086, 0.612, 0.126, 0.008, 0.343, 0.287, 0.101, 0.415]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:04:11 (running for 00:20:48.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.02  |      0.379 |                   64 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.367 |      0.252 |                   40 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 14.665 |      0.058 |                   38 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.527 |      0.024 |                   39 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.017723880597014924
[2m[36m(func pid=122462)[0m top5: 0.5708955223880597
[2m[36m(func pid=122462)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=122462)[0m f1_macro: 0.024419213425662244
[2m[36m(func pid=122462)[0m f1_weighted: 0.019571403290706436
[2m[36m(func pid=122462)[0m f1_per_class: [0.109, 0.093, 0.001, 0.0, 0.0, 0.0, 0.0, 0.018, 0.0, 0.023]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.4221082089552239
[2m[36m(func pid=116143)[0m top5: 0.933768656716418
[2m[36m(func pid=116143)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=116143)[0m f1_macro: 0.3770936714986624
[2m[36m(func pid=116143)[0m f1_weighted: 0.43655378436611364
[2m[36m(func pid=116143)[0m f1_per_class: [0.629, 0.588, 0.274, 0.525, 0.103, 0.178, 0.383, 0.434, 0.294, 0.364]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.0700 | Steps: 4 | Val loss: 418.9290 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4907 | Steps: 4 | Val loss: 6.8048 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.7042 | Steps: 4 | Val loss: 16.8957 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0147 | Steps: 4 | Val loss: 1.6009 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=121898)[0m top1: 0.3316231343283582
[2m[36m(func pid=121898)[0m top5: 0.8115671641791045
[2m[36m(func pid=121898)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=121898)[0m f1_macro: 0.2608307066292362
[2m[36m(func pid=121898)[0m f1_weighted: 0.32395592683131236
[2m[36m(func pid=121898)[0m f1_per_class: [0.452, 0.356, 0.042, 0.583, 0.257, 0.008, 0.247, 0.101, 0.08, 0.483]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.2271455223880597
[2m[36m(func pid=122441)[0m top5: 0.5209888059701493
[2m[36m(func pid=122441)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=122441)[0m f1_macro: 0.09647959498904564
[2m[36m(func pid=122441)[0m f1_weighted: 0.1668705507975602
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.378, 0.037, 0.235, 0.0, 0.315, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:04:16 (running for 00:20:53.83)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.078 |      0.377 |                   65 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.491 |      0.261 |                   41 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.07  |      0.096 |                   39 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.704 |      0.048 |                   40 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.14738805970149255
[2m[36m(func pid=122462)[0m top5: 0.2980410447761194
[2m[36m(func pid=122462)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=122462)[0m f1_macro: 0.04828123587067868
[2m[36m(func pid=122462)[0m f1_weighted: 0.0529022216154553
[2m[36m(func pid=122462)[0m f1_per_class: [0.091, 0.277, 0.032, 0.0, 0.0, 0.0, 0.0, 0.016, 0.066, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.4361007462686567
[2m[36m(func pid=116143)[0m top5: 0.9398320895522388
[2m[36m(func pid=116143)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=116143)[0m f1_macro: 0.3815195507356821
[2m[36m(func pid=116143)[0m f1_weighted: 0.4494786903688422
[2m[36m(func pid=116143)[0m f1_per_class: [0.595, 0.586, 0.283, 0.536, 0.125, 0.211, 0.41, 0.409, 0.305, 0.356]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 11.2951 | Steps: 4 | Val loss: 606.8486 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5134 | Steps: 4 | Val loss: 5.8736 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 4.6451 | Steps: 4 | Val loss: 13.1282 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0711 | Steps: 4 | Val loss: 1.7010 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=121898)[0m top1: 0.363339552238806
[2m[36m(func pid=121898)[0m top5: 0.8073694029850746
[2m[36m(func pid=121898)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=121898)[0m f1_macro: 0.3069329929283895
[2m[36m(func pid=121898)[0m f1_weighted: 0.3957408659947628
[2m[36m(func pid=121898)[0m f1_per_class: [0.4, 0.444, 0.044, 0.537, 0.4, 0.016, 0.47, 0.123, 0.088, 0.549]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.23647388059701493
[2m[36m(func pid=122441)[0m top5: 0.675839552238806
[2m[36m(func pid=122441)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=122441)[0m f1_macro: 0.09632992291210388
[2m[36m(func pid=122441)[0m f1_weighted: 0.16563648348146004
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.383, 0.038, 0.217, 0.0, 0.316, 0.009, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:04:22 (running for 00:20:59.24)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.015 |      0.382 |                   66 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.513 |      0.307 |                   42 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 11.295 |      0.096 |                   40 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.645 |      0.045 |                   41 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.1478544776119403
[2m[36m(func pid=122462)[0m top5: 0.37966417910447764
[2m[36m(func pid=122462)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=122462)[0m f1_macro: 0.04455486945820193
[2m[36m(func pid=122462)[0m f1_weighted: 0.05238680200098988
[2m[36m(func pid=122462)[0m f1_per_class: [0.058, 0.28, 0.029, 0.0, 0.0, 0.0, 0.0, 0.009, 0.069, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.42117537313432835
[2m[36m(func pid=116143)[0m top5: 0.9351679104477612
[2m[36m(func pid=116143)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=116143)[0m f1_macro: 0.3737060601460589
[2m[36m(func pid=116143)[0m f1_weighted: 0.43868115773751937
[2m[36m(func pid=116143)[0m f1_per_class: [0.595, 0.571, 0.257, 0.49, 0.103, 0.184, 0.436, 0.414, 0.292, 0.395]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.2780 | Steps: 4 | Val loss: 5.0540 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 10.5784 | Steps: 4 | Val loss: 580.7930 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.7375 | Steps: 4 | Val loss: 17.5636 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1074 | Steps: 4 | Val loss: 1.7049 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=121898)[0m top1: 0.4281716417910448
[2m[36m(func pid=121898)[0m top5: 0.8125
[2m[36m(func pid=121898)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=121898)[0m f1_macro: 0.3192111195724602
[2m[36m(func pid=121898)[0m f1_weighted: 0.4355591810801051
[2m[36m(func pid=121898)[0m f1_per_class: [0.364, 0.421, 0.078, 0.508, 0.341, 0.023, 0.621, 0.239, 0.107, 0.491]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.24720149253731344
[2m[36m(func pid=122441)[0m top5: 0.6982276119402985
[2m[36m(func pid=122441)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=122441)[0m f1_macro: 0.11065974863787756
[2m[36m(func pid=122441)[0m f1_weighted: 0.17506051901282194
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.367, 0.158, 0.253, 0.0, 0.313, 0.015, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:04:27 (running for 00:21:04.65)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.071 |      0.374 |                   67 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.278 |      0.319 |                   43 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 10.578 |      0.111 |                   41 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.738 |      0.02  |                   42 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.014458955223880597
[2m[36m(func pid=122462)[0m top5: 0.39972014925373134
[2m[36m(func pid=122462)[0m f1_micro: 0.014458955223880597
[2m[36m(func pid=122462)[0m f1_macro: 0.019686945904762127
[2m[36m(func pid=122462)[0m f1_weighted: 0.005587722253182447
[2m[36m(func pid=122462)[0m f1_per_class: [0.059, 0.01, 0.053, 0.0, 0.018, 0.0, 0.0, 0.011, 0.046, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m top1: 0.41884328358208955
[2m[36m(func pid=116143)[0m top5: 0.9356343283582089
[2m[36m(func pid=116143)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=116143)[0m f1_macro: 0.3755834470657461
[2m[36m(func pid=116143)[0m f1_weighted: 0.4348478798731115
[2m[36m(func pid=116143)[0m f1_per_class: [0.618, 0.57, 0.313, 0.474, 0.106, 0.185, 0.441, 0.4, 0.276, 0.372]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1811 | Steps: 4 | Val loss: 7.2744 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.1961 | Steps: 4 | Val loss: 514.9976 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0703 | Steps: 4 | Val loss: 1.6898 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 5.3117 | Steps: 4 | Val loss: 24.2482 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=121898)[0m top1: 0.3064365671641791
[2m[36m(func pid=121898)[0m top5: 0.7220149253731343
[2m[36m(func pid=121898)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=121898)[0m f1_macro: 0.21743818906704937
[2m[36m(func pid=121898)[0m f1_weighted: 0.27164329684763944
[2m[36m(func pid=121898)[0m f1_per_class: [0.393, 0.173, 0.057, 0.136, 0.211, 0.022, 0.575, 0.247, 0.078, 0.283]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.20708955223880596
[2m[36m(func pid=122441)[0m top5: 0.7159514925373134
[2m[36m(func pid=122441)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=122441)[0m f1_macro: 0.12923860148068522
[2m[36m(func pid=122441)[0m f1_weighted: 0.14441221629459597
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.386, 0.333, 0.167, 0.0, 0.076, 0.012, 0.269, 0.049, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:04:33 (running for 00:21:10.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.07  |      0.379 |                   69 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.181 |      0.217 |                   44 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.196 |      0.129 |                   42 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.738 |      0.02  |                   42 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.43843283582089554
[2m[36m(func pid=116143)[0m top5: 0.9347014925373134
[2m[36m(func pid=116143)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=116143)[0m f1_macro: 0.37914046528636003
[2m[36m(func pid=116143)[0m f1_weighted: 0.4511910529208994
[2m[36m(func pid=116143)[0m f1_per_class: [0.569, 0.585, 0.28, 0.497, 0.112, 0.189, 0.459, 0.429, 0.311, 0.361]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.01585820895522388
[2m[36m(func pid=122462)[0m top5: 0.40625
[2m[36m(func pid=122462)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=122462)[0m f1_macro: 0.02157805714427847
[2m[36m(func pid=122462)[0m f1_weighted: 0.0053223737572874645
[2m[36m(func pid=122462)[0m f1_per_class: [0.072, 0.005, 0.043, 0.0, 0.019, 0.0, 0.0, 0.0, 0.076, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1682 | Steps: 4 | Val loss: 9.9455 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6620 | Steps: 4 | Val loss: 344.3229 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4454 | Steps: 4 | Val loss: 1.7543 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.3327 | Steps: 4 | Val loss: 33.6349 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=121898)[0m top1: 0.2896455223880597
[2m[36m(func pid=121898)[0m top5: 0.5363805970149254
[2m[36m(func pid=121898)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=121898)[0m f1_macro: 0.19103303836005098
[2m[36m(func pid=121898)[0m f1_weighted: 0.20388460115066726
[2m[36m(func pid=121898)[0m f1_per_class: [0.344, 0.037, 0.156, 0.003, 0.263, 0.047, 0.527, 0.349, 0.086, 0.098]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:04:38 (running for 00:21:15.28)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.07  |      0.379 |                   69 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.168 |      0.191 |                   45 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.662 |      0.161 |                   43 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  5.312 |      0.022 |                   43 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122441)[0m top1: 0.2080223880597015
[2m[36m(func pid=122441)[0m top5: 0.7313432835820896
[2m[36m(func pid=122441)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=122441)[0m f1_macro: 0.16137361407247686
[2m[36m(func pid=122441)[0m f1_weighted: 0.16420272553471182
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.343, 0.64, 0.287, 0.0, 0.033, 0.006, 0.23, 0.076, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m top1: 0.43330223880597013
[2m[36m(func pid=116143)[0m top5: 0.9267723880597015
[2m[36m(func pid=116143)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=116143)[0m f1_macro: 0.36739884580335197
[2m[36m(func pid=116143)[0m f1_weighted: 0.4415974055914942
[2m[36m(func pid=116143)[0m f1_per_class: [0.555, 0.584, 0.257, 0.504, 0.104, 0.113, 0.454, 0.42, 0.305, 0.38]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.03451492537313433
[2m[36m(func pid=122462)[0m top5: 0.4141791044776119
[2m[36m(func pid=122462)[0m f1_micro: 0.03451492537313433
[2m[36m(func pid=122462)[0m f1_macro: 0.02099389728551119
[2m[36m(func pid=122462)[0m f1_weighted: 0.011346125987768678
[2m[36m(func pid=122462)[0m f1_per_class: [0.089, 0.005, 0.032, 0.023, 0.0, 0.0, 0.0, 0.0, 0.061, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.9755 | Steps: 4 | Val loss: 10.3977 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.1570 | Steps: 4 | Val loss: 179.1498 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1044 | Steps: 4 | Val loss: 1.8449 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 4.0898 | Steps: 4 | Val loss: 19.9638 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:04:43 (running for 00:21:20.69)
Memory usage on this node: 26.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.445 |      0.367 |                   70 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.168 |      0.191 |                   45 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.662 |      0.161 |                   43 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.333 |      0.021 |                   44 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121898)[0m top1: 0.27238805970149255
[2m[36m(func pid=121898)[0m top5: 0.49486940298507465
[2m[36m(func pid=121898)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=121898)[0m f1_macro: 0.17006347209627684
[2m[36m(func pid=121898)[0m f1_weighted: 0.1938373727003714
[2m[36m(func pid=121898)[0m f1_per_class: [0.256, 0.016, 0.111, 0.0, 0.205, 0.008, 0.529, 0.335, 0.136, 0.105]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.21361940298507462
[2m[36m(func pid=122441)[0m top5: 0.7294776119402985
[2m[36m(func pid=122441)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=122441)[0m f1_macro: 0.16580440014668169
[2m[36m(func pid=122441)[0m f1_weighted: 0.18914845314160708
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.307, 0.64, 0.371, 0.0, 0.058, 0.03, 0.201, 0.051, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m top1: 0.4076492537313433
[2m[36m(func pid=116143)[0m top5: 0.909981343283582
[2m[36m(func pid=116143)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=116143)[0m f1_macro: 0.3551050481955378
[2m[36m(func pid=116143)[0m f1_weighted: 0.40970202380877013
[2m[36m(func pid=116143)[0m f1_per_class: [0.545, 0.577, 0.236, 0.488, 0.098, 0.12, 0.362, 0.42, 0.308, 0.395]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.04617537313432836
[2m[36m(func pid=122462)[0m top5: 0.498134328358209
[2m[36m(func pid=122462)[0m f1_micro: 0.04617537313432836
[2m[36m(func pid=122462)[0m f1_macro: 0.029147327340003366
[2m[36m(func pid=122462)[0m f1_weighted: 0.03288331284886257
[2m[36m(func pid=122462)[0m f1_per_class: [0.069, 0.005, 0.057, 0.101, 0.0, 0.0, 0.0, 0.0, 0.059, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4729 | Steps: 4 | Val loss: 9.4886 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.5844 | Steps: 4 | Val loss: 153.1425 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0192 | Steps: 4 | Val loss: 1.9025 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.8303 | Steps: 4 | Val loss: 18.5592 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 14:04:49 (running for 00:21:26.21)
Memory usage on this node: 26.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.104 |      0.355 |                   71 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.976 |      0.17  |                   46 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.157 |      0.166 |                   44 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.09  |      0.029 |                   45 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122441)[0m top1: 0.14925373134328357
[2m[36m(func pid=122441)[0m top5: 0.715018656716418
[2m[36m(func pid=122441)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=122441)[0m f1_macro: 0.13672516850834676
[2m[36m(func pid=122441)[0m f1_weighted: 0.1201595078433241
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.298, 0.571, 0.147, 0.018, 0.043, 0.018, 0.203, 0.068, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m top1: 0.15625
[2m[36m(func pid=121898)[0m top5: 0.5251865671641791
[2m[36m(func pid=121898)[0m f1_micro: 0.15625
[2m[36m(func pid=121898)[0m f1_macro: 0.1336413106793189
[2m[36m(func pid=121898)[0m f1_weighted: 0.15629106353024788
[2m[36m(func pid=121898)[0m f1_per_class: [0.092, 0.165, 0.1, 0.026, 0.333, 0.006, 0.359, 0.06, 0.104, 0.09]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.14832089552238806
[2m[36m(func pid=122462)[0m top5: 0.5051305970149254
[2m[36m(func pid=122462)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=122462)[0m f1_macro: 0.05991178422268029
[2m[36m(func pid=122462)[0m f1_weighted: 0.0862112077645391
[2m[36m(func pid=122462)[0m f1_per_class: [0.071, 0.0, 0.042, 0.2, 0.0, 0.236, 0.0, 0.0, 0.05, 0.0]
[2m[36m(func pid=116143)[0m top1: 0.3885261194029851
[2m[36m(func pid=116143)[0m top5: 0.9095149253731343
[2m[36m(func pid=116143)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=116143)[0m f1_macro: 0.35239814114402584
[2m[36m(func pid=116143)[0m f1_weighted: 0.3896106784450484
[2m[36m(func pid=116143)[0m f1_per_class: [0.564, 0.571, 0.252, 0.466, 0.092, 0.133, 0.314, 0.397, 0.34, 0.395]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2430 | Steps: 4 | Val loss: 9.9666 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.5898 | Steps: 4 | Val loss: 201.1006 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0168 | Steps: 4 | Val loss: 1.8041 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.8154 | Steps: 4 | Val loss: 7.5726 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 14:04:54 (running for 00:21:31.77)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.019 |      0.352 |                   72 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.473 |      0.134 |                   47 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.584 |      0.137 |                   45 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.83  |      0.06  |                   46 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4006529850746269
[2m[36m(func pid=116143)[0m top5: 0.9211753731343284
[2m[36m(func pid=116143)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=116143)[0m f1_macro: 0.3613719825741737
[2m[36m(func pid=116143)[0m f1_weighted: 0.4112807232576977
[2m[36m(func pid=116143)[0m f1_per_class: [0.591, 0.581, 0.241, 0.496, 0.093, 0.172, 0.339, 0.397, 0.308, 0.395]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.11520522388059702
[2m[36m(func pid=121898)[0m top5: 0.5023320895522388
[2m[36m(func pid=121898)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=121898)[0m f1_macro: 0.11313435664595854
[2m[36m(func pid=121898)[0m f1_weighted: 0.12258969678534583
[2m[36m(func pid=121898)[0m f1_per_class: [0.093, 0.202, 0.0, 0.075, 0.409, 0.023, 0.188, 0.01, 0.062, 0.07]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.197294776119403
[2m[36m(func pid=122441)[0m top5: 0.7229477611940298
[2m[36m(func pid=122441)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=122441)[0m f1_macro: 0.13539560238701004
[2m[36m(func pid=122441)[0m f1_weighted: 0.16037584996792054
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.349, 0.392, 0.251, 0.0, 0.034, 0.03, 0.21, 0.088, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m top1: 0.1525186567164179
[2m[36m(func pid=122462)[0m top5: 0.46361940298507465
[2m[36m(func pid=122462)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=122462)[0m f1_macro: 0.06961269567306769
[2m[36m(func pid=122462)[0m f1_weighted: 0.09058559670331331
[2m[36m(func pid=122462)[0m f1_per_class: [0.056, 0.0, 0.16, 0.218, 0.0, 0.223, 0.003, 0.0, 0.035, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2665 | Steps: 4 | Val loss: 1.9263 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4894 | Steps: 4 | Val loss: 8.6215 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 6.1618 | Steps: 4 | Val loss: 182.0795 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 3.3801 | Steps: 4 | Val loss: 7.9551 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:05:00 (running for 00:21:37.33)
Memory usage on this node: 26.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.017 |      0.361 |                   73 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.243 |      0.113 |                   48 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.59  |      0.135 |                   46 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.815 |      0.07  |                   47 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3833955223880597
[2m[36m(func pid=116143)[0m top5: 0.9057835820895522
[2m[36m(func pid=116143)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=116143)[0m f1_macro: 0.34674494941207745
[2m[36m(func pid=116143)[0m f1_weighted: 0.38159398754984414
[2m[36m(func pid=116143)[0m f1_per_class: [0.556, 0.587, 0.213, 0.499, 0.088, 0.131, 0.25, 0.399, 0.322, 0.423]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.12779850746268656
[2m[36m(func pid=121898)[0m top5: 0.5839552238805971
[2m[36m(func pid=121898)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=121898)[0m f1_macro: 0.12832815593126257
[2m[36m(func pid=121898)[0m f1_weighted: 0.14539740619597097
[2m[36m(func pid=121898)[0m f1_per_class: [0.18, 0.188, 0.038, 0.151, 0.333, 0.03, 0.183, 0.066, 0.064, 0.05]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.20755597014925373
[2m[36m(func pid=122441)[0m top5: 0.7257462686567164
[2m[36m(func pid=122441)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=122441)[0m f1_macro: 0.12948171726012098
[2m[36m(func pid=122441)[0m f1_weighted: 0.18170889102826251
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.33, 0.281, 0.326, 0.0, 0.029, 0.048, 0.214, 0.067, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m top1: 0.14692164179104478
[2m[36m(func pid=122462)[0m top5: 0.46408582089552236
[2m[36m(func pid=122462)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=122462)[0m f1_macro: 0.07668856125117154
[2m[36m(func pid=122462)[0m f1_weighted: 0.0842474925735466
[2m[36m(func pid=122462)[0m f1_per_class: [0.07, 0.0, 0.2, 0.189, 0.0, 0.224, 0.006, 0.0, 0.034, 0.044]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0125 | Steps: 4 | Val loss: 1.8603 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3802 | Steps: 4 | Val loss: 8.7034 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.8884 | Steps: 4 | Val loss: 6.0313 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 10.1519 | Steps: 4 | Val loss: 140.2841 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 14:05:06 (running for 00:21:42.90)
Memory usage on this node: 26.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.35000000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.266 |      0.347 |                   74 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.489 |      0.128 |                   49 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  6.162 |      0.129 |                   47 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.38  |      0.077 |                   48 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3941231343283582
[2m[36m(func pid=116143)[0m top5: 0.9151119402985075
[2m[36m(func pid=116143)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=116143)[0m f1_macro: 0.35549042028092653
[2m[36m(func pid=116143)[0m f1_weighted: 0.39998693930864726
[2m[36m(func pid=116143)[0m f1_per_class: [0.532, 0.59, 0.248, 0.502, 0.092, 0.17, 0.294, 0.398, 0.325, 0.405]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.12360074626865672
[2m[36m(func pid=122462)[0m top5: 0.7234141791044776
[2m[36m(func pid=122462)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=122462)[0m f1_macro: 0.07777855964854205
[2m[36m(func pid=122462)[0m f1_weighted: 0.043670276835915865
[2m[36m(func pid=122462)[0m f1_per_class: [0.023, 0.0, 0.409, 0.042, 0.0, 0.221, 0.006, 0.0, 0.047, 0.029]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.11473880597014925
[2m[36m(func pid=121898)[0m top5: 0.7024253731343284
[2m[36m(func pid=121898)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=121898)[0m f1_macro: 0.1285976577850353
[2m[36m(func pid=121898)[0m f1_weighted: 0.14162744628172358
[2m[36m(func pid=121898)[0m f1_per_class: [0.182, 0.117, 0.043, 0.127, 0.23, 0.075, 0.188, 0.241, 0.036, 0.048]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.1455223880597015
[2m[36m(func pid=122441)[0m top5: 0.6926305970149254
[2m[36m(func pid=122441)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=122441)[0m f1_macro: 0.12619162783789875
[2m[36m(func pid=122441)[0m f1_weighted: 0.12692097244650694
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.291, 0.439, 0.16, 0.0, 0.041, 0.032, 0.219, 0.073, 0.006]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0516 | Steps: 4 | Val loss: 1.8697 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 4.2814 | Steps: 4 | Val loss: 8.7933 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6307 | Steps: 4 | Val loss: 9.9631 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.9579 | Steps: 4 | Val loss: 116.5626 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:05:11 (running for 00:21:48.44)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.013 |      0.355 |                   75 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.38  |      0.129 |                   50 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 | 10.152 |      0.126 |                   48 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.888 |      0.078 |                   49 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.38526119402985076
[2m[36m(func pid=116143)[0m top5: 0.9174440298507462
[2m[36m(func pid=116143)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=116143)[0m f1_macro: 0.3421274928641249
[2m[36m(func pid=116143)[0m f1_weighted: 0.39123840255842124
[2m[36m(func pid=116143)[0m f1_per_class: [0.521, 0.59, 0.211, 0.524, 0.094, 0.171, 0.254, 0.374, 0.282, 0.4]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.11847014925373134
[2m[36m(func pid=122462)[0m top5: 0.7159514925373134
[2m[36m(func pid=122462)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=122462)[0m f1_macro: 0.06420936170837958
[2m[36m(func pid=122462)[0m f1_weighted: 0.036396436069903414
[2m[36m(func pid=122462)[0m f1_per_class: [0.057, 0.02, 0.246, 0.003, 0.0, 0.227, 0.003, 0.0, 0.066, 0.019]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.12779850746268656
[2m[36m(func pid=121898)[0m top5: 0.6590485074626866
[2m[36m(func pid=121898)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=121898)[0m f1_macro: 0.11970287283130351
[2m[36m(func pid=121898)[0m f1_weighted: 0.1407610471535377
[2m[36m(func pid=121898)[0m f1_per_class: [0.0, 0.093, 0.038, 0.096, 0.131, 0.24, 0.164, 0.294, 0.083, 0.058]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.07276119402985075
[2m[36m(func pid=122441)[0m top5: 0.5825559701492538
[2m[36m(func pid=122441)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=122441)[0m f1_macro: 0.07909956391551191
[2m[36m(func pid=122441)[0m f1_weighted: 0.07266937486795497
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.245, 0.351, 0.073, 0.021, 0.028, 0.009, 0.0, 0.06, 0.004]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.1127 | Steps: 4 | Val loss: 1.8559 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.4414 | Steps: 4 | Val loss: 9.8061 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.8707 | Steps: 4 | Val loss: 9.6784 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 4.7885 | Steps: 4 | Val loss: 106.5305 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:05:17 (running for 00:21:53.97)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.052 |      0.342 |                   76 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.631 |      0.12  |                   51 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.958 |      0.079 |                   49 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.281 |      0.064 |                   50 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.38152985074626866
[2m[36m(func pid=116143)[0m top5: 0.9216417910447762
[2m[36m(func pid=116143)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=116143)[0m f1_macro: 0.3485581668908301
[2m[36m(func pid=116143)[0m f1_weighted: 0.39474819257279353
[2m[36m(func pid=116143)[0m f1_per_class: [0.553, 0.577, 0.184, 0.489, 0.101, 0.193, 0.294, 0.357, 0.322, 0.416]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.018656716417910446
[2m[36m(func pid=122462)[0m top5: 0.7192164179104478
[2m[36m(func pid=122462)[0m f1_micro: 0.018656716417910446
[2m[36m(func pid=122462)[0m f1_macro: 0.03438588061300659
[2m[36m(func pid=122462)[0m f1_weighted: 0.009866903727997381
[2m[36m(func pid=122462)[0m f1_per_class: [0.043, 0.015, 0.188, 0.003, 0.0, 0.0, 0.006, 0.0, 0.065, 0.023]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.1525186567164179
[2m[36m(func pid=121898)[0m top5: 0.6576492537313433
[2m[36m(func pid=121898)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=121898)[0m f1_macro: 0.11668215707653698
[2m[36m(func pid=121898)[0m f1_weighted: 0.17386672334888656
[2m[36m(func pid=121898)[0m f1_per_class: [0.0, 0.078, 0.007, 0.256, 0.082, 0.217, 0.165, 0.152, 0.145, 0.065]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.0853544776119403
[2m[36m(func pid=122441)[0m top5: 0.5471082089552238
[2m[36m(func pid=122441)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=122441)[0m f1_macro: 0.07926778623773222
[2m[36m(func pid=122441)[0m f1_weighted: 0.08469509927159327
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.27, 0.299, 0.111, 0.019, 0.028, 0.0, 0.0, 0.06, 0.006]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.1169 | Steps: 4 | Val loss: 1.7797 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.9055 | Steps: 4 | Val loss: 13.4311 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4168 | Steps: 4 | Val loss: 9.2998 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.8629 | Steps: 4 | Val loss: 93.4357 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:05:22 (running for 00:21:59.34)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.117 |      0.357 |                   78 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.871 |      0.117 |                   52 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  4.788 |      0.079 |                   50 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.441 |      0.034 |                   51 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.3969216417910448
[2m[36m(func pid=116143)[0m top5: 0.933768656716418
[2m[36m(func pid=116143)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=116143)[0m f1_macro: 0.3572077275597565
[2m[36m(func pid=116143)[0m f1_weighted: 0.41488110864330496
[2m[36m(func pid=116143)[0m f1_per_class: [0.586, 0.581, 0.203, 0.492, 0.112, 0.208, 0.355, 0.357, 0.263, 0.416]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.020522388059701493
[2m[36m(func pid=122462)[0m top5: 0.7159514925373134
[2m[36m(func pid=122462)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=122462)[0m f1_macro: 0.032143847344438016
[2m[36m(func pid=122462)[0m f1_weighted: 0.012201289065068167
[2m[36m(func pid=122462)[0m f1_per_class: [0.044, 0.036, 0.149, 0.003, 0.0, 0.0, 0.003, 0.0, 0.062, 0.023]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.17257462686567165
[2m[36m(func pid=121898)[0m top5: 0.675839552238806
[2m[36m(func pid=121898)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=121898)[0m f1_macro: 0.12333356450469975
[2m[36m(func pid=121898)[0m f1_weighted: 0.19942142027807927
[2m[36m(func pid=121898)[0m f1_per_class: [0.0, 0.082, 0.022, 0.282, 0.106, 0.244, 0.231, 0.066, 0.143, 0.058]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.09328358208955224
[2m[36m(func pid=122441)[0m top5: 0.5625
[2m[36m(func pid=122441)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=122441)[0m f1_macro: 0.07192163531487772
[2m[36m(func pid=122441)[0m f1_weighted: 0.08719808309102747
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.292, 0.206, 0.103, 0.015, 0.043, 0.0, 0.0, 0.06, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0090 | Steps: 4 | Val loss: 1.6909 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 5.9980 | Steps: 4 | Val loss: 7.9132 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.1734 | Steps: 4 | Val loss: 10.7198 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.0477 | Steps: 4 | Val loss: 74.9518 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:05:27 (running for 00:22:04.59)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.009 |      0.366 |                   79 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.417 |      0.123 |                   53 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.863 |      0.072 |                   51 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.906 |      0.032 |                   52 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4137126865671642
[2m[36m(func pid=116143)[0m top5: 0.9416977611940298
[2m[36m(func pid=116143)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=116143)[0m f1_macro: 0.36558351554832347
[2m[36m(func pid=116143)[0m f1_weighted: 0.4312757963085338
[2m[36m(func pid=116143)[0m f1_per_class: [0.593, 0.598, 0.232, 0.527, 0.118, 0.227, 0.358, 0.372, 0.252, 0.378]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.017723880597014924
[2m[36m(func pid=122462)[0m top5: 0.6688432835820896
[2m[36m(func pid=122462)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=122462)[0m f1_macro: 0.024646582952649633
[2m[36m(func pid=122462)[0m f1_weighted: 0.011035711537197138
[2m[36m(func pid=122462)[0m f1_per_class: [0.024, 0.015, 0.125, 0.013, 0.0, 0.0, 0.006, 0.0, 0.04, 0.023]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.16277985074626866
[2m[36m(func pid=121898)[0m top5: 0.6898320895522388
[2m[36m(func pid=121898)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=121898)[0m f1_macro: 0.1338863191794992
[2m[36m(func pid=121898)[0m f1_weighted: 0.1792030453532318
[2m[36m(func pid=121898)[0m f1_per_class: [0.122, 0.073, 0.134, 0.179, 0.071, 0.267, 0.25, 0.046, 0.135, 0.062]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.12126865671641791
[2m[36m(func pid=122441)[0m top5: 0.5904850746268657
[2m[36m(func pid=122441)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=122441)[0m f1_macro: 0.10403972561196786
[2m[36m(func pid=122441)[0m f1_weighted: 0.11180000153193174
[2m[36m(func pid=122441)[0m f1_per_class: [0.069, 0.327, 0.351, 0.16, 0.029, 0.045, 0.0, 0.0, 0.059, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.1350 | Steps: 4 | Val loss: 1.6491 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.4296 | Steps: 4 | Val loss: 7.8318 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2642 | Steps: 4 | Val loss: 12.2230 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 3.0551 | Steps: 4 | Val loss: 87.6285 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 14:05:33 (running for 00:22:10.00)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.135 |      0.375 |                   80 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.173 |      0.134 |                   54 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.048 |      0.104 |                   52 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  5.998 |      0.025 |                   53 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4295708955223881
[2m[36m(func pid=116143)[0m top5: 0.9426305970149254
[2m[36m(func pid=116143)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=116143)[0m f1_macro: 0.375138696521608
[2m[36m(func pid=116143)[0m f1_weighted: 0.4466723773753578
[2m[36m(func pid=116143)[0m f1_per_class: [0.593, 0.597, 0.22, 0.557, 0.132, 0.239, 0.377, 0.368, 0.248, 0.419]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.01958955223880597
[2m[36m(func pid=122462)[0m top5: 0.6665111940298507
[2m[36m(func pid=122462)[0m f1_micro: 0.01958955223880597
[2m[36m(func pid=122462)[0m f1_macro: 0.01520327765840158
[2m[36m(func pid=122462)[0m f1_weighted: 0.015394612312635458
[2m[36m(func pid=122462)[0m f1_per_class: [0.022, 0.0, 0.0, 0.032, 0.0, 0.0, 0.012, 0.0, 0.062, 0.023]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.15485074626865672
[2m[36m(func pid=121898)[0m top5: 0.6902985074626866
[2m[36m(func pid=121898)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=121898)[0m f1_macro: 0.1195667610714709
[2m[36m(func pid=121898)[0m f1_weighted: 0.17984760498270688
[2m[36m(func pid=121898)[0m f1_per_class: [0.0, 0.176, 0.091, 0.154, 0.087, 0.17, 0.265, 0.0, 0.192, 0.061]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.14925373134328357
[2m[36m(func pid=122441)[0m top5: 0.5904850746268657
[2m[36m(func pid=122441)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=122441)[0m f1_macro: 0.12664748761304517
[2m[36m(func pid=122441)[0m f1_weighted: 0.13712309314356003
[2m[36m(func pid=122441)[0m f1_per_class: [0.057, 0.382, 0.5, 0.233, 0.014, 0.0, 0.0, 0.0, 0.05, 0.03]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.1339 | Steps: 4 | Val loss: 1.7137 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.8954 | Steps: 4 | Val loss: 15.9933 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.4261 | Steps: 4 | Val loss: 4.5893 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.8046 | Steps: 4 | Val loss: 32.0108 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 14:05:38 (running for 00:22:15.42)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.134 |      0.36  |                   81 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  2.264 |      0.12  |                   55 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.055 |      0.127 |                   53 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.43  |      0.015 |                   54 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4137126865671642
[2m[36m(func pid=116143)[0m top5: 0.9323694029850746
[2m[36m(func pid=116143)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=116143)[0m f1_macro: 0.36030146713117106
[2m[36m(func pid=116143)[0m f1_weighted: 0.4335634737183067
[2m[36m(func pid=116143)[0m f1_per_class: [0.537, 0.583, 0.23, 0.507, 0.137, 0.242, 0.395, 0.36, 0.225, 0.386]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m top1: 0.10587686567164178
[2m[36m(func pid=121898)[0m top5: 0.5746268656716418
[2m[36m(func pid=121898)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=121898)[0m f1_macro: 0.09553148997007327
[2m[36m(func pid=121898)[0m f1_weighted: 0.1235275802691232
[2m[36m(func pid=121898)[0m f1_per_class: [0.053, 0.167, 0.07, 0.003, 0.069, 0.117, 0.24, 0.012, 0.171, 0.054]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.0648320895522388
[2m[36m(func pid=122462)[0m top5: 0.8064365671641791
[2m[36m(func pid=122462)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=122462)[0m f1_macro: 0.027394253988082724
[2m[36m(func pid=122462)[0m f1_weighted: 0.022377946414495333
[2m[36m(func pid=122462)[0m f1_per_class: [0.028, 0.01, 0.0, 0.026, 0.0, 0.0, 0.015, 0.113, 0.035, 0.045]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.12826492537313433
[2m[36m(func pid=122441)[0m top5: 0.6296641791044776
[2m[36m(func pid=122441)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=122441)[0m f1_macro: 0.11620392738078737
[2m[36m(func pid=122441)[0m f1_weighted: 0.1384758587513554
[2m[36m(func pid=122441)[0m f1_per_class: [0.071, 0.22, 0.421, 0.335, 0.034, 0.0, 0.003, 0.0, 0.044, 0.033]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0863 | Steps: 4 | Val loss: 1.6236 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.0734 | Steps: 4 | Val loss: 3.5925 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.0871 | Steps: 4 | Val loss: 18.4760 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.5856 | Steps: 4 | Val loss: 47.6459 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=116143)[0m top1: 0.44169776119402987
[2m[36m(func pid=116143)[0m top5: 0.9370335820895522
[2m[36m(func pid=116143)[0m f1_micro: 0.4416977611940298
[2m[36m(func pid=116143)[0m f1_macro: 0.37307203718452253
[2m[36m(func pid=116143)[0m f1_weighted: 0.4663543814580483
[2m[36m(func pid=116143)[0m f1_per_class: [0.526, 0.561, 0.22, 0.579, 0.183, 0.287, 0.436, 0.36, 0.218, 0.362]
== Status ==
Current time: 2024-01-07 14:05:43 (running for 00:22:20.83)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.086 |      0.373 |                   82 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.895 |      0.096 |                   56 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.805 |      0.116 |                   54 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.426 |      0.027 |                   55 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.06296641791044776
[2m[36m(func pid=122462)[0m top5: 0.8069029850746269
[2m[36m(func pid=122462)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=122462)[0m f1_macro: 0.06137606651986218
[2m[36m(func pid=122462)[0m f1_weighted: 0.015462484342148471
[2m[36m(func pid=122462)[0m f1_per_class: [0.032, 0.005, 0.414, 0.01, 0.0, 0.0, 0.003, 0.112, 0.037, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.1296641791044776
[2m[36m(func pid=121898)[0m top5: 0.47527985074626866
[2m[36m(func pid=121898)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=121898)[0m f1_macro: 0.13530632534073514
[2m[36m(func pid=121898)[0m f1_weighted: 0.1079335721621433
[2m[36m(func pid=121898)[0m f1_per_class: [0.131, 0.021, 0.194, 0.0, 0.222, 0.3, 0.161, 0.278, 0.0, 0.045]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.2751865671641791
[2m[36m(func pid=122441)[0m top5: 0.6180037313432836
[2m[36m(func pid=122441)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=122441)[0m f1_macro: 0.1103446116631773
[2m[36m(func pid=122441)[0m f1_weighted: 0.27426032597800415
[2m[36m(func pid=122441)[0m f1_per_class: [0.067, 0.056, 0.062, 0.4, 0.012, 0.0, 0.507, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3264 | Steps: 4 | Val loss: 1.6022 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.6498 | Steps: 4 | Val loss: 3.6987 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3770 | Steps: 4 | Val loss: 14.0393 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.9985 | Steps: 4 | Val loss: 36.5218 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:05:49 (running for 00:22:26.09)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.326 |      0.384 |                   83 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  3.087 |      0.135 |                   57 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.586 |      0.11  |                   55 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.073 |      0.061 |                   56 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4458955223880597
[2m[36m(func pid=116143)[0m top5: 0.9416977611940298
[2m[36m(func pid=116143)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=116143)[0m f1_macro: 0.3842331490729753
[2m[36m(func pid=116143)[0m f1_weighted: 0.4665502810911082
[2m[36m(func pid=116143)[0m f1_per_class: [0.532, 0.583, 0.325, 0.56, 0.186, 0.295, 0.435, 0.339, 0.27, 0.317]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.06529850746268656
[2m[36m(func pid=122462)[0m top5: 0.5410447761194029
[2m[36m(func pid=122462)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=122462)[0m f1_macro: 0.05342539518999846
[2m[36m(func pid=122462)[0m f1_weighted: 0.018954089724584507
[2m[36m(func pid=122462)[0m f1_per_class: [0.034, 0.005, 0.333, 0.026, 0.0, 0.0, 0.003, 0.113, 0.02, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.15298507462686567
[2m[36m(func pid=121898)[0m top5: 0.5424440298507462
[2m[36m(func pid=121898)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=121898)[0m f1_macro: 0.17085436603057622
[2m[36m(func pid=121898)[0m f1_weighted: 0.13176607811968133
[2m[36m(func pid=121898)[0m f1_per_class: [0.199, 0.034, 0.299, 0.0, 0.389, 0.305, 0.239, 0.179, 0.0, 0.065]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.2248134328358209
[2m[36m(func pid=122441)[0m top5: 0.6119402985074627
[2m[36m(func pid=122441)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=122441)[0m f1_macro: 0.08621983654247822
[2m[36m(func pid=122441)[0m f1_weighted: 0.21642625080268027
[2m[36m(func pid=122441)[0m f1_per_class: [0.076, 0.035, 0.038, 0.205, 0.0, 0.0, 0.508, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0300 | Steps: 4 | Val loss: 1.6454 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.4850 | Steps: 4 | Val loss: 3.4329 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.7239 | Steps: 4 | Val loss: 13.0717 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 6.6270 | Steps: 4 | Val loss: 41.9842 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=116143)[0m top1: 0.43236940298507465
[2m[36m(func pid=116143)[0m top5: 0.9351679104477612
[2m[36m(func pid=116143)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=116143)[0m f1_macro: 0.37448499178317335
[2m[36m(func pid=116143)[0m f1_weighted: 0.4490604527557107
[2m[36m(func pid=116143)[0m f1_per_class: [0.462, 0.578, 0.302, 0.555, 0.175, 0.303, 0.381, 0.365, 0.259, 0.364]
[2m[36m(func pid=116143)[0m 
== Status ==
Current time: 2024-01-07 14:05:54 (running for 00:22:31.40)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.03  |      0.374 |                   84 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.377 |      0.171 |                   58 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.999 |      0.086 |                   56 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.65  |      0.053 |                   57 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122462)[0m top1: 0.16977611940298507
[2m[36m(func pid=122462)[0m top5: 0.5419776119402985
[2m[36m(func pid=122462)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=122462)[0m f1_macro: 0.07977633968082828
[2m[36m(func pid=122462)[0m f1_weighted: 0.05730961924572253
[2m[36m(func pid=122462)[0m f1_per_class: [0.031, 0.294, 0.414, 0.007, 0.0, 0.0, 0.0, 0.0, 0.052, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.18050373134328357
[2m[36m(func pid=121898)[0m top5: 0.5732276119402985
[2m[36m(func pid=121898)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=121898)[0m f1_macro: 0.19135575556924783
[2m[36m(func pid=121898)[0m f1_weighted: 0.1673802382049493
[2m[36m(func pid=121898)[0m f1_per_class: [0.0, 0.07, 0.625, 0.0, 0.27, 0.394, 0.319, 0.144, 0.0, 0.092]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.23880597014925373
[2m[36m(func pid=122441)[0m top5: 0.6072761194029851
[2m[36m(func pid=122441)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=122441)[0m f1_macro: 0.13628177241581815
[2m[36m(func pid=122441)[0m f1_weighted: 0.2330619058599661
[2m[36m(func pid=122441)[0m f1_per_class: [0.075, 0.066, 0.452, 0.233, 0.026, 0.0, 0.511, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0088 | Steps: 4 | Val loss: 1.6711 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.5526 | Steps: 4 | Val loss: 4.4988 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.4150 | Steps: 4 | Val loss: 10.3314 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=116143)[0m top1: 0.42257462686567165
[2m[36m(func pid=116143)[0m top5: 0.9351679104477612
[2m[36m(func pid=116143)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=116143)[0m f1_macro: 0.36482376968432806
[2m[36m(func pid=116143)[0m f1_weighted: 0.43695157605770923
[2m[36m(func pid=116143)[0m f1_per_class: [0.43, 0.577, 0.329, 0.574, 0.173, 0.284, 0.334, 0.366, 0.247, 0.333]
== Status ==
Current time: 2024-01-07 14:05:59 (running for 00:22:36.67)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.009 |      0.365 |                   85 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.724 |      0.191 |                   59 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  6.627 |      0.136 |                   57 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.485 |      0.08  |                   58 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 6.1412 | Steps: 4 | Val loss: 46.4967 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m top1: 0.166044776119403
[2m[36m(func pid=122462)[0m top5: 0.40625
[2m[36m(func pid=122462)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=122462)[0m f1_macro: 0.055694630788003976
[2m[36m(func pid=122462)[0m f1_weighted: 0.05655053578984498
[2m[36m(func pid=122462)[0m f1_per_class: [0.13, 0.29, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.066, 0.067]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.18330223880597016
[2m[36m(func pid=121898)[0m top5: 0.6128731343283582
[2m[36m(func pid=121898)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=121898)[0m f1_macro: 0.19381472280287934
[2m[36m(func pid=121898)[0m f1_weighted: 0.1712689988006221
[2m[36m(func pid=121898)[0m f1_per_class: [0.0, 0.247, 0.643, 0.0, 0.148, 0.345, 0.244, 0.166, 0.018, 0.128]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.21688432835820895
[2m[36m(func pid=122441)[0m top5: 0.5923507462686567
[2m[36m(func pid=122441)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=122441)[0m f1_macro: 0.13536277149328235
[2m[36m(func pid=122441)[0m f1_weighted: 0.20080720516442016
[2m[36m(func pid=122441)[0m f1_per_class: [0.075, 0.076, 0.556, 0.109, 0.028, 0.0, 0.511, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0071 | Steps: 4 | Val loss: 1.6230 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.3917 | Steps: 4 | Val loss: 3.8964 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:06:05 (running for 00:22:42.04)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.386 |                   86 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  2.415 |      0.194 |                   60 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  6.141 |      0.135 |                   58 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.553 |      0.056 |                   59 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4375
[2m[36m(func pid=116143)[0m top5: 0.9393656716417911
[2m[36m(func pid=116143)[0m f1_micro: 0.4375
[2m[36m(func pid=116143)[0m f1_macro: 0.3858748078967399
[2m[36m(func pid=116143)[0m f1_weighted: 0.4439178440766757
[2m[36m(func pid=116143)[0m f1_per_class: [0.493, 0.589, 0.366, 0.579, 0.197, 0.314, 0.324, 0.376, 0.266, 0.354]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.7576 | Steps: 4 | Val loss: 9.2225 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.3498 | Steps: 4 | Val loss: 19.8021 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=122462)[0m top1: 0.16277985074626866
[2m[36m(func pid=122462)[0m top5: 0.4155783582089552
[2m[36m(func pid=122462)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=122462)[0m f1_macro: 0.06926348629704114
[2m[36m(func pid=122462)[0m f1_weighted: 0.054901494707400966
[2m[36m(func pid=122462)[0m f1_per_class: [0.031, 0.289, 0.346, 0.007, 0.0, 0.0, 0.0, 0.0, 0.021, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.1478544776119403
[2m[36m(func pid=121898)[0m top5: 0.6413246268656716
[2m[36m(func pid=121898)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=121898)[0m f1_macro: 0.1403607624958891
[2m[36m(func pid=121898)[0m f1_weighted: 0.12051622967473533
[2m[36m(func pid=121898)[0m f1_per_class: [0.0, 0.211, 0.17, 0.016, 0.27, 0.255, 0.11, 0.196, 0.056, 0.119]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.23880597014925373
[2m[36m(func pid=122441)[0m top5: 0.6767723880597015
[2m[36m(func pid=122441)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=122441)[0m f1_macro: 0.14361383668460337
[2m[36m(func pid=122441)[0m f1_weighted: 0.22815765285762113
[2m[36m(func pid=122441)[0m f1_per_class: [0.072, 0.23, 0.471, 0.11, 0.039, 0.0, 0.515, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3057 | Steps: 4 | Val loss: 1.6553 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 4.2126 | Steps: 4 | Val loss: 3.9720 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 14:06:10 (running for 00:22:47.26)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.306 |      0.389 |                   87 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.758 |      0.14  |                   61 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.35  |      0.144 |                   59 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.392 |      0.069 |                   60 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.44169776119402987
[2m[36m(func pid=116143)[0m top5: 0.9300373134328358
[2m[36m(func pid=116143)[0m f1_micro: 0.4416977611940298
[2m[36m(func pid=116143)[0m f1_macro: 0.3893499584887897
[2m[36m(func pid=116143)[0m f1_weighted: 0.44168118183407123
[2m[36m(func pid=116143)[0m f1_per_class: [0.497, 0.608, 0.338, 0.578, 0.191, 0.309, 0.301, 0.396, 0.305, 0.372]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1121 | Steps: 4 | Val loss: 10.6776 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6737 | Steps: 4 | Val loss: 14.3110 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=122462)[0m top1: 0.014925373134328358
[2m[36m(func pid=122462)[0m top5: 0.417910447761194
[2m[36m(func pid=122462)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=122462)[0m f1_macro: 0.04268302080674416
[2m[36m(func pid=122462)[0m f1_weighted: 0.008105648253621596
[2m[36m(func pid=122462)[0m f1_per_class: [0.03, 0.005, 0.34, 0.013, 0.016, 0.0, 0.0, 0.0, 0.023, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0898 | Steps: 4 | Val loss: 1.5866 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=122441)[0m top1: 0.279384328358209
[2m[36m(func pid=122441)[0m top5: 0.7644589552238806
[2m[36m(func pid=122441)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=122441)[0m f1_macro: 0.15913446774284545
[2m[36m(func pid=122441)[0m f1_weighted: 0.27984698036389605
[2m[36m(func pid=122441)[0m f1_per_class: [0.072, 0.251, 0.471, 0.284, 0.0, 0.0, 0.513, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m top1: 0.13059701492537312
[2m[36m(func pid=121898)[0m top5: 0.6749067164179104
[2m[36m(func pid=121898)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=121898)[0m f1_macro: 0.14127076329556812
[2m[36m(func pid=121898)[0m f1_weighted: 0.0950719906839077
[2m[36m(func pid=121898)[0m f1_per_class: [0.043, 0.163, 0.117, 0.086, 0.48, 0.047, 0.047, 0.247, 0.097, 0.086]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.2238 | Steps: 4 | Val loss: 3.2063 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:06:15 (running for 00:22:52.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.09  |      0.391 |                   88 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  2.112 |      0.141 |                   62 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.674 |      0.159 |                   60 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  4.213 |      0.043 |                   61 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.45382462686567165
[2m[36m(func pid=116143)[0m top5: 0.9393656716417911
[2m[36m(func pid=116143)[0m f1_micro: 0.45382462686567165
[2m[36m(func pid=116143)[0m f1_macro: 0.39130474824772477
[2m[36m(func pid=116143)[0m f1_weighted: 0.45833863561324856
[2m[36m(func pid=116143)[0m f1_per_class: [0.465, 0.61, 0.317, 0.58, 0.186, 0.316, 0.352, 0.417, 0.27, 0.4]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8317 | Steps: 4 | Val loss: 12.1922 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.5990 | Steps: 4 | Val loss: 11.5179 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=122462)[0m top1: 0.12126865671641791
[2m[36m(func pid=122462)[0m top5: 0.6208022388059702
[2m[36m(func pid=122462)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=122462)[0m f1_macro: 0.07009093792814172
[2m[36m(func pid=122462)[0m f1_weighted: 0.03807968336932292
[2m[36m(func pid=122462)[0m f1_per_class: [0.03, 0.0, 0.4, 0.036, 0.0, 0.213, 0.0, 0.0, 0.022, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0254 | Steps: 4 | Val loss: 1.5628 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=121898)[0m top1: 0.15625
[2m[36m(func pid=121898)[0m top5: 0.7504664179104478
[2m[36m(func pid=121898)[0m f1_micro: 0.15625
[2m[36m(func pid=121898)[0m f1_macro: 0.16412435981099152
[2m[36m(func pid=121898)[0m f1_weighted: 0.1291520513603664
[2m[36m(func pid=121898)[0m f1_per_class: [0.128, 0.122, 0.163, 0.279, 0.467, 0.024, 0.003, 0.255, 0.115, 0.086]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.18516791044776118
[2m[36m(func pid=122441)[0m top5: 0.7597947761194029
[2m[36m(func pid=122441)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=122441)[0m f1_macro: 0.16524788676480395
[2m[36m(func pid=122441)[0m f1_weighted: 0.1867279720130459
[2m[36m(func pid=122441)[0m f1_per_class: [0.073, 0.119, 0.643, 0.343, 0.0, 0.0, 0.163, 0.258, 0.053, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:06:20 (running for 00:22:57.84)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.025 |      0.381 |                   89 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.832 |      0.164 |                   63 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.599 |      0.165 |                   61 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.224 |      0.07  |                   62 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4533582089552239
[2m[36m(func pid=116143)[0m top5: 0.9379664179104478
[2m[36m(func pid=116143)[0m f1_micro: 0.4533582089552239
[2m[36m(func pid=116143)[0m f1_macro: 0.38066208871992957
[2m[36m(func pid=116143)[0m f1_weighted: 0.4700366093832222
[2m[36m(func pid=116143)[0m f1_per_class: [0.451, 0.583, 0.295, 0.578, 0.194, 0.323, 0.419, 0.385, 0.233, 0.344]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.9806 | Steps: 4 | Val loss: 2.9825 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3864 | Steps: 4 | Val loss: 12.3672 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.7737 | Steps: 4 | Val loss: 11.2611 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=122462)[0m top1: 0.12173507462686567
[2m[36m(func pid=122462)[0m top5: 0.7052238805970149
[2m[36m(func pid=122462)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=122462)[0m f1_macro: 0.0612989227311478
[2m[36m(func pid=122462)[0m f1_weighted: 0.03938610009319442
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.333, 0.044, 0.0, 0.212, 0.0, 0.0, 0.024, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0520 | Steps: 4 | Val loss: 1.5621 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=121898)[0m top1: 0.18050373134328357
[2m[36m(func pid=121898)[0m top5: 0.7486007462686567
[2m[36m(func pid=121898)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=121898)[0m f1_macro: 0.14580998290213332
[2m[36m(func pid=121898)[0m f1_weighted: 0.1487545795112083
[2m[36m(func pid=121898)[0m f1_per_class: [0.044, 0.111, 0.0, 0.357, 0.383, 0.031, 0.0, 0.305, 0.116, 0.111]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.16044776119402984
[2m[36m(func pid=122441)[0m top5: 0.753731343283582
[2m[36m(func pid=122441)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=122441)[0m f1_macro: 0.10888599132916651
[2m[36m(func pid=122441)[0m f1_weighted: 0.14572440031684922
[2m[36m(func pid=122441)[0m f1_per_class: [0.07, 0.01, 0.308, 0.337, 0.0, 0.036, 0.098, 0.23, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:06:26 (running for 00:23:03.16)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.052 |      0.378 |                   90 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.386 |      0.146 |                   64 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.774 |      0.109 |                   62 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.981 |      0.061 |                   63 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.45848880597014924
[2m[36m(func pid=116143)[0m top5: 0.9393656716417911
[2m[36m(func pid=116143)[0m f1_micro: 0.45848880597014924
[2m[36m(func pid=116143)[0m f1_macro: 0.37829823473150304
[2m[36m(func pid=116143)[0m f1_weighted: 0.480282227170576
[2m[36m(func pid=116143)[0m f1_per_class: [0.428, 0.54, 0.338, 0.596, 0.186, 0.313, 0.473, 0.373, 0.208, 0.33]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 3.2111 | Steps: 4 | Val loss: 2.7168 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.9671 | Steps: 4 | Val loss: 13.0869 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 3.2421 | Steps: 4 | Val loss: 11.1858 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=122462)[0m top1: 0.12733208955223882
[2m[36m(func pid=122462)[0m top5: 0.7154850746268657
[2m[36m(func pid=122462)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=122462)[0m f1_macro: 0.07780217858818492
[2m[36m(func pid=122462)[0m f1_weighted: 0.048745901228456914
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.467, 0.071, 0.0, 0.213, 0.003, 0.0, 0.024, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0244 | Steps: 4 | Val loss: 1.5628 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=121898)[0m top1: 0.19029850746268656
[2m[36m(func pid=121898)[0m top5: 0.7551305970149254
[2m[36m(func pid=121898)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=121898)[0m f1_macro: 0.14358782930027839
[2m[36m(func pid=121898)[0m f1_weighted: 0.17204356100516058
[2m[36m(func pid=121898)[0m f1_per_class: [0.087, 0.183, 0.0, 0.384, 0.218, 0.008, 0.018, 0.332, 0.104, 0.102]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.15811567164179105
[2m[36m(func pid=122441)[0m top5: 0.7509328358208955
[2m[36m(func pid=122441)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=122441)[0m f1_macro: 0.09713004362963826
[2m[36m(func pid=122441)[0m f1_weighted: 0.13266048223682633
[2m[36m(func pid=122441)[0m f1_per_class: [0.067, 0.021, 0.2, 0.349, 0.0, 0.083, 0.021, 0.23, 0.0, 0.0]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:06:31 (running for 00:23:08.49)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.024 |      0.371 |                   91 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.967 |      0.144 |                   65 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.242 |      0.097 |                   63 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.211 |      0.078 |                   64 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4608208955223881
[2m[36m(func pid=116143)[0m top5: 0.9347014925373134
[2m[36m(func pid=116143)[0m f1_micro: 0.4608208955223881
[2m[36m(func pid=116143)[0m f1_macro: 0.3705698500037884
[2m[36m(func pid=116143)[0m f1_weighted: 0.486736814834428
[2m[36m(func pid=116143)[0m f1_per_class: [0.396, 0.542, 0.321, 0.587, 0.17, 0.294, 0.515, 0.352, 0.226, 0.304]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.8579 | Steps: 4 | Val loss: 2.5352 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9246 | Steps: 4 | Val loss: 15.1371 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6302 | Steps: 4 | Val loss: 9.3016 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0639 | Steps: 4 | Val loss: 1.4853 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=122462)[0m top1: 0.02751865671641791
[2m[36m(func pid=122462)[0m top5: 0.7196828358208955
[2m[36m(func pid=122462)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=122462)[0m f1_macro: 0.06576478529777698
[2m[36m(func pid=122462)[0m f1_weighted: 0.027495275629142103
[2m[36m(func pid=122462)[0m f1_per_class: [0.033, 0.0, 0.519, 0.084, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.16184701492537312
[2m[36m(func pid=121898)[0m top5: 0.7318097014925373
[2m[36m(func pid=121898)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=121898)[0m f1_macro: 0.14212569117843313
[2m[36m(func pid=121898)[0m f1_weighted: 0.15632098385193405
[2m[36m(func pid=121898)[0m f1_per_class: [0.167, 0.258, 0.0, 0.233, 0.038, 0.008, 0.04, 0.447, 0.095, 0.135]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.17257462686567165
[2m[36m(func pid=122441)[0m top5: 0.7588619402985075
[2m[36m(func pid=122441)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=122441)[0m f1_macro: 0.10503892056740391
[2m[36m(func pid=122441)[0m f1_weighted: 0.1530330973138165
[2m[36m(func pid=122441)[0m f1_per_class: [0.056, 0.116, 0.115, 0.371, 0.0, 0.089, 0.009, 0.237, 0.024, 0.033]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:06:36 (running for 00:23:13.84)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.064 |      0.398 |                   92 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.925 |      0.142 |                   66 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.63  |      0.105 |                   64 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.858 |      0.066 |                   65 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4822761194029851
[2m[36m(func pid=116143)[0m top5: 0.9449626865671642
[2m[36m(func pid=116143)[0m f1_micro: 0.4822761194029851
[2m[36m(func pid=116143)[0m f1_macro: 0.39788203803225786
[2m[36m(func pid=116143)[0m f1_weighted: 0.5033761392225301
[2m[36m(func pid=116143)[0m f1_per_class: [0.484, 0.571, 0.426, 0.605, 0.191, 0.286, 0.533, 0.344, 0.218, 0.321]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.0192 | Steps: 4 | Val loss: 2.6821 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.6539 | Steps: 4 | Val loss: 15.5007 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.8437 | Steps: 4 | Val loss: 7.5088 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0240 | Steps: 4 | Val loss: 1.5287 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=122462)[0m top1: 0.02751865671641791
[2m[36m(func pid=122462)[0m top5: 0.7201492537313433
[2m[36m(func pid=122462)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=122462)[0m f1_macro: 0.061309772704857254
[2m[36m(func pid=122462)[0m f1_weighted: 0.02769888141781447
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.483, 0.082, 0.0, 0.0, 0.003, 0.0, 0.023, 0.022]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.15625
[2m[36m(func pid=121898)[0m top5: 0.722481343283582
[2m[36m(func pid=121898)[0m f1_micro: 0.15625
[2m[36m(func pid=121898)[0m f1_macro: 0.15476494394253926
[2m[36m(func pid=121898)[0m f1_weighted: 0.1544429211083416
[2m[36m(func pid=121898)[0m f1_per_class: [0.308, 0.358, 0.027, 0.13, 0.0, 0.015, 0.062, 0.443, 0.088, 0.118]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122441)[0m top1: 0.1226679104477612
[2m[36m(func pid=122441)[0m top5: 0.7513992537313433
[2m[36m(func pid=122441)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=122441)[0m f1_macro: 0.08246250365411037
[2m[36m(func pid=122441)[0m f1_weighted: 0.10154755086991667
[2m[36m(func pid=122441)[0m f1_per_class: [0.04, 0.203, 0.086, 0.173, 0.0, 0.021, 0.0, 0.235, 0.017, 0.05]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:06:42 (running for 00:23:19.15)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.024 |      0.391 |                   93 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.654 |      0.155 |                   67 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.844 |      0.082 |                   65 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.019 |      0.061 |                   66 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.47201492537313433
[2m[36m(func pid=116143)[0m top5: 0.9388992537313433
[2m[36m(func pid=116143)[0m f1_micro: 0.47201492537313433
[2m[36m(func pid=116143)[0m f1_macro: 0.39137370271446076
[2m[36m(func pid=116143)[0m f1_weighted: 0.4947297527080651
[2m[36m(func pid=116143)[0m f1_per_class: [0.446, 0.571, 0.421, 0.553, 0.195, 0.294, 0.552, 0.348, 0.216, 0.318]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.9704 | Steps: 4 | Val loss: 2.9215 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1748 | Steps: 4 | Val loss: 11.9444 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6756 | Steps: 4 | Val loss: 8.7346 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0188 | Steps: 4 | Val loss: 1.5086 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=121898)[0m top1: 0.21455223880597016
[2m[36m(func pid=121898)[0m top5: 0.7653917910447762
[2m[36m(func pid=121898)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=121898)[0m f1_macro: 0.19936191408320925
[2m[36m(func pid=121898)[0m f1_weighted: 0.22130349442974456
[2m[36m(func pid=121898)[0m f1_per_class: [0.321, 0.471, 0.11, 0.159, 0.138, 0.065, 0.174, 0.426, 0.098, 0.032]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.022388059701492536
[2m[36m(func pid=122462)[0m top5: 0.632929104477612
[2m[36m(func pid=122462)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=122462)[0m f1_macro: 0.06838638803197243
[2m[36m(func pid=122462)[0m f1_weighted: 0.019011134208898745
[2m[36m(func pid=122462)[0m f1_per_class: [0.034, 0.0, 0.538, 0.045, 0.0, 0.0, 0.003, 0.0, 0.041, 0.022]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=122441)[0m top1: 0.10727611940298508
[2m[36m(func pid=122441)[0m top5: 0.7047574626865671
[2m[36m(func pid=122441)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=122441)[0m f1_macro: 0.06531639427831173
[2m[36m(func pid=122441)[0m f1_weighted: 0.0846117686968173
[2m[36m(func pid=122441)[0m f1_per_class: [0.028, 0.054, 0.092, 0.217, 0.0, 0.0, 0.0, 0.226, 0.013, 0.024]
[2m[36m(func pid=122441)[0m 
== Status ==
Current time: 2024-01-07 14:06:47 (running for 00:23:24.30)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.019 |      0.397 |                   94 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  2.175 |      0.199 |                   68 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.676 |      0.065 |                   66 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.97  |      0.068 |                   67 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.470615671641791
[2m[36m(func pid=116143)[0m top5: 0.9435634328358209
[2m[36m(func pid=116143)[0m f1_micro: 0.470615671641791
[2m[36m(func pid=116143)[0m f1_macro: 0.396764850976935
[2m[36m(func pid=116143)[0m f1_weighted: 0.49195578408748786
[2m[36m(func pid=116143)[0m f1_per_class: [0.435, 0.572, 0.453, 0.565, 0.206, 0.309, 0.523, 0.349, 0.228, 0.327]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4939 | Steps: 4 | Val loss: 9.6468 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.9598 | Steps: 4 | Val loss: 3.1570 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.9092 | Steps: 4 | Val loss: 11.6100 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0124 | Steps: 4 | Val loss: 1.4787 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=121898)[0m top1: 0.23087686567164178
[2m[36m(func pid=121898)[0m top5: 0.7681902985074627
[2m[36m(func pid=121898)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=121898)[0m f1_macro: 0.23585389554360328
[2m[36m(func pid=121898)[0m f1_weighted: 0.24052075917533505
[2m[36m(func pid=121898)[0m f1_per_class: [0.348, 0.49, 0.14, 0.153, 0.364, 0.132, 0.205, 0.389, 0.118, 0.021]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.01632462686567164
[2m[36m(func pid=122462)[0m top5: 0.6319962686567164
[2m[36m(func pid=122462)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=122462)[0m f1_macro: 0.04730412075253448
[2m[36m(func pid=122462)[0m f1_weighted: 0.007599855563190713
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.4, 0.013, 0.0, 0.0, 0.0, 0.0, 0.037, 0.023]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:06:52 (running for 00:23:29.49)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.019 |      0.397 |                   94 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  0.494 |      0.236 |                   69 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.909 |      0.071 |                   67 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.96  |      0.047 |                   68 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122441)[0m top1: 0.1166044776119403
[2m[36m(func pid=122441)[0m top5: 0.691231343283582
[2m[36m(func pid=122441)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=122441)[0m f1_macro: 0.0707105935029556
[2m[36m(func pid=122441)[0m f1_weighted: 0.09119225966655684
[2m[36m(func pid=122441)[0m f1_per_class: [0.023, 0.005, 0.122, 0.266, 0.0, 0.0, 0.0, 0.234, 0.028, 0.028]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=116143)[0m top1: 0.48507462686567165
[2m[36m(func pid=116143)[0m top5: 0.9458955223880597
[2m[36m(func pid=116143)[0m f1_micro: 0.48507462686567165
[2m[36m(func pid=116143)[0m f1_macro: 0.40267418629694063
[2m[36m(func pid=116143)[0m f1_weighted: 0.5073248149801182
[2m[36m(func pid=116143)[0m f1_per_class: [0.474, 0.568, 0.414, 0.587, 0.205, 0.317, 0.549, 0.35, 0.241, 0.321]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1412 | Steps: 4 | Val loss: 7.6768 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 3.4000 | Steps: 4 | Val loss: 3.0333 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0779 | Steps: 4 | Val loss: 1.4373 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.7912 | Steps: 4 | Val loss: 11.2220 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=121898)[0m top1: 0.25419776119402987
[2m[36m(func pid=121898)[0m top5: 0.7980410447761194
[2m[36m(func pid=121898)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=121898)[0m f1_macro: 0.2703461164153416
[2m[36m(func pid=121898)[0m f1_weighted: 0.26637058746132275
[2m[36m(func pid=121898)[0m f1_per_class: [0.385, 0.509, 0.174, 0.172, 0.449, 0.207, 0.219, 0.421, 0.149, 0.018]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.015391791044776119
[2m[36m(func pid=122462)[0m top5: 0.605410447761194
[2m[36m(func pid=122462)[0m f1_micro: 0.015391791044776119
[2m[36m(func pid=122462)[0m f1_macro: 0.0363146974367449
[2m[36m(func pid=122462)[0m f1_weighted: 0.005207904272132991
[2m[36m(func pid=122462)[0m f1_per_class: [0.0, 0.0, 0.294, 0.007, 0.0, 0.0, 0.0, 0.0, 0.04, 0.023]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:06:57 (running for 00:23:34.82)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.078 |      0.409 |                   96 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.141 |      0.27  |                   70 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.909 |      0.071 |                   67 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.4   |      0.036 |                   69 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.49486940298507465
[2m[36m(func pid=116143)[0m top5: 0.9500932835820896
[2m[36m(func pid=116143)[0m f1_micro: 0.49486940298507465
[2m[36m(func pid=116143)[0m f1_macro: 0.4088311358343014
[2m[36m(func pid=116143)[0m f1_weighted: 0.5149980068552362
[2m[36m(func pid=116143)[0m f1_per_class: [0.468, 0.576, 0.448, 0.603, 0.211, 0.333, 0.557, 0.32, 0.219, 0.353]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m top1: 0.1287313432835821
[2m[36m(func pid=122441)[0m top5: 0.7122201492537313
[2m[36m(func pid=122441)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=122441)[0m f1_macro: 0.07848420849882931
[2m[36m(func pid=122441)[0m f1_weighted: 0.1011583662576301
[2m[36m(func pid=122441)[0m f1_per_class: [0.033, 0.01, 0.166, 0.298, 0.0, 0.0, 0.0, 0.245, 0.0, 0.032]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.6865 | Steps: 4 | Val loss: 7.1292 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.3243 | Steps: 4 | Val loss: 2.9432 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0060 | Steps: 4 | Val loss: 1.4387 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6229 | Steps: 4 | Val loss: 8.2590 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=121898)[0m top1: 0.22434701492537312
[2m[36m(func pid=121898)[0m top5: 0.7882462686567164
[2m[36m(func pid=121898)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=121898)[0m f1_macro: 0.22928829118421742
[2m[36m(func pid=121898)[0m f1_weighted: 0.27214659064033686
[2m[36m(func pid=121898)[0m f1_per_class: [0.2, 0.474, 0.069, 0.152, 0.4, 0.116, 0.337, 0.383, 0.131, 0.031]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=122462)[0m top1: 0.017723880597014924
[2m[36m(func pid=122462)[0m top5: 0.3917910447761194
[2m[36m(func pid=122462)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=122462)[0m f1_macro: 0.025668688766477772
[2m[36m(func pid=122462)[0m f1_weighted: 0.005183401983045986
[2m[36m(func pid=122462)[0m f1_per_class: [0.031, 0.0, 0.15, 0.007, 0.0, 0.0, 0.0, 0.0, 0.046, 0.023]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:07:03 (running for 00:23:40.12)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.006 |      0.403 |                   97 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.687 |      0.229 |                   71 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.791 |      0.078 |                   68 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.324 |      0.026 |                   70 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.49300373134328357
[2m[36m(func pid=116143)[0m top5: 0.9486940298507462
[2m[36m(func pid=116143)[0m f1_micro: 0.49300373134328357
[2m[36m(func pid=116143)[0m f1_macro: 0.4030956086248869
[2m[36m(func pid=116143)[0m f1_weighted: 0.5120043650631645
[2m[36m(func pid=116143)[0m f1_per_class: [0.454, 0.559, 0.388, 0.608, 0.22, 0.349, 0.547, 0.322, 0.212, 0.372]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m top1: 0.1455223880597015
[2m[36m(func pid=122441)[0m top5: 0.738339552238806
[2m[36m(func pid=122441)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=122441)[0m f1_macro: 0.09934480055423642
[2m[36m(func pid=122441)[0m f1_weighted: 0.12172777971255665
[2m[36m(func pid=122441)[0m f1_per_class: [0.033, 0.077, 0.237, 0.329, 0.048, 0.0, 0.0, 0.24, 0.0, 0.031]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.7505 | Steps: 4 | Val loss: 2.7919 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.4213 | Steps: 4 | Val loss: 10.9208 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0158 | Steps: 4 | Val loss: 1.4184 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3533 | Steps: 4 | Val loss: 9.1991 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=122462)[0m top1: 0.00792910447761194
[2m[36m(func pid=122462)[0m top5: 0.4183768656716418
[2m[36m(func pid=122462)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=122462)[0m f1_macro: 0.008092938501880934
[2m[36m(func pid=122462)[0m f1_weighted: 0.004064406041066666
[2m[36m(func pid=122462)[0m f1_per_class: [0.06, 0.0, 0.011, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.11567164179104478
[2m[36m(func pid=121898)[0m top5: 0.7215485074626866
[2m[36m(func pid=121898)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=121898)[0m f1_macro: 0.11794902401051814
[2m[36m(func pid=121898)[0m f1_weighted: 0.1515217354124766
[2m[36m(func pid=121898)[0m f1_per_class: [0.146, 0.219, 0.047, 0.072, 0.188, 0.048, 0.258, 0.031, 0.134, 0.037]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:07:08 (running for 00:23:45.38)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.016 |      0.404 |                   98 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.421 |      0.118 |                   72 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.623 |      0.099 |                   69 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.75  |      0.008 |                   71 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.5013992537313433
[2m[36m(func pid=116143)[0m top5: 0.9486940298507462
[2m[36m(func pid=116143)[0m f1_micro: 0.5013992537313433
[2m[36m(func pid=116143)[0m f1_macro: 0.40447480543575426
[2m[36m(func pid=116143)[0m f1_weighted: 0.5174503443617163
[2m[36m(func pid=116143)[0m f1_per_class: [0.444, 0.57, 0.377, 0.614, 0.224, 0.357, 0.553, 0.304, 0.221, 0.38]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m top1: 0.1837686567164179
[2m[36m(func pid=122441)[0m top5: 0.7434701492537313
[2m[36m(func pid=122441)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=122441)[0m f1_macro: 0.11939753176550198
[2m[36m(func pid=122441)[0m f1_weighted: 0.15868495055970083
[2m[36m(func pid=122441)[0m f1_per_class: [0.061, 0.273, 0.238, 0.339, 0.0, 0.0, 0.0, 0.243, 0.0, 0.04]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.9164 | Steps: 4 | Val loss: 2.6682 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.7935 | Steps: 4 | Val loss: 14.9857 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0117 | Steps: 4 | Val loss: 1.4361 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.0130 | Steps: 4 | Val loss: 9.1765 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=122462)[0m top1: 0.0065298507462686565
[2m[36m(func pid=122462)[0m top5: 0.375
[2m[36m(func pid=122462)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=122462)[0m f1_macro: 0.004683184974881488
[2m[36m(func pid=122462)[0m f1_weighted: 0.0016553696614726118
[2m[36m(func pid=122462)[0m f1_per_class: [0.032, 0.0, 0.011, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.06716417910447761
[2m[36m(func pid=121898)[0m top5: 0.6711753731343284
[2m[36m(func pid=121898)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=121898)[0m f1_macro: 0.08073721459760094
[2m[36m(func pid=121898)[0m f1_weighted: 0.0808856673140095
[2m[36m(func pid=121898)[0m f1_per_class: [0.248, 0.096, 0.051, 0.075, 0.065, 0.041, 0.098, 0.0, 0.101, 0.032]
[2m[36m(func pid=121898)[0m 
== Status ==
Current time: 2024-01-07 14:07:13 (running for 00:23:50.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.012 |      0.404 |                   99 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.793 |      0.081 |                   73 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.353 |      0.119 |                   70 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.916 |      0.005 |                   72 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=116143)[0m top1: 0.4976679104477612
[2m[36m(func pid=116143)[0m top5: 0.9463619402985075
[2m[36m(func pid=116143)[0m f1_micro: 0.4976679104477612
[2m[36m(func pid=116143)[0m f1_macro: 0.40350494427261074
[2m[36m(func pid=116143)[0m f1_weighted: 0.5156407794648044
[2m[36m(func pid=116143)[0m f1_per_class: [0.428, 0.589, 0.356, 0.598, 0.211, 0.344, 0.554, 0.313, 0.237, 0.405]
[2m[36m(func pid=116143)[0m 
[2m[36m(func pid=122441)[0m top1: 0.19496268656716417
[2m[36m(func pid=122441)[0m top5: 0.7453358208955224
[2m[36m(func pid=122441)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=122441)[0m f1_macro: 0.1177175425214205
[2m[36m(func pid=122441)[0m f1_weighted: 0.16324060811701668
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.272, 0.263, 0.36, 0.0, 0.0, 0.0, 0.239, 0.0, 0.043]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.9897 | Steps: 4 | Val loss: 2.6920 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5944 | Steps: 4 | Val loss: 12.2932 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=116143)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0577 | Steps: 4 | Val loss: 1.4140 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6814 | Steps: 4 | Val loss: 10.9617 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=122462)[0m top1: 0.24860074626865672
[2m[36m(func pid=122462)[0m top5: 0.5354477611940298
[2m[36m(func pid=122462)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=122462)[0m f1_macro: 0.06061925353572001
[2m[36m(func pid=122462)[0m f1_weighted: 0.11489900759237845
[2m[36m(func pid=122462)[0m f1_per_class: [0.059, 0.0, 0.143, 0.405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
== Status ==
Current time: 2024-01-07 14:07:18 (running for 00:23:55.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00008 | RUNNING    | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.012 |      0.404 |                   99 |
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.594 |      0.131 |                   74 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  3.013 |      0.118 |                   71 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  2.99  |      0.061 |                   73 |
| train_5806f_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121898)[0m top1: 0.12173507462686567
[2m[36m(func pid=121898)[0m top5: 0.722481343283582
[2m[36m(func pid=121898)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=121898)[0m f1_macro: 0.13050822682797195
[2m[36m(func pid=121898)[0m f1_weighted: 0.1598325698556519
[2m[36m(func pid=121898)[0m f1_per_class: [0.293, 0.136, 0.075, 0.183, 0.041, 0.116, 0.175, 0.173, 0.075, 0.038]
[2m[36m(func pid=121898)[0m 
[2m[36m(func pid=116143)[0m top1: 0.5074626865671642
[2m[36m(func pid=116143)[0m top5: 0.9500932835820896
[2m[36m(func pid=116143)[0m f1_micro: 0.5074626865671642
[2m[36m(func pid=116143)[0m f1_macro: 0.4143948106675756
[2m[36m(func pid=116143)[0m f1_weighted: 0.5229850048715952
[2m[36m(func pid=116143)[0m f1_per_class: [0.448, 0.601, 0.351, 0.6, 0.245, 0.366, 0.558, 0.306, 0.254, 0.415]
[2m[36m(func pid=122441)[0m top1: 0.20569029850746268
[2m[36m(func pid=122441)[0m top5: 0.7551305970149254
[2m[36m(func pid=122441)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=122441)[0m f1_macro: 0.11831018848343858
[2m[36m(func pid=122441)[0m f1_weighted: 0.17223687415249592
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.289, 0.165, 0.38, 0.043, 0.0, 0.0, 0.253, 0.0, 0.052]
[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 3.5965 | Steps: 4 | Val loss: 2.8088 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=121898)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.4888 | Steps: 4 | Val loss: 12.3407 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 7.1987 | Steps: 4 | Val loss: 10.8312 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=122462)[0m top1: 0.2658582089552239
[2m[36m(func pid=122462)[0m top5: 0.5461753731343284
[2m[36m(func pid=122462)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=122462)[0m f1_macro: 0.0772825543448066
[2m[36m(func pid=122462)[0m f1_weighted: 0.12071558804003545
[2m[36m(func pid=122462)[0m f1_per_class: [0.058, 0.0, 0.293, 0.422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122462)[0m 
[2m[36m(func pid=121898)[0m top1: 0.1935634328358209
[2m[36m(func pid=121898)[0m top5: 0.769589552238806
[2m[36m(func pid=121898)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=121898)[0m f1_macro: 0.16391292443401043
[2m[36m(func pid=121898)[0m f1_weighted: 0.24331399329806505
[2m[36m(func pid=121898)[0m f1_per_class: [0.133, 0.147, 0.0, 0.282, 0.05, 0.226, 0.277, 0.458, 0.022, 0.044]
[2m[36m(func pid=122441)[0m top1: 0.2126865671641791
[2m[36m(func pid=122441)[0m top5: 0.7630597014925373
[2m[36m(func pid=122441)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=122441)[0m f1_macro: 0.11744916688782223
[2m[36m(func pid=122441)[0m f1_weighted: 0.17926293651723504
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.136, 0.136, 0.468, 0.038, 0.074, 0.0, 0.263, 0.0, 0.061]
[2m[36m(func pid=122462)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 3.0543 | Steps: 4 | Val loss: 2.6897 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 14:07:23 (running for 00:24:00.82)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00009 | RUNNING    | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.594 |      0.131 |                   74 |
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.681 |      0.118 |                   72 |
| train_5806f_00011 | RUNNING    | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.597 |      0.077 |                   74 |
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=140272)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140272)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=140272)[0m Configuration completed!
[2m[36m(func pid=140272)[0m New optimizer parameters:
[2m[36m(func pid=140272)[0m SGD (
[2m[36m(func pid=140272)[0m Parameter Group 0
[2m[36m(func pid=140272)[0m     dampening: 0
[2m[36m(func pid=140272)[0m     differentiable: False
[2m[36m(func pid=140272)[0m     foreach: None
[2m[36m(func pid=140272)[0m     lr: 0.0001
[2m[36m(func pid=140272)[0m     maximize: False
[2m[36m(func pid=140272)[0m     momentum: 0.9
[2m[36m(func pid=140272)[0m     nesterov: False
[2m[36m(func pid=140272)[0m     weight_decay: 0.0001
[2m[36m(func pid=140272)[0m )
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=122462)[0m top1: 0.18983208955223882
[2m[36m(func pid=122462)[0m top5: 0.5513059701492538
[2m[36m(func pid=122462)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=122462)[0m f1_macro: 0.09904081378040205
[2m[36m(func pid=122462)[0m f1_weighted: 0.10218718479479527
[2m[36m(func pid=122462)[0m f1_per_class: [0.061, 0.287, 0.468, 0.175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.4285 | Steps: 4 | Val loss: 8.9870 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0850 | Steps: 4 | Val loss: 2.5328 | Batch size: 32 | lr: 0.0001 | Duration: 4.97s
[2m[36m(func pid=122441)[0m top1: 0.15578358208955223
[2m[36m(func pid=122441)[0m top5: 0.7560634328358209
[2m[36m(func pid=122441)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=122441)[0m f1_macro: 0.0851783529630598
[2m[36m(func pid=122441)[0m f1_weighted: 0.1242118283694316
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.185, 0.388, 0.032, 0.007, 0.0, 0.24, 0.0, 0.0]
[2m[36m(func pid=140272)[0m top1: 0.06203358208955224
[2m[36m(func pid=140272)[0m top5: 0.4878731343283582
[2m[36m(func pid=140272)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=140272)[0m f1_macro: 0.03442763805892379
[2m[36m(func pid=140272)[0m f1_weighted: 0.034505981728838564
[2m[36m(func pid=140272)[0m f1_per_class: [0.079, 0.015, 0.0, 0.076, 0.0, 0.019, 0.0, 0.1, 0.024, 0.032]
== Status ==
Current time: 2024-01-07 14:07:29 (running for 00:24:06.52)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 3 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.199 |      0.117 |                   73 |
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 2 TERMINATED)


[2m[36m(func pid=140840)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140840)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=140840)[0m Configuration completed!
[2m[36m(func pid=140840)[0m New optimizer parameters:
[2m[36m(func pid=140840)[0m SGD (
[2m[36m(func pid=140840)[0m Parameter Group 0
[2m[36m(func pid=140840)[0m     dampening: 0
[2m[36m(func pid=140840)[0m     differentiable: False
[2m[36m(func pid=140840)[0m     foreach: None
[2m[36m(func pid=140840)[0m     lr: 0.001
[2m[36m(func pid=140840)[0m     maximize: False
[2m[36m(func pid=140840)[0m     momentum: 0.9
[2m[36m(func pid=140840)[0m     nesterov: False
[2m[36m(func pid=140840)[0m     weight_decay: 0.0001
[2m[36m(func pid=140840)[0m )
[2m[36m(func pid=140840)[0m 
== Status ==
Current time: 2024-01-07 14:07:36 (running for 00:24:13.34)
Memory usage on this node: 20.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 3 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00010 | RUNNING    | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  7.199 |      0.117 |                   73 |
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  3.085 |      0.034 |                    1 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 2 TERMINATED)


[2m[36m(func pid=122441)[0m 
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=122441)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6781 | Steps: 4 | Val loss: 7.7821 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0933 | Steps: 4 | Val loss: 2.5498 | Batch size: 32 | lr: 0.0001 | Duration: 3.29s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0337 | Steps: 4 | Val loss: 2.4548 | Batch size: 32 | lr: 0.001 | Duration: 5.05s
[2m[36m(func pid=122441)[0m top1: 0.11613805970149253
[2m[36m(func pid=122441)[0m top5: 0.7513992537313433
[2m[36m(func pid=122441)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=122441)[0m f1_macro: 0.07722774210057763
[2m[36m(func pid=122441)[0m f1_weighted: 0.09213830643607133
[2m[36m(func pid=122441)[0m f1_per_class: [0.0, 0.0, 0.231, 0.276, 0.03, 0.0, 0.0, 0.235, 0.0, 0.0]
[2m[36m(func pid=140272)[0m top1: 0.06296641791044776
[2m[36m(func pid=140272)[0m top5: 0.4916044776119403
[2m[36m(func pid=140272)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=140272)[0m f1_macro: 0.034444434445630494
[2m[36m(func pid=140272)[0m f1_weighted: 0.04088069748785305
[2m[36m(func pid=140272)[0m f1_per_class: [0.056, 0.032, 0.0, 0.093, 0.0, 0.02, 0.0, 0.095, 0.0, 0.049]
[2m[36m(func pid=140840)[0m top1: 0.07369402985074627
[2m[36m(func pid=140840)[0m top5: 0.5009328358208955
[2m[36m(func pid=140840)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=140840)[0m f1_macro: 0.05256938754728825
[2m[36m(func pid=140840)[0m f1_weighted: 0.0634964457096935
[2m[36m(func pid=140840)[0m f1_per_class: [0.087, 0.069, 0.0, 0.139, 0.0, 0.027, 0.0, 0.094, 0.055, 0.055]
== Status ==
Current time: 2024-01-07 14:07:42 (running for 00:24:19.13)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 3 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  3.085 |      0.034 |                    1 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 TERMINATED)


[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141385)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=141385)[0m Configuration completed!
[2m[36m(func pid=141385)[0m New optimizer parameters:
[2m[36m(func pid=141385)[0m SGD (
[2m[36m(func pid=141385)[0m Parameter Group 0
[2m[36m(func pid=141385)[0m     dampening: 0
[2m[36m(func pid=141385)[0m     differentiable: False
[2m[36m(func pid=141385)[0m     foreach: None
[2m[36m(func pid=141385)[0m     lr: 0.01
[2m[36m(func pid=141385)[0m     maximize: False
[2m[36m(func pid=141385)[0m     momentum: 0.9
[2m[36m(func pid=141385)[0m     nesterov: False
[2m[36m(func pid=141385)[0m     weight_decay: 0.0001
[2m[36m(func pid=141385)[0m )
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1157 | Steps: 4 | Val loss: 2.5167 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7802 | Steps: 4 | Val loss: 2.3858 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9557 | Steps: 4 | Val loss: 2.3619 | Batch size: 32 | lr: 0.01 | Duration: 4.67s
[2m[36m(func pid=140840)[0m top1: 0.08861940298507463
[2m[36m(func pid=140840)[0m top5: 0.49300373134328357
[2m[36m(func pid=140840)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=140840)[0m f1_macro: 0.09105686597205427
[2m[36m(func pid=140840)[0m f1_weighted: 0.08857101333035662
[2m[36m(func pid=140840)[0m f1_per_class: [0.11, 0.193, 0.154, 0.111, 0.0, 0.052, 0.012, 0.13, 0.108, 0.041]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.07649253731343283
[2m[36m(func pid=140272)[0m top5: 0.490205223880597
[2m[36m(func pid=140272)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=140272)[0m f1_macro: 0.041549191037952765
[2m[36m(func pid=140272)[0m f1_weighted: 0.0636343457218278
[2m[36m(func pid=140272)[0m f1_per_class: [0.041, 0.091, 0.0, 0.142, 0.0, 0.013, 0.0, 0.099, 0.0, 0.03]
[2m[36m(func pid=141385)[0m top1: 0.19029850746268656
[2m[36m(func pid=141385)[0m top5: 0.6049440298507462
[2m[36m(func pid=141385)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=141385)[0m f1_macro: 0.07705104484149525
[2m[36m(func pid=141385)[0m f1_weighted: 0.1751354129427657
[2m[36m(func pid=141385)[0m f1_per_class: [0.043, 0.074, 0.02, 0.0, 0.095, 0.0, 0.539, 0.0, 0.0, 0.0]
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4544 | Steps: 4 | Val loss: 2.3027 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:07:50 (running for 00:24:27.87)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  3.093 |      0.034 |                    2 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  2.78  |      0.091 |                    2 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141940)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=141940)[0m Configuration completed!
[2m[36m(func pid=141940)[0m New optimizer parameters:
[2m[36m(func pid=141940)[0m SGD (
[2m[36m(func pid=141940)[0m Parameter Group 0
[2m[36m(func pid=141940)[0m     dampening: 0
[2m[36m(func pid=141940)[0m     differentiable: False
[2m[36m(func pid=141940)[0m     foreach: None
[2m[36m(func pid=141940)[0m     lr: 0.1
[2m[36m(func pid=141940)[0m     maximize: False
[2m[36m(func pid=141940)[0m     momentum: 0.9
[2m[36m(func pid=141940)[0m     nesterov: False
[2m[36m(func pid=141940)[0m     weight_decay: 0.0001
[2m[36m(func pid=141940)[0m )
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m 
== Status ==
Current time: 2024-01-07 14:07:56 (running for 00:24:33.38)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  3.116 |      0.042 |                    3 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  2.454 |      0.094 |                    3 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.956 |      0.077 |                    1 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.13199626865671643
[2m[36m(func pid=140840)[0m top5: 0.5802238805970149
[2m[36m(func pid=140840)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=140840)[0m f1_macro: 0.09430205929190145
[2m[36m(func pid=140840)[0m f1_weighted: 0.1459015483550274
[2m[36m(func pid=140840)[0m f1_per_class: [0.161, 0.147, 0.083, 0.039, 0.0, 0.065, 0.32, 0.0, 0.079, 0.05]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0344 | Steps: 4 | Val loss: 2.5075 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.2600 | Steps: 4 | Val loss: 2.7752 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.2128 | Steps: 4 | Val loss: 2.1721 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 17.5890 | Steps: 4 | Val loss: 96287.3672 | Batch size: 32 | lr: 0.1 | Duration: 4.94s
[2m[36m(func pid=140272)[0m top1: 0.0830223880597015
[2m[36m(func pid=140272)[0m top5: 0.4780783582089552
[2m[36m(func pid=140272)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=140272)[0m f1_macro: 0.05111363166726805
[2m[36m(func pid=140272)[0m f1_weighted: 0.07415043887460653
[2m[36m(func pid=140272)[0m f1_per_class: [0.081, 0.113, 0.0, 0.16, 0.0, 0.013, 0.0, 0.103, 0.023, 0.018]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m top1: 0.08022388059701492
[2m[36m(func pid=141385)[0m top5: 0.6553171641791045
[2m[36m(func pid=141385)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=141385)[0m f1_macro: 0.18649093871693395
[2m[36m(func pid=141385)[0m f1_weighted: 0.04162691448560688
[2m[36m(func pid=141385)[0m f1_per_class: [0.422, 0.0, 0.7, 0.007, 0.265, 0.056, 0.0, 0.259, 0.08, 0.077]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m top1: 0.01166044776119403
[2m[36m(func pid=141940)[0m top5: 0.5149253731343284
[2m[36m(func pid=141940)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=141940)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=141940)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
== Status ==
Current time: 2024-01-07 14:08:01 (running for 00:24:38.84)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  3.034 |      0.051 |                    4 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  2.454 |      0.094 |                    3 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.26  |      0.186 |                    2 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 17.589 |      0.002 |                    1 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.24766791044776118
[2m[36m(func pid=140840)[0m top5: 0.644589552238806
[2m[36m(func pid=140840)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=140840)[0m f1_macro: 0.10659273460657466
[2m[36m(func pid=140840)[0m f1_weighted: 0.19073908456879066
[2m[36m(func pid=140840)[0m f1_per_class: [0.182, 0.05, 0.034, 0.017, 0.0, 0.021, 0.56, 0.0, 0.095, 0.108]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0421 | Steps: 4 | Val loss: 3.6290 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9321 | Steps: 4 | Val loss: 2.4683 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 17.8835 | Steps: 4 | Val loss: 51215429632.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8363 | Steps: 4 | Val loss: 1.9650 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=140272)[0m top1: 0.09235074626865672
[2m[36m(func pid=140272)[0m top5: 0.478544776119403
[2m[36m(func pid=140272)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=140272)[0m f1_macro: 0.06125195764094983
[2m[36m(func pid=140272)[0m f1_weighted: 0.08387536382293947
[2m[36m(func pid=140272)[0m f1_per_class: [0.102, 0.154, 0.0, 0.159, 0.0, 0.018, 0.003, 0.11, 0.041, 0.026]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m top1: 0.30223880597014924
[2m[36m(func pid=141385)[0m top5: 0.5289179104477612
[2m[36m(func pid=141385)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=141385)[0m f1_macro: 0.2306802785830721
[2m[36m(func pid=141385)[0m f1_weighted: 0.2081369631331004
[2m[36m(func pid=141385)[0m f1_per_class: [0.469, 0.016, 0.48, 0.599, 0.105, 0.015, 0.0, 0.274, 0.152, 0.196]
[2m[36m(func pid=141385)[0m 
== Status ==
Current time: 2024-01-07 14:08:07 (running for 00:24:44.32)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.932 |      0.061 |                    5 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  2.213 |      0.107 |                    4 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.042 |      0.231 |                    3 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 17.884 |      0.029 |                    2 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.17210820895522388
[2m[36m(func pid=141940)[0m top5: 0.5727611940298507
[2m[36m(func pid=141940)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=141940)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=141940)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3306902985074627
[2m[36m(func pid=140840)[0m top5: 0.773320895522388
[2m[36m(func pid=140840)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=140840)[0m f1_macro: 0.15988559061442512
[2m[36m(func pid=140840)[0m f1_weighted: 0.29550264384030783
[2m[36m(func pid=140840)[0m f1_per_class: [0.25, 0.079, 0.055, 0.346, 0.042, 0.03, 0.574, 0.0, 0.119, 0.104]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0358 | Steps: 4 | Val loss: 3.1532 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8309 | Steps: 4 | Val loss: 2.4312 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4943 | Steps: 4 | Val loss: 1.9007 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=141385)[0m top1: 0.3414179104477612
[2m[36m(func pid=141385)[0m top5: 0.6711753731343284
[2m[36m(func pid=141385)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=141385)[0m f1_macro: 0.29681219240105555
[2m[36m(func pid=141385)[0m f1_weighted: 0.2969607852015177
[2m[36m(func pid=141385)[0m f1_per_class: [0.557, 0.622, 0.426, 0.453, 0.093, 0.214, 0.003, 0.355, 0.0, 0.245]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 17.4636 | Steps: 4 | Val loss: 90065768.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=140272)[0m top1: 0.10074626865671642
[2m[36m(func pid=140272)[0m top5: 0.48367537313432835
[2m[36m(func pid=140272)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=140272)[0m f1_macro: 0.06798291313136104
[2m[36m(func pid=140272)[0m f1_weighted: 0.09700649318138138
[2m[36m(func pid=140272)[0m f1_per_class: [0.096, 0.184, 0.0, 0.175, 0.0, 0.022, 0.012, 0.11, 0.056, 0.025]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:13 (running for 00:24:49.98)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.831 |      0.068 |                    6 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  1.494 |      0.197 |                    6 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.036 |      0.297 |                    4 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 17.884 |      0.029 |                    2 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.3498134328358209
[2m[36m(func pid=140840)[0m top5: 0.824160447761194
[2m[36m(func pid=140840)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=140840)[0m f1_macro: 0.19708212260266458
[2m[36m(func pid=140840)[0m f1_weighted: 0.3261508739339235
[2m[36m(func pid=140840)[0m f1_per_class: [0.251, 0.139, 0.183, 0.482, 0.119, 0.061, 0.495, 0.015, 0.117, 0.109]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m top1: 0.006063432835820896
[2m[36m(func pid=141940)[0m top5: 0.5093283582089553
[2m[36m(func pid=141940)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=141940)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=141940)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.6549 | Steps: 4 | Val loss: 4.6490 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7046 | Steps: 4 | Val loss: 2.4117 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.2752 | Steps: 4 | Val loss: 1.9412 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=141385)[0m top1: 0.27052238805970147
[2m[36m(func pid=141385)[0m top5: 0.5069962686567164
[2m[36m(func pid=141385)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=141385)[0m f1_macro: 0.2172831071733691
[2m[36m(func pid=141385)[0m f1_weighted: 0.1778110757438776
[2m[36m(func pid=141385)[0m f1_per_class: [0.344, 0.56, 0.0, 0.054, 0.114, 0.264, 0.0, 0.257, 0.306, 0.272]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 14.7391 | Steps: 4 | Val loss: 1943198.5000 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=140272)[0m top1: 0.09934701492537314
[2m[36m(func pid=140272)[0m top5: 0.47947761194029853
[2m[36m(func pid=140272)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=140272)[0m f1_macro: 0.07594882257461491
[2m[36m(func pid=140272)[0m f1_weighted: 0.0995573539529642
[2m[36m(func pid=140272)[0m f1_per_class: [0.094, 0.177, 0.055, 0.166, 0.0, 0.044, 0.021, 0.118, 0.061, 0.024]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:18 (running for 00:24:55.51)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.705 |      0.076 |                    7 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  1.494 |      0.197 |                    6 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.655 |      0.217 |                    5 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 14.739 |      0.001 |                    4 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.30363805970149255
[2m[36m(func pid=140840)[0m top5: 0.8083022388059702
[2m[36m(func pid=140840)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=140840)[0m f1_macro: 0.2398047526157506
[2m[36m(func pid=140840)[0m f1_weighted: 0.2886509497410051
[2m[36m(func pid=140840)[0m f1_per_class: [0.22, 0.323, 0.308, 0.451, 0.11, 0.107, 0.197, 0.377, 0.176, 0.129]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m top1: 0.006063432835820896
[2m[36m(func pid=141940)[0m top5: 0.5093283582089553
[2m[36m(func pid=141940)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=141940)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=141940)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6082 | Steps: 4 | Val loss: 8.0562 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.5576 | Steps: 4 | Val loss: 2.3913 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.0711 | Steps: 4 | Val loss: 1.9423 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m top1: 0.048507462686567165
[2m[36m(func pid=141385)[0m top5: 0.4976679104477612
[2m[36m(func pid=141385)[0m f1_micro: 0.048507462686567165
[2m[36m(func pid=141385)[0m f1_macro: 0.15196955991151115
[2m[36m(func pid=141385)[0m f1_weighted: 0.02576045905462284
[2m[36m(func pid=141385)[0m f1_per_class: [0.087, 0.031, 0.741, 0.0, 0.036, 0.0, 0.0, 0.104, 0.082, 0.439]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 12.8471 | Steps: 4 | Val loss: 5181311.5000 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=140272)[0m top1: 0.10121268656716417
[2m[36m(func pid=140272)[0m top5: 0.48600746268656714
[2m[36m(func pid=140272)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=140272)[0m f1_macro: 0.07961181486851995
[2m[36m(func pid=140272)[0m f1_weighted: 0.10123900603230984
[2m[36m(func pid=140272)[0m f1_per_class: [0.109, 0.171, 0.07, 0.159, 0.0, 0.059, 0.03, 0.128, 0.043, 0.028]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:24 (running for 00:25:00.94)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.558 |      0.08  |                    8 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  1.275 |      0.24  |                    7 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.608 |      0.152 |                    6 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 12.847 |      0.001 |                    5 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.006063432835820896
[2m[36m(func pid=141940)[0m top5: 0.5093283582089553
[2m[36m(func pid=141940)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=141940)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=141940)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.32975746268656714
[2m[36m(func pid=140840)[0m top5: 0.7882462686567164
[2m[36m(func pid=140840)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=140840)[0m f1_macro: 0.2883895659051932
[2m[36m(func pid=140840)[0m f1_weighted: 0.27687185309655993
[2m[36m(func pid=140840)[0m f1_per_class: [0.326, 0.529, 0.429, 0.442, 0.115, 0.115, 0.025, 0.331, 0.277, 0.295]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.0911 | Steps: 4 | Val loss: 4.2781 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6057 | Steps: 4 | Val loss: 2.3679 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=141385)[0m top1: 0.16837686567164178
[2m[36m(func pid=141385)[0m top5: 0.7084888059701493
[2m[36m(func pid=141385)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=141385)[0m f1_macro: 0.19780037094927277
[2m[36m(func pid=141385)[0m f1_weighted: 0.1618487285071477
[2m[36m(func pid=141385)[0m f1_per_class: [0.43, 0.296, 0.048, 0.169, 0.061, 0.058, 0.079, 0.237, 0.149, 0.452]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 6.7820 | Steps: 4 | Val loss: 239311.4688 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9626 | Steps: 4 | Val loss: 2.0024 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=140272)[0m top1: 0.10914179104477612
[2m[36m(func pid=140272)[0m top5: 0.5023320895522388
[2m[36m(func pid=140272)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=140272)[0m f1_macro: 0.0945337996729332
[2m[36m(func pid=140272)[0m f1_weighted: 0.11048120771368297
[2m[36m(func pid=140272)[0m f1_per_class: [0.119, 0.177, 0.15, 0.17, 0.0, 0.055, 0.041, 0.142, 0.064, 0.028]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:29 (running for 00:25:06.62)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.606 |      0.095 |                    9 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  1.071 |      0.288 |                    8 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.091 |      0.198 |                    7 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  6.782 |      0.006 |                    6 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.32136194029850745
[2m[36m(func pid=140840)[0m top5: 0.7621268656716418
[2m[36m(func pid=140840)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=140840)[0m f1_macro: 0.30031273956619564
[2m[36m(func pid=140840)[0m f1_weighted: 0.2655303892618542
[2m[36m(func pid=140840)[0m f1_per_class: [0.439, 0.534, 0.348, 0.395, 0.137, 0.153, 0.006, 0.323, 0.258, 0.41]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m top1: 0.03311567164179104
[2m[36m(func pid=141940)[0m top5: 0.5149253731343284
[2m[36m(func pid=141940)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=141940)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=141940)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.2593 | Steps: 4 | Val loss: 3.9411 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5468 | Steps: 4 | Val loss: 2.3455 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=141385)[0m top1: 0.3344216417910448
[2m[36m(func pid=141385)[0m top5: 0.8372201492537313
[2m[36m(func pid=141385)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=141385)[0m f1_macro: 0.25291333186405185
[2m[36m(func pid=141385)[0m f1_weighted: 0.27934452890874134
[2m[36m(func pid=141385)[0m f1_per_class: [0.395, 0.445, 0.324, 0.54, 0.088, 0.204, 0.027, 0.092, 0.0, 0.415]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6121 | Steps: 4 | Val loss: 1.8745 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 10.3313 | Steps: 4 | Val loss: 162790.5000 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=140272)[0m top1: 0.1142723880597015
[2m[36m(func pid=140272)[0m top5: 0.523320895522388
[2m[36m(func pid=140272)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=140272)[0m f1_macro: 0.09832008525534755
[2m[36m(func pid=140272)[0m f1_weighted: 0.12149156564199125
[2m[36m(func pid=140272)[0m f1_per_class: [0.122, 0.168, 0.131, 0.181, 0.0, 0.063, 0.069, 0.149, 0.059, 0.041]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:35 (running for 00:25:12.01)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.547 |      0.098 |                   10 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.612 |      0.321 |                   10 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.259 |      0.253 |                    8 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  6.782 |      0.006 |                    6 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.3512126865671642
[2m[36m(func pid=140840)[0m top5: 0.8194962686567164
[2m[36m(func pid=140840)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=140840)[0m f1_macro: 0.32131811208272004
[2m[36m(func pid=140840)[0m f1_weighted: 0.31041627577180186
[2m[36m(func pid=140840)[0m f1_per_class: [0.508, 0.559, 0.258, 0.477, 0.168, 0.245, 0.025, 0.344, 0.247, 0.381]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m top1: 0.057369402985074626
[2m[36m(func pid=141940)[0m top5: 0.5121268656716418
[2m[36m(func pid=141940)[0m f1_micro: 0.057369402985074626
[2m[36m(func pid=141940)[0m f1_macro: 0.01354054657752705
[2m[36m(func pid=141940)[0m f1_weighted: 0.007579963994718857
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.131, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0430 | Steps: 4 | Val loss: 5.4224 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4892 | Steps: 4 | Val loss: 2.3274 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=141385)[0m top1: 0.21595149253731344
[2m[36m(func pid=141385)[0m top5: 0.6702425373134329
[2m[36m(func pid=141385)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=141385)[0m f1_macro: 0.18430576862982057
[2m[36m(func pid=141385)[0m f1_weighted: 0.20608827158550083
[2m[36m(func pid=141385)[0m f1_per_class: [0.137, 0.45, 0.111, 0.18, 0.085, 0.0, 0.146, 0.426, 0.128, 0.182]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 14.9970 | Steps: 4 | Val loss: 26594.4570 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8639 | Steps: 4 | Val loss: 1.7559 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=140272)[0m top1: 0.11380597014925373
[2m[36m(func pid=140272)[0m top5: 0.5373134328358209
[2m[36m(func pid=140272)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=140272)[0m f1_macro: 0.09974917473981085
[2m[36m(func pid=140272)[0m f1_weighted: 0.12263104211243088
[2m[36m(func pid=140272)[0m f1_per_class: [0.125, 0.187, 0.117, 0.156, 0.0, 0.057, 0.085, 0.152, 0.072, 0.046]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:40 (running for 00:25:17.36)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.489 |      0.1   |                   11 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.612 |      0.321 |                   10 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.043 |      0.184 |                    9 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 14.997 |      0.011 |                    8 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.05783582089552239
[2m[36m(func pid=141940)[0m top5: 0.5186567164179104
[2m[36m(func pid=141940)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=141940)[0m f1_macro: 0.01106648817492191
[2m[36m(func pid=141940)[0m f1_weighted: 0.006400394280271999
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6476 | Steps: 4 | Val loss: 5.1136 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=140840)[0m top1: 0.38386194029850745
[2m[36m(func pid=140840)[0m top5: 0.8768656716417911
[2m[36m(func pid=140840)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=140840)[0m f1_macro: 0.33427916198279617
[2m[36m(func pid=140840)[0m f1_weighted: 0.3483389663727154
[2m[36m(func pid=140840)[0m f1_per_class: [0.496, 0.51, 0.296, 0.572, 0.173, 0.263, 0.083, 0.349, 0.267, 0.333]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.3703 | Steps: 4 | Val loss: 2.3048 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=141385)[0m top1: 0.16417910447761194
[2m[36m(func pid=141385)[0m top5: 0.6907649253731343
[2m[36m(func pid=141385)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=141385)[0m f1_macro: 0.16822601438988724
[2m[36m(func pid=141385)[0m f1_weighted: 0.13909440064487302
[2m[36m(func pid=141385)[0m f1_per_class: [0.182, 0.286, 0.344, 0.124, 0.082, 0.0, 0.08, 0.323, 0.159, 0.103]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 13.7527 | Steps: 4 | Val loss: 6545.7441 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4945 | Steps: 4 | Val loss: 1.7411 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=140272)[0m top1: 0.12313432835820895
[2m[36m(func pid=140272)[0m top5: 0.5620335820895522
[2m[36m(func pid=140272)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=140272)[0m f1_macro: 0.1069109079042416
[2m[36m(func pid=140272)[0m f1_weighted: 0.1347173060685801
[2m[36m(func pid=140272)[0m f1_per_class: [0.126, 0.187, 0.115, 0.157, 0.016, 0.057, 0.121, 0.177, 0.052, 0.061]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:45 (running for 00:25:22.74)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.37  |      0.107 |                   12 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.864 |      0.334 |                   11 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.648 |      0.168 |                   10 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 13.753 |      0.018 |                    9 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.06203358208955224
[2m[36m(func pid=141940)[0m top5: 0.6240671641791045
[2m[36m(func pid=141940)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=141940)[0m f1_macro: 0.017690324408556453
[2m[36m(func pid=141940)[0m f1_weighted: 0.008375980277109986
[2m[36m(func pid=141940)[0m f1_per_class: [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.127, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7457 | Steps: 4 | Val loss: 4.2159 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=140840)[0m top1: 0.35774253731343286
[2m[36m(func pid=140840)[0m top5: 0.8861940298507462
[2m[36m(func pid=140840)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=140840)[0m f1_macro: 0.31283404627267697
[2m[36m(func pid=140840)[0m f1_weighted: 0.34417079543492396
[2m[36m(func pid=140840)[0m f1_per_class: [0.547, 0.292, 0.436, 0.604, 0.144, 0.246, 0.183, 0.329, 0.216, 0.131]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.3340 | Steps: 4 | Val loss: 2.2847 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=141385)[0m top1: 0.1525186567164179
[2m[36m(func pid=141385)[0m top5: 0.7369402985074627
[2m[36m(func pid=141385)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=141385)[0m f1_macro: 0.16276710408573658
[2m[36m(func pid=141385)[0m f1_weighted: 0.12681380610186496
[2m[36m(func pid=141385)[0m f1_per_class: [0.122, 0.14, 0.302, 0.019, 0.32, 0.016, 0.231, 0.23, 0.175, 0.072]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 6.8167 | Steps: 4 | Val loss: 1078.3412 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5860 | Steps: 4 | Val loss: 1.7799 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=140272)[0m top1: 0.13292910447761194
[2m[36m(func pid=140272)[0m top5: 0.582089552238806
[2m[36m(func pid=140272)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=140272)[0m f1_macro: 0.11885649720186656
[2m[36m(func pid=140272)[0m f1_weighted: 0.14689704814352025
[2m[36m(func pid=140272)[0m f1_per_class: [0.143, 0.188, 0.149, 0.182, 0.02, 0.061, 0.135, 0.158, 0.081, 0.072]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:51 (running for 00:25:28.19)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.334 |      0.119 |                   13 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.495 |      0.313 |                   12 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.746 |      0.163 |                   11 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  6.817 |      0.033 |                   10 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.0853544776119403
[2m[36m(func pid=141940)[0m top5: 0.6749067164179104
[2m[36m(func pid=141940)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=141940)[0m f1_macro: 0.0330017711835192
[2m[36m(func pid=141940)[0m f1_weighted: 0.03506944427185189
[2m[36m(func pid=141940)[0m f1_per_class: [0.077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.079, 0.174, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8208 | Steps: 4 | Val loss: 6.6976 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=140840)[0m top1: 0.3138992537313433
[2m[36m(func pid=140840)[0m top5: 0.886660447761194
[2m[36m(func pid=140840)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=140840)[0m f1_macro: 0.29792720240840964
[2m[36m(func pid=140840)[0m f1_weighted: 0.3170676289681693
[2m[36m(func pid=140840)[0m f1_per_class: [0.569, 0.3, 0.429, 0.564, 0.134, 0.183, 0.154, 0.284, 0.25, 0.113]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2813 | Steps: 4 | Val loss: 2.2624 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=141385)[0m top1: 0.12639925373134328
[2m[36m(func pid=141385)[0m top5: 0.6702425373134329
[2m[36m(func pid=141385)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=141385)[0m f1_macro: 0.14297449914067836
[2m[36m(func pid=141385)[0m f1_weighted: 0.13655386540216147
[2m[36m(func pid=141385)[0m f1_per_class: [0.125, 0.005, 0.043, 0.122, 0.4, 0.016, 0.245, 0.266, 0.152, 0.056]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 10.6154 | Steps: 4 | Val loss: 737.9058 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4716 | Steps: 4 | Val loss: 1.7603 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=140272)[0m top1: 0.13852611940298507
[2m[36m(func pid=140272)[0m top5: 0.6044776119402985
[2m[36m(func pid=140272)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=140272)[0m f1_macro: 0.11935850939606898
[2m[36m(func pid=140272)[0m f1_weighted: 0.15290346260334906
[2m[36m(func pid=140272)[0m f1_per_class: [0.138, 0.179, 0.154, 0.197, 0.026, 0.062, 0.144, 0.173, 0.073, 0.047]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:08:56 (running for 00:25:33.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.281 |      0.119 |                   14 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.586 |      0.298 |                   13 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.821 |      0.143 |                   12 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 10.615 |      0.033 |                   11 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.08348880597014925
[2m[36m(func pid=141940)[0m top5: 0.6268656716417911
[2m[36m(func pid=141940)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=141940)[0m f1_macro: 0.03261783604563216
[2m[36m(func pid=141940)[0m f1_weighted: 0.03267370431395317
[2m[36m(func pid=141940)[0m f1_per_class: [0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.068, 0.189, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6426 | Steps: 4 | Val loss: 7.2188 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=140840)[0m top1: 0.32649253731343286
[2m[36m(func pid=140840)[0m top5: 0.8824626865671642
[2m[36m(func pid=140840)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=140840)[0m f1_macro: 0.3038109304570228
[2m[36m(func pid=140840)[0m f1_weighted: 0.3492549160655197
[2m[36m(func pid=140840)[0m f1_per_class: [0.365, 0.46, 0.522, 0.433, 0.136, 0.175, 0.308, 0.281, 0.241, 0.117]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.2723 | Steps: 4 | Val loss: 2.2240 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=141385)[0m top1: 0.25093283582089554
[2m[36m(func pid=141385)[0m top5: 0.6958955223880597
[2m[36m(func pid=141385)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=141385)[0m f1_macro: 0.17220855210382463
[2m[36m(func pid=141385)[0m f1_weighted: 0.17521587531164687
[2m[36m(func pid=141385)[0m f1_per_class: [0.279, 0.011, 0.106, 0.48, 0.4, 0.0, 0.054, 0.13, 0.161, 0.102]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 6.5182 | Steps: 4 | Val loss: 51.6842 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3011 | Steps: 4 | Val loss: 1.7970 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=140272)[0m top1: 0.16324626865671643
[2m[36m(func pid=140272)[0m top5: 0.6478544776119403
[2m[36m(func pid=140272)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=140272)[0m f1_macro: 0.1342453112706254
[2m[36m(func pid=140272)[0m f1_weighted: 0.1792187203477398
[2m[36m(func pid=140272)[0m f1_per_class: [0.152, 0.227, 0.167, 0.23, 0.031, 0.062, 0.172, 0.18, 0.068, 0.054]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6873 | Steps: 4 | Val loss: 5.7968 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=141940)[0m top1: 0.23460820895522388
[2m[36m(func pid=141940)[0m top5: 0.5965485074626866
[2m[36m(func pid=141940)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=141940)[0m f1_macro: 0.08971528607999414
[2m[36m(func pid=141940)[0m f1_weighted: 0.13960752985501906
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.447, 0.0, 0.0, 0.0, 0.0, 0.157, 0.268, 0.0, 0.025]
== Status ==
Current time: 2024-01-07 14:09:02 (running for 00:25:38.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.272 |      0.134 |                   15 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.472 |      0.304 |                   14 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.643 |      0.172 |                   13 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  6.518 |      0.09  |                   12 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.34048507462686567
[2m[36m(func pid=140840)[0m top5: 0.8652052238805971
[2m[36m(func pid=140840)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=140840)[0m f1_macro: 0.32106065796543415
[2m[36m(func pid=140840)[0m f1_weighted: 0.3545538408389126
[2m[36m(func pid=140840)[0m f1_per_class: [0.266, 0.511, 0.6, 0.352, 0.119, 0.222, 0.348, 0.308, 0.275, 0.209]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.1715 | Steps: 4 | Val loss: 2.2120 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=141385)[0m top1: 0.33955223880597013
[2m[36m(func pid=141385)[0m top5: 0.7649253731343284
[2m[36m(func pid=141385)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=141385)[0m f1_macro: 0.2874133263231501
[2m[36m(func pid=141385)[0m f1_weighted: 0.227205375034868
[2m[36m(func pid=141385)[0m f1_per_class: [0.519, 0.173, 0.688, 0.515, 0.062, 0.0, 0.027, 0.369, 0.135, 0.386]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 7.9179 | Steps: 4 | Val loss: 7.7513 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3284 | Steps: 4 | Val loss: 1.7511 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=140272)[0m top1: 0.16417910447761194
[2m[36m(func pid=140272)[0m top5: 0.652518656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=140272)[0m f1_macro: 0.13539129600643346
[2m[36m(func pid=140272)[0m f1_weighted: 0.1795207328095824
[2m[36m(func pid=140272)[0m f1_per_class: [0.145, 0.23, 0.149, 0.225, 0.034, 0.097, 0.164, 0.178, 0.071, 0.062]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:09:07 (running for 00:25:44.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.171 |      0.135 |                   16 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.301 |      0.321 |                   15 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.687 |      0.287 |                   14 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  7.918 |      0.134 |                   13 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5301 | Steps: 4 | Val loss: 13.2752 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=141940)[0m top1: 0.37220149253731344
[2m[36m(func pid=141940)[0m top5: 0.5592350746268657
[2m[36m(func pid=141940)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=141940)[0m f1_macro: 0.13369258891570363
[2m[36m(func pid=141940)[0m f1_weighted: 0.287366674360428
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.45, 0.0, 0.0, 0.0, 0.296, 0.591, 0.0, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.373134328358209
[2m[36m(func pid=140840)[0m top5: 0.8782649253731343
[2m[36m(func pid=140840)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=140840)[0m f1_macro: 0.34702802186761883
[2m[36m(func pid=140840)[0m f1_weighted: 0.3827575023049125
[2m[36m(func pid=140840)[0m f1_per_class: [0.288, 0.53, 0.615, 0.345, 0.135, 0.233, 0.424, 0.337, 0.26, 0.302]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.1531 | Steps: 4 | Val loss: 2.2032 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=141385)[0m top1: 0.2980410447761194
[2m[36m(func pid=141385)[0m top5: 0.683768656716418
[2m[36m(func pid=141385)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=141385)[0m f1_macro: 0.20485411067965228
[2m[36m(func pid=141385)[0m f1_weighted: 0.16997447698602758
[2m[36m(func pid=141385)[0m f1_per_class: [0.308, 0.122, 0.632, 0.45, 0.0, 0.0, 0.0, 0.088, 0.146, 0.304]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 7.0105 | Steps: 4 | Val loss: 3.2122 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3938 | Steps: 4 | Val loss: 1.6721 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=140272)[0m top1: 0.17210820895522388
[2m[36m(func pid=140272)[0m top5: 0.6646455223880597
[2m[36m(func pid=140272)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=140272)[0m f1_macro: 0.13453980908807467
[2m[36m(func pid=140272)[0m f1_weighted: 0.18764662359529508
[2m[36m(func pid=140272)[0m f1_per_class: [0.149, 0.267, 0.125, 0.193, 0.02, 0.107, 0.199, 0.165, 0.066, 0.055]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6578 | Steps: 4 | Val loss: 7.0840 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:09:13 (running for 00:25:49.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.153 |      0.135 |                   17 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.328 |      0.347 |                   16 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.53  |      0.205 |                   15 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  7.01  |      0.055 |                   14 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.1478544776119403
[2m[36m(func pid=141940)[0m top5: 0.25279850746268656
[2m[36m(func pid=141940)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=141940)[0m f1_macro: 0.05457436551734771
[2m[36m(func pid=141940)[0m f1_weighted: 0.0761499179815422
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.348, 0.0, 0.0, 0.0, 0.018, 0.024, 0.079, 0.077, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3987873134328358
[2m[36m(func pid=140840)[0m top5: 0.8997201492537313
[2m[36m(func pid=140840)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=140840)[0m f1_macro: 0.35521247582174226
[2m[36m(func pid=140840)[0m f1_weighted: 0.40494887903761007
[2m[36m(func pid=140840)[0m f1_per_class: [0.333, 0.546, 0.545, 0.367, 0.184, 0.242, 0.469, 0.31, 0.252, 0.304]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.1055 | Steps: 4 | Val loss: 2.2000 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m top1: 0.30223880597014924
[2m[36m(func pid=141385)[0m top5: 0.7220149253731343
[2m[36m(func pid=141385)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=141385)[0m f1_macro: 0.22160403571461904
[2m[36m(func pid=141385)[0m f1_weighted: 0.21868247839206215
[2m[36m(func pid=141385)[0m f1_per_class: [0.252, 0.534, 0.276, 0.336, 0.176, 0.0, 0.0, 0.375, 0.0, 0.267]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 11.4582 | Steps: 4 | Val loss: 2.9231 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1634 | Steps: 4 | Val loss: 1.6663 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=140272)[0m top1: 0.17210820895522388
[2m[36m(func pid=140272)[0m top5: 0.6707089552238806
[2m[36m(func pid=140272)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=140272)[0m f1_macro: 0.13723120859230542
[2m[36m(func pid=140272)[0m f1_weighted: 0.18649026375625422
[2m[36m(func pid=140272)[0m f1_per_class: [0.147, 0.258, 0.117, 0.193, 0.027, 0.092, 0.199, 0.193, 0.076, 0.071]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7262 | Steps: 4 | Val loss: 13.1587 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=141940)[0m top1: 0.04291044776119403
[2m[36m(func pid=141940)[0m top5: 0.386660447761194
[2m[36m(func pid=141940)[0m f1_micro: 0.04291044776119403
[2m[36m(func pid=141940)[0m f1_macro: 0.04603103839845229
[2m[36m(func pid=141940)[0m f1_weighted: 0.02496184489505202
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.41, 0.034, 0.0]
== Status ==
Current time: 2024-01-07 14:09:18 (running for 00:25:55.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.106 |      0.137 |                   18 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.394 |      0.355 |                   17 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.658 |      0.222 |                   16 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 | 11.458 |      0.046 |                   15 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.396455223880597
[2m[36m(func pid=140840)[0m top5: 0.9011194029850746
[2m[36m(func pid=140840)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=140840)[0m f1_macro: 0.3565630564526677
[2m[36m(func pid=140840)[0m f1_weighted: 0.39966008925955565
[2m[36m(func pid=140840)[0m f1_per_class: [0.458, 0.566, 0.471, 0.373, 0.18, 0.242, 0.429, 0.313, 0.223, 0.312]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.0095 | Steps: 4 | Val loss: 2.1963 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=141385)[0m top1: 0.12313432835820895
[2m[36m(func pid=141385)[0m top5: 0.664179104477612
[2m[36m(func pid=141385)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=141385)[0m f1_macro: 0.11102015387292238
[2m[36m(func pid=141385)[0m f1_weighted: 0.08906269387263095
[2m[36m(func pid=141385)[0m f1_per_class: [0.099, 0.385, 0.029, 0.003, 0.213, 0.008, 0.0, 0.277, 0.0, 0.095]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 4.2002 | Steps: 4 | Val loss: 9.9092 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3596 | Steps: 4 | Val loss: 1.6494 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=140272)[0m top1: 0.17723880597014927
[2m[36m(func pid=140272)[0m top5: 0.6707089552238806
[2m[36m(func pid=140272)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=140272)[0m f1_macro: 0.1388753978457901
[2m[36m(func pid=140272)[0m f1_weighted: 0.19502630361383372
[2m[36m(func pid=140272)[0m f1_per_class: [0.15, 0.259, 0.118, 0.205, 0.028, 0.092, 0.215, 0.201, 0.075, 0.045]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:09:23 (running for 00:26:00.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  2.009 |      0.139 |                   19 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.163 |      0.357 |                   18 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.726 |      0.111 |                   17 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  4.2   |      0.031 |                   16 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.9505 | Steps: 4 | Val loss: 15.1651 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=141940)[0m top1: 0.07555970149253731
[2m[36m(func pid=141940)[0m top5: 0.5867537313432836
[2m[36m(func pid=141940)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=141940)[0m f1_macro: 0.03145419095974469
[2m[36m(func pid=141940)[0m f1_weighted: 0.033422291121856525
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0.069, 0.221, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.38759328358208955
[2m[36m(func pid=140840)[0m top5: 0.9067164179104478
[2m[36m(func pid=140840)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=140840)[0m f1_macro: 0.3673242871892665
[2m[36m(func pid=140840)[0m f1_weighted: 0.38605570917872395
[2m[36m(func pid=140840)[0m f1_per_class: [0.507, 0.56, 0.522, 0.36, 0.207, 0.243, 0.391, 0.304, 0.245, 0.333]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.9471 | Steps: 4 | Val loss: 2.1987 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m top1: 0.07602611940298508
[2m[36m(func pid=141385)[0m top5: 0.6226679104477612
[2m[36m(func pid=141385)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=141385)[0m f1_macro: 0.12570259839863973
[2m[36m(func pid=141385)[0m f1_weighted: 0.08020620857314656
[2m[36m(func pid=141385)[0m f1_per_class: [0.136, 0.0, 0.017, 0.0, 0.429, 0.309, 0.081, 0.237, 0.016, 0.032]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.1502 | Steps: 4 | Val loss: 6.0995 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3157 | Steps: 4 | Val loss: 1.6699 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=140272)[0m top1: 0.17817164179104478
[2m[36m(func pid=140272)[0m top5: 0.6707089552238806
[2m[36m(func pid=140272)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=140272)[0m f1_macro: 0.1405800250535228
[2m[36m(func pid=140272)[0m f1_weighted: 0.1991487477305836
[2m[36m(func pid=140272)[0m f1_per_class: [0.147, 0.26, 0.114, 0.197, 0.028, 0.094, 0.233, 0.205, 0.093, 0.036]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:09:28 (running for 00:26:05.86)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.947 |      0.141 |                   20 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.36  |      0.367 |                   19 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  0.951 |      0.126 |                   18 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  3.15  |      0.09  |                   17 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.27238805970149255
[2m[36m(func pid=141940)[0m top5: 0.7952425373134329
[2m[36m(func pid=141940)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=141940)[0m f1_macro: 0.09011478109330125
[2m[36m(func pid=141940)[0m f1_weighted: 0.20089023289956284
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.02, 0.0, 0.517, 0.0, 0.0, 0.134, 0.23, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.6381 | Steps: 4 | Val loss: 5.0084 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=140840)[0m top1: 0.37826492537313433
[2m[36m(func pid=140840)[0m top5: 0.8997201492537313
[2m[36m(func pid=140840)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=140840)[0m f1_macro: 0.3570534115228664
[2m[36m(func pid=140840)[0m f1_weighted: 0.3744806581770447
[2m[36m(func pid=140840)[0m f1_per_class: [0.595, 0.555, 0.48, 0.37, 0.186, 0.225, 0.35, 0.313, 0.24, 0.256]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.9336 | Steps: 4 | Val loss: 2.1794 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=141385)[0m top1: 0.2887126865671642
[2m[36m(func pid=141385)[0m top5: 0.7560634328358209
[2m[36m(func pid=141385)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=141385)[0m f1_macro: 0.17828148767769053
[2m[36m(func pid=141385)[0m f1_weighted: 0.22543463243975506
[2m[36m(func pid=141385)[0m f1_per_class: [0.115, 0.056, 0.0, 0.115, 0.513, 0.075, 0.498, 0.338, 0.0, 0.072]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.1171 | Steps: 4 | Val loss: 3.9906 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2198 | Steps: 4 | Val loss: 1.7218 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=140272)[0m top1: 0.18796641791044777
[2m[36m(func pid=140272)[0m top5: 0.6842350746268657
[2m[36m(func pid=140272)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=140272)[0m f1_macro: 0.14895064362600755
[2m[36m(func pid=140272)[0m f1_weighted: 0.20846232350449131
[2m[36m(func pid=140272)[0m f1_per_class: [0.161, 0.229, 0.113, 0.21, 0.035, 0.086, 0.267, 0.226, 0.087, 0.075]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:09:34 (running for 00:26:11.28)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.934 |      0.149 |                   21 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.316 |      0.357 |                   20 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.638 |      0.178 |                   19 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  3.117 |      0.086 |                   18 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.2826492537313433
[2m[36m(func pid=141940)[0m top5: 0.8246268656716418
[2m[36m(func pid=141940)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=141940)[0m f1_macro: 0.08597156780126773
[2m[36m(func pid=141940)[0m f1_weighted: 0.19059214892178902
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.537, 0.0, 0.0, 0.092, 0.23, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.6631 | Steps: 4 | Val loss: 9.1936 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=140840)[0m top1: 0.3712686567164179
[2m[36m(func pid=140840)[0m top5: 0.8857276119402985
[2m[36m(func pid=140840)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=140840)[0m f1_macro: 0.33728468987050764
[2m[36m(func pid=140840)[0m f1_weighted: 0.36370568260624864
[2m[36m(func pid=140840)[0m f1_per_class: [0.591, 0.551, 0.333, 0.363, 0.126, 0.208, 0.324, 0.343, 0.281, 0.252]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.7725 | Steps: 4 | Val loss: 2.1831 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m top1: 0.09468283582089553
[2m[36m(func pid=141385)[0m top5: 0.7248134328358209
[2m[36m(func pid=141385)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=141385)[0m f1_macro: 0.05416469446139062
[2m[36m(func pid=141385)[0m f1_weighted: 0.10175404090138668
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.032, 0.05, 0.187, 0.077, 0.039, 0.129, 0.0, 0.0, 0.028]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6005 | Steps: 4 | Val loss: 3.8179 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2239 | Steps: 4 | Val loss: 1.7004 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=140272)[0m top1: 0.1828358208955224
[2m[36m(func pid=140272)[0m top5: 0.6823694029850746
[2m[36m(func pid=140272)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=140272)[0m f1_macro: 0.14728562724425212
[2m[36m(func pid=140272)[0m f1_weighted: 0.20619726188238294
[2m[36m(func pid=140272)[0m f1_per_class: [0.175, 0.211, 0.109, 0.23, 0.044, 0.074, 0.257, 0.217, 0.076, 0.078]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:09:39 (running for 00:26:16.63)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.773 |      0.147 |                   22 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.22  |      0.337 |                   21 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.663 |      0.054 |                   20 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.6   |      0.043 |                   19 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.1905 | Steps: 4 | Val loss: 21.7893 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=141940)[0m top1: 0.08628731343283583
[2m[36m(func pid=141940)[0m top5: 0.7910447761194029
[2m[36m(func pid=141940)[0m f1_micro: 0.08628731343283583
[2m[36m(func pid=141940)[0m f1_macro: 0.042764253176555904
[2m[36m(func pid=141940)[0m f1_weighted: 0.05166398416267292
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.118, 0.277, 0.0, 0.032]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.38992537313432835
[2m[36m(func pid=140840)[0m top5: 0.8880597014925373
[2m[36m(func pid=140840)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=140840)[0m f1_macro: 0.34482996860276866
[2m[36m(func pid=140840)[0m f1_weighted: 0.39021339504453284
[2m[36m(func pid=140840)[0m f1_per_class: [0.515, 0.58, 0.333, 0.455, 0.106, 0.179, 0.318, 0.37, 0.299, 0.291]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.8728 | Steps: 4 | Val loss: 2.1740 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=141385)[0m top1: 0.1394589552238806
[2m[36m(func pid=141385)[0m top5: 0.7136194029850746
[2m[36m(func pid=141385)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=141385)[0m f1_macro: 0.10230998624391563
[2m[36m(func pid=141385)[0m f1_weighted: 0.17513904163679891
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.102, 0.0, 0.193, 0.269, 0.145, 0.281, 0.013, 0.02, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.0851 | Steps: 4 | Val loss: 2.9514 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2164 | Steps: 4 | Val loss: 1.6740 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=140272)[0m top1: 0.18563432835820895
[2m[36m(func pid=140272)[0m top5: 0.6819029850746269
[2m[36m(func pid=140272)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=140272)[0m f1_macro: 0.153914536807663
[2m[36m(func pid=140272)[0m f1_weighted: 0.20432404324223957
[2m[36m(func pid=140272)[0m f1_per_class: [0.197, 0.226, 0.096, 0.252, 0.061, 0.089, 0.21, 0.226, 0.097, 0.085]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:09:45 (running for 00:26:22.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.873 |      0.154 |                   23 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.224 |      0.345 |                   22 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.19  |      0.102 |                   21 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  3.085 |      0.057 |                   20 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 3.1453 | Steps: 4 | Val loss: 60.9527 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=141940)[0m top1: 0.09888059701492537
[2m[36m(func pid=141940)[0m top5: 0.5438432835820896
[2m[36m(func pid=141940)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=141940)[0m f1_macro: 0.056691825968062326
[2m[36m(func pid=141940)[0m f1_weighted: 0.07891389046154376
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.198, 0.341, 0.0, 0.028]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.394589552238806
[2m[36m(func pid=140840)[0m top5: 0.894589552238806
[2m[36m(func pid=140840)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=140840)[0m f1_macro: 0.3563607794318932
[2m[36m(func pid=140840)[0m f1_weighted: 0.4012767778523663
[2m[36m(func pid=140840)[0m f1_per_class: [0.476, 0.58, 0.407, 0.499, 0.142, 0.248, 0.293, 0.371, 0.256, 0.292]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.9177 | Steps: 4 | Val loss: 2.1561 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=141385)[0m top1: 0.12779850746268656
[2m[36m(func pid=141385)[0m top5: 0.7201492537313433
[2m[36m(func pid=141385)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=141385)[0m f1_macro: 0.11105939827117753
[2m[36m(func pid=141385)[0m f1_weighted: 0.1156908561576903
[2m[36m(func pid=141385)[0m f1_per_class: [0.204, 0.381, 0.0, 0.0, 0.222, 0.015, 0.128, 0.0, 0.115, 0.045]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7947 | Steps: 4 | Val loss: 2.3349 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7144 | Steps: 4 | Val loss: 1.7177 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=140272)[0m top1: 0.20149253731343283
[2m[36m(func pid=140272)[0m top5: 0.6986940298507462
[2m[36m(func pid=140272)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=140272)[0m f1_macro: 0.162550892502921
[2m[36m(func pid=140272)[0m f1_weighted: 0.22644199064153148
[2m[36m(func pid=140272)[0m f1_per_class: [0.215, 0.258, 0.108, 0.264, 0.039, 0.073, 0.257, 0.243, 0.1, 0.07]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:09:50 (running for 00:26:27.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.918 |      0.163 |                   24 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.216 |      0.356 |                   23 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.145 |      0.111 |                   22 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.795 |      0.092 |                   21 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6662 | Steps: 4 | Val loss: 11.2078 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=141940)[0m top1: 0.1609141791044776
[2m[36m(func pid=141940)[0m top5: 0.5410447761194029
[2m[36m(func pid=141940)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=141940)[0m f1_macro: 0.09234169328953395
[2m[36m(func pid=141940)[0m f1_weighted: 0.15859835465848396
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.443, 0.452, 0.0, 0.028]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.396455223880597
[2m[36m(func pid=140840)[0m top5: 0.8782649253731343
[2m[36m(func pid=140840)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=140840)[0m f1_macro: 0.36114117982731897
[2m[36m(func pid=140840)[0m f1_weighted: 0.39886381544548816
[2m[36m(func pid=140840)[0m f1_per_class: [0.37, 0.567, 0.4, 0.492, 0.132, 0.265, 0.286, 0.388, 0.31, 0.4]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.7407 | Steps: 4 | Val loss: 2.1448 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=141385)[0m top1: 0.13899253731343283
[2m[36m(func pid=141385)[0m top5: 0.5904850746268657
[2m[36m(func pid=141385)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=141385)[0m f1_macro: 0.11622804589619715
[2m[36m(func pid=141385)[0m f1_weighted: 0.10772244211544969
[2m[36m(func pid=141385)[0m f1_per_class: [0.136, 0.353, 0.0, 0.01, 0.25, 0.213, 0.024, 0.089, 0.088, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7827 | Steps: 4 | Val loss: 2.2673 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.1360 | Steps: 4 | Val loss: 1.7128 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=140272)[0m top1: 0.19962686567164178
[2m[36m(func pid=140272)[0m top5: 0.7122201492537313
[2m[36m(func pid=140272)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=140272)[0m f1_macro: 0.16490574868044622
[2m[36m(func pid=140272)[0m f1_weighted: 0.22380017215244946
[2m[36m(func pid=140272)[0m f1_per_class: [0.218, 0.261, 0.141, 0.262, 0.046, 0.085, 0.245, 0.231, 0.098, 0.062]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.7937 | Steps: 4 | Val loss: 13.1712 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:09:55 (running for 00:26:32.88)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.741 |      0.165 |                   25 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.714 |      0.361 |                   24 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.666 |      0.116 |                   23 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.783 |      0.056 |                   22 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.08255597014925373
[2m[36m(func pid=141940)[0m top5: 0.5055970149253731
[2m[36m(func pid=141940)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=141940)[0m f1_macro: 0.05554548344771252
[2m[36m(func pid=141940)[0m f1_weighted: 0.10895458965488507
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.161, 0.287, 0.083, 0.0, 0.024]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.38619402985074625
[2m[36m(func pid=140840)[0m top5: 0.886660447761194
[2m[36m(func pid=140840)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=140840)[0m f1_macro: 0.3614748527886505
[2m[36m(func pid=140840)[0m f1_weighted: 0.3830401361195056
[2m[36m(func pid=140840)[0m f1_per_class: [0.323, 0.58, 0.453, 0.5, 0.178, 0.283, 0.225, 0.311, 0.299, 0.462]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.8186 | Steps: 4 | Val loss: 2.1205 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=141385)[0m top1: 0.09514925373134328
[2m[36m(func pid=141385)[0m top5: 0.42490671641791045
[2m[36m(func pid=141385)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=141385)[0m f1_macro: 0.08083205295412761
[2m[36m(func pid=141385)[0m f1_weighted: 0.08306383434653755
[2m[36m(func pid=141385)[0m f1_per_class: [0.055, 0.405, 0.02, 0.023, 0.119, 0.008, 0.006, 0.0, 0.0, 0.171]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6456 | Steps: 4 | Val loss: 2.2710 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2722 | Steps: 4 | Val loss: 1.7034 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=140272)[0m top1: 0.2103544776119403
[2m[36m(func pid=140272)[0m top5: 0.7308768656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=140272)[0m f1_macro: 0.17375144643521756
[2m[36m(func pid=140272)[0m f1_weighted: 0.23548055907158816
[2m[36m(func pid=140272)[0m f1_per_class: [0.226, 0.245, 0.182, 0.306, 0.057, 0.091, 0.249, 0.222, 0.103, 0.056]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3291 | Steps: 4 | Val loss: 15.8623 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:10:01 (running for 00:26:38.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.819 |      0.174 |                   26 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.136 |      0.361 |                   25 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.794 |      0.081 |                   24 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.646 |      0.089 |                   23 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.20522388059701493
[2m[36m(func pid=141940)[0m top5: 0.4808768656716418
[2m[36m(func pid=141940)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=141940)[0m f1_macro: 0.0887977358740734
[2m[36m(func pid=141940)[0m f1_weighted: 0.11094026069277199
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.409, 0.0, 0.0, 0.0, 0.183, 0.012, 0.272, 0.0, 0.011]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.38152985074626866
[2m[36m(func pid=140840)[0m top5: 0.8997201492537313
[2m[36m(func pid=140840)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=140840)[0m f1_macro: 0.3557086007330851
[2m[36m(func pid=140840)[0m f1_weighted: 0.3780198093493534
[2m[36m(func pid=140840)[0m f1_per_class: [0.335, 0.564, 0.5, 0.535, 0.202, 0.285, 0.195, 0.291, 0.245, 0.406]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.7524 | Steps: 4 | Val loss: 2.1119 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=141385)[0m top1: 0.10634328358208955
[2m[36m(func pid=141385)[0m top5: 0.6259328358208955
[2m[36m(func pid=141385)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=141385)[0m f1_macro: 0.12509978748344988
[2m[36m(func pid=141385)[0m f1_weighted: 0.12026767201230863
[2m[36m(func pid=141385)[0m f1_per_class: [0.118, 0.213, 0.029, 0.062, 0.032, 0.097, 0.079, 0.468, 0.0, 0.153]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6162 | Steps: 4 | Val loss: 2.1817 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1652 | Steps: 4 | Val loss: 1.7130 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=140272)[0m top1: 0.22621268656716417
[2m[36m(func pid=140272)[0m top5: 0.7392723880597015
[2m[36m(func pid=140272)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=140272)[0m f1_macro: 0.18681868429609572
[2m[36m(func pid=140272)[0m f1_weighted: 0.25152438940755695
[2m[36m(func pid=140272)[0m f1_per_class: [0.269, 0.271, 0.21, 0.35, 0.053, 0.093, 0.244, 0.217, 0.1, 0.061]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.2709 | Steps: 4 | Val loss: 12.2033 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:10:06 (running for 00:26:43.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.752 |      0.187 |                   27 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.272 |      0.356 |                   26 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.329 |      0.125 |                   25 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.616 |      0.107 |                   24 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.23694029850746268
[2m[36m(func pid=141940)[0m top5: 0.4780783582089552
[2m[36m(func pid=141940)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=141940)[0m f1_macro: 0.10655221820058416
[2m[36m(func pid=141940)[0m f1_weighted: 0.1158955103705219
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.394, 0.0, 0.0, 0.0, 0.175, 0.0, 0.469, 0.027, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.37033582089552236
[2m[36m(func pid=140840)[0m top5: 0.8978544776119403
[2m[36m(func pid=140840)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=140840)[0m f1_macro: 0.33129525859876185
[2m[36m(func pid=140840)[0m f1_weighted: 0.3682999334880423
[2m[36m(func pid=140840)[0m f1_per_class: [0.342, 0.509, 0.471, 0.566, 0.229, 0.29, 0.175, 0.273, 0.224, 0.234]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6622 | Steps: 4 | Val loss: 2.0914 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=141385)[0m top1: 0.19076492537313433
[2m[36m(func pid=141385)[0m top5: 0.8568097014925373
[2m[36m(func pid=141385)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=141385)[0m f1_macro: 0.16105974594734646
[2m[36m(func pid=141385)[0m f1_weighted: 0.2175793690735307
[2m[36m(func pid=141385)[0m f1_per_class: [0.167, 0.097, 0.202, 0.196, 0.028, 0.047, 0.37, 0.427, 0.0, 0.076]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5999 | Steps: 4 | Val loss: 2.0832 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1452 | Steps: 4 | Val loss: 1.7092 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=140272)[0m top1: 0.2355410447761194
[2m[36m(func pid=140272)[0m top5: 0.7625932835820896
[2m[36m(func pid=140272)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=140272)[0m f1_macro: 0.19550361853940107
[2m[36m(func pid=140272)[0m f1_weighted: 0.26254579961116625
[2m[36m(func pid=140272)[0m f1_per_class: [0.249, 0.307, 0.25, 0.322, 0.055, 0.092, 0.285, 0.229, 0.106, 0.06]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4723 | Steps: 4 | Val loss: 6.2806 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:10:12 (running for 00:26:49.19)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.662 |      0.196 |                   28 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.165 |      0.331 |                   27 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.271 |      0.161 |                   26 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.6   |      0.105 |                   25 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.29757462686567165
[2m[36m(func pid=141940)[0m top5: 0.478544776119403
[2m[36m(func pid=141940)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=141940)[0m f1_macro: 0.1053033889465819
[2m[36m(func pid=141940)[0m f1_weighted: 0.2317607037835142
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.404, 0.0, 0.0, 0.0, 0.13, 0.492, 0.0, 0.028, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3763992537313433
[2m[36m(func pid=140840)[0m top5: 0.8964552238805971
[2m[36m(func pid=140840)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=140840)[0m f1_macro: 0.3313519350414437
[2m[36m(func pid=140840)[0m f1_weighted: 0.37160561135267006
[2m[36m(func pid=140840)[0m f1_per_class: [0.392, 0.476, 0.511, 0.593, 0.214, 0.287, 0.179, 0.29, 0.193, 0.179]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.2719216417910448
[2m[36m(func pid=141385)[0m top5: 0.8521455223880597
[2m[36m(func pid=141385)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=141385)[0m f1_macro: 0.20086981675207638
[2m[36m(func pid=141385)[0m f1_weighted: 0.2582294489184587
[2m[36m(func pid=141385)[0m f1_per_class: [0.133, 0.513, 0.0, 0.381, 0.4, 0.069, 0.099, 0.286, 0.115, 0.012]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.7083 | Steps: 4 | Val loss: 2.0740 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8112 | Steps: 4 | Val loss: 2.0278 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0886 | Steps: 4 | Val loss: 1.7376 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=140272)[0m top1: 0.24766791044776118
[2m[36m(func pid=140272)[0m top5: 0.7644589552238806
[2m[36m(func pid=140272)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=140272)[0m f1_macro: 0.19387190525891146
[2m[36m(func pid=140272)[0m f1_weighted: 0.2732961237385756
[2m[36m(func pid=140272)[0m f1_per_class: [0.236, 0.327, 0.155, 0.33, 0.068, 0.106, 0.298, 0.23, 0.107, 0.082]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5580 | Steps: 4 | Val loss: 5.5054 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:10:17 (running for 00:26:54.67)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.708 |      0.194 |                   29 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.145 |      0.331 |                   28 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.472 |      0.201 |                   27 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.811 |      0.128 |                   26 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.1912313432835821
[2m[36m(func pid=141940)[0m top5: 0.5242537313432836
[2m[36m(func pid=141940)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=141940)[0m f1_macro: 0.12766013157608574
[2m[36m(func pid=141940)[0m f1_weighted: 0.20589827910612774
[2m[36m(func pid=141940)[0m f1_per_class: [0.063, 0.0, 0.0, 0.0, 0.0, 0.19, 0.521, 0.449, 0.054, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3656716417910448
[2m[36m(func pid=140840)[0m top5: 0.8889925373134329
[2m[36m(func pid=140840)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=140840)[0m f1_macro: 0.3246114476370488
[2m[36m(func pid=140840)[0m f1_weighted: 0.3721333525261849
[2m[36m(func pid=140840)[0m f1_per_class: [0.43, 0.438, 0.511, 0.582, 0.145, 0.185, 0.241, 0.315, 0.259, 0.14]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.27005597014925375
[2m[36m(func pid=141385)[0m top5: 0.8344216417910447
[2m[36m(func pid=141385)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=141385)[0m f1_macro: 0.22426622021707945
[2m[36m(func pid=141385)[0m f1_weighted: 0.2622870185324094
[2m[36m(func pid=141385)[0m f1_per_class: [0.333, 0.294, 0.068, 0.429, 0.164, 0.164, 0.123, 0.366, 0.159, 0.141]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.6165 | Steps: 4 | Val loss: 2.0658 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4908 | Steps: 4 | Val loss: 2.1626 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2024 | Steps: 4 | Val loss: 1.7418 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 14:10:22 (running for 00:26:59.70)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.617 |      0.191 |                   30 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.089 |      0.325 |                   29 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.558 |      0.224 |                   28 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.811 |      0.128 |                   26 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140272)[0m top1: 0.24207089552238806
[2m[36m(func pid=140272)[0m top5: 0.7728544776119403
[2m[36m(func pid=140272)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=140272)[0m f1_macro: 0.1913403913371553
[2m[36m(func pid=140272)[0m f1_weighted: 0.27000988837722517
[2m[36m(func pid=140272)[0m f1_per_class: [0.234, 0.33, 0.136, 0.305, 0.078, 0.107, 0.309, 0.231, 0.104, 0.08]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.8818 | Steps: 4 | Val loss: 7.1002 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=141940)[0m top1: 0.1259328358208955
[2m[36m(func pid=141940)[0m top5: 0.5634328358208955
[2m[36m(func pid=141940)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=141940)[0m f1_macro: 0.1028500349400296
[2m[36m(func pid=141940)[0m f1_weighted: 0.13979069270336728
[2m[36m(func pid=141940)[0m f1_per_class: [0.06, 0.0, 0.0, 0.0, 0.0, 0.073, 0.331, 0.517, 0.048, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.375
[2m[36m(func pid=140840)[0m top5: 0.8768656716417911
[2m[36m(func pid=140840)[0m f1_micro: 0.375
[2m[36m(func pid=140840)[0m f1_macro: 0.3379782964612597
[2m[36m(func pid=140840)[0m f1_weighted: 0.3760377102605982
[2m[36m(func pid=140840)[0m f1_per_class: [0.497, 0.504, 0.522, 0.563, 0.113, 0.178, 0.223, 0.359, 0.27, 0.152]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.2789179104477612
[2m[36m(func pid=141385)[0m top5: 0.7723880597014925
[2m[36m(func pid=141385)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=141385)[0m f1_macro: 0.2294294035439234
[2m[36m(func pid=141385)[0m f1_weighted: 0.2826559316092067
[2m[36m(func pid=141385)[0m f1_per_class: [0.4, 0.097, 0.104, 0.422, 0.299, 0.138, 0.348, 0.227, 0.074, 0.186]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.6298 | Steps: 4 | Val loss: 2.0604 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5589 | Steps: 4 | Val loss: 2.2605 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2066 | Steps: 4 | Val loss: 1.6908 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:10:28 (running for 00:27:05.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.63  |      0.189 |                   31 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.202 |      0.338 |                   30 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.882 |      0.229 |                   29 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.491 |      0.103 |                   27 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140272)[0m top1: 0.24113805970149255
[2m[36m(func pid=140272)[0m top5: 0.777518656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=140272)[0m f1_macro: 0.18909145089317836
[2m[36m(func pid=140272)[0m f1_weighted: 0.2705704719650412
[2m[36m(func pid=140272)[0m f1_per_class: [0.216, 0.322, 0.118, 0.29, 0.088, 0.101, 0.334, 0.218, 0.11, 0.094]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7480 | Steps: 4 | Val loss: 3.8893 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=141940)[0m top1: 0.06203358208955224
[2m[36m(func pid=141940)[0m top5: 0.5513059701492538
[2m[36m(func pid=141940)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=141940)[0m f1_macro: 0.05300146433499479
[2m[36m(func pid=141940)[0m f1_weighted: 0.028092449517429324
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.0, 0.0, 0.0, 0.449, 0.06, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.38759328358208955
[2m[36m(func pid=140840)[0m top5: 0.8922574626865671
[2m[36m(func pid=140840)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=140840)[0m f1_macro: 0.339358399665894
[2m[36m(func pid=140840)[0m f1_weighted: 0.39557710362103626
[2m[36m(func pid=140840)[0m f1_per_class: [0.503, 0.497, 0.5, 0.566, 0.114, 0.162, 0.302, 0.342, 0.234, 0.173]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.353544776119403
[2m[36m(func pid=141385)[0m top5: 0.878731343283582
[2m[36m(func pid=141385)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=141385)[0m f1_macro: 0.2763193354174188
[2m[36m(func pid=141385)[0m f1_weighted: 0.3639546564811361
[2m[36m(func pid=141385)[0m f1_per_class: [0.452, 0.551, 0.15, 0.355, 0.174, 0.34, 0.329, 0.329, 0.084, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6753 | Steps: 4 | Val loss: 2.2040 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.5398 | Steps: 4 | Val loss: 2.0268 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.1246 | Steps: 4 | Val loss: 1.6080 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:10:33 (running for 00:27:10.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.63  |      0.189 |                   31 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.207 |      0.339 |                   31 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.748 |      0.276 |                   30 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.675 |      0.046 |                   29 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.6314 | Steps: 4 | Val loss: 6.8506 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=141940)[0m top1: 0.06436567164179105
[2m[36m(func pid=141940)[0m top5: 0.6072761194029851
[2m[36m(func pid=141940)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=141940)[0m f1_macro: 0.04629548588746449
[2m[36m(func pid=141940)[0m f1_weighted: 0.025746451402466664
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.006, 0.377, 0.059, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.25466417910447764
[2m[36m(func pid=140272)[0m top5: 0.8092350746268657
[2m[36m(func pid=140272)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=140272)[0m f1_macro: 0.19707324457240422
[2m[36m(func pid=140272)[0m f1_weighted: 0.2827977291876941
[2m[36m(func pid=140272)[0m f1_per_class: [0.259, 0.3, 0.135, 0.315, 0.084, 0.101, 0.363, 0.209, 0.11, 0.094]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.417910447761194
[2m[36m(func pid=140840)[0m top5: 0.9081156716417911
[2m[36m(func pid=140840)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=140840)[0m f1_macro: 0.36173583338382004
[2m[36m(func pid=140840)[0m f1_weighted: 0.4277667770804976
[2m[36m(func pid=140840)[0m f1_per_class: [0.534, 0.563, 0.444, 0.556, 0.125, 0.207, 0.352, 0.381, 0.253, 0.202]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.2462686567164179
[2m[36m(func pid=141385)[0m top5: 0.6427238805970149
[2m[36m(func pid=141385)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=141385)[0m f1_macro: 0.17294621326399967
[2m[36m(func pid=141385)[0m f1_weighted: 0.181850290351458
[2m[36m(func pid=141385)[0m f1_per_class: [0.056, 0.547, 0.056, 0.062, 0.079, 0.356, 0.019, 0.324, 0.026, 0.204]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6300 | Steps: 4 | Val loss: 2.1807 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.5529 | Steps: 4 | Val loss: 2.0447 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1171 | Steps: 4 | Val loss: 1.5042 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3949 | Steps: 4 | Val loss: 9.4687 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 14:10:39 (running for 00:27:16.20)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.54  |      0.197 |                   32 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.125 |      0.362 |                   32 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.631 |      0.173 |                   31 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.63  |      0.102 |                   30 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.12173507462686567
[2m[36m(func pid=141940)[0m top5: 0.5680970149253731
[2m[36m(func pid=141940)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=141940)[0m f1_macro: 0.10230359199585018
[2m[36m(func pid=141940)[0m f1_weighted: 0.15074548001537552
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.016, 0.0, 0.0, 0.074, 0.367, 0.565, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.2462686567164179
[2m[36m(func pid=140272)[0m top5: 0.7975746268656716
[2m[36m(func pid=140272)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=140272)[0m f1_macro: 0.19585683552041577
[2m[36m(func pid=140272)[0m f1_weighted: 0.2746973439034503
[2m[36m(func pid=140272)[0m f1_per_class: [0.234, 0.291, 0.161, 0.284, 0.085, 0.098, 0.368, 0.235, 0.111, 0.092]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.457089552238806
[2m[36m(func pid=140840)[0m top5: 0.9263059701492538
[2m[36m(func pid=140840)[0m f1_micro: 0.457089552238806
[2m[36m(func pid=140840)[0m f1_macro: 0.39642434496849716
[2m[36m(func pid=140840)[0m f1_weighted: 0.465857572375173
[2m[36m(func pid=140840)[0m f1_per_class: [0.562, 0.59, 0.471, 0.546, 0.164, 0.254, 0.447, 0.376, 0.273, 0.281]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.22621268656716417
[2m[36m(func pid=141385)[0m top5: 0.5443097014925373
[2m[36m(func pid=141385)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=141385)[0m f1_macro: 0.14178589830795577
[2m[36m(func pid=141385)[0m f1_weighted: 0.17151780468926656
[2m[36m(func pid=141385)[0m f1_per_class: [0.2, 0.44, 0.0, 0.197, 0.184, 0.277, 0.003, 0.031, 0.0, 0.085]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7403 | Steps: 4 | Val loss: 2.1365 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5711 | Steps: 4 | Val loss: 2.0361 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1823 | Steps: 4 | Val loss: 1.4775 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6630 | Steps: 4 | Val loss: 12.5574 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:10:44 (running for 00:27:21.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.553 |      0.196 |                   33 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.117 |      0.396 |                   33 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.395 |      0.142 |                   32 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.74  |      0.1   |                   31 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.13526119402985073
[2m[36m(func pid=141940)[0m top5: 0.710820895522388
[2m[36m(func pid=141940)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=141940)[0m f1_macro: 0.1003786202964222
[2m[36m(func pid=141940)[0m f1_weighted: 0.17052429961629578
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.017, 0.0, 0.0, 0.185, 0.428, 0.373, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.23740671641791045
[2m[36m(func pid=140272)[0m top5: 0.8050373134328358
[2m[36m(func pid=140272)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=140272)[0m f1_macro: 0.19404742558721988
[2m[36m(func pid=140272)[0m f1_weighted: 0.2658129737111923
[2m[36m(func pid=140272)[0m f1_per_class: [0.249, 0.288, 0.137, 0.267, 0.095, 0.131, 0.343, 0.227, 0.11, 0.094]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.4650186567164179
[2m[36m(func pid=140840)[0m top5: 0.9319029850746269
[2m[36m(func pid=140840)[0m f1_micro: 0.46501865671641784
[2m[36m(func pid=140840)[0m f1_macro: 0.40465275217064056
[2m[36m(func pid=140840)[0m f1_weighted: 0.47480626556359085
[2m[36m(func pid=140840)[0m f1_per_class: [0.607, 0.598, 0.471, 0.562, 0.151, 0.234, 0.467, 0.348, 0.251, 0.357]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.18003731343283583
[2m[36m(func pid=141385)[0m top5: 0.6814365671641791
[2m[36m(func pid=141385)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=141385)[0m f1_macro: 0.12375946936938864
[2m[36m(func pid=141385)[0m f1_weighted: 0.2063412726364112
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.337, 0.353, 0.287, 0.0, 0.0, 0.22, 0.0, 0.0, 0.04]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7194 | Steps: 4 | Val loss: 2.0993 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.4686 | Steps: 4 | Val loss: 2.0427 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2363 | Steps: 4 | Val loss: 1.4870 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.4706 | Steps: 4 | Val loss: 24.4774 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:10:50 (running for 00:27:27.17)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.571 |      0.194 |                   34 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.182 |      0.405 |                   34 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.663 |      0.124 |                   33 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.719 |      0.107 |                   32 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.14598880597014927
[2m[36m(func pid=141940)[0m top5: 0.7243470149253731
[2m[36m(func pid=141940)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=141940)[0m f1_macro: 0.10667764900849312
[2m[36m(func pid=141940)[0m f1_weighted: 0.17727969613799968
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.017, 0.0, 0.0, 0.233, 0.43, 0.386, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.2355410447761194
[2m[36m(func pid=140272)[0m top5: 0.8017723880597015
[2m[36m(func pid=140272)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=140272)[0m f1_macro: 0.1991875179534216
[2m[36m(func pid=140272)[0m f1_weighted: 0.2658601277833681
[2m[36m(func pid=140272)[0m f1_per_class: [0.272, 0.261, 0.179, 0.272, 0.101, 0.131, 0.349, 0.244, 0.113, 0.07]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.447294776119403
[2m[36m(func pid=140840)[0m top5: 0.9300373134328358
[2m[36m(func pid=140840)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=140840)[0m f1_macro: 0.3960132956788686
[2m[36m(func pid=140840)[0m f1_weighted: 0.4549160770796892
[2m[36m(func pid=140840)[0m f1_per_class: [0.595, 0.572, 0.48, 0.576, 0.157, 0.231, 0.399, 0.372, 0.271, 0.308]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.07742537313432836
[2m[36m(func pid=141385)[0m top5: 0.7877798507462687
[2m[36m(func pid=141385)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=141385)[0m f1_macro: 0.0401838597996721
[2m[36m(func pid=141385)[0m f1_weighted: 0.07663629454265279
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.132, 0.023, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.4980 | Steps: 4 | Val loss: 2.0543 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6156 | Steps: 4 | Val loss: 2.0799 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2655 | Steps: 4 | Val loss: 1.5596 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 7.6207 | Steps: 4 | Val loss: 48.6425 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:10:55 (running for 00:27:32.47)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.469 |      0.199 |                   35 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.236 |      0.396 |                   35 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  6.471 |      0.04  |                   34 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.616 |      0.118 |                   33 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.14832089552238806
[2m[36m(func pid=141940)[0m top5: 0.7476679104477612
[2m[36m(func pid=141940)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=141940)[0m f1_macro: 0.11777634719688948
[2m[36m(func pid=141940)[0m f1_weighted: 0.1800986067347358
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.019, 0.0, 0.0, 0.215, 0.424, 0.473, 0.046, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.22901119402985073
[2m[36m(func pid=140272)[0m top5: 0.7919776119402985
[2m[36m(func pid=140272)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=140272)[0m f1_macro: 0.19826128916592917
[2m[36m(func pid=140272)[0m f1_weighted: 0.25958114582356073
[2m[36m(func pid=140272)[0m f1_per_class: [0.252, 0.252, 0.194, 0.279, 0.103, 0.138, 0.323, 0.259, 0.103, 0.08]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.43236940298507465
[2m[36m(func pid=140840)[0m top5: 0.917910447761194
[2m[36m(func pid=140840)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=140840)[0m f1_macro: 0.3863555180635433
[2m[36m(func pid=140840)[0m f1_weighted: 0.4419650923288612
[2m[36m(func pid=140840)[0m f1_per_class: [0.595, 0.573, 0.48, 0.575, 0.113, 0.195, 0.369, 0.374, 0.295, 0.295]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.006063432835820896
[2m[36m(func pid=141385)[0m top5: 0.4748134328358209
[2m[36m(func pid=141385)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=141385)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=141385)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7317 | Steps: 4 | Val loss: 2.1314 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.4028 | Steps: 4 | Val loss: 2.0462 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.1053 | Steps: 4 | Val loss: 1.6458 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 7.3143 | Steps: 4 | Val loss: 124.8907 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 14:11:01 (running for 00:27:37.96)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.498 |      0.198 |                   36 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.265 |      0.386 |                   36 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  7.621 |      0.001 |                   35 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.732 |      0.074 |                   34 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.08022388059701492
[2m[36m(func pid=141940)[0m top5: 0.7318097014925373
[2m[36m(func pid=141940)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=141940)[0m f1_macro: 0.07412200933600323
[2m[36m(func pid=141940)[0m f1_weighted: 0.05845340021553456
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.022, 0.181, 0.031, 0.471, 0.037, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.22154850746268656
[2m[36m(func pid=140272)[0m top5: 0.7966417910447762
[2m[36m(func pid=140272)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=140272)[0m f1_macro: 0.19554024839687284
[2m[36m(func pid=140272)[0m f1_weighted: 0.25086298315998534
[2m[36m(func pid=140272)[0m f1_per_class: [0.251, 0.219, 0.237, 0.286, 0.083, 0.134, 0.309, 0.242, 0.126, 0.069]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3908582089552239
[2m[36m(func pid=140840)[0m top5: 0.9211753731343284
[2m[36m(func pid=140840)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=140840)[0m f1_macro: 0.3755006568278124
[2m[36m(func pid=140840)[0m f1_weighted: 0.4147194967174466
[2m[36m(func pid=140840)[0m f1_per_class: [0.602, 0.492, 0.49, 0.563, 0.08, 0.238, 0.324, 0.35, 0.279, 0.338]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.008395522388059701
[2m[36m(func pid=141385)[0m top5: 0.4281716417910448
[2m[36m(func pid=141385)[0m f1_micro: 0.008395522388059701
[2m[36m(func pid=141385)[0m f1_macro: 0.006041028556651754
[2m[36m(func pid=141385)[0m f1_weighted: 0.00354556025365643
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.011, 0.013, 0.0, 0.0, 0.0, 0.0, 0.016, 0.021, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5492 | Steps: 4 | Val loss: 2.1272 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.3500 | Steps: 4 | Val loss: 2.0264 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0965 | Steps: 4 | Val loss: 1.7153 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.1893 | Steps: 4 | Val loss: 84.1102 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:11:06 (running for 00:27:43.28)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.403 |      0.196 |                   37 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.105 |      0.376 |                   37 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  7.314 |      0.006 |                   36 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.549 |      0.126 |                   35 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.3031716417910448
[2m[36m(func pid=141940)[0m top5: 0.769589552238806
[2m[36m(func pid=141940)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=141940)[0m f1_macro: 0.12565606987873096
[2m[36m(func pid=141940)[0m f1_weighted: 0.2023938547826089
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.536, 0.0, 0.226, 0.0, 0.435, 0.059, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.23414179104477612
[2m[36m(func pid=140272)[0m top5: 0.808768656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=140272)[0m f1_macro: 0.20326622792724064
[2m[36m(func pid=140272)[0m f1_weighted: 0.26312736043514506
[2m[36m(func pid=140272)[0m f1_per_class: [0.269, 0.207, 0.265, 0.303, 0.103, 0.121, 0.347, 0.228, 0.116, 0.074]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3628731343283582
[2m[36m(func pid=140840)[0m top5: 0.9104477611940298
[2m[36m(func pid=140840)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=140840)[0m f1_macro: 0.36747615261598876
[2m[36m(func pid=140840)[0m f1_weighted: 0.39077933872873183
[2m[36m(func pid=140840)[0m f1_per_class: [0.636, 0.485, 0.462, 0.555, 0.06, 0.215, 0.262, 0.365, 0.243, 0.394]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.0228544776119403
[2m[36m(func pid=141385)[0m top5: 0.5722947761194029
[2m[36m(func pid=141385)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=141385)[0m f1_macro: 0.017677267597593367
[2m[36m(func pid=141385)[0m f1_weighted: 0.016245341768131166
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.087, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.056]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5518 | Steps: 4 | Val loss: 2.1080 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2685 | Steps: 4 | Val loss: 2.0297 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0574 | Steps: 4 | Val loss: 1.7278 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.9510 | Steps: 4 | Val loss: 108.8307 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:11:11 (running for 00:27:48.72)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.35  |      0.203 |                   38 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.096 |      0.367 |                   38 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.189 |      0.018 |                   37 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.552 |      0.095 |                   36 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.11287313432835822
[2m[36m(func pid=141940)[0m top5: 0.761660447761194
[2m[36m(func pid=141940)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=141940)[0m f1_macro: 0.09528268200528643
[2m[36m(func pid=141940)[0m f1_weighted: 0.0972009069897892
[2m[36m(func pid=141940)[0m f1_per_class: [0.063, 0.0, 0.0, 0.117, 0.0, 0.194, 0.037, 0.489, 0.052, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.22994402985074627
[2m[36m(func pid=140272)[0m top5: 0.8055037313432836
[2m[36m(func pid=140272)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=140272)[0m f1_macro: 0.20953770728715146
[2m[36m(func pid=140272)[0m f1_weighted: 0.25683880122076436
[2m[36m(func pid=140272)[0m f1_per_class: [0.241, 0.243, 0.349, 0.321, 0.097, 0.117, 0.289, 0.226, 0.127, 0.086]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.353544776119403
[2m[36m(func pid=140840)[0m top5: 0.914179104477612
[2m[36m(func pid=140840)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=140840)[0m f1_macro: 0.35213820003180546
[2m[36m(func pid=140840)[0m f1_weighted: 0.37989455575326603
[2m[36m(func pid=140840)[0m f1_per_class: [0.627, 0.427, 0.522, 0.563, 0.069, 0.247, 0.253, 0.332, 0.191, 0.289]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.06623134328358209
[2m[36m(func pid=141385)[0m top5: 0.6184701492537313
[2m[36m(func pid=141385)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=141385)[0m f1_macro: 0.04188562742561662
[2m[36m(func pid=141385)[0m f1_weighted: 0.06795645595103658
[2m[36m(func pid=141385)[0m f1_per_class: [0.077, 0.182, 0.0, 0.0, 0.0, 0.008, 0.113, 0.0, 0.0, 0.039]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6214 | Steps: 4 | Val loss: 2.0243 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.4193 | Steps: 4 | Val loss: 2.0466 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2934 | Steps: 4 | Val loss: 1.7491 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.8957 | Steps: 4 | Val loss: 7.3308 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:11:17 (running for 00:27:54.26)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.268 |      0.21  |                   39 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.057 |      0.352 |                   39 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.951 |      0.042 |                   38 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.621 |      0.124 |                   37 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.17537313432835822
[2m[36m(func pid=141940)[0m top5: 0.7583955223880597
[2m[36m(func pid=141940)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=141940)[0m f1_macro: 0.12384411435374558
[2m[36m(func pid=141940)[0m f1_weighted: 0.19903569730533419
[2m[36m(func pid=141940)[0m f1_per_class: [0.064, 0.0, 0.0, 0.032, 0.0, 0.146, 0.482, 0.464, 0.05, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.22574626865671643
[2m[36m(func pid=140272)[0m top5: 0.8003731343283582
[2m[36m(func pid=140272)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=140272)[0m f1_macro: 0.20445907673264835
[2m[36m(func pid=140272)[0m f1_weighted: 0.25146028802210224
[2m[36m(func pid=140272)[0m f1_per_class: [0.247, 0.261, 0.289, 0.296, 0.112, 0.117, 0.285, 0.222, 0.133, 0.083]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.34888059701492535
[2m[36m(func pid=140840)[0m top5: 0.9207089552238806
[2m[36m(func pid=140840)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=140840)[0m f1_macro: 0.3582491056519242
[2m[36m(func pid=140840)[0m f1_weighted: 0.3779835666096417
[2m[36m(func pid=140840)[0m f1_per_class: [0.653, 0.449, 0.545, 0.548, 0.071, 0.245, 0.244, 0.349, 0.189, 0.289]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.21875
[2m[36m(func pid=141385)[0m top5: 0.746268656716418
[2m[36m(func pid=141385)[0m f1_micro: 0.21875
[2m[36m(func pid=141385)[0m f1_macro: 0.14644099562048965
[2m[36m(func pid=141385)[0m f1_weighted: 0.16558880676481774
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.336, 0.329, 0.016, 0.0, 0.322, 0.163, 0.269, 0.0, 0.03]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5273 | Steps: 4 | Val loss: 2.1034 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.3666 | Steps: 4 | Val loss: 2.0163 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1910 | Steps: 4 | Val loss: 1.7244 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1216 | Steps: 4 | Val loss: 6.4473 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:11:22 (running for 00:27:59.83)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.419 |      0.204 |                   40 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.293 |      0.358 |                   40 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.896 |      0.146 |                   39 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.527 |      0.107 |                   38 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.1515858208955224
[2m[36m(func pid=141940)[0m top5: 0.7159514925373134
[2m[36m(func pid=141940)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=141940)[0m f1_macro: 0.10711619599909997
[2m[36m(func pid=141940)[0m f1_weighted: 0.17890974917204172
[2m[36m(func pid=141940)[0m f1_per_class: [0.059, 0.0, 0.0, 0.037, 0.0, 0.11, 0.44, 0.389, 0.036, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.2392723880597015
[2m[36m(func pid=140272)[0m top5: 0.8138992537313433
[2m[36m(func pid=140272)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=140272)[0m f1_macro: 0.20536290439107313
[2m[36m(func pid=140272)[0m f1_weighted: 0.26390682106531854
[2m[36m(func pid=140272)[0m f1_per_class: [0.264, 0.237, 0.265, 0.313, 0.113, 0.099, 0.332, 0.223, 0.106, 0.101]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3656716417910448
[2m[36m(func pid=140840)[0m top5: 0.9127798507462687
[2m[36m(func pid=140840)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=140840)[0m f1_macro: 0.34475655816740536
[2m[36m(func pid=140840)[0m f1_weighted: 0.3966025682289983
[2m[36m(func pid=140840)[0m f1_per_class: [0.634, 0.447, 0.338, 0.549, 0.084, 0.249, 0.307, 0.369, 0.192, 0.279]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.18330223880597016
[2m[36m(func pid=141385)[0m top5: 0.7658582089552238
[2m[36m(func pid=141385)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=141385)[0m f1_macro: 0.1811726134831411
[2m[36m(func pid=141385)[0m f1_weighted: 0.14442936958847594
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.37, 0.696, 0.035, 0.161, 0.044, 0.15, 0.217, 0.065, 0.073]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4802 | Steps: 4 | Val loss: 2.1607 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.3667 | Steps: 4 | Val loss: 2.0090 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2713 | Steps: 4 | Val loss: 1.5863 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.9625 | Steps: 4 | Val loss: 57.8815 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:11:28 (running for 00:28:05.25)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.367 |      0.205 |                   41 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.191 |      0.345 |                   41 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.122 |      0.181 |                   40 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.48  |      0.096 |                   39 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.1226679104477612
[2m[36m(func pid=141940)[0m top5: 0.6963619402985075
[2m[36m(func pid=141940)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=141940)[0m f1_macro: 0.09644173507182505
[2m[36m(func pid=141940)[0m f1_weighted: 0.14667860615941533
[2m[36m(func pid=141940)[0m f1_per_class: [0.057, 0.0, 0.0, 0.022, 0.0, 0.093, 0.349, 0.406, 0.037, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.25326492537313433
[2m[36m(func pid=140272)[0m top5: 0.8138992537313433
[2m[36m(func pid=140272)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=140272)[0m f1_macro: 0.21621606916446195
[2m[36m(func pid=140272)[0m f1_weighted: 0.27730484080978784
[2m[36m(func pid=140272)[0m f1_per_class: [0.279, 0.313, 0.22, 0.312, 0.125, 0.117, 0.32, 0.245, 0.124, 0.105]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.404384328358209
[2m[36m(func pid=140840)[0m top5: 0.9291044776119403
[2m[36m(func pid=140840)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=140840)[0m f1_macro: 0.3616349590999758
[2m[36m(func pid=140840)[0m f1_weighted: 0.43627850312124405
[2m[36m(func pid=140840)[0m f1_per_class: [0.624, 0.49, 0.338, 0.516, 0.113, 0.264, 0.444, 0.341, 0.21, 0.279]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.14598880597014927
[2m[36m(func pid=141385)[0m top5: 0.6324626865671642
[2m[36m(func pid=141385)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=141385)[0m f1_macro: 0.11225423793690129
[2m[36m(func pid=141385)[0m f1_weighted: 0.15359523756110577
[2m[36m(func pid=141385)[0m f1_per_class: [0.085, 0.375, 0.0, 0.013, 0.286, 0.0, 0.266, 0.0, 0.052, 0.046]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6410 | Steps: 4 | Val loss: 2.0211 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.2450 | Steps: 4 | Val loss: 1.9873 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0795 | Steps: 4 | Val loss: 1.5894 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.8880 | Steps: 4 | Val loss: 8.2650 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:11:33 (running for 00:28:10.67)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.367 |      0.216 |                   42 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.271 |      0.362 |                   42 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.963 |      0.112 |                   41 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.641 |      0.125 |                   40 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.18796641791044777
[2m[36m(func pid=141940)[0m top5: 0.753731343283582
[2m[36m(func pid=141940)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=141940)[0m f1_macro: 0.12500095492244945
[2m[36m(func pid=141940)[0m f1_weighted: 0.21008834516803707
[2m[36m(func pid=141940)[0m f1_per_class: [0.064, 0.0, 0.0, 0.062, 0.0, 0.097, 0.509, 0.464, 0.054, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.26399253731343286
[2m[36m(func pid=140272)[0m top5: 0.8218283582089553
[2m[36m(func pid=140272)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=140272)[0m f1_macro: 0.22313305862357508
[2m[36m(func pid=140272)[0m f1_weighted: 0.2903892161847954
[2m[36m(func pid=140272)[0m f1_per_class: [0.322, 0.358, 0.231, 0.288, 0.102, 0.119, 0.359, 0.244, 0.118, 0.091]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.4207089552238806
[2m[36m(func pid=140840)[0m top5: 0.9174440298507462
[2m[36m(func pid=140840)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=140840)[0m f1_macro: 0.35747473087768294
[2m[36m(func pid=140840)[0m f1_weighted: 0.4506271083230413
[2m[36m(func pid=140840)[0m f1_per_class: [0.627, 0.521, 0.22, 0.491, 0.144, 0.262, 0.502, 0.326, 0.214, 0.267]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.27052238805970147
[2m[36m(func pid=141385)[0m top5: 0.7117537313432836
[2m[36m(func pid=141385)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=141385)[0m f1_macro: 0.15683161433975465
[2m[36m(func pid=141385)[0m f1_weighted: 0.25924700568847747
[2m[36m(func pid=141385)[0m f1_per_class: [0.069, 0.482, 0.0, 0.023, 0.231, 0.044, 0.528, 0.0, 0.096, 0.095]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3429 | Steps: 4 | Val loss: 1.9624 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.2134 | Steps: 4 | Val loss: 1.9533 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0972 | Steps: 4 | Val loss: 1.6224 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 4.1274 | Steps: 4 | Val loss: 4.2656 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:11:39 (running for 00:28:16.16)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.245 |      0.223 |                   43 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.08  |      0.357 |                   43 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.888 |      0.157 |                   42 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.343 |      0.147 |                   41 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.20009328358208955
[2m[36m(func pid=141940)[0m top5: 0.7840485074626866
[2m[36m(func pid=141940)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=141940)[0m f1_macro: 0.14696007955372442
[2m[36m(func pid=141940)[0m f1_weighted: 0.2244071634020669
[2m[36m(func pid=141940)[0m f1_per_class: [0.066, 0.0, 0.0, 0.189, 0.0, 0.265, 0.363, 0.502, 0.085, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.291044776119403
[2m[36m(func pid=140272)[0m top5: 0.8311567164179104
[2m[36m(func pid=140272)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=140272)[0m f1_macro: 0.23598393866934106
[2m[36m(func pid=140272)[0m f1_weighted: 0.3163797006295682
[2m[36m(func pid=140272)[0m f1_per_class: [0.292, 0.354, 0.261, 0.316, 0.122, 0.119, 0.422, 0.249, 0.12, 0.105]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.41324626865671643
[2m[36m(func pid=140840)[0m top5: 0.9071828358208955
[2m[36m(func pid=140840)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=140840)[0m f1_macro: 0.3450376698061719
[2m[36m(func pid=140840)[0m f1_weighted: 0.43868654616617286
[2m[36m(func pid=140840)[0m f1_per_class: [0.582, 0.515, 0.182, 0.45, 0.167, 0.245, 0.513, 0.318, 0.255, 0.225]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.24766791044776118
[2m[36m(func pid=141385)[0m top5: 0.8418843283582089
[2m[36m(func pid=141385)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=141385)[0m f1_macro: 0.16122497224114415
[2m[36m(func pid=141385)[0m f1_weighted: 0.22150030337564822
[2m[36m(func pid=141385)[0m f1_per_class: [0.16, 0.011, 0.0, 0.411, 0.0, 0.022, 0.249, 0.298, 0.108, 0.353]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3542 | Steps: 4 | Val loss: 2.0106 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0980 | Steps: 4 | Val loss: 1.9085 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0676 | Steps: 4 | Val loss: 1.6527 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3437 | Steps: 4 | Val loss: 4.0841 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:11:44 (running for 00:28:21.65)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.213 |      0.236 |                   44 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.097 |      0.345 |                   44 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  4.127 |      0.161 |                   43 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.354 |      0.137 |                   42 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.28451492537313433
[2m[36m(func pid=141940)[0m top5: 0.7495335820895522
[2m[36m(func pid=141940)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=141940)[0m f1_macro: 0.13653369710903532
[2m[36m(func pid=141940)[0m f1_weighted: 0.21309828893877866
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.506, 0.0, 0.299, 0.033, 0.429, 0.099, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3162313432835821
[2m[36m(func pid=140272)[0m top5: 0.8572761194029851
[2m[36m(func pid=140272)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=140272)[0m f1_macro: 0.25486717965703287
[2m[36m(func pid=140272)[0m f1_weighted: 0.34157305960405465
[2m[36m(func pid=140272)[0m f1_per_class: [0.319, 0.377, 0.308, 0.352, 0.136, 0.131, 0.452, 0.242, 0.123, 0.109]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.40718283582089554
[2m[36m(func pid=140840)[0m top5: 0.8987873134328358
[2m[36m(func pid=140840)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=140840)[0m f1_macro: 0.33821587266044806
[2m[36m(func pid=140840)[0m f1_weighted: 0.4376619902102596
[2m[36m(func pid=140840)[0m f1_per_class: [0.541, 0.475, 0.231, 0.467, 0.16, 0.236, 0.525, 0.33, 0.214, 0.203]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.31203358208955223
[2m[36m(func pid=141385)[0m top5: 0.8535447761194029
[2m[36m(func pid=141385)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=141385)[0m f1_macro: 0.15734167692874007
[2m[36m(func pid=141385)[0m f1_weighted: 0.1938470743783127
[2m[36m(func pid=141385)[0m f1_per_class: [0.261, 0.082, 0.0, 0.539, 0.269, 0.016, 0.006, 0.294, 0.0, 0.105]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4047 | Steps: 4 | Val loss: 2.0122 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.3890 | Steps: 4 | Val loss: 1.8885 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1300 | Steps: 4 | Val loss: 1.6542 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.1806 | Steps: 4 | Val loss: 4.2678 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 14:11:50 (running for 00:28:27.07)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.098 |      0.255 |                   45 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.068 |      0.338 |                   45 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.344 |      0.157 |                   44 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.405 |      0.125 |                   43 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.19263059701492538
[2m[36m(func pid=141940)[0m top5: 0.7793843283582089
[2m[36m(func pid=141940)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=141940)[0m f1_macro: 0.12451328383640466
[2m[36m(func pid=141940)[0m f1_weighted: 0.16495889222361426
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.05, 0.337, 0.0, 0.292, 0.027, 0.46, 0.079, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3302238805970149
[2m[36m(func pid=140272)[0m top5: 0.8684701492537313
[2m[36m(func pid=140272)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=140272)[0m f1_macro: 0.2743088810813571
[2m[36m(func pid=140272)[0m f1_weighted: 0.3521397412735057
[2m[36m(func pid=140272)[0m f1_per_class: [0.32, 0.366, 0.423, 0.365, 0.127, 0.177, 0.461, 0.225, 0.157, 0.121]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.4039179104477612
[2m[36m(func pid=140840)[0m top5: 0.9006529850746269
[2m[36m(func pid=140840)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=140840)[0m f1_macro: 0.33964268350992616
[2m[36m(func pid=140840)[0m f1_weighted: 0.431004391630228
[2m[36m(func pid=140840)[0m f1_per_class: [0.532, 0.494, 0.222, 0.468, 0.16, 0.233, 0.489, 0.337, 0.224, 0.238]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.4375
[2m[36m(func pid=141385)[0m top5: 0.8470149253731343
[2m[36m(func pid=141385)[0m f1_micro: 0.4375
[2m[36m(func pid=141385)[0m f1_macro: 0.2201720836065944
[2m[36m(func pid=141385)[0m f1_weighted: 0.3854211691354899
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.22, 0.136, 0.52, 0.147, 0.015, 0.555, 0.545, 0.064, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5085 | Steps: 4 | Val loss: 2.0219 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0695 | Steps: 4 | Val loss: 1.6585 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1667 | Steps: 4 | Val loss: 1.8817 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.5953 | Steps: 4 | Val loss: 8.8520 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:11:55 (running for 00:28:32.44)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.389 |      0.274 |                   46 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.13  |      0.34  |                   46 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.181 |      0.22  |                   45 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.508 |      0.102 |                   44 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.11473880597014925
[2m[36m(func pid=141940)[0m top5: 0.7723880597014925
[2m[36m(func pid=141940)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=141940)[0m f1_macro: 0.10243008197787058
[2m[36m(func pid=141940)[0m f1_weighted: 0.10633263394500962
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.023, 0.126, 0.0, 0.248, 0.034, 0.531, 0.063, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.39505597014925375
[2m[36m(func pid=140840)[0m top5: 0.9043843283582089
[2m[36m(func pid=140840)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=140840)[0m f1_macro: 0.35381467899817026
[2m[36m(func pid=140840)[0m f1_weighted: 0.42056615917720336
[2m[36m(func pid=140840)[0m f1_per_class: [0.611, 0.51, 0.261, 0.434, 0.181, 0.251, 0.46, 0.352, 0.212, 0.267]
[2m[36m(func pid=140272)[0m top1: 0.3400186567164179
[2m[36m(func pid=140272)[0m top5: 0.867070895522388
[2m[36m(func pid=140272)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=140272)[0m f1_macro: 0.27188038931649094
[2m[36m(func pid=140272)[0m f1_weighted: 0.36422432686861494
[2m[36m(func pid=140272)[0m f1_per_class: [0.316, 0.369, 0.364, 0.397, 0.114, 0.172, 0.472, 0.241, 0.142, 0.132]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.13386194029850745
[2m[36m(func pid=141385)[0m top5: 0.784981343283582
[2m[36m(func pid=141385)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=141385)[0m f1_macro: 0.10201351554014344
[2m[36m(func pid=141385)[0m f1_weighted: 0.10191885182687964
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.408, 0.168, 0.025, 0.148, 0.029, 0.027, 0.172, 0.028, 0.014]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7495 | Steps: 4 | Val loss: 1.9748 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1182 | Steps: 4 | Val loss: 1.8710 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2965 | Steps: 4 | Val loss: 1.7434 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.2609 | Steps: 4 | Val loss: 8.0454 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:12:00 (running for 00:28:37.75)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.167 |      0.272 |                   47 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.069 |      0.354 |                   47 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.595 |      0.102 |                   46 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.749 |      0.123 |                   45 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.18423507462686567
[2m[36m(func pid=141940)[0m top5: 0.7495335820895522
[2m[36m(func pid=141940)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=141940)[0m f1_macro: 0.1226879848012861
[2m[36m(func pid=141940)[0m f1_weighted: 0.21472504779152538
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.021, 0.098, 0.0, 0.054, 0.506, 0.492, 0.056, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.34841417910447764
[2m[36m(func pid=140272)[0m top5: 0.8717350746268657
[2m[36m(func pid=140272)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=140272)[0m f1_macro: 0.2766762142242238
[2m[36m(func pid=140272)[0m f1_weighted: 0.37038498756276134
[2m[36m(func pid=140272)[0m f1_per_class: [0.298, 0.397, 0.355, 0.379, 0.132, 0.171, 0.492, 0.248, 0.151, 0.144]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.363339552238806
[2m[36m(func pid=140840)[0m top5: 0.8922574626865671
[2m[36m(func pid=140840)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=140840)[0m f1_macro: 0.3444464399852949
[2m[36m(func pid=140840)[0m f1_weighted: 0.3855987841706885
[2m[36m(func pid=140840)[0m f1_per_class: [0.606, 0.5, 0.343, 0.395, 0.168, 0.25, 0.388, 0.347, 0.197, 0.252]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.20662313432835822
[2m[36m(func pid=141385)[0m top5: 0.8134328358208955
[2m[36m(func pid=141385)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=141385)[0m f1_macro: 0.2013160020046815
[2m[36m(func pid=141385)[0m f1_weighted: 0.17366691001456527
[2m[36m(func pid=141385)[0m f1_per_class: [0.42, 0.458, 0.367, 0.227, 0.213, 0.044, 0.0, 0.207, 0.053, 0.024]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4234 | Steps: 4 | Val loss: 2.0827 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1595 | Steps: 4 | Val loss: 1.8730 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.1679 | Steps: 4 | Val loss: 1.7803 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.9553 | Steps: 4 | Val loss: 13.7052 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=141940)[0m top1: 0.1417910447761194
[2m[36m(func pid=141940)[0m top5: 0.7280783582089553
[2m[36m(func pid=141940)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=141940)[0m f1_macro: 0.11046688304138205
[2m[36m(func pid=141940)[0m f1_weighted: 0.1775754620135794
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.0, 0.131, 0.447, 0.507, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 14:12:06 (running for 00:28:43.12)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.118 |      0.277 |                   48 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.297 |      0.344 |                   48 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.261 |      0.201 |                   47 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.423 |      0.11  |                   46 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m top1: 0.34654850746268656
[2m[36m(func pid=140272)[0m top5: 0.8680037313432836
[2m[36m(func pid=140272)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=140272)[0m f1_macro: 0.27084686304144845
[2m[36m(func pid=140272)[0m f1_weighted: 0.36917250752180225
[2m[36m(func pid=140272)[0m f1_per_class: [0.311, 0.415, 0.304, 0.38, 0.126, 0.146, 0.486, 0.25, 0.145, 0.145]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3614738805970149
[2m[36m(func pid=140840)[0m top5: 0.878731343283582
[2m[36m(func pid=140840)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=140840)[0m f1_macro: 0.3321049298494968
[2m[36m(func pid=140840)[0m f1_weighted: 0.3788886354891054
[2m[36m(func pid=140840)[0m f1_per_class: [0.417, 0.501, 0.436, 0.378, 0.179, 0.231, 0.4, 0.337, 0.218, 0.224]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.09934701492537314
[2m[36m(func pid=141385)[0m top5: 0.6963619402985075
[2m[36m(func pid=141385)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=141385)[0m f1_macro: 0.06318602214878175
[2m[36m(func pid=141385)[0m f1_weighted: 0.05295531458473408
[2m[36m(func pid=141385)[0m f1_per_class: [0.14, 0.011, 0.0, 0.087, 0.043, 0.016, 0.015, 0.267, 0.053, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.5824 | Steps: 4 | Val loss: 2.1897 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0177 | Steps: 4 | Val loss: 1.8705 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0475 | Steps: 4 | Val loss: 1.8089 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.3354 | Steps: 4 | Val loss: 5.6301 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=141940)[0m top1: 0.22901119402985073
[2m[36m(func pid=141940)[0m top5: 0.7182835820895522
[2m[36m(func pid=141940)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=141940)[0m f1_macro: 0.12016365949083951
[2m[36m(func pid=141940)[0m f1_weighted: 0.14687266663702112
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.411, 0.0, 0.0, 0.0, 0.191, 0.082, 0.518, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 14:12:11 (running for 00:28:48.52)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.16  |      0.271 |                   49 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.168 |      0.332 |                   49 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.955 |      0.063 |                   48 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.582 |      0.12  |                   47 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.35261194029850745
[2m[36m(func pid=140840)[0m top5: 0.8703358208955224
[2m[36m(func pid=140840)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=140840)[0m f1_macro: 0.32809185393026485
[2m[36m(func pid=140840)[0m f1_weighted: 0.36695579687410956
[2m[36m(func pid=140840)[0m f1_per_class: [0.338, 0.503, 0.511, 0.367, 0.144, 0.233, 0.366, 0.35, 0.257, 0.212]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.33722014925373134
[2m[36m(func pid=140272)[0m top5: 0.871268656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=140272)[0m f1_macro: 0.2708388199544092
[2m[36m(func pid=140272)[0m f1_weighted: 0.3632877693118844
[2m[36m(func pid=140272)[0m f1_per_class: [0.328, 0.383, 0.338, 0.411, 0.107, 0.148, 0.454, 0.259, 0.121, 0.159]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m top1: 0.19962686567164178
[2m[36m(func pid=141385)[0m top5: 0.6749067164179104
[2m[36m(func pid=141385)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=141385)[0m f1_macro: 0.15478587012137485
[2m[36m(func pid=141385)[0m f1_weighted: 0.18799611411414657
[2m[36m(func pid=141385)[0m f1_per_class: [0.138, 0.281, 0.0, 0.033, 0.126, 0.308, 0.239, 0.292, 0.086, 0.045]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4262 | Steps: 4 | Val loss: 2.2051 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0608 | Steps: 4 | Val loss: 1.8144 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1284 | Steps: 4 | Val loss: 1.8591 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4862 | Steps: 4 | Val loss: 4.7199 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:12:17 (running for 00:28:54.12)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.018 |      0.271 |                   50 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.047 |      0.328 |                   50 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.335 |      0.155 |                   49 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.426 |      0.129 |                   48 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.24347014925373134
[2m[36m(func pid=141940)[0m top5: 0.6930970149253731
[2m[36m(func pid=141940)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=141940)[0m f1_macro: 0.1288960483883938
[2m[36m(func pid=141940)[0m f1_weighted: 0.1613873136237484
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.414, 0.0, 0.109, 0.0, 0.206, 0.019, 0.531, 0.0, 0.011]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3521455223880597
[2m[36m(func pid=140840)[0m top5: 0.8708022388059702
[2m[36m(func pid=140840)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=140840)[0m f1_macro: 0.3195946434935338
[2m[36m(func pid=140840)[0m f1_weighted: 0.3756499808877021
[2m[36m(func pid=140840)[0m f1_per_class: [0.27, 0.497, 0.471, 0.367, 0.117, 0.217, 0.409, 0.352, 0.264, 0.233]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.28171641791044777
[2m[36m(func pid=141385)[0m top5: 0.7789179104477612
[2m[36m(func pid=141385)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=141385)[0m f1_macro: 0.19362925721084293
[2m[36m(func pid=141385)[0m f1_weighted: 0.24551278734354223
[2m[36m(func pid=141385)[0m f1_per_class: [0.391, 0.554, 0.012, 0.036, 0.137, 0.016, 0.365, 0.306, 0.073, 0.048]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m top1: 0.34328358208955223
[2m[36m(func pid=140272)[0m top5: 0.8754664179104478
[2m[36m(func pid=140272)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=140272)[0m f1_macro: 0.2743213184877514
[2m[36m(func pid=140272)[0m f1_weighted: 0.3676948776725522
[2m[36m(func pid=140272)[0m f1_per_class: [0.36, 0.388, 0.289, 0.43, 0.122, 0.154, 0.445, 0.249, 0.13, 0.176]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4645 | Steps: 4 | Val loss: 2.0722 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6625 | Steps: 4 | Val loss: 2.6135 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1530 | Steps: 4 | Val loss: 1.8228 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1606 | Steps: 4 | Val loss: 1.8671 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 14:12:22 (running for 00:28:59.47)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.128 |      0.274 |                   51 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.061 |      0.32  |                   51 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.486 |      0.194 |                   50 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.465 |      0.16  |                   49 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.29617537313432835
[2m[36m(func pid=141940)[0m top5: 0.7318097014925373
[2m[36m(func pid=141940)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=141940)[0m f1_macro: 0.15958493259999199
[2m[36m(func pid=141940)[0m f1_weighted: 0.242885280821687
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.44, 0.0, 0.197, 0.0, 0.263, 0.174, 0.522, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m top1: 0.35494402985074625
[2m[36m(func pid=141385)[0m top5: 0.8348880597014925
[2m[36m(func pid=141385)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=141385)[0m f1_macro: 0.2967305981841291
[2m[36m(func pid=141385)[0m f1_weighted: 0.36405619124698035
[2m[36m(func pid=141385)[0m f1_per_class: [0.614, 0.502, 0.076, 0.279, 0.185, 0.222, 0.443, 0.379, 0.144, 0.125]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3530783582089552
[2m[36m(func pid=140840)[0m top5: 0.871268656716418
[2m[36m(func pid=140840)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=140840)[0m f1_macro: 0.3223009912834502
[2m[36m(func pid=140840)[0m f1_weighted: 0.38494226991384667
[2m[36m(func pid=140840)[0m f1_per_class: [0.262, 0.485, 0.462, 0.409, 0.097, 0.204, 0.41, 0.362, 0.263, 0.269]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.33675373134328357
[2m[36m(func pid=140272)[0m top5: 0.8796641791044776
[2m[36m(func pid=140272)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=140272)[0m f1_macro: 0.26867347685966314
[2m[36m(func pid=140272)[0m f1_weighted: 0.36338353933609013
[2m[36m(func pid=140272)[0m f1_per_class: [0.375, 0.341, 0.255, 0.468, 0.089, 0.165, 0.411, 0.273, 0.153, 0.155]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3038 | Steps: 4 | Val loss: 1.9828 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1923 | Steps: 4 | Val loss: 1.8513 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.0260 | Steps: 4 | Val loss: 2.8092 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0792 | Steps: 4 | Val loss: 1.8500 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:12:27 (running for 00:29:04.66)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.161 |      0.269 |                   52 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.153 |      0.322 |                   52 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.663 |      0.297 |                   51 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.304 |      0.176 |                   50 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.3064365671641791
[2m[36m(func pid=141940)[0m top5: 0.7621268656716418
[2m[36m(func pid=141940)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=141940)[0m f1_macro: 0.1760308386547953
[2m[36m(func pid=141940)[0m f1_weighted: 0.2853763867613532
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.447, 0.0, 0.218, 0.018, 0.272, 0.291, 0.515, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.35261194029850745
[2m[36m(func pid=140840)[0m top5: 0.8703358208955224
[2m[36m(func pid=140840)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=140840)[0m f1_macro: 0.3355280632095582
[2m[36m(func pid=140840)[0m f1_weighted: 0.3753419921202391
[2m[36m(func pid=140840)[0m f1_per_class: [0.338, 0.528, 0.571, 0.392, 0.081, 0.187, 0.366, 0.369, 0.295, 0.228]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.22994402985074627
[2m[36m(func pid=141385)[0m top5: 0.8232276119402985
[2m[36m(func pid=141385)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=141385)[0m f1_macro: 0.20629865552202448
[2m[36m(func pid=141385)[0m f1_weighted: 0.22893361088759767
[2m[36m(func pid=141385)[0m f1_per_class: [0.46, 0.3, 0.02, 0.385, 0.114, 0.277, 0.006, 0.38, 0.12, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3362873134328358
[2m[36m(func pid=140272)[0m top5: 0.8815298507462687
[2m[36m(func pid=140272)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=140272)[0m f1_macro: 0.27340767971849933
[2m[36m(func pid=140272)[0m f1_weighted: 0.3640254340075541
[2m[36m(func pid=140272)[0m f1_per_class: [0.355, 0.35, 0.3, 0.463, 0.084, 0.148, 0.42, 0.266, 0.157, 0.19]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3789 | Steps: 4 | Val loss: 1.9187 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0456 | Steps: 4 | Val loss: 1.7886 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 3.8480 | Steps: 4 | Val loss: 6.8039 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0321 | Steps: 4 | Val loss: 1.8529 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 14:12:33 (running for 00:29:10.08)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.079 |      0.273 |                   53 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.192 |      0.336 |                   53 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.026 |      0.206 |                   52 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.379 |      0.123 |                   51 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.1646455223880597
[2m[36m(func pid=141940)[0m top5: 0.7910447761194029
[2m[36m(func pid=141940)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=141940)[0m f1_macro: 0.12333889309178914
[2m[36m(func pid=141940)[0m f1_weighted: 0.1563540530465258
[2m[36m(func pid=141940)[0m f1_per_class: [0.085, 0.0, 0.0, 0.0, 0.036, 0.314, 0.301, 0.498, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.36427238805970147
[2m[36m(func pid=140840)[0m top5: 0.8824626865671642
[2m[36m(func pid=140840)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=140840)[0m f1_macro: 0.33774756650606597
[2m[36m(func pid=140840)[0m f1_weighted: 0.39434657427601866
[2m[36m(func pid=140840)[0m f1_per_class: [0.311, 0.499, 0.579, 0.418, 0.095, 0.188, 0.427, 0.377, 0.23, 0.252]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.05083955223880597
[2m[36m(func pid=141385)[0m top5: 0.4351679104477612
[2m[36m(func pid=141385)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=141385)[0m f1_macro: 0.03732699280585536
[2m[36m(func pid=141385)[0m f1_weighted: 0.0280969429548377
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.036, 0.0, 0.0, 0.0, 0.089, 0.0, 0.156, 0.078, 0.015]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3283582089552239
[2m[36m(func pid=140272)[0m top5: 0.8791977611940298
[2m[36m(func pid=140272)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=140272)[0m f1_macro: 0.27353099063994885
[2m[36m(func pid=140272)[0m f1_weighted: 0.3588891075017641
[2m[36m(func pid=140272)[0m f1_per_class: [0.357, 0.383, 0.32, 0.418, 0.087, 0.169, 0.423, 0.245, 0.151, 0.183]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.5117 | Steps: 4 | Val loss: 1.9816 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 5.2344 | Steps: 4 | Val loss: 22.3951 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0357 | Steps: 4 | Val loss: 1.7953 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9158 | Steps: 4 | Val loss: 1.8428 | Batch size: 32 | lr: 0.0001 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 14:12:38 (running for 00:29:15.64)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.032 |      0.274 |                   54 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.046 |      0.338 |                   54 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.848 |      0.037 |                   53 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.512 |      0.118 |                   52 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.16184701492537312
[2m[36m(func pid=141940)[0m top5: 0.773320895522388
[2m[36m(func pid=141940)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=141940)[0m f1_macro: 0.11823714314289784
[2m[36m(func pid=141940)[0m f1_weighted: 0.14204410161016223
[2m[36m(func pid=141940)[0m f1_per_class: [0.09, 0.0, 0.0, 0.0, 0.027, 0.333, 0.248, 0.484, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m top1: 0.1142723880597015
[2m[36m(func pid=141385)[0m top5: 0.597481343283582
[2m[36m(func pid=141385)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=141385)[0m f1_macro: 0.060346363744587406
[2m[36m(func pid=141385)[0m f1_weighted: 0.08847111045260114
[2m[36m(func pid=141385)[0m f1_per_class: [0.019, 0.47, 0.0, 0.0, 0.0, 0.042, 0.0, 0.0, 0.072, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.36613805970149255
[2m[36m(func pid=140840)[0m top5: 0.8847947761194029
[2m[36m(func pid=140840)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=140840)[0m f1_macro: 0.349582378190339
[2m[36m(func pid=140840)[0m f1_weighted: 0.39913585309839245
[2m[36m(func pid=140840)[0m f1_per_class: [0.347, 0.476, 0.615, 0.447, 0.089, 0.203, 0.416, 0.401, 0.223, 0.278]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3460820895522388
[2m[36m(func pid=140272)[0m top5: 0.8852611940298507
[2m[36m(func pid=140272)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=140272)[0m f1_macro: 0.284922069941448
[2m[36m(func pid=140272)[0m f1_weighted: 0.3715345056910567
[2m[36m(func pid=140272)[0m f1_per_class: [0.377, 0.429, 0.324, 0.444, 0.088, 0.146, 0.414, 0.267, 0.171, 0.189]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3312 | Steps: 4 | Val loss: 2.2538 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0588 | Steps: 4 | Val loss: 1.8409 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.1715 | Steps: 4 | Val loss: 53.4278 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.0636 | Steps: 4 | Val loss: 1.8452 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 14:12:44 (running for 00:29:21.06)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.916 |      0.285 |                   55 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.036 |      0.35  |                   55 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  5.234 |      0.06  |                   54 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.331 |      0.095 |                   53 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.11287313432835822
[2m[36m(func pid=141940)[0m top5: 0.7425373134328358
[2m[36m(func pid=141940)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=141940)[0m f1_macro: 0.09467588319194722
[2m[36m(func pid=141940)[0m f1_weighted: 0.06951905555209854
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.049, 0.0, 0.023, 0.292, 0.016, 0.498, 0.07, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3582089552238806
[2m[36m(func pid=140840)[0m top5: 0.8768656716417911
[2m[36m(func pid=140840)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=140840)[0m f1_macro: 0.3436942216413187
[2m[36m(func pid=140840)[0m f1_weighted: 0.3970317503153184
[2m[36m(func pid=140840)[0m f1_per_class: [0.339, 0.475, 0.632, 0.444, 0.083, 0.187, 0.421, 0.412, 0.199, 0.246]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141385)[0m top1: 0.052705223880597014
[2m[36m(func pid=141385)[0m top5: 0.28824626865671643
[2m[36m(func pid=141385)[0m f1_micro: 0.05270522388059702
[2m[36m(func pid=141385)[0m f1_macro: 0.024677825685208994
[2m[36m(func pid=141385)[0m f1_weighted: 0.025741279301575447
[2m[36m(func pid=141385)[0m f1_per_class: [0.037, 0.103, 0.0, 0.0, 0.0, 0.045, 0.0, 0.0, 0.061, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m top1: 0.34048507462686567
[2m[36m(func pid=140272)[0m top5: 0.8815298507462687
[2m[36m(func pid=140272)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=140272)[0m f1_macro: 0.2818917563581188
[2m[36m(func pid=140272)[0m f1_weighted: 0.3681307468020057
[2m[36m(func pid=140272)[0m f1_per_class: [0.377, 0.432, 0.31, 0.406, 0.094, 0.164, 0.433, 0.257, 0.167, 0.179]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3070 | Steps: 4 | Val loss: 2.2501 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 4.7910 | Steps: 4 | Val loss: 48.4249 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1471 | Steps: 4 | Val loss: 1.7923 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.9902 | Steps: 4 | Val loss: 1.8293 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 14:12:49 (running for 00:29:26.42)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.064 |      0.282 |                   56 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.059 |      0.344 |                   56 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.171 |      0.025 |                   55 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.085 |                   54 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.08582089552238806
[2m[36m(func pid=141940)[0m top5: 0.7201492537313433
[2m[36m(func pid=141940)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=141940)[0m f1_macro: 0.08518724530471436
[2m[36m(func pid=141940)[0m f1_weighted: 0.05372692102237876
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.03, 0.0, 0.061, 0.193, 0.0, 0.496, 0.072, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m top1: 0.04477611940298507
[2m[36m(func pid=141385)[0m top5: 0.2733208955223881
[2m[36m(func pid=141385)[0m f1_micro: 0.04477611940298508
[2m[36m(func pid=141385)[0m f1_macro: 0.10000779197717462
[2m[36m(func pid=141385)[0m f1_weighted: 0.03692739151883063
[2m[36m(func pid=141385)[0m f1_per_class: [0.045, 0.071, 0.6, 0.058, 0.062, 0.0, 0.003, 0.0, 0.039, 0.122]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.384794776119403
[2m[36m(func pid=140840)[0m top5: 0.886660447761194
[2m[36m(func pid=140840)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=140840)[0m f1_macro: 0.3493864820648502
[2m[36m(func pid=140840)[0m f1_weighted: 0.418073635517142
[2m[36m(func pid=140840)[0m f1_per_class: [0.385, 0.502, 0.558, 0.445, 0.082, 0.144, 0.486, 0.423, 0.216, 0.254]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3530783582089552
[2m[36m(func pid=140272)[0m top5: 0.8871268656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=140272)[0m f1_macro: 0.3012123820640989
[2m[36m(func pid=140272)[0m f1_weighted: 0.3794092623487852
[2m[36m(func pid=140272)[0m f1_per_class: [0.418, 0.43, 0.379, 0.452, 0.115, 0.173, 0.419, 0.275, 0.144, 0.207]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4082 | Steps: 4 | Val loss: 2.1000 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.0242 | Steps: 4 | Val loss: 38.1523 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2366 | Steps: 4 | Val loss: 1.8303 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.2076 | Steps: 4 | Val loss: 1.8412 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:12:54 (running for 00:29:31.75)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.99  |      0.301 |                   57 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.147 |      0.349 |                   57 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  4.791 |      0.1   |                   56 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.408 |      0.091 |                   55 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.12080223880597014
[2m[36m(func pid=141940)[0m top5: 0.7541977611940298
[2m[36m(func pid=141940)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=141940)[0m f1_macro: 0.09081069459510335
[2m[36m(func pid=141940)[0m f1_weighted: 0.13338077158883124
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.029, 0.0, 0.058, 0.0, 0.364, 0.372, 0.086, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141385)[0m top1: 0.07555970149253731
[2m[36m(func pid=141385)[0m top5: 0.40298507462686567
[2m[36m(func pid=141385)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=141385)[0m f1_macro: 0.08849764703728086
[2m[36m(func pid=141385)[0m f1_weighted: 0.07994982725105165
[2m[36m(func pid=141385)[0m f1_per_class: [0.068, 0.131, 0.333, 0.175, 0.0, 0.0, 0.0, 0.0, 0.151, 0.027]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.376865671641791
[2m[36m(func pid=140840)[0m top5: 0.8810634328358209
[2m[36m(func pid=140840)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=140840)[0m f1_macro: 0.3550630744841004
[2m[36m(func pid=140840)[0m f1_weighted: 0.4082173504546251
[2m[36m(func pid=140840)[0m f1_per_class: [0.443, 0.509, 0.558, 0.433, 0.097, 0.142, 0.459, 0.419, 0.181, 0.309]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.345615671641791
[2m[36m(func pid=140272)[0m top5: 0.882929104477612
[2m[36m(func pid=140272)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=140272)[0m f1_macro: 0.3084423865924552
[2m[36m(func pid=140272)[0m f1_weighted: 0.3706612891346208
[2m[36m(func pid=140272)[0m f1_per_class: [0.367, 0.437, 0.524, 0.423, 0.111, 0.19, 0.403, 0.292, 0.164, 0.175]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.7545 | Steps: 4 | Val loss: 2.0194 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.8615 | Steps: 4 | Val loss: 24.6385 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0447 | Steps: 4 | Val loss: 1.7694 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:13:00 (running for 00:29:37.08)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  1.208 |      0.308 |                   58 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.237 |      0.355 |                   58 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.024 |      0.088 |                   57 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.755 |      0.073 |                   56 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.16744402985074627
[2m[36m(func pid=141940)[0m top5: 0.7509328358208955
[2m[36m(func pid=141940)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=141940)[0m f1_macro: 0.0730211635068067
[2m[36m(func pid=141940)[0m f1_weighted: 0.16593862042417287
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.03, 0.0, 0.056, 0.0, 0.541, 0.032, 0.072, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.9022 | Steps: 4 | Val loss: 1.8405 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=141385)[0m top1: 0.21875
[2m[36m(func pid=141385)[0m top5: 0.6068097014925373
[2m[36m(func pid=141385)[0m f1_micro: 0.21875
[2m[36m(func pid=141385)[0m f1_macro: 0.14202369950202107
[2m[36m(func pid=141385)[0m f1_weighted: 0.1980350357108005
[2m[36m(func pid=141385)[0m f1_per_class: [0.038, 0.468, 0.267, 0.296, 0.0, 0.138, 0.03, 0.088, 0.073, 0.021]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.39225746268656714
[2m[36m(func pid=140840)[0m top5: 0.8903917910447762
[2m[36m(func pid=140840)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=140840)[0m f1_macro: 0.3821101862698297
[2m[36m(func pid=140840)[0m f1_weighted: 0.42771169118216357
[2m[36m(func pid=140840)[0m f1_per_class: [0.504, 0.511, 0.6, 0.483, 0.107, 0.215, 0.442, 0.397, 0.2, 0.361]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.34701492537313433
[2m[36m(func pid=140272)[0m top5: 0.882929104477612
[2m[36m(func pid=140272)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=140272)[0m f1_macro: 0.30131369932344365
[2m[36m(func pid=140272)[0m f1_weighted: 0.37010298263682884
[2m[36m(func pid=140272)[0m f1_per_class: [0.391, 0.445, 0.44, 0.427, 0.109, 0.191, 0.392, 0.285, 0.176, 0.157]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.4518 | Steps: 4 | Val loss: 2.0821 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.3256 | Steps: 4 | Val loss: 14.9556 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0375 | Steps: 4 | Val loss: 1.7287 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:13:05 (running for 00:29:42.46)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.902 |      0.301 |                   59 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.045 |      0.382 |                   59 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.861 |      0.142 |                   58 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.452 |      0.11  |                   57 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.13152985074626866
[2m[36m(func pid=141940)[0m top5: 0.7299440298507462
[2m[36m(func pid=141940)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=141940)[0m f1_macro: 0.10984713964177122
[2m[36m(func pid=141940)[0m f1_weighted: 0.14671863783513514
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.023, 0.0, 0.025, 0.076, 0.348, 0.54, 0.087, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.9904 | Steps: 4 | Val loss: 1.8605 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=141385)[0m top1: 0.3064365671641791
[2m[36m(func pid=141385)[0m top5: 0.8353544776119403
[2m[36m(func pid=141385)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=141385)[0m f1_macro: 0.16929008568512566
[2m[36m(func pid=141385)[0m f1_weighted: 0.32685843277711096
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.203, 0.027, 0.189, 0.0, 0.384, 0.598, 0.291, 0.0, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.38899253731343286
[2m[36m(func pid=140840)[0m top5: 0.8992537313432836
[2m[36m(func pid=140840)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=140840)[0m f1_macro: 0.3725522230700959
[2m[36m(func pid=140840)[0m f1_weighted: 0.42108738249899064
[2m[36m(func pid=140840)[0m f1_per_class: [0.52, 0.499, 0.533, 0.508, 0.124, 0.263, 0.395, 0.354, 0.195, 0.333]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3400186567164179
[2m[36m(func pid=140272)[0m top5: 0.8754664179104478
[2m[36m(func pid=140272)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=140272)[0m f1_macro: 0.28805668109026217
[2m[36m(func pid=140272)[0m f1_weighted: 0.3621042310974359
[2m[36m(func pid=140272)[0m f1_per_class: [0.356, 0.422, 0.355, 0.446, 0.109, 0.188, 0.363, 0.307, 0.157, 0.176]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.5048 | Steps: 4 | Val loss: 2.0718 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.8973 | Steps: 4 | Val loss: 11.7886 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.1005 | Steps: 4 | Val loss: 1.7543 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:13:10 (running for 00:29:47.84)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.99  |      0.288 |                   60 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.038 |      0.373 |                   60 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.326 |      0.169 |                   59 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.505 |      0.09  |                   58 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.09981343283582089
[2m[36m(func pid=141940)[0m top5: 0.738339552238806
[2m[36m(func pid=141940)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=141940)[0m f1_macro: 0.09033949558000606
[2m[36m(func pid=141940)[0m f1_weighted: 0.10561864592118629
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.03, 0.0, 0.0, 0.156, 0.197, 0.494, 0.0, 0.026]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8354 | Steps: 4 | Val loss: 1.8696 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=141385)[0m top1: 0.3185634328358209
[2m[36m(func pid=141385)[0m top5: 0.8330223880597015
[2m[36m(func pid=141385)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=141385)[0m f1_macro: 0.19459892031552586
[2m[36m(func pid=141385)[0m f1_weighted: 0.314430365305022
[2m[36m(func pid=141385)[0m f1_per_class: [0.11, 0.097, 0.083, 0.179, 0.0, 0.337, 0.602, 0.409, 0.079, 0.049]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3908582089552239
[2m[36m(func pid=140840)[0m top5: 0.8955223880597015
[2m[36m(func pid=140840)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=140840)[0m f1_macro: 0.3753439633366174
[2m[36m(func pid=140840)[0m f1_weighted: 0.4176512951685028
[2m[36m(func pid=140840)[0m f1_per_class: [0.532, 0.53, 0.522, 0.525, 0.122, 0.267, 0.344, 0.374, 0.198, 0.341]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3306902985074627
[2m[36m(func pid=140272)[0m top5: 0.871268656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=140272)[0m f1_macro: 0.28365815025161273
[2m[36m(func pid=140272)[0m f1_weighted: 0.35164532614246913
[2m[36m(func pid=140272)[0m f1_per_class: [0.337, 0.406, 0.4, 0.444, 0.115, 0.172, 0.348, 0.3, 0.165, 0.151]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6595 | Steps: 4 | Val loss: 2.0791 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3620 | Steps: 4 | Val loss: 3.7969 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 14:13:16 (running for 00:29:53.25)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.835 |      0.284 |                   61 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.101 |      0.375 |                   61 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.897 |      0.195 |                   60 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.66  |      0.153 |                   59 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.18936567164179105
[2m[36m(func pid=141940)[0m top5: 0.7476679104477612
[2m[36m(func pid=141940)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=141940)[0m f1_macro: 0.15269403797550227
[2m[36m(func pid=141940)[0m f1_weighted: 0.2193695593445519
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.29, 0.043, 0.172, 0.0, 0.275, 0.199, 0.527, 0.0, 0.022]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0310 | Steps: 4 | Val loss: 1.7175 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.8548 | Steps: 4 | Val loss: 1.8503 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=141385)[0m top1: 0.292910447761194
[2m[36m(func pid=141385)[0m top5: 0.816231343283582
[2m[36m(func pid=141385)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=141385)[0m f1_macro: 0.21621243036901333
[2m[36m(func pid=141385)[0m f1_weighted: 0.30225959594179475
[2m[36m(func pid=141385)[0m f1_per_class: [0.155, 0.435, 0.333, 0.385, 0.065, 0.074, 0.278, 0.343, 0.093, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3927238805970149
[2m[36m(func pid=140840)[0m top5: 0.9067164179104478
[2m[36m(func pid=140840)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=140840)[0m f1_macro: 0.39641988375908566
[2m[36m(func pid=140840)[0m f1_weighted: 0.4087580459476795
[2m[36m(func pid=140840)[0m f1_per_class: [0.533, 0.548, 0.632, 0.51, 0.169, 0.288, 0.304, 0.352, 0.222, 0.406]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.34375
[2m[36m(func pid=140272)[0m top5: 0.8740671641791045
[2m[36m(func pid=140272)[0m f1_micro: 0.34375
[2m[36m(func pid=140272)[0m f1_macro: 0.2960491075525729
[2m[36m(func pid=140272)[0m f1_weighted: 0.3606077341019134
[2m[36m(func pid=140272)[0m f1_per_class: [0.374, 0.441, 0.421, 0.459, 0.133, 0.19, 0.333, 0.304, 0.158, 0.147]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.4360 | Steps: 4 | Val loss: 2.0571 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.3292 | Steps: 4 | Val loss: 3.2367 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:13:21 (running for 00:29:58.64)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.855 |      0.296 |                   62 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.031 |      0.396 |                   62 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.362 |      0.216 |                   61 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.436 |      0.153 |                   60 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.24813432835820895
[2m[36m(func pid=141940)[0m top5: 0.7602611940298507
[2m[36m(func pid=141940)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=141940)[0m f1_macro: 0.15287362099636453
[2m[36m(func pid=141940)[0m f1_weighted: 0.22224324984841776
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.349, 0.0, 0.226, 0.0, 0.327, 0.113, 0.443, 0.07, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2366 | Steps: 4 | Val loss: 1.7392 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.9198 | Steps: 4 | Val loss: 1.8886 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m top1: 0.25652985074626866
[2m[36m(func pid=141385)[0m top5: 0.7896455223880597
[2m[36m(func pid=141385)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=141385)[0m f1_macro: 0.15957983489733313
[2m[36m(func pid=141385)[0m f1_weighted: 0.2540002022803486
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.31, 0.0, 0.456, 0.049, 0.0, 0.168, 0.292, 0.108, 0.212]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.39225746268656714
[2m[36m(func pid=140840)[0m top5: 0.8987873134328358
[2m[36m(func pid=140840)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=140840)[0m f1_macro: 0.38765071861462064
[2m[36m(func pid=140840)[0m f1_weighted: 0.40674850775517823
[2m[36m(func pid=140840)[0m f1_per_class: [0.555, 0.568, 0.558, 0.488, 0.156, 0.283, 0.312, 0.342, 0.221, 0.395]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.5953 | Steps: 4 | Val loss: 1.9550 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=140272)[0m top1: 0.31902985074626866
[2m[36m(func pid=140272)[0m top5: 0.8558768656716418
[2m[36m(func pid=140272)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=140272)[0m f1_macro: 0.28619411637061265
[2m[36m(func pid=140272)[0m f1_weighted: 0.3322779206475506
[2m[36m(func pid=140272)[0m f1_per_class: [0.357, 0.43, 0.471, 0.439, 0.124, 0.181, 0.267, 0.299, 0.173, 0.12]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.7138 | Steps: 4 | Val loss: 2.9366 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:13:26 (running for 00:30:03.83)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.92  |      0.286 |                   63 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.237 |      0.388 |                   63 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.329 |      0.16  |                   62 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.595 |      0.195 |                   61 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.3199626865671642
[2m[36m(func pid=141940)[0m top5: 0.7975746268656716
[2m[36m(func pid=141940)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=141940)[0m f1_macro: 0.19484450264074613
[2m[36m(func pid=141940)[0m f1_weighted: 0.3158861679190922
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.413, 0.0, 0.233, 0.0, 0.299, 0.374, 0.529, 0.099, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0817 | Steps: 4 | Val loss: 1.5908 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8140 | Steps: 4 | Val loss: 1.8656 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=141385)[0m top1: 0.18889925373134328
[2m[36m(func pid=141385)[0m top5: 0.7639925373134329
[2m[36m(func pid=141385)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=141385)[0m f1_macro: 0.20712234788762984
[2m[36m(func pid=141385)[0m f1_weighted: 0.18623652301181923
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.369, 0.818, 0.229, 0.043, 0.264, 0.036, 0.114, 0.178, 0.02]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.4222 | Steps: 4 | Val loss: 2.0681 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=140840)[0m top1: 0.42723880597014924
[2m[36m(func pid=140840)[0m top5: 0.9235074626865671
[2m[36m(func pid=140840)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=140840)[0m f1_macro: 0.41598367547252746
[2m[36m(func pid=140840)[0m f1_weighted: 0.441405571359256
[2m[36m(func pid=140840)[0m f1_per_class: [0.653, 0.574, 0.558, 0.553, 0.169, 0.295, 0.351, 0.332, 0.227, 0.448]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3283582089552239
[2m[36m(func pid=140272)[0m top5: 0.8698694029850746
[2m[36m(func pid=140272)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=140272)[0m f1_macro: 0.28805047544284995
[2m[36m(func pid=140272)[0m f1_weighted: 0.3493622740388176
[2m[36m(func pid=140272)[0m f1_per_class: [0.385, 0.413, 0.414, 0.448, 0.134, 0.192, 0.322, 0.3, 0.163, 0.109]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.5684 | Steps: 4 | Val loss: 2.9953 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:13:32 (running for 00:30:09.13)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.814 |      0.288 |                   64 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.082 |      0.416 |                   64 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.714 |      0.207 |                   63 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.422 |      0.116 |                   62 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.18610074626865672
[2m[36m(func pid=141940)[0m top5: 0.7593283582089553
[2m[36m(func pid=141940)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=141940)[0m f1_macro: 0.11588252891197515
[2m[36m(func pid=141940)[0m f1_weighted: 0.19191553607465128
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.026, 0.276, 0.0, 0.07, 0.26, 0.485, 0.042, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1135 | Steps: 4 | Val loss: 1.5416 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8648 | Steps: 4 | Val loss: 1.8517 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=141385)[0m top1: 0.17723880597014927
[2m[36m(func pid=141385)[0m top5: 0.7243470149253731
[2m[36m(func pid=141385)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=141385)[0m f1_macro: 0.2014758013379514
[2m[36m(func pid=141385)[0m f1_weighted: 0.16603263569679538
[2m[36m(func pid=141385)[0m f1_per_class: [0.177, 0.168, 0.474, 0.177, 0.05, 0.299, 0.051, 0.469, 0.128, 0.023]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.44169776119402987
[2m[36m(func pid=140840)[0m top5: 0.9221082089552238
[2m[36m(func pid=140840)[0m f1_micro: 0.4416977611940298
[2m[36m(func pid=140840)[0m f1_macro: 0.4122138081301717
[2m[36m(func pid=140840)[0m f1_weighted: 0.4513893721066448
[2m[36m(func pid=140840)[0m f1_per_class: [0.635, 0.573, 0.558, 0.575, 0.189, 0.305, 0.363, 0.33, 0.257, 0.337]
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.5317 | Steps: 4 | Val loss: 2.1874 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.33722014925373134
[2m[36m(func pid=140272)[0m top5: 0.8740671641791045
[2m[36m(func pid=140272)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=140272)[0m f1_macro: 0.28786927836374987
[2m[36m(func pid=140272)[0m f1_weighted: 0.3597488108182874
[2m[36m(func pid=140272)[0m f1_per_class: [0.359, 0.411, 0.4, 0.466, 0.134, 0.19, 0.346, 0.288, 0.158, 0.127]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.5437 | Steps: 4 | Val loss: 2.7721 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:13:37 (running for 00:30:14.71)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.865 |      0.288 |                   65 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.113 |      0.412 |                   65 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.568 |      0.201 |                   64 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.532 |      0.117 |                   63 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.3414179104477612
[2m[36m(func pid=141940)[0m top5: 0.7168843283582089
[2m[36m(func pid=141940)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=141940)[0m f1_macro: 0.11724245960689288
[2m[36m(func pid=141940)[0m f1_weighted: 0.209931800516708
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.0, 0.481, 0.0, 0.0, 0.149, 0.542, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0523 | Steps: 4 | Val loss: 1.4281 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7544 | Steps: 4 | Val loss: 1.8585 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=141385)[0m top1: 0.24720149253731344
[2m[36m(func pid=141385)[0m top5: 0.6819029850746269
[2m[36m(func pid=141385)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=141385)[0m f1_macro: 0.18609540022219118
[2m[36m(func pid=141385)[0m f1_weighted: 0.24134354045673678
[2m[36m(func pid=141385)[0m f1_per_class: [0.127, 0.16, 0.229, 0.128, 0.071, 0.0, 0.484, 0.424, 0.101, 0.138]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.47574626865671643
[2m[36m(func pid=140840)[0m top5: 0.9384328358208955
[2m[36m(func pid=140840)[0m f1_micro: 0.47574626865671643
[2m[36m(func pid=140840)[0m f1_macro: 0.4200717622484132
[2m[36m(func pid=140840)[0m f1_weighted: 0.4927485774418797
[2m[36m(func pid=140840)[0m f1_per_class: [0.641, 0.563, 0.558, 0.591, 0.197, 0.321, 0.493, 0.318, 0.234, 0.286]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.5912 | Steps: 4 | Val loss: 2.0828 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=140272)[0m top1: 0.333955223880597
[2m[36m(func pid=140272)[0m top5: 0.8722014925373134
[2m[36m(func pid=140272)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=140272)[0m f1_macro: 0.29043962769418874
[2m[36m(func pid=140272)[0m f1_weighted: 0.35507827751094184
[2m[36m(func pid=140272)[0m f1_per_class: [0.361, 0.41, 0.415, 0.463, 0.144, 0.185, 0.335, 0.267, 0.185, 0.138]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.8879 | Steps: 4 | Val loss: 3.2021 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:13:43 (running for 00:30:20.45)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.754 |      0.29  |                   66 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.052 |      0.42  |                   66 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.544 |      0.186 |                   65 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.591 |      0.124 |                   64 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.19309701492537312
[2m[36m(func pid=141940)[0m top5: 0.7793843283582089
[2m[36m(func pid=141940)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=141940)[0m f1_macro: 0.12375739498942114
[2m[36m(func pid=141940)[0m f1_weighted: 0.21324117661956654
[2m[36m(func pid=141940)[0m f1_per_class: [0.066, 0.0, 0.0, 0.255, 0.0, 0.0, 0.366, 0.551, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0363 | Steps: 4 | Val loss: 1.4225 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.8130 | Steps: 4 | Val loss: 1.8699 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m top1: 0.2462686567164179
[2m[36m(func pid=141385)[0m top5: 0.6585820895522388
[2m[36m(func pid=141385)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=141385)[0m f1_macro: 0.1807591619496745
[2m[36m(func pid=141385)[0m f1_weighted: 0.23484995089132615
[2m[36m(func pid=141385)[0m f1_per_class: [0.11, 0.436, 0.15, 0.056, 0.118, 0.0, 0.386, 0.369, 0.072, 0.111]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.4818097014925373
[2m[36m(func pid=140840)[0m top5: 0.9393656716417911
[2m[36m(func pid=140840)[0m f1_micro: 0.4818097014925373
[2m[36m(func pid=140840)[0m f1_macro: 0.41424923150585685
[2m[36m(func pid=140840)[0m f1_weighted: 0.49564711082156343
[2m[36m(func pid=140840)[0m f1_per_class: [0.54, 0.586, 0.558, 0.582, 0.176, 0.283, 0.514, 0.344, 0.222, 0.337]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.5347 | Steps: 4 | Val loss: 1.9272 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=140272)[0m top1: 0.3269589552238806
[2m[36m(func pid=140272)[0m top5: 0.8600746268656716
[2m[36m(func pid=140272)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=140272)[0m f1_macro: 0.2833260489090393
[2m[36m(func pid=140272)[0m f1_weighted: 0.3518974800658711
[2m[36m(func pid=140272)[0m f1_per_class: [0.324, 0.409, 0.393, 0.433, 0.161, 0.184, 0.359, 0.259, 0.181, 0.131]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3288 | Steps: 4 | Val loss: 3.2948 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:13:48 (running for 00:30:25.73)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.813 |      0.283 |                   67 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.036 |      0.414 |                   67 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.888 |      0.181 |                   66 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.535 |      0.147 |                   65 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.2756529850746269
[2m[36m(func pid=141940)[0m top5: 0.8166977611940298
[2m[36m(func pid=141940)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=141940)[0m f1_macro: 0.14735509159057364
[2m[36m(func pid=141940)[0m f1_weighted: 0.27119534057608613
[2m[36m(func pid=141940)[0m f1_per_class: [0.083, 0.0, 0.0, 0.268, 0.046, 0.0, 0.55, 0.526, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0433 | Steps: 4 | Val loss: 1.3971 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8883 | Steps: 4 | Val loss: 1.8353 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=141385)[0m top1: 0.18516791044776118
[2m[36m(func pid=141385)[0m top5: 0.648320895522388
[2m[36m(func pid=141385)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=141385)[0m f1_macro: 0.1570262852526768
[2m[36m(func pid=141385)[0m f1_weighted: 0.1858409440568055
[2m[36m(func pid=141385)[0m f1_per_class: [0.186, 0.38, 0.084, 0.141, 0.076, 0.0, 0.161, 0.433, 0.088, 0.02]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140840)[0m top1: 0.48973880597014924
[2m[36m(func pid=140840)[0m top5: 0.9426305970149254
[2m[36m(func pid=140840)[0m f1_micro: 0.48973880597014924
[2m[36m(func pid=140840)[0m f1_macro: 0.42059470380202973
[2m[36m(func pid=140840)[0m f1_weighted: 0.5048072492470496
[2m[36m(func pid=140840)[0m f1_per_class: [0.556, 0.596, 0.533, 0.577, 0.163, 0.298, 0.533, 0.359, 0.227, 0.364]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.4100 | Steps: 4 | Val loss: 1.9989 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=140272)[0m top1: 0.3423507462686567
[2m[36m(func pid=140272)[0m top5: 0.878731343283582
[2m[36m(func pid=140272)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=140272)[0m f1_macro: 0.30203408807703847
[2m[36m(func pid=140272)[0m f1_weighted: 0.36720170597479396
[2m[36m(func pid=140272)[0m f1_per_class: [0.346, 0.431, 0.449, 0.421, 0.161, 0.198, 0.394, 0.287, 0.189, 0.145]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7343 | Steps: 4 | Val loss: 3.3292 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 14:13:54 (running for 00:30:31.13)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.888 |      0.302 |                   68 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.043 |      0.421 |                   68 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.329 |      0.157 |                   67 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.41  |      0.138 |                   66 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.2416044776119403
[2m[36m(func pid=141940)[0m top5: 0.8041044776119403
[2m[36m(func pid=141940)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=141940)[0m f1_macro: 0.1377521725069144
[2m[36m(func pid=141940)[0m f1_weighted: 0.24454801050296748
[2m[36m(func pid=141940)[0m f1_per_class: [0.089, 0.0, 0.0, 0.169, 0.041, 0.0, 0.561, 0.441, 0.077, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7521 | Steps: 4 | Val loss: 1.8155 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0446 | Steps: 4 | Val loss: 1.4086 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=141385)[0m top1: 0.14925373134328357
[2m[36m(func pid=141385)[0m top5: 0.7644589552238806
[2m[36m(func pid=141385)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=141385)[0m f1_macro: 0.1348988541281832
[2m[36m(func pid=141385)[0m f1_weighted: 0.1586622497106162
[2m[36m(func pid=141385)[0m f1_per_class: [0.19, 0.292, 0.0, 0.155, 0.042, 0.0, 0.101, 0.485, 0.084, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.5299 | Steps: 4 | Val loss: 2.0359 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=140840)[0m top1: 0.48507462686567165
[2m[36m(func pid=140840)[0m top5: 0.945429104477612
[2m[36m(func pid=140840)[0m f1_micro: 0.48507462686567165
[2m[36m(func pid=140840)[0m f1_macro: 0.4175468138768398
[2m[36m(func pid=140840)[0m f1_weighted: 0.49967547011768626
[2m[36m(func pid=140840)[0m f1_per_class: [0.54, 0.595, 0.545, 0.576, 0.168, 0.304, 0.518, 0.349, 0.228, 0.352]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.35494402985074625
[2m[36m(func pid=140272)[0m top5: 0.8857276119402985
[2m[36m(func pid=140272)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=140272)[0m f1_macro: 0.30923562886755607
[2m[36m(func pid=140272)[0m f1_weighted: 0.38002177076889104
[2m[36m(func pid=140272)[0m f1_per_class: [0.4, 0.417, 0.444, 0.466, 0.153, 0.21, 0.397, 0.28, 0.184, 0.141]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.4112 | Steps: 4 | Val loss: 2.9788 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:13:59 (running for 00:30:36.57)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.752 |      0.309 |                   69 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.045 |      0.418 |                   69 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.734 |      0.135 |                   68 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.53  |      0.094 |                   67 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.17583955223880596
[2m[36m(func pid=141940)[0m top5: 0.7644589552238806
[2m[36m(func pid=141940)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=141940)[0m f1_macro: 0.09437343634011466
[2m[36m(func pid=141940)[0m f1_weighted: 0.1852822503462907
[2m[36m(func pid=141940)[0m f1_per_class: [0.071, 0.0, 0.0, 0.089, 0.043, 0.0, 0.492, 0.158, 0.091, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0261 | Steps: 4 | Val loss: 1.4500 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6849 | Steps: 4 | Val loss: 1.8048 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=141385)[0m top1: 0.2490671641791045
[2m[36m(func pid=141385)[0m top5: 0.7915111940298507
[2m[36m(func pid=141385)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=141385)[0m f1_macro: 0.19934161601915595
[2m[36m(func pid=141385)[0m f1_weighted: 0.2525339780038024
[2m[36m(func pid=141385)[0m f1_per_class: [0.169, 0.482, 0.0, 0.263, 0.067, 0.317, 0.086, 0.458, 0.105, 0.046]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6074 | Steps: 4 | Val loss: 2.1238 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=140840)[0m top1: 0.46595149253731344
[2m[36m(func pid=140840)[0m top5: 0.9388992537313433
[2m[36m(func pid=140840)[0m f1_micro: 0.46595149253731344
[2m[36m(func pid=140840)[0m f1_macro: 0.4089787536030757
[2m[36m(func pid=140840)[0m f1_weighted: 0.48263674082294133
[2m[36m(func pid=140840)[0m f1_per_class: [0.525, 0.58, 0.571, 0.528, 0.148, 0.267, 0.527, 0.357, 0.239, 0.348]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3596082089552239
[2m[36m(func pid=140272)[0m top5: 0.8931902985074627
[2m[36m(func pid=140272)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=140272)[0m f1_macro: 0.309633200702364
[2m[36m(func pid=140272)[0m f1_weighted: 0.3794464986435006
[2m[36m(func pid=140272)[0m f1_per_class: [0.387, 0.444, 0.436, 0.484, 0.14, 0.216, 0.361, 0.282, 0.179, 0.168]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9957 | Steps: 4 | Val loss: 2.4263 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:14:04 (running for 00:30:41.73)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.685 |      0.31  |                   70 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.026 |      0.409 |                   70 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.411 |      0.199 |                   69 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.607 |      0.068 |                   68 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.12919776119402984
[2m[36m(func pid=141940)[0m top5: 0.7332089552238806
[2m[36m(func pid=141940)[0m f1_micro: 0.12919776119402984
[2m[36m(func pid=141940)[0m f1_macro: 0.06787869782887931
[2m[36m(func pid=141940)[0m f1_weighted: 0.149451312119672
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.024, 0.0, 0.475, 0.091, 0.068, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0478 | Steps: 4 | Val loss: 1.4294 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.8669 | Steps: 4 | Val loss: 1.8023 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=141385)[0m top1: 0.27798507462686567
[2m[36m(func pid=141385)[0m top5: 0.8278917910447762
[2m[36m(func pid=141385)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=141385)[0m f1_macro: 0.246583676952319
[2m[36m(func pid=141385)[0m f1_weighted: 0.26886291437921617
[2m[36m(func pid=141385)[0m f1_per_class: [0.198, 0.54, 0.741, 0.472, 0.0, 0.15, 0.027, 0.0, 0.303, 0.035]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6548 | Steps: 4 | Val loss: 2.1868 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=140840)[0m top1: 0.47947761194029853
[2m[36m(func pid=140840)[0m top5: 0.9379664179104478
[2m[36m(func pid=140840)[0m f1_micro: 0.47947761194029853
[2m[36m(func pid=140840)[0m f1_macro: 0.4042503764634553
[2m[36m(func pid=140840)[0m f1_weighted: 0.4968183976422986
[2m[36m(func pid=140840)[0m f1_per_class: [0.545, 0.58, 0.545, 0.543, 0.156, 0.265, 0.568, 0.339, 0.222, 0.278]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3493470149253731
[2m[36m(func pid=140272)[0m top5: 0.8927238805970149
[2m[36m(func pid=140272)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=140272)[0m f1_macro: 0.3119432357859604
[2m[36m(func pid=140272)[0m f1_weighted: 0.36609463402978826
[2m[36m(func pid=140272)[0m f1_per_class: [0.398, 0.459, 0.49, 0.441, 0.144, 0.215, 0.341, 0.311, 0.177, 0.144]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0429 | Steps: 4 | Val loss: 2.8509 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:14:10 (running for 00:30:47.02)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.867 |      0.312 |                   71 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.048 |      0.404 |                   71 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.996 |      0.247 |                   70 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.655 |      0.085 |                   69 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.10261194029850747
[2m[36m(func pid=141940)[0m top5: 0.7257462686567164
[2m[36m(func pid=141940)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=141940)[0m f1_macro: 0.08473459421006377
[2m[36m(func pid=141940)[0m f1_weighted: 0.12578622041077256
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.02, 0.0, 0.016, 0.0, 0.327, 0.484, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0716 | Steps: 4 | Val loss: 1.4227 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.8650 | Steps: 4 | Val loss: 1.7544 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=141385)[0m top1: 0.38572761194029853
[2m[36m(func pid=141385)[0m top5: 0.8568097014925373
[2m[36m(func pid=141385)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=141385)[0m f1_macro: 0.2798615620214554
[2m[36m(func pid=141385)[0m f1_weighted: 0.4105792442609395
[2m[36m(func pid=141385)[0m f1_per_class: [0.0, 0.4, 0.364, 0.496, 0.0, 0.354, 0.425, 0.421, 0.269, 0.07]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6368 | Steps: 4 | Val loss: 2.3058 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=140840)[0m top1: 0.48600746268656714
[2m[36m(func pid=140840)[0m top5: 0.9393656716417911
[2m[36m(func pid=140840)[0m f1_micro: 0.48600746268656714
[2m[36m(func pid=140840)[0m f1_macro: 0.40694054715115435
[2m[36m(func pid=140840)[0m f1_weighted: 0.5031693812990493
[2m[36m(func pid=140840)[0m f1_per_class: [0.552, 0.585, 0.5, 0.584, 0.145, 0.264, 0.545, 0.354, 0.221, 0.32]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.376865671641791
[2m[36m(func pid=140272)[0m top5: 0.90625
[2m[36m(func pid=140272)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=140272)[0m f1_macro: 0.3314374634970155
[2m[36m(func pid=140272)[0m f1_weighted: 0.3928828480390509
[2m[36m(func pid=140272)[0m f1_per_class: [0.454, 0.496, 0.48, 0.463, 0.155, 0.219, 0.386, 0.28, 0.195, 0.186]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.4602 | Steps: 4 | Val loss: 4.5128 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:14:15 (running for 00:30:52.35)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.865 |      0.331 |                   72 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.072 |      0.407 |                   72 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.043 |      0.28  |                   71 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.637 |      0.06  |                   70 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.054104477611940295
[2m[36m(func pid=141940)[0m top5: 0.7103544776119403
[2m[36m(func pid=141940)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=141940)[0m f1_macro: 0.060242484247864894
[2m[36m(func pid=141940)[0m f1_weighted: 0.040923961207927406
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.02, 0.0, 0.014, 0.046, 0.025, 0.484, 0.0, 0.015]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2212 | Steps: 4 | Val loss: 1.4400 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=141385)[0m top1: 0.3591417910447761
[2m[36m(func pid=141385)[0m top5: 0.8624067164179104
[2m[36m(func pid=141385)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=141385)[0m f1_macro: 0.2946794263755639
[2m[36m(func pid=141385)[0m f1_weighted: 0.3290076412412351
[2m[36m(func pid=141385)[0m f1_per_class: [0.037, 0.278, 0.667, 0.528, 0.075, 0.411, 0.153, 0.49, 0.192, 0.117]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.8004 | Steps: 4 | Val loss: 1.7625 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.5926 | Steps: 4 | Val loss: 2.2956 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=140840)[0m top1: 0.47901119402985076
[2m[36m(func pid=140840)[0m top5: 0.941231343283582
[2m[36m(func pid=140840)[0m f1_micro: 0.47901119402985076
[2m[36m(func pid=140840)[0m f1_macro: 0.41019806004538006
[2m[36m(func pid=140840)[0m f1_weighted: 0.5011820145755421
[2m[36m(func pid=140840)[0m f1_per_class: [0.593, 0.568, 0.585, 0.568, 0.161, 0.256, 0.571, 0.308, 0.227, 0.263]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.3763992537313433
[2m[36m(func pid=140272)[0m top5: 0.8997201492537313
[2m[36m(func pid=140272)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=140272)[0m f1_macro: 0.32296878701260995
[2m[36m(func pid=140272)[0m f1_weighted: 0.3884258571430458
[2m[36m(func pid=140272)[0m f1_per_class: [0.471, 0.496, 0.393, 0.463, 0.164, 0.219, 0.371, 0.291, 0.182, 0.178]
[2m[36m(func pid=140272)[0m 
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 3.0566 | Steps: 4 | Val loss: 3.9193 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 14:14:20 (running for 00:30:57.68)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.8   |      0.323 |                   73 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.221 |      0.41  |                   73 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.46  |      0.295 |                   72 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.593 |      0.067 |                   71 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.061567164179104475
[2m[36m(func pid=141940)[0m top5: 0.7210820895522388
[2m[36m(func pid=141940)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=141940)[0m f1_macro: 0.06705071579120304
[2m[36m(func pid=141940)[0m f1_weighted: 0.04724273985934402
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.012, 0.141, 0.009, 0.487, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0599 | Steps: 4 | Val loss: 1.5452 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7216 | Steps: 4 | Val loss: 1.7792 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=141385)[0m top1: 0.39505597014925375
[2m[36m(func pid=141385)[0m top5: 0.8605410447761194
[2m[36m(func pid=141385)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=141385)[0m f1_macro: 0.34265885445061167
[2m[36m(func pid=141385)[0m f1_weighted: 0.41035586038350785
[2m[36m(func pid=141385)[0m f1_per_class: [0.273, 0.481, 0.7, 0.423, 0.079, 0.363, 0.407, 0.522, 0.178, 0.0]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.4155 | Steps: 4 | Val loss: 2.2692 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=140840)[0m top1: 0.447294776119403
[2m[36m(func pid=140840)[0m top5: 0.9272388059701493
[2m[36m(func pid=140840)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=140840)[0m f1_macro: 0.387393464900389
[2m[36m(func pid=140840)[0m f1_weighted: 0.473769894938453
[2m[36m(func pid=140840)[0m f1_per_class: [0.529, 0.552, 0.522, 0.556, 0.133, 0.232, 0.504, 0.382, 0.213, 0.252]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.36986940298507465
[2m[36m(func pid=140272)[0m top5: 0.8955223880597015
[2m[36m(func pid=140272)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=140272)[0m f1_macro: 0.31587064688806865
[2m[36m(func pid=140272)[0m f1_weighted: 0.3819118673594737
[2m[36m(func pid=140272)[0m f1_per_class: [0.4, 0.47, 0.387, 0.466, 0.148, 0.207, 0.362, 0.319, 0.215, 0.184]
[2m[36m(func pid=140272)[0m 
== Status ==
Current time: 2024-01-07 14:14:26 (running for 00:31:03.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00012 | RUNNING    | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.722 |      0.316 |                   74 |
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.06  |      0.387 |                   74 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  3.057 |      0.343 |                   73 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.415 |      0.077 |                   72 |
| train_5806f_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6127 | Steps: 4 | Val loss: 3.2400 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=141940)[0m top1: 0.07462686567164178
[2m[36m(func pid=141940)[0m top5: 0.7458022388059702
[2m[36m(func pid=141940)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=141940)[0m f1_macro: 0.07681896582692817
[2m[36m(func pid=141940)[0m f1_weighted: 0.056489795113852145
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.022, 0.0, 0.021, 0.213, 0.009, 0.503, 0.0, 0.0]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0562 | Steps: 4 | Val loss: 1.5933 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=140272)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6876 | Steps: 4 | Val loss: 1.7979 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=141385)[0m top1: 0.28404850746268656
[2m[36m(func pid=141385)[0m top5: 0.789179104477612
[2m[36m(func pid=141385)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=141385)[0m f1_macro: 0.24546198152269372
[2m[36m(func pid=141385)[0m f1_weighted: 0.2993219741401028
[2m[36m(func pid=141385)[0m f1_per_class: [0.202, 0.46, 0.408, 0.34, 0.078, 0.272, 0.23, 0.197, 0.175, 0.092]
[2m[36m(func pid=141385)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5660 | Steps: 4 | Val loss: 2.2353 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=140840)[0m top1: 0.4197761194029851
[2m[36m(func pid=140840)[0m top5: 0.9174440298507462
[2m[36m(func pid=140840)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=140840)[0m f1_macro: 0.36698852123843384
[2m[36m(func pid=140840)[0m f1_weighted: 0.439497505680044
[2m[36m(func pid=140840)[0m f1_per_class: [0.526, 0.553, 0.453, 0.475, 0.149, 0.224, 0.472, 0.372, 0.194, 0.252]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=140272)[0m top1: 0.36007462686567165
[2m[36m(func pid=140272)[0m top5: 0.8847947761194029
[2m[36m(func pid=140272)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=140272)[0m f1_macro: 0.3034816763186613
[2m[36m(func pid=140272)[0m f1_weighted: 0.3717685762747222
[2m[36m(func pid=140272)[0m f1_per_class: [0.351, 0.467, 0.329, 0.448, 0.173, 0.206, 0.352, 0.307, 0.233, 0.17]
[2m[36m(func pid=141385)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.3995 | Steps: 4 | Val loss: 3.1752 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=141940)[0m top1: 0.07975746268656717
[2m[36m(func pid=141940)[0m top5: 0.7565298507462687
[2m[36m(func pid=141940)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=141940)[0m f1_macro: 0.07649925481940284
[2m[36m(func pid=141940)[0m f1_weighted: 0.06542875108870375
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.025, 0.0, 0.032, 0.171, 0.063, 0.458, 0.0, 0.015]
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0629 | Steps: 4 | Val loss: 1.6480 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=141385)[0m top1: 0.17537313432835822
[2m[36m(func pid=141385)[0m top5: 0.7649253731343284
[2m[36m(func pid=141385)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=141385)[0m f1_macro: 0.18603021592145674
[2m[36m(func pid=141385)[0m f1_weighted: 0.19845532778647826
[2m[36m(func pid=141385)[0m f1_per_class: [0.2, 0.254, 0.07, 0.212, 0.042, 0.203, 0.114, 0.459, 0.155, 0.15]
[2m[36m(func pid=140840)[0m top1: 0.4001865671641791
[2m[36m(func pid=140840)[0m top5: 0.9118470149253731
[2m[36m(func pid=140840)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=140840)[0m f1_macro: 0.3534181714357158
[2m[36m(func pid=140840)[0m f1_weighted: 0.4143998084656841
[2m[36m(func pid=140840)[0m f1_per_class: [0.562, 0.549, 0.358, 0.439, 0.157, 0.238, 0.419, 0.362, 0.197, 0.252]
== Status ==
Current time: 2024-01-07 14:14:31 (running for 00:31:08.66)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.35025
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.056 |      0.367 |                   75 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.613 |      0.245 |                   74 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.415 |      0.077 |                   72 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159021)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=159021)[0m Configuration completed!
[2m[36m(func pid=159021)[0m New optimizer parameters:
[2m[36m(func pid=159021)[0m SGD (
[2m[36m(func pid=159021)[0m Parameter Group 0
[2m[36m(func pid=159021)[0m     dampening: 0
[2m[36m(func pid=159021)[0m     differentiable: False
[2m[36m(func pid=159021)[0m     foreach: None
[2m[36m(func pid=159021)[0m     lr: 0.0001
[2m[36m(func pid=159021)[0m     maximize: False
[2m[36m(func pid=159021)[0m     momentum: 0.99
[2m[36m(func pid=159021)[0m     nesterov: False
[2m[36m(func pid=159021)[0m     weight_decay: 1e-05
[2m[36m(func pid=159021)[0m )
[2m[36m(func pid=159021)[0m 
== Status ==
Current time: 2024-01-07 14:14:40 (running for 00:31:17.07)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.35025
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.056 |      0.367 |                   75 |
| train_5806f_00014 | RUNNING    | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  2.613 |      0.245 |                   74 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.566 |      0.076 |                   73 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2782 | Steps: 4 | Val loss: 2.2188 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0221 | Steps: 4 | Val loss: 1.7574 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0227 | Steps: 4 | Val loss: 2.5142 | Batch size: 32 | lr: 0.0001 | Duration: 4.82s
[2m[36m(func pid=141940)[0m top1: 0.07229477611940298
[2m[36m(func pid=141940)[0m top5: 0.7681902985074627
[2m[36m(func pid=141940)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=141940)[0m f1_macro: 0.06786597297382205
[2m[36m(func pid=141940)[0m f1_weighted: 0.06700680683014551
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.025, 0.0, 0.031, 0.0, 0.131, 0.473, 0.0, 0.019]
[2m[36m(func pid=141940)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3675373134328358
[2m[36m(func pid=140840)[0m top5: 0.8973880597014925
[2m[36m(func pid=140840)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=140840)[0m f1_macro: 0.32141341303014076
[2m[36m(func pid=140840)[0m f1_weighted: 0.3767999081001061
[2m[36m(func pid=140840)[0m f1_per_class: [0.43, 0.545, 0.32, 0.39, 0.145, 0.249, 0.352, 0.34, 0.188, 0.254]
[2m[36m(func pid=159021)[0m top1: 0.06343283582089553
[2m[36m(func pid=159021)[0m top5: 0.488339552238806
[2m[36m(func pid=159021)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=159021)[0m f1_macro: 0.034228818095853965
[2m[36m(func pid=159021)[0m f1_weighted: 0.0361923073855302
[2m[36m(func pid=159021)[0m f1_per_class: [0.067, 0.01, 0.0, 0.084, 0.0, 0.024, 0.0, 0.102, 0.022, 0.034]
[2m[36m(func pid=141940)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.3065 | Steps: 4 | Val loss: 2.1661 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 14:14:45 (running for 00:31:22.47)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.063 |      0.353 |                   76 |
| train_5806f_00015 | RUNNING    | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.278 |      0.068 |                   74 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159478)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=159478)[0m Configuration completed!
[2m[36m(func pid=159478)[0m New optimizer parameters:
[2m[36m(func pid=159478)[0m SGD (
[2m[36m(func pid=159478)[0m Parameter Group 0
[2m[36m(func pid=159478)[0m     dampening: 0
[2m[36m(func pid=159478)[0m     differentiable: False
[2m[36m(func pid=159478)[0m     foreach: None
[2m[36m(func pid=159478)[0m     lr: 0.001
[2m[36m(func pid=159478)[0m     maximize: False
[2m[36m(func pid=159478)[0m     momentum: 0.99
[2m[36m(func pid=159478)[0m     nesterov: False
[2m[36m(func pid=159478)[0m     weight_decay: 1e-05
[2m[36m(func pid=159478)[0m )
[2m[36m(func pid=159478)[0m 
== Status ==
Current time: 2024-01-07 14:14:51 (running for 00:31:27.97)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 3 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.022 |      0.321 |                   77 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  3.023 |      0.034 |                    1 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=141940)[0m top1: 0.10634328358208955
[2m[36m(func pid=141940)[0m top5: 0.7784514925373134
[2m[36m(func pid=141940)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=141940)[0m f1_macro: 0.08414267997079058
[2m[36m(func pid=141940)[0m f1_weighted: 0.11206151997810665
[2m[36m(func pid=141940)[0m f1_per_class: [0.0, 0.0, 0.028, 0.0, 0.04, 0.0, 0.283, 0.469, 0.0, 0.021]
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0459 | Steps: 4 | Val loss: 1.8528 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1882 | Steps: 4 | Val loss: 2.5508 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9480 | Steps: 4 | Val loss: 2.4501 | Batch size: 32 | lr: 0.001 | Duration: 4.84s
[2m[36m(func pid=140840)[0m top1: 0.34281716417910446
[2m[36m(func pid=140840)[0m top5: 0.8754664179104478
[2m[36m(func pid=140840)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=140840)[0m f1_macro: 0.3049395436078578
[2m[36m(func pid=140840)[0m f1_weighted: 0.34285010207759753
[2m[36m(func pid=140840)[0m f1_per_class: [0.385, 0.539, 0.3, 0.353, 0.136, 0.235, 0.283, 0.337, 0.216, 0.265]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.06110074626865672
[2m[36m(func pid=159021)[0m top5: 0.4748134328358209
[2m[36m(func pid=159021)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=159021)[0m f1_macro: 0.0358946453315633
[2m[36m(func pid=159021)[0m f1_weighted: 0.041598195949574604
[2m[36m(func pid=159021)[0m f1_per_class: [0.046, 0.035, 0.0, 0.09, 0.0, 0.024, 0.0, 0.096, 0.024, 0.045]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.08255597014925373
[2m[36m(func pid=159478)[0m top5: 0.488339552238806
[2m[36m(func pid=159478)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=159478)[0m f1_macro: 0.05990873960835149
[2m[36m(func pid=159478)[0m f1_weighted: 0.07033042324665484
[2m[36m(func pid=159478)[0m f1_per_class: [0.151, 0.049, 0.0, 0.171, 0.0, 0.027, 0.0, 0.103, 0.04, 0.058]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0299 | Steps: 4 | Val loss: 2.5566 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0939 | Steps: 4 | Val loss: 1.8437 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7845 | Steps: 4 | Val loss: 2.3806 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=160131)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=160131)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=160131)[0m Configuration completed!
[2m[36m(func pid=160131)[0m New optimizer parameters:
[2m[36m(func pid=160131)[0m SGD (
[2m[36m(func pid=160131)[0m Parameter Group 0
[2m[36m(func pid=160131)[0m     dampening: 0
[2m[36m(func pid=160131)[0m     differentiable: False
[2m[36m(func pid=160131)[0m     foreach: None
[2m[36m(func pid=160131)[0m     lr: 0.01
[2m[36m(func pid=160131)[0m     maximize: False
[2m[36m(func pid=160131)[0m     momentum: 0.99
[2m[36m(func pid=160131)[0m     nesterov: False
[2m[36m(func pid=160131)[0m     weight_decay: 1e-05
[2m[36m(func pid=160131)[0m )
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:14:59 (running for 00:31:36.56)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.046 |      0.305 |                   78 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  3.03  |      0.044 |                    3 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  2.948 |      0.06  |                    1 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.06623134328358209
[2m[36m(func pid=159021)[0m top5: 0.4496268656716418
[2m[36m(func pid=159021)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=159021)[0m f1_macro: 0.043777889448381666
[2m[36m(func pid=159021)[0m f1_weighted: 0.05461295522635893
[2m[36m(func pid=159021)[0m f1_per_class: [0.038, 0.077, 0.0, 0.108, 0.0, 0.023, 0.0, 0.102, 0.044, 0.046]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3498134328358209
[2m[36m(func pid=140840)[0m top5: 0.8759328358208955
[2m[36m(func pid=140840)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=140840)[0m f1_macro: 0.308096278958286
[2m[36m(func pid=140840)[0m f1_weighted: 0.36027898145342185
[2m[36m(func pid=140840)[0m f1_per_class: [0.357, 0.522, 0.289, 0.412, 0.138, 0.236, 0.295, 0.346, 0.216, 0.268]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.08162313432835822
[2m[36m(func pid=159478)[0m top5: 0.5247201492537313
[2m[36m(func pid=159478)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=159478)[0m f1_macro: 0.07011583317671942
[2m[36m(func pid=159478)[0m f1_weighted: 0.09040297065514924
[2m[36m(func pid=159478)[0m f1_per_class: [0.137, 0.01, 0.087, 0.267, 0.015, 0.02, 0.009, 0.051, 0.078, 0.028]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8442 | Steps: 4 | Val loss: 2.5346 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0345 | Steps: 4 | Val loss: 1.8697 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.8358 | Steps: 4 | Val loss: 2.2942 | Batch size: 32 | lr: 0.01 | Duration: 4.54s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4016 | Steps: 4 | Val loss: 2.2276 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:15:05 (running for 00:31:42.06)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.094 |      0.308 |                   79 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  2.844 |      0.058 |                    4 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  2.784 |      0.07  |                    2 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.07835820895522388
[2m[36m(func pid=159021)[0m top5: 0.4319029850746269
[2m[36m(func pid=159021)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=159021)[0m f1_macro: 0.05805610455360347
[2m[36m(func pid=159021)[0m f1_weighted: 0.07833563206164669
[2m[36m(func pid=159021)[0m f1_per_class: [0.049, 0.128, 0.0, 0.148, 0.0, 0.041, 0.0, 0.116, 0.064, 0.034]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.33908582089552236
[2m[36m(func pid=140840)[0m top5: 0.8722014925373134
[2m[36m(func pid=140840)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=140840)[0m f1_macro: 0.30606842504438503
[2m[36m(func pid=140840)[0m f1_weighted: 0.3462553359582858
[2m[36m(func pid=140840)[0m f1_per_class: [0.394, 0.525, 0.289, 0.397, 0.148, 0.236, 0.269, 0.296, 0.195, 0.312]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.14832089552238806
[2m[36m(func pid=159478)[0m top5: 0.6632462686567164
[2m[36m(func pid=159478)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=159478)[0m f1_macro: 0.12824868916394813
[2m[36m(func pid=159478)[0m f1_weighted: 0.160251977856705
[2m[36m(func pid=159478)[0m f1_per_class: [0.201, 0.011, 0.241, 0.231, 0.054, 0.166, 0.218, 0.0, 0.109, 0.052]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.20335820895522388
[2m[36m(func pid=160131)[0m top5: 0.6002798507462687
[2m[36m(func pid=160131)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=160131)[0m f1_macro: 0.11306666639797791
[2m[36m(func pid=160131)[0m f1_weighted: 0.1726622058959607
[2m[36m(func pid=160131)[0m f1_per_class: [0.23, 0.0, 0.075, 0.0, 0.118, 0.123, 0.503, 0.0, 0.083, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6604 | Steps: 4 | Val loss: 2.5095 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.1120 | Steps: 4 | Val loss: 1.7917 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.8877 | Steps: 4 | Val loss: 2.0269 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9622 | Steps: 4 | Val loss: 2.4552 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:15:10 (running for 00:31:47.53)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.035 |      0.306 |                   80 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  2.66  |      0.07  |                    5 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  2.402 |      0.128 |                    3 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  2.836 |      0.113 |                    1 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.07929104477611941
[2m[36m(func pid=159021)[0m top5: 0.4099813432835821
[2m[36m(func pid=159021)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=159021)[0m f1_macro: 0.07015261000735894
[2m[36m(func pid=159021)[0m f1_weighted: 0.08159709550434248
[2m[36m(func pid=159021)[0m f1_per_class: [0.079, 0.141, 0.06, 0.135, 0.0, 0.048, 0.006, 0.137, 0.066, 0.03]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.3614738805970149
[2m[36m(func pid=140840)[0m top5: 0.8941231343283582
[2m[36m(func pid=140840)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=140840)[0m f1_macro: 0.33023074113354906
[2m[36m(func pid=140840)[0m f1_weighted: 0.3689606779058037
[2m[36m(func pid=140840)[0m f1_per_class: [0.465, 0.556, 0.32, 0.425, 0.173, 0.238, 0.295, 0.282, 0.195, 0.353]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.22154850746268656
[2m[36m(func pid=159478)[0m top5: 0.7947761194029851
[2m[36m(func pid=159478)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=159478)[0m f1_macro: 0.15707513556074065
[2m[36m(func pid=159478)[0m f1_weighted: 0.2130343898896904
[2m[36m(func pid=159478)[0m f1_per_class: [0.185, 0.079, 0.208, 0.293, 0.098, 0.247, 0.267, 0.0, 0.085, 0.109]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.2416044776119403
[2m[36m(func pid=160131)[0m top5: 0.6781716417910447
[2m[36m(func pid=160131)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=160131)[0m f1_macro: 0.1954907133066664
[2m[36m(func pid=160131)[0m f1_weighted: 0.22557655539395394
[2m[36m(func pid=160131)[0m f1_per_class: [0.271, 0.458, 0.049, 0.345, 0.276, 0.258, 0.03, 0.0, 0.038, 0.229]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6539 | Steps: 4 | Val loss: 2.4889 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.1151 | Steps: 4 | Val loss: 1.7739 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.5002 | Steps: 4 | Val loss: 1.8734 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9517 | Steps: 4 | Val loss: 4.2669 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:15:16 (running for 00:31:53.04)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.112 |      0.33  |                   81 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  2.654 |      0.073 |                    6 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.888 |      0.157 |                    4 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  1.962 |      0.195 |                    2 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.07649253731343283
[2m[36m(func pid=159021)[0m top5: 0.40718283582089554
[2m[36m(func pid=159021)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=159021)[0m f1_macro: 0.07301756142664097
[2m[36m(func pid=159021)[0m f1_weighted: 0.08387013819745162
[2m[36m(func pid=159021)[0m f1_per_class: [0.073, 0.122, 0.064, 0.132, 0.0, 0.063, 0.019, 0.142, 0.086, 0.03]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.37033582089552236
[2m[36m(func pid=140840)[0m top5: 0.898320895522388
[2m[36m(func pid=140840)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=140840)[0m f1_macro: 0.3537375016032011
[2m[36m(func pid=140840)[0m f1_weighted: 0.3780421062839328
[2m[36m(func pid=140840)[0m f1_per_class: [0.462, 0.554, 0.436, 0.437, 0.165, 0.225, 0.31, 0.29, 0.222, 0.435]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3031716417910448
[2m[36m(func pid=159478)[0m top5: 0.8843283582089553
[2m[36m(func pid=159478)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=159478)[0m f1_macro: 0.23251889830353578
[2m[36m(func pid=159478)[0m f1_weighted: 0.32610218053565376
[2m[36m(func pid=159478)[0m f1_per_class: [0.147, 0.339, 0.276, 0.402, 0.131, 0.277, 0.37, 0.0, 0.16, 0.222]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.2775186567164179
[2m[36m(func pid=160131)[0m top5: 0.621268656716418
[2m[36m(func pid=160131)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=160131)[0m f1_macro: 0.16791749306664028
[2m[36m(func pid=160131)[0m f1_weighted: 0.1851756183324065
[2m[36m(func pid=160131)[0m f1_per_class: [0.115, 0.446, 0.258, 0.287, 0.2, 0.024, 0.0, 0.349, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5057 | Steps: 4 | Val loss: 2.4566 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0786 | Steps: 4 | Val loss: 1.6863 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.0853 | Steps: 4 | Val loss: 1.7848 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9685 | Steps: 4 | Val loss: 6.4441 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:15:21 (running for 00:31:58.51)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.115 |      0.354 |                   82 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  2.506 |      0.07  |                    7 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.5   |      0.233 |                    5 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  0.952 |      0.168 |                    3 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.07136194029850747
[2m[36m(func pid=159021)[0m top5: 0.435634328358209
[2m[36m(func pid=159021)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=159021)[0m f1_macro: 0.07046708913542567
[2m[36m(func pid=159021)[0m f1_weighted: 0.08130500104252106
[2m[36m(func pid=159021)[0m f1_per_class: [0.082, 0.102, 0.048, 0.129, 0.0, 0.069, 0.021, 0.15, 0.08, 0.023]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.404384328358209
[2m[36m(func pid=140840)[0m top5: 0.9081156716417911
[2m[36m(func pid=140840)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=140840)[0m f1_macro: 0.38523227367112567
[2m[36m(func pid=140840)[0m f1_weighted: 0.41408680612721527
[2m[36m(func pid=140840)[0m f1_per_class: [0.545, 0.567, 0.511, 0.427, 0.159, 0.224, 0.417, 0.327, 0.246, 0.429]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3619402985074627
[2m[36m(func pid=159478)[0m top5: 0.8675373134328358
[2m[36m(func pid=159478)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=159478)[0m f1_macro: 0.2700824152860327
[2m[36m(func pid=159478)[0m f1_weighted: 0.3287313844779514
[2m[36m(func pid=159478)[0m f1_per_class: [0.33, 0.558, 0.233, 0.459, 0.123, 0.111, 0.191, 0.323, 0.122, 0.25]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.15764925373134328
[2m[36m(func pid=160131)[0m top5: 0.550839552238806
[2m[36m(func pid=160131)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=160131)[0m f1_macro: 0.1643222304312601
[2m[36m(func pid=160131)[0m f1_weighted: 0.15674588939703574
[2m[36m(func pid=160131)[0m f1_per_class: [0.38, 0.191, 0.031, 0.366, 0.275, 0.063, 0.0, 0.0, 0.034, 0.304]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2992 | Steps: 4 | Val loss: 2.4014 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0314 | Steps: 4 | Val loss: 1.6830 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7025 | Steps: 4 | Val loss: 1.8548 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.5861 | Steps: 4 | Val loss: 10.0612 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:15:26 (running for 00:32:03.88)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.079 |      0.385 |                   83 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  2.299 |      0.073 |                    8 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.085 |      0.27  |                    6 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  0.969 |      0.164 |                    4 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.07555970149253731
[2m[36m(func pid=159021)[0m top5: 0.47761194029850745
[2m[36m(func pid=159021)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=159021)[0m f1_macro: 0.07260152458764658
[2m[36m(func pid=159021)[0m f1_weighted: 0.08955363581560158
[2m[36m(func pid=159021)[0m f1_per_class: [0.114, 0.106, 0.034, 0.119, 0.0, 0.1, 0.053, 0.097, 0.067, 0.035]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.40345149253731344
[2m[36m(func pid=140840)[0m top5: 0.9085820895522388
[2m[36m(func pid=140840)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=140840)[0m f1_macro: 0.3708575357995544
[2m[36m(func pid=140840)[0m f1_weighted: 0.4240943593086464
[2m[36m(func pid=140840)[0m f1_per_class: [0.507, 0.535, 0.436, 0.466, 0.148, 0.183, 0.455, 0.342, 0.191, 0.444]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.38899253731343286
[2m[36m(func pid=159478)[0m top5: 0.8232276119402985
[2m[36m(func pid=159478)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=159478)[0m f1_macro: 0.33284219462975273
[2m[36m(func pid=159478)[0m f1_weighted: 0.295501109355333
[2m[36m(func pid=159478)[0m f1_per_class: [0.574, 0.572, 0.343, 0.504, 0.186, 0.016, 0.003, 0.463, 0.177, 0.489]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.03544776119402985
[2m[36m(func pid=160131)[0m top5: 0.4221082089552239
[2m[36m(func pid=160131)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=160131)[0m f1_macro: 0.03824043618164229
[2m[36m(func pid=160131)[0m f1_weighted: 0.026910810173163352
[2m[36m(func pid=160131)[0m f1_per_class: [0.08, 0.0, 0.025, 0.029, 0.06, 0.008, 0.043, 0.0, 0.064, 0.074]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2144 | Steps: 4 | Val loss: 2.3842 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0231 | Steps: 4 | Val loss: 1.7021 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6186 | Steps: 4 | Val loss: 1.9989 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.8491 | Steps: 4 | Val loss: 23.5205 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:15:32 (running for 00:32:09.59)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.031 |      0.371 |                   84 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  2.214 |      0.076 |                    9 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.703 |      0.333 |                    7 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  1.586 |      0.038 |                    5 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.07276119402985075
[2m[36m(func pid=159021)[0m top5: 0.5130597014925373
[2m[36m(func pid=159021)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=159021)[0m f1_macro: 0.07612444059551052
[2m[36m(func pid=159021)[0m f1_weighted: 0.08441561564552237
[2m[36m(func pid=159021)[0m f1_per_class: [0.178, 0.077, 0.031, 0.081, 0.015, 0.098, 0.088, 0.068, 0.079, 0.047]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.41138059701492535
[2m[36m(func pid=140840)[0m top5: 0.9006529850746269
[2m[36m(func pid=140840)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=140840)[0m f1_macro: 0.36788698709864176
[2m[36m(func pid=140840)[0m f1_weighted: 0.43597459946242084
[2m[36m(func pid=140840)[0m f1_per_class: [0.438, 0.534, 0.393, 0.462, 0.155, 0.213, 0.492, 0.351, 0.187, 0.455]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.396455223880597
[2m[36m(func pid=159478)[0m top5: 0.8120335820895522
[2m[36m(func pid=159478)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=159478)[0m f1_macro: 0.33319390789303627
[2m[36m(func pid=159478)[0m f1_weighted: 0.2997788043179034
[2m[36m(func pid=159478)[0m f1_per_class: [0.634, 0.572, 0.381, 0.534, 0.208, 0.008, 0.0, 0.404, 0.203, 0.388]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.032182835820895525
[2m[36m(func pid=160131)[0m top5: 0.39598880597014924
[2m[36m(func pid=160131)[0m f1_micro: 0.032182835820895525
[2m[36m(func pid=160131)[0m f1_macro: 0.02604271016650873
[2m[36m(func pid=160131)[0m f1_weighted: 0.03502405616512671
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.2, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.042]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1528 | Steps: 4 | Val loss: 2.3274 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0662 | Steps: 4 | Val loss: 1.6302 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4611 | Steps: 4 | Val loss: 2.0500 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.0851 | Steps: 4 | Val loss: 48.6606 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=159021)[0m top1: 0.08815298507462686
[2m[36m(func pid=159021)[0m top5: 0.5848880597014925
[2m[36m(func pid=159021)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=159021)[0m f1_macro: 0.08628295545289405
[2m[36m(func pid=159021)[0m f1_weighted: 0.10951757615704841
[2m[36m(func pid=159021)[0m f1_per_class: [0.191, 0.038, 0.027, 0.074, 0.046, 0.092, 0.205, 0.056, 0.074, 0.06]
== Status ==
Current time: 2024-01-07 14:15:38 (running for 00:32:14.98)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.023 |      0.368 |                   85 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  2.153 |      0.086 |                   10 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.619 |      0.333 |                    8 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  1.849 |      0.026 |                    6 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.43097014925373134
[2m[36m(func pid=140840)[0m top5: 0.9090485074626866
[2m[36m(func pid=140840)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=140840)[0m f1_macro: 0.371515601500017
[2m[36m(func pid=140840)[0m f1_weighted: 0.45540196814580597
[2m[36m(func pid=140840)[0m f1_per_class: [0.402, 0.535, 0.429, 0.491, 0.161, 0.219, 0.529, 0.36, 0.178, 0.411]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3591417910447761
[2m[36m(func pid=159478)[0m top5: 0.8246268656716418
[2m[36m(func pid=159478)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=159478)[0m f1_macro: 0.28676094715692424
[2m[36m(func pid=159478)[0m f1_weighted: 0.28912606643816136
[2m[36m(func pid=159478)[0m f1_per_class: [0.529, 0.537, 0.179, 0.544, 0.18, 0.008, 0.0, 0.329, 0.241, 0.319]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.018190298507462687
[2m[36m(func pid=160131)[0m top5: 0.261660447761194
[2m[36m(func pid=160131)[0m f1_micro: 0.018190298507462687
[2m[36m(func pid=160131)[0m f1_macro: 0.016050239614853568
[2m[36m(func pid=160131)[0m f1_weighted: 0.01736164509577869
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.097, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.9458 | Steps: 4 | Val loss: 2.2316 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.1677 | Steps: 4 | Val loss: 1.6028 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3255 | Steps: 4 | Val loss: 2.1470 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.3492 | Steps: 4 | Val loss: 230.3376 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:15:43 (running for 00:32:20.47)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.066 |      0.372 |                   86 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.946 |      0.113 |                   11 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.461 |      0.287 |                    9 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  2.085 |      0.016 |                    7 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.14085820895522388
[2m[36m(func pid=159021)[0m top5: 0.6632462686567164
[2m[36m(func pid=159021)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=159021)[0m f1_macro: 0.1125309352148165
[2m[36m(func pid=159021)[0m f1_weighted: 0.16749658096954304
[2m[36m(func pid=159021)[0m f1_per_class: [0.215, 0.047, 0.031, 0.106, 0.054, 0.115, 0.355, 0.044, 0.075, 0.083]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.44029850746268656
[2m[36m(func pid=140840)[0m top5: 0.9155783582089553
[2m[36m(func pid=140840)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=140840)[0m f1_macro: 0.38330395093279146
[2m[36m(func pid=140840)[0m f1_weighted: 0.4625245103295635
[2m[36m(func pid=140840)[0m f1_per_class: [0.451, 0.518, 0.48, 0.481, 0.162, 0.228, 0.564, 0.362, 0.193, 0.394]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3269589552238806
[2m[36m(func pid=159478)[0m top5: 0.824160447761194
[2m[36m(func pid=159478)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=159478)[0m f1_macro: 0.27412487582759143
[2m[36m(func pid=159478)[0m f1_weighted: 0.2846421608936552
[2m[36m(func pid=159478)[0m f1_per_class: [0.484, 0.555, 0.091, 0.477, 0.169, 0.095, 0.009, 0.366, 0.184, 0.312]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.004197761194029851
[2m[36m(func pid=160131)[0m top5: 0.28404850746268656
[2m[36m(func pid=160131)[0m f1_micro: 0.004197761194029851
[2m[36m(func pid=160131)[0m f1_macro: 0.00459337292573783
[2m[36m(func pid=160131)[0m f1_weighted: 0.0004965815988238739
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.7705 | Steps: 4 | Val loss: 2.1690 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.1202 | Steps: 4 | Val loss: 1.5993 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3807 | Steps: 4 | Val loss: 2.1795 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 7.8509 | Steps: 4 | Val loss: 87.9222 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:15:49 (running for 00:32:25.93)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.12  |      0.392 |                   88 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.946 |      0.113 |                   11 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.326 |      0.274 |                   10 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.349 |      0.005 |                    8 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.44776119402985076
[2m[36m(func pid=140840)[0m top5: 0.9169776119402985
[2m[36m(func pid=140840)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=140840)[0m f1_macro: 0.39210917351846586
[2m[36m(func pid=140840)[0m f1_weighted: 0.4653156395446037
[2m[36m(func pid=140840)[0m f1_per_class: [0.451, 0.522, 0.558, 0.47, 0.146, 0.179, 0.594, 0.374, 0.192, 0.435]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.16930970149253732
[2m[36m(func pid=159021)[0m top5: 0.71875
[2m[36m(func pid=159021)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=159021)[0m f1_macro: 0.12743321529094062
[2m[36m(func pid=159021)[0m f1_weighted: 0.19477309893297332
[2m[36m(func pid=159021)[0m f1_per_class: [0.224, 0.065, 0.033, 0.125, 0.072, 0.131, 0.415, 0.015, 0.073, 0.121]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.31902985074626866
[2m[36m(func pid=159478)[0m top5: 0.8339552238805971
[2m[36m(func pid=159478)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=159478)[0m f1_macro: 0.2828439142752325
[2m[36m(func pid=159478)[0m f1_weighted: 0.2891063197319588
[2m[36m(func pid=159478)[0m f1_per_class: [0.459, 0.582, 0.087, 0.433, 0.149, 0.143, 0.022, 0.414, 0.189, 0.35]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.0648320895522388
[2m[36m(func pid=160131)[0m top5: 0.5205223880597015
[2m[36m(func pid=160131)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=160131)[0m f1_macro: 0.03100673583000233
[2m[36m(func pid=160131)[0m f1_weighted: 0.058973739607439415
[2m[36m(func pid=160131)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.188, 0.0, 0.052, 0.03]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.7020 | Steps: 4 | Val loss: 2.1137 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0269 | Steps: 4 | Val loss: 1.6672 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5636 | Steps: 4 | Val loss: 2.0367 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 7.7105 | Steps: 4 | Val loss: 616.7476 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 14:15:54 (running for 00:32:31.41)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.027 |      0.377 |                   89 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.77  |      0.127 |                   12 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.381 |      0.283 |                   11 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  7.851 |      0.031 |                    9 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.4253731343283582
[2m[36m(func pid=140840)[0m top5: 0.9053171641791045
[2m[36m(func pid=140840)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=140840)[0m f1_macro: 0.3769525413150959
[2m[36m(func pid=140840)[0m f1_weighted: 0.4437223122005514
[2m[36m(func pid=140840)[0m f1_per_class: [0.404, 0.486, 0.558, 0.424, 0.131, 0.151, 0.599, 0.387, 0.167, 0.462]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.19962686567164178
[2m[36m(func pid=159021)[0m top5: 0.7444029850746269
[2m[36m(func pid=159021)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=159021)[0m f1_macro: 0.1496935401273753
[2m[36m(func pid=159021)[0m f1_weighted: 0.21968580493528841
[2m[36m(func pid=159021)[0m f1_per_class: [0.284, 0.066, 0.035, 0.146, 0.085, 0.114, 0.475, 0.03, 0.068, 0.194]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3353544776119403
[2m[36m(func pid=159478)[0m top5: 0.8791977611940298
[2m[36m(func pid=159478)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=159478)[0m f1_macro: 0.30920608529868554
[2m[36m(func pid=159478)[0m f1_weighted: 0.32132979998827293
[2m[36m(func pid=159478)[0m f1_per_class: [0.551, 0.579, 0.105, 0.45, 0.128, 0.156, 0.102, 0.401, 0.216, 0.404]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.19402985074626866
[2m[36m(func pid=160131)[0m top5: 0.6963619402985075
[2m[36m(func pid=160131)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=160131)[0m f1_macro: 0.055491530962365886
[2m[36m(func pid=160131)[0m f1_weighted: 0.151144968204021
[2m[36m(func pid=160131)[0m f1_per_class: [0.014, 0.0, 0.0, 0.541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0211 | Steps: 4 | Val loss: 1.7323 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.6047 | Steps: 4 | Val loss: 2.0311 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1117 | Steps: 4 | Val loss: 1.9422 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.9594 | Steps: 4 | Val loss: 5462.2979 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:15:59 (running for 00:32:36.82)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.021 |      0.371 |                   90 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.702 |      0.15  |                   13 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.564 |      0.309 |                   12 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  7.711 |      0.055 |                   10 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.4146455223880597
[2m[36m(func pid=140840)[0m top5: 0.8992537313432836
[2m[36m(func pid=140840)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=140840)[0m f1_macro: 0.37076435986930345
[2m[36m(func pid=140840)[0m f1_weighted: 0.4369838285029546
[2m[36m(func pid=140840)[0m f1_per_class: [0.423, 0.48, 0.5, 0.427, 0.119, 0.136, 0.582, 0.389, 0.168, 0.484]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.25
[2m[36m(func pid=159021)[0m top5: 0.7863805970149254
[2m[36m(func pid=159021)[0m f1_micro: 0.25
[2m[36m(func pid=159021)[0m f1_macro: 0.17196200834956912
[2m[36m(func pid=159021)[0m f1_weighted: 0.26352776102299824
[2m[36m(func pid=159021)[0m f1_per_class: [0.263, 0.087, 0.042, 0.221, 0.136, 0.124, 0.53, 0.058, 0.081, 0.176]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3833955223880597
[2m[36m(func pid=159478)[0m top5: 0.8913246268656716
[2m[36m(func pid=159478)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=159478)[0m f1_macro: 0.36492328111944355
[2m[36m(func pid=159478)[0m f1_weighted: 0.378377750790555
[2m[36m(func pid=159478)[0m f1_per_class: [0.706, 0.6, 0.231, 0.425, 0.124, 0.257, 0.251, 0.392, 0.249, 0.415]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.027052238805970148
[2m[36m(func pid=160131)[0m top5: 0.5209888059701493
[2m[36m(func pid=160131)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=160131)[0m f1_macro: 0.016599648969647378
[2m[36m(func pid=160131)[0m f1_weighted: 0.019320928329090454
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.104, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0273 | Steps: 4 | Val loss: 1.7603 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.5573 | Steps: 4 | Val loss: 1.9669 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0762 | Steps: 4 | Val loss: 2.1566 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 4.1462 | Steps: 4 | Val loss: 5175.5718 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=140840)[0m top1: 0.3987873134328358
[2m[36m(func pid=140840)[0m top5: 0.894589552238806
[2m[36m(func pid=140840)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=140840)[0m f1_macro: 0.3661068906622173
[2m[36m(func pid=140840)[0m f1_weighted: 0.4239563319031058
[2m[36m(func pid=140840)[0m f1_per_class: [0.396, 0.464, 0.558, 0.392, 0.131, 0.157, 0.574, 0.39, 0.173, 0.426]
== Status ==
Current time: 2024-01-07 14:16:05 (running for 00:32:42.41)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.027 |      0.366 |                   91 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.172 |                   14 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.112 |      0.365 |                   13 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.959 |      0.017 |                   11 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.310634328358209
[2m[36m(func pid=159021)[0m top5: 0.800839552238806
[2m[36m(func pid=159021)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=159021)[0m f1_macro: 0.19250147727781228
[2m[36m(func pid=159021)[0m f1_weighted: 0.3082978975891221
[2m[36m(func pid=159021)[0m f1_per_class: [0.279, 0.15, 0.068, 0.301, 0.119, 0.065, 0.584, 0.072, 0.107, 0.179]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.38152985074626866
[2m[36m(func pid=159478)[0m top5: 0.8773320895522388
[2m[36m(func pid=159478)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=159478)[0m f1_macro: 0.3784042688295456
[2m[36m(func pid=159478)[0m f1_weighted: 0.37774873442930773
[2m[36m(func pid=159478)[0m f1_per_class: [0.729, 0.551, 0.369, 0.286, 0.116, 0.267, 0.402, 0.332, 0.332, 0.4]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.15345149253731344
[2m[36m(func pid=160131)[0m top5: 0.5657649253731343
[2m[36m(func pid=160131)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=160131)[0m f1_macro: 0.029752872067394333
[2m[36m(func pid=160131)[0m f1_weighted: 0.04908511175749738
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0588 | Steps: 4 | Val loss: 1.7215 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.2771 | Steps: 4 | Val loss: 1.9034 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5481 | Steps: 4 | Val loss: 2.3807 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 5.6570 | Steps: 4 | Val loss: 12690.9336 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:16:11 (running for 00:32:47.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.027 |      0.366 |                   91 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.277 |      0.211 |                   16 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.076 |      0.378 |                   14 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.146 |      0.03  |                   12 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.41044776119402987
[2m[36m(func pid=140840)[0m top5: 0.9015858208955224
[2m[36m(func pid=140840)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=140840)[0m f1_macro: 0.3839151189167833
[2m[36m(func pid=140840)[0m f1_weighted: 0.43419256489262154
[2m[36m(func pid=140840)[0m f1_per_class: [0.433, 0.469, 0.585, 0.416, 0.159, 0.147, 0.58, 0.397, 0.161, 0.492]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.34421641791044777
[2m[36m(func pid=159021)[0m top5: 0.8297574626865671
[2m[36m(func pid=159021)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=159021)[0m f1_macro: 0.21059828560691876
[2m[36m(func pid=159021)[0m f1_weighted: 0.3371008125880965
[2m[36m(func pid=159021)[0m f1_per_class: [0.263, 0.177, 0.099, 0.371, 0.129, 0.063, 0.6, 0.058, 0.126, 0.22]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3656716417910448
[2m[36m(func pid=159478)[0m top5: 0.8619402985074627
[2m[36m(func pid=159478)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=159478)[0m f1_macro: 0.36575381018695674
[2m[36m(func pid=159478)[0m f1_weighted: 0.35169254503230585
[2m[36m(func pid=159478)[0m f1_per_class: [0.681, 0.54, 0.393, 0.217, 0.121, 0.279, 0.383, 0.329, 0.36, 0.355]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.03311567164179104
[2m[36m(func pid=160131)[0m top5: 0.5149253731343284
[2m[36m(func pid=160131)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=160131)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=160131)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.2379 | Steps: 4 | Val loss: 1.8482 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.1721 | Steps: 4 | Val loss: 1.7808 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3343 | Steps: 4 | Val loss: 2.2012 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.8834 | Steps: 4 | Val loss: 3249.0540 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:16:16 (running for 00:32:53.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.172 |      0.368 |                   93 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.277 |      0.211 |                   16 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.548 |      0.366 |                   15 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  5.657 |      0.006 |                   13 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.4001865671641791
[2m[36m(func pid=140840)[0m top5: 0.8931902985074627
[2m[36m(func pid=140840)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=140840)[0m f1_macro: 0.3677498099337841
[2m[36m(func pid=140840)[0m f1_weighted: 0.43377007159562786
[2m[36m(func pid=140840)[0m f1_per_class: [0.365, 0.451, 0.558, 0.443, 0.129, 0.141, 0.573, 0.406, 0.161, 0.452]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.36800373134328357
[2m[36m(func pid=159021)[0m top5: 0.8614738805970149
[2m[36m(func pid=159021)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=159021)[0m f1_macro: 0.23781127989795087
[2m[36m(func pid=159021)[0m f1_weighted: 0.36829704722646983
[2m[36m(func pid=159021)[0m f1_per_class: [0.299, 0.292, 0.173, 0.419, 0.12, 0.059, 0.586, 0.085, 0.136, 0.211]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3694029850746269
[2m[36m(func pid=159478)[0m top5: 0.8777985074626866
[2m[36m(func pid=159478)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=159478)[0m f1_macro: 0.36652641529964963
[2m[36m(func pid=159478)[0m f1_weighted: 0.35658250419282495
[2m[36m(func pid=159478)[0m f1_per_class: [0.645, 0.567, 0.348, 0.364, 0.114, 0.278, 0.239, 0.383, 0.354, 0.373]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.03311567164179104
[2m[36m(func pid=160131)[0m top5: 0.5153917910447762
[2m[36m(func pid=160131)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=160131)[0m f1_macro: 0.0064516129032258064
[2m[36m(func pid=160131)[0m f1_weighted: 0.0021364949446316802
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.065, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0183 | Steps: 4 | Val loss: 1.6249 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.1037 | Steps: 4 | Val loss: 1.7832 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0293 | Steps: 4 | Val loss: 2.1443 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.4941 | Steps: 4 | Val loss: 1077.6510 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:16:22 (running for 00:32:58.99)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.172 |      0.368 |                   93 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  1.104 |      0.279 |                   18 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.334 |      0.367 |                   16 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.883 |      0.006 |                   14 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.4468283582089552
[2m[36m(func pid=140840)[0m top5: 0.9169776119402985
[2m[36m(func pid=140840)[0m f1_micro: 0.4468283582089552
[2m[36m(func pid=140840)[0m f1_macro: 0.38897434715565554
[2m[36m(func pid=140840)[0m f1_weighted: 0.4760543833825368
[2m[36m(func pid=140840)[0m f1_per_class: [0.457, 0.462, 0.49, 0.575, 0.117, 0.158, 0.566, 0.434, 0.174, 0.457]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.39598880597014924
[2m[36m(func pid=159021)[0m top5: 0.8833955223880597
[2m[36m(func pid=159021)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=159021)[0m f1_macro: 0.2792665688790544
[2m[36m(func pid=159021)[0m f1_weighted: 0.3990944023719821
[2m[36m(func pid=159021)[0m f1_per_class: [0.342, 0.351, 0.304, 0.486, 0.123, 0.062, 0.562, 0.183, 0.168, 0.211]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3558768656716418
[2m[36m(func pid=159478)[0m top5: 0.8838619402985075
[2m[36m(func pid=159478)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=159478)[0m f1_macro: 0.3587790182095965
[2m[36m(func pid=159478)[0m f1_weighted: 0.34426587923708485
[2m[36m(func pid=159478)[0m f1_per_class: [0.653, 0.601, 0.369, 0.418, 0.116, 0.253, 0.145, 0.357, 0.315, 0.358]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.021455223880597014
[2m[36m(func pid=160131)[0m top5: 0.5405783582089553
[2m[36m(func pid=160131)[0m f1_micro: 0.021455223880597014
[2m[36m(func pid=160131)[0m f1_macro: 0.0051027287714377105
[2m[36m(func pid=160131)[0m f1_weighted: 0.002677156863501199
[2m[36m(func pid=160131)[0m f1_per_class: [0.04, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0240 | Steps: 4 | Val loss: 1.6058 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8560 | Steps: 4 | Val loss: 1.7340 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2666 | Steps: 4 | Val loss: 2.4183 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 4.1972 | Steps: 4 | Val loss: 2076.3340 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:16:27 (running for 00:33:04.46)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.018 |      0.389 |                   94 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.856 |      0.307 |                   19 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.029 |      0.359 |                   17 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.494 |      0.005 |                   15 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.4603544776119403
[2m[36m(func pid=140840)[0m top5: 0.9188432835820896
[2m[36m(func pid=140840)[0m f1_micro: 0.4603544776119403
[2m[36m(func pid=140840)[0m f1_macro: 0.40403178060538086
[2m[36m(func pid=140840)[0m f1_weighted: 0.4829816098876806
[2m[36m(func pid=140840)[0m f1_per_class: [0.537, 0.483, 0.545, 0.61, 0.119, 0.164, 0.536, 0.428, 0.189, 0.429]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.404384328358209
[2m[36m(func pid=159021)[0m top5: 0.9067164179104478
[2m[36m(func pid=159021)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=159021)[0m f1_macro: 0.3074826361698678
[2m[36m(func pid=159021)[0m f1_weighted: 0.4146223217187224
[2m[36m(func pid=159021)[0m f1_per_class: [0.419, 0.422, 0.375, 0.504, 0.114, 0.097, 0.524, 0.242, 0.186, 0.192]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.332089552238806
[2m[36m(func pid=159478)[0m top5: 0.8698694029850746
[2m[36m(func pid=159478)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=159478)[0m f1_macro: 0.3406558417045408
[2m[36m(func pid=159478)[0m f1_weighted: 0.32445427792246645
[2m[36m(func pid=159478)[0m f1_per_class: [0.604, 0.603, 0.286, 0.423, 0.095, 0.233, 0.088, 0.356, 0.281, 0.438]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.01585820895522388
[2m[36m(func pid=160131)[0m top5: 0.43423507462686567
[2m[36m(func pid=160131)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=160131)[0m f1_macro: 0.00815703598783372
[2m[36m(func pid=160131)[0m f1_weighted: 0.0038276842321305913
[2m[36m(func pid=160131)[0m f1_per_class: [0.049, 0.016, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8852 | Steps: 4 | Val loss: 1.7217 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2666 | Steps: 4 | Val loss: 1.6166 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1219 | Steps: 4 | Val loss: 2.7309 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.6389 | Steps: 4 | Val loss: 2584.6841 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=159021)[0m top1: 0.3894589552238806
[2m[36m(func pid=159021)[0m top5: 0.9165111940298507
[2m[36m(func pid=159021)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=159021)[0m f1_macro: 0.32795350313428173
[2m[36m(func pid=159021)[0m f1_weighted: 0.4068796381841211
[2m[36m(func pid=159021)[0m f1_per_class: [0.412, 0.45, 0.5, 0.478, 0.12, 0.133, 0.477, 0.297, 0.2, 0.212]
== Status ==
Current time: 2024-01-07 14:16:33 (running for 00:33:09.97)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.024 |      0.404 |                   95 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.885 |      0.328 |                   20 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.267 |      0.341 |                   18 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.197 |      0.008 |                   16 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.466884328358209
[2m[36m(func pid=140840)[0m top5: 0.9127798507462687
[2m[36m(func pid=140840)[0m f1_micro: 0.46688432835820903
[2m[36m(func pid=140840)[0m f1_macro: 0.40782101988509734
[2m[36m(func pid=140840)[0m f1_weighted: 0.48319745847928947
[2m[36m(func pid=140840)[0m f1_per_class: [0.571, 0.514, 0.5, 0.616, 0.12, 0.132, 0.518, 0.444, 0.199, 0.464]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3180970149253731
[2m[36m(func pid=159478)[0m top5: 0.8796641791044776
[2m[36m(func pid=159478)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=159478)[0m f1_macro: 0.3306368259013929
[2m[36m(func pid=159478)[0m f1_weighted: 0.31623278644556063
[2m[36m(func pid=159478)[0m f1_per_class: [0.611, 0.482, 0.321, 0.497, 0.092, 0.251, 0.063, 0.332, 0.244, 0.415]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.03404850746268657
[2m[36m(func pid=160131)[0m top5: 0.5083955223880597
[2m[36m(func pid=160131)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=160131)[0m f1_macro: 0.008951515151515151
[2m[36m(func pid=160131)[0m f1_weighted: 0.005929528493894166
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.068, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8019 | Steps: 4 | Val loss: 1.6929 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0493 | Steps: 4 | Val loss: 1.5555 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2966 | Steps: 4 | Val loss: 2.8469 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 4.3129 | Steps: 4 | Val loss: 1362.6520 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:16:38 (running for 00:33:15.43)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.267 |      0.408 |                   96 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.802 |      0.347 |                   21 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.122 |      0.331 |                   19 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.639 |      0.009 |                   17 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.39132462686567165
[2m[36m(func pid=159021)[0m top5: 0.9202425373134329
[2m[36m(func pid=159021)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=159021)[0m f1_macro: 0.347495376913178
[2m[36m(func pid=159021)[0m f1_weighted: 0.40683812156791593
[2m[36m(func pid=159021)[0m f1_per_class: [0.4, 0.476, 0.632, 0.488, 0.122, 0.173, 0.424, 0.355, 0.216, 0.19]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.458955223880597
[2m[36m(func pid=140840)[0m top5: 0.9249067164179104
[2m[36m(func pid=140840)[0m f1_micro: 0.458955223880597
[2m[36m(func pid=140840)[0m f1_macro: 0.4064911440363451
[2m[36m(func pid=140840)[0m f1_weighted: 0.4796894618711742
[2m[36m(func pid=140840)[0m f1_per_class: [0.533, 0.534, 0.522, 0.605, 0.133, 0.215, 0.483, 0.414, 0.198, 0.429]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3208955223880597
[2m[36m(func pid=159478)[0m top5: 0.8694029850746269
[2m[36m(func pid=159478)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=159478)[0m f1_macro: 0.3063436263886993
[2m[36m(func pid=159478)[0m f1_weighted: 0.3131863871348512
[2m[36m(func pid=159478)[0m f1_per_class: [0.556, 0.389, 0.292, 0.545, 0.1, 0.256, 0.08, 0.299, 0.153, 0.392]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.016791044776119403
[2m[36m(func pid=160131)[0m top5: 0.4244402985074627
[2m[36m(func pid=160131)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=160131)[0m f1_macro: 0.012001962169735595
[2m[36m(func pid=160131)[0m f1_weighted: 0.018688328348625667
[2m[36m(func pid=160131)[0m f1_per_class: [0.013, 0.011, 0.015, 0.04, 0.0, 0.0, 0.015, 0.0, 0.027, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7708 | Steps: 4 | Val loss: 1.6954 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0244 | Steps: 4 | Val loss: 1.5593 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8070 | Steps: 4 | Val loss: 3.3217 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 6.2995 | Steps: 4 | Val loss: 745.0767 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:16:44 (running for 00:33:20.92)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.049 |      0.406 |                   97 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.771 |      0.341 |                   22 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.297 |      0.306 |                   20 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.313 |      0.012 |                   18 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.3787313432835821
[2m[36m(func pid=159021)[0m top5: 0.9048507462686567
[2m[36m(func pid=159021)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=159021)[0m f1_macro: 0.34110364237412755
[2m[36m(func pid=159021)[0m f1_weighted: 0.3754290803560428
[2m[36m(func pid=159021)[0m f1_per_class: [0.412, 0.496, 0.615, 0.516, 0.122, 0.165, 0.278, 0.356, 0.255, 0.196]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.4566231343283582
[2m[36m(func pid=140840)[0m top5: 0.9239738805970149
[2m[36m(func pid=140840)[0m f1_micro: 0.4566231343283582
[2m[36m(func pid=140840)[0m f1_macro: 0.41454344349016237
[2m[36m(func pid=140840)[0m f1_weighted: 0.48092452458279156
[2m[36m(func pid=140840)[0m f1_per_class: [0.565, 0.533, 0.533, 0.607, 0.14, 0.259, 0.463, 0.426, 0.208, 0.411]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159478)[0m top1: 0.30597014925373134
[2m[36m(func pid=159478)[0m top5: 0.8642723880597015
[2m[36m(func pid=159478)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=159478)[0m f1_macro: 0.3111878324713304
[2m[36m(func pid=159478)[0m f1_weighted: 0.30609660087450274
[2m[36m(func pid=159478)[0m f1_per_class: [0.604, 0.445, 0.419, 0.58, 0.058, 0.106, 0.045, 0.288, 0.157, 0.408]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.06389925373134328
[2m[36m(func pid=160131)[0m top5: 0.3978544776119403
[2m[36m(func pid=160131)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=160131)[0m f1_macro: 0.048583326835373086
[2m[36m(func pid=160131)[0m f1_weighted: 0.04053668365648613
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.045, 0.02, 0.0, 0.122, 0.267, 0.0, 0.008, 0.024, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.1221 | Steps: 4 | Val loss: 1.6446 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5900 | Steps: 4 | Val loss: 1.6689 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0676 | Steps: 4 | Val loss: 3.5486 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 14.1308 | Steps: 4 | Val loss: 2016.2303 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 14:16:49 (running for 00:33:26.38)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.122 |      0.398 |                   99 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.771 |      0.341 |                   22 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.807 |      0.311 |                   21 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  6.299 |      0.049 |                   19 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140840)[0m top1: 0.4295708955223881
[2m[36m(func pid=140840)[0m top5: 0.9104477611940298
[2m[36m(func pid=140840)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=140840)[0m f1_macro: 0.3977970817482328
[2m[36m(func pid=140840)[0m f1_weighted: 0.4422053736427177
[2m[36m(func pid=140840)[0m f1_per_class: [0.554, 0.486, 0.49, 0.626, 0.135, 0.269, 0.34, 0.424, 0.207, 0.448]
[2m[36m(func pid=140840)[0m 
[2m[36m(func pid=159021)[0m top1: 0.3885261194029851
[2m[36m(func pid=159021)[0m top5: 0.9104477611940298
[2m[36m(func pid=159021)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=159021)[0m f1_macro: 0.35452390918740545
[2m[36m(func pid=159021)[0m f1_weighted: 0.372837226980798
[2m[36m(func pid=159021)[0m f1_per_class: [0.405, 0.509, 0.686, 0.526, 0.155, 0.188, 0.24, 0.348, 0.283, 0.206]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3306902985074627
[2m[36m(func pid=159478)[0m top5: 0.8927238805970149
[2m[36m(func pid=159478)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=159478)[0m f1_macro: 0.3509060412356842
[2m[36m(func pid=159478)[0m f1_weighted: 0.3152260591667062
[2m[36m(func pid=159478)[0m f1_per_class: [0.667, 0.356, 0.636, 0.6, 0.054, 0.036, 0.12, 0.266, 0.202, 0.571]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.07789179104477612
[2m[36m(func pid=160131)[0m top5: 0.5443097014925373
[2m[36m(func pid=160131)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=160131)[0m f1_macro: 0.06292583015946207
[2m[36m(func pid=160131)[0m f1_weighted: 0.047411377485204666
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.141, 0.026, 0.0, 0.175, 0.09, 0.0, 0.197, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=140840)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0138 | Steps: 4 | Val loss: 1.5287 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6115 | Steps: 4 | Val loss: 1.6589 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7385 | Steps: 4 | Val loss: 3.9085 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 11.8060 | Steps: 4 | Val loss: 2384.3809 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:16:54 (running for 00:33:31.60)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00013 | RUNNING    | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.122 |      0.398 |                   99 |
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.612 |      0.366 |                   24 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.068 |      0.351 |                   22 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 14.131 |      0.063 |                   20 |
| train_5806f_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.38992537313432835
[2m[36m(func pid=159021)[0m top5: 0.9057835820895522
[2m[36m(func pid=159021)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=159021)[0m f1_macro: 0.36550288667279746
[2m[36m(func pid=159021)[0m f1_weighted: 0.3697757699473944
[2m[36m(func pid=159021)[0m f1_per_class: [0.415, 0.525, 0.75, 0.533, 0.179, 0.193, 0.207, 0.35, 0.304, 0.2]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=140840)[0m top1: 0.45988805970149255
[2m[36m(func pid=140840)[0m top5: 0.9244402985074627
[2m[36m(func pid=140840)[0m f1_micro: 0.45988805970149255
[2m[36m(func pid=140840)[0m f1_macro: 0.4034260482072557
[2m[36m(func pid=140840)[0m f1_weighted: 0.4708118219937742
[2m[36m(func pid=140840)[0m f1_per_class: [0.556, 0.418, 0.453, 0.641, 0.167, 0.297, 0.457, 0.378, 0.227, 0.441]
[2m[36m(func pid=159478)[0m top1: 0.33861940298507465
[2m[36m(func pid=159478)[0m top5: 0.8913246268656716
[2m[36m(func pid=159478)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=159478)[0m f1_macro: 0.31327417479953196
[2m[36m(func pid=159478)[0m f1_weighted: 0.33166709760757973
[2m[36m(func pid=159478)[0m f1_per_class: [0.632, 0.355, 0.267, 0.594, 0.056, 0.027, 0.199, 0.251, 0.194, 0.558]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.3185634328358209
[2m[36m(func pid=160131)[0m top5: 0.590018656716418
[2m[36m(func pid=160131)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=160131)[0m f1_macro: 0.11250440476539528
[2m[36m(func pid=160131)[0m f1_weighted: 0.23626661939104565
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.392, 0.009, 0.0, 0.0, 0.0, 0.528, 0.196, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4496 | Steps: 4 | Val loss: 1.6290 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3949 | Steps: 4 | Val loss: 3.6720 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=159021)[0m top1: 0.39598880597014924
[2m[36m(func pid=159021)[0m top5: 0.9118470149253731
[2m[36m(func pid=159021)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=159021)[0m f1_macro: 0.3722519843928949
[2m[36m(func pid=159021)[0m f1_weighted: 0.36671357995303194
[2m[36m(func pid=159021)[0m f1_per_class: [0.479, 0.559, 0.71, 0.534, 0.165, 0.198, 0.169, 0.342, 0.311, 0.255]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 8.2143 | Steps: 4 | Val loss: 338.0440 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=159478)[0m top1: 0.3628731343283582
[2m[36m(func pid=159478)[0m top5: 0.8605410447761194
[2m[36m(func pid=159478)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=159478)[0m f1_macro: 0.3645083268330515
[2m[36m(func pid=159478)[0m f1_weighted: 0.36639355282949077
[2m[36m(func pid=159478)[0m f1_per_class: [0.674, 0.33, 0.645, 0.603, 0.067, 0.014, 0.318, 0.237, 0.205, 0.553]
[2m[36m(func pid=160131)[0m top1: 0.31529850746268656
[2m[36m(func pid=160131)[0m top5: 0.6352611940298507
[2m[36m(func pid=160131)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=160131)[0m f1_macro: 0.09789488810535249
[2m[36m(func pid=160131)[0m f1_weighted: 0.22603095676516957
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.323, 0.059, 0.045, 0.0, 0.0, 0.526, 0.0, 0.026, 0.0]
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4418 | Steps: 4 | Val loss: 1.6145 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:16:59 (running for 00:33:36.86)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.45  |      0.372 |                   25 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.738 |      0.313 |                   23 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 11.806 |      0.113 |                   21 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=165522)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=165522)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=165522)[0m Configuration completed!
[2m[36m(func pid=165522)[0m New optimizer parameters:
[2m[36m(func pid=165522)[0m SGD (
[2m[36m(func pid=165522)[0m Parameter Group 0
[2m[36m(func pid=165522)[0m     dampening: 0
[2m[36m(func pid=165522)[0m     differentiable: False
[2m[36m(func pid=165522)[0m     foreach: None
[2m[36m(func pid=165522)[0m     lr: 0.1
[2m[36m(func pid=165522)[0m     maximize: False
[2m[36m(func pid=165522)[0m     momentum: 0.99
[2m[36m(func pid=165522)[0m     nesterov: False
[2m[36m(func pid=165522)[0m     weight_decay: 1e-05
[2m[36m(func pid=165522)[0m )
[2m[36m(func pid=165522)[0m 
== Status ==
Current time: 2024-01-07 14:17:05 (running for 00:33:42.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.442 |      0.377 |                   26 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.395 |      0.365 |                   24 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  8.214 |      0.098 |                   22 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.3978544776119403
[2m[36m(func pid=159021)[0m top5: 0.9155783582089553
[2m[36m(func pid=159021)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=159021)[0m f1_macro: 0.3772203886812195
[2m[36m(func pid=159021)[0m f1_weighted: 0.36298989340496196
[2m[36m(func pid=159021)[0m f1_per_class: [0.515, 0.578, 0.71, 0.533, 0.175, 0.207, 0.141, 0.348, 0.299, 0.268]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.0199 | Steps: 4 | Val loss: 3.2209 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 7.2660 | Steps: 4 | Val loss: 187.8435 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4793 | Steps: 4 | Val loss: 1.6144 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.4099 | Steps: 4 | Val loss: 85.6291 | Batch size: 32 | lr: 0.1 | Duration: 4.76s
[2m[36m(func pid=159478)[0m top1: 0.394589552238806
[2m[36m(func pid=159478)[0m top5: 0.8236940298507462
[2m[36m(func pid=159478)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=159478)[0m f1_macro: 0.3224908242245251
[2m[36m(func pid=159478)[0m f1_weighted: 0.39638597832363553
[2m[36m(func pid=159478)[0m f1_per_class: [0.468, 0.493, 0.369, 0.585, 0.08, 0.015, 0.36, 0.28, 0.192, 0.383]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.10587686567164178
[2m[36m(func pid=160131)[0m top5: 0.6651119402985075
[2m[36m(func pid=160131)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=160131)[0m f1_macro: 0.05616136176805884
[2m[36m(func pid=160131)[0m f1_weighted: 0.07380730960854077
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.005, 0.042, 0.196, 0.0, 0.037, 0.0, 0.18, 0.102, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.39225746268656714
[2m[36m(func pid=159021)[0m top5: 0.914179104477612
[2m[36m(func pid=159021)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=159021)[0m f1_macro: 0.3805324604640408
[2m[36m(func pid=159021)[0m f1_weighted: 0.3647491709160718
[2m[36m(func pid=159021)[0m f1_per_class: [0.515, 0.571, 0.71, 0.537, 0.162, 0.209, 0.143, 0.343, 0.33, 0.286]
[2m[36m(func pid=159021)[0m 
== Status ==
Current time: 2024-01-07 14:17:10 (running for 00:33:47.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.479 |      0.381 |                   27 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.02  |      0.322 |                   25 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  7.266 |      0.056 |                   23 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165522)[0m top1: 0.041044776119402986
[2m[36m(func pid=165522)[0m top5: 0.3829291044776119
[2m[36m(func pid=165522)[0m f1_micro: 0.041044776119402986
[2m[36m(func pid=165522)[0m f1_macro: 0.04328640544127927
[2m[36m(func pid=165522)[0m f1_weighted: 0.022332441079163923
[2m[36m(func pid=165522)[0m f1_per_class: [0.044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.366, 0.0, 0.023]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1812 | Steps: 4 | Val loss: 3.1031 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 4.7431 | Steps: 4 | Val loss: 8.3425 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5849 | Steps: 4 | Val loss: 1.5939 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 18.4656 | Steps: 4 | Val loss: 195166384.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=160131)[0m top1: 0.11613805970149253
[2m[36m(func pid=160131)[0m top5: 0.6744402985074627
[2m[36m(func pid=160131)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=160131)[0m f1_macro: 0.09567175425027843
[2m[36m(func pid=160131)[0m f1_weighted: 0.04586554706288101
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.5, 0.032, 0.0, 0.164, 0.0, 0.26, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159478)[0m top1: 0.3773320895522388
[2m[36m(func pid=159478)[0m top5: 0.8540111940298507
[2m[36m(func pid=159478)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=159478)[0m f1_macro: 0.30783674093330027
[2m[36m(func pid=159478)[0m f1_weighted: 0.3608183980829818
[2m[36m(func pid=159478)[0m f1_per_class: [0.547, 0.499, 0.333, 0.518, 0.11, 0.052, 0.277, 0.341, 0.168, 0.234]
[2m[36m(func pid=159478)[0m 
== Status ==
Current time: 2024-01-07 14:17:16 (running for 00:33:53.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.585 |      0.387 |                   28 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.181 |      0.308 |                   26 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.743 |      0.096 |                   24 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.41  |      0.043 |                    1 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.394589552238806
[2m[36m(func pid=159021)[0m top5: 0.9277052238805971
[2m[36m(func pid=159021)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=159021)[0m f1_macro: 0.38698737237483916
[2m[36m(func pid=159021)[0m f1_weighted: 0.36979750278069007
[2m[36m(func pid=159021)[0m f1_per_class: [0.571, 0.555, 0.733, 0.564, 0.162, 0.247, 0.131, 0.326, 0.313, 0.268]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.007462686567164179
[2m[36m(func pid=165522)[0m top5: 0.5107276119402985
[2m[36m(func pid=165522)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=165522)[0m f1_macro: 0.0014814814814814816
[2m[36m(func pid=165522)[0m f1_weighted: 0.0001105583195135434
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1756 | Steps: 4 | Val loss: 3.5607 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 11.6003 | Steps: 4 | Val loss: 7.4705 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3083 | Steps: 4 | Val loss: 1.6083 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 48.4806 | Steps: 4 | Val loss: 473815040.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=159478)[0m top1: 0.3208955223880597
[2m[36m(func pid=159478)[0m top5: 0.8460820895522388
[2m[36m(func pid=159478)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=159478)[0m f1_macro: 0.272862202939393
[2m[36m(func pid=159478)[0m f1_weighted: 0.3094287732095221
[2m[36m(func pid=159478)[0m f1_per_class: [0.538, 0.472, 0.353, 0.423, 0.087, 0.107, 0.213, 0.257, 0.114, 0.164]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.09981343283582089
[2m[36m(func pid=160131)[0m top5: 0.5214552238805971
[2m[36m(func pid=160131)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=160131)[0m f1_macro: 0.050550082545112104
[2m[36m(func pid=160131)[0m f1_weighted: 0.036626988826089724
[2m[36m(func pid=160131)[0m f1_per_class: [0.005, 0.0, 0.0, 0.0, 0.0, 0.149, 0.0, 0.317, 0.035, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:21 (running for 00:33:58.49)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.308 |      0.38  |                   29 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.176 |      0.273 |                   27 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 11.6   |      0.051 |                   25 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  | 18.466 |      0.001 |                    2 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.3880597014925373
[2m[36m(func pid=159021)[0m top5: 0.9207089552238806
[2m[36m(func pid=159021)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=159021)[0m f1_macro: 0.38004267226428123
[2m[36m(func pid=159021)[0m f1_weighted: 0.34926477772599074
[2m[36m(func pid=159021)[0m f1_per_class: [0.567, 0.574, 0.733, 0.564, 0.167, 0.233, 0.057, 0.329, 0.306, 0.271]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0320 | Steps: 4 | Val loss: 3.7479 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 12.7102 | Steps: 4 | Val loss: 9.4682 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4438 | Steps: 4 | Val loss: 1.6364 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 40.3443 | Steps: 4 | Val loss: 2456303872.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=159478)[0m top1: 0.322294776119403
[2m[36m(func pid=159478)[0m top5: 0.855410447761194
[2m[36m(func pid=159478)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=159478)[0m f1_macro: 0.2755847508222274
[2m[36m(func pid=159478)[0m f1_weighted: 0.30745127046965226
[2m[36m(func pid=159478)[0m f1_per_class: [0.495, 0.446, 0.338, 0.433, 0.167, 0.21, 0.182, 0.205, 0.152, 0.129]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.06343283582089553
[2m[36m(func pid=160131)[0m top5: 0.46548507462686567
[2m[36m(func pid=160131)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=160131)[0m f1_macro: 0.021537317949771477
[2m[36m(func pid=160131)[0m f1_weighted: 0.010817366975503076
[2m[36m(func pid=160131)[0m f1_per_class: [0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.153, 0.056, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:26 (running for 00:34:03.84)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.444 |      0.383 |                   30 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.032 |      0.276 |                   28 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 12.71  |      0.022 |                   26 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  | 48.481 |      0.011 |                    3 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.3908582089552239
[2m[36m(func pid=159021)[0m top5: 0.9053171641791045
[2m[36m(func pid=159021)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=159021)[0m f1_macro: 0.3828304400556678
[2m[36m(func pid=159021)[0m f1_weighted: 0.3519297622337946
[2m[36m(func pid=159021)[0m f1_per_class: [0.571, 0.603, 0.632, 0.542, 0.151, 0.222, 0.063, 0.352, 0.366, 0.327]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2521 | Steps: 4 | Val loss: 4.1318 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 7.2424 | Steps: 4 | Val loss: 10.5385 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2692 | Steps: 4 | Val loss: 1.6201 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 23.3390 | Steps: 4 | Val loss: 27270658.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=159478)[0m top1: 0.2989738805970149
[2m[36m(func pid=159478)[0m top5: 0.8339552238805971
[2m[36m(func pid=159478)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=159478)[0m f1_macro: 0.253906451207863
[2m[36m(func pid=159478)[0m f1_weighted: 0.2866979466431581
[2m[36m(func pid=159478)[0m f1_per_class: [0.495, 0.439, 0.183, 0.385, 0.125, 0.146, 0.173, 0.286, 0.149, 0.158]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.06436567164179105
[2m[36m(func pid=160131)[0m top5: 0.4123134328358209
[2m[36m(func pid=160131)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=160131)[0m f1_macro: 0.02211913888507099
[2m[36m(func pid=160131)[0m f1_weighted: 0.009317013723399012
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.056, 0.0, 0.0, 0.138, 0.027, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:32 (running for 00:34:09.09)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.269 |      0.375 |                   31 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.252 |      0.254 |                   29 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  7.242 |      0.022 |                   27 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  | 40.344 |      0.011 |                    4 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.3880597014925373
[2m[36m(func pid=159021)[0m top5: 0.9090485074626866
[2m[36m(func pid=159021)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=159021)[0m f1_macro: 0.3750701373942032
[2m[36m(func pid=159021)[0m f1_weighted: 0.3530991595741837
[2m[36m(func pid=159021)[0m f1_per_class: [0.559, 0.586, 0.585, 0.54, 0.149, 0.227, 0.079, 0.358, 0.346, 0.321]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.29850746268656714
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=165522)[0m f1_macro: 0.04869441858403156
[2m[36m(func pid=165522)[0m f1_weighted: 0.137871124494564
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46, 0.0, 0.027, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0949 | Steps: 4 | Val loss: 4.5718 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 10.8759 | Steps: 4 | Val loss: 6.4947 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5047 | Steps: 4 | Val loss: 1.5977 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 11.4492 | Steps: 4 | Val loss: 4472948.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=159478)[0m top1: 0.29757462686567165
[2m[36m(func pid=159478)[0m top5: 0.7761194029850746
[2m[36m(func pid=159478)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=159478)[0m f1_macro: 0.2584859112153365
[2m[36m(func pid=159478)[0m f1_weighted: 0.31367355438149935
[2m[36m(func pid=159478)[0m f1_per_class: [0.524, 0.381, 0.108, 0.375, 0.148, 0.171, 0.295, 0.289, 0.145, 0.147]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.08208955223880597
[2m[36m(func pid=160131)[0m top5: 0.6982276119402985
[2m[36m(func pid=160131)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=160131)[0m f1_macro: 0.09237206224774683
[2m[36m(func pid=160131)[0m f1_weighted: 0.038725373248232955
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.131, 0.522, 0.0, 0.029, 0.0, 0.0, 0.216, 0.0, 0.026]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:37 (running for 00:34:14.57)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.505 |      0.379 |                   32 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.095 |      0.258 |                   30 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 10.876 |      0.092 |                   28 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  | 23.339 |      0.049 |                    5 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4039179104477612
[2m[36m(func pid=159021)[0m top5: 0.909981343283582
[2m[36m(func pid=159021)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=159021)[0m f1_macro: 0.37902308082833924
[2m[36m(func pid=159021)[0m f1_weighted: 0.37473826545491007
[2m[36m(func pid=159021)[0m f1_per_class: [0.579, 0.613, 0.49, 0.526, 0.138, 0.22, 0.144, 0.401, 0.354, 0.327]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4678 | Steps: 4 | Val loss: 5.2279 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 14.4520 | Steps: 4 | Val loss: 4.4631 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2469 | Steps: 4 | Val loss: 1.5620 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 10.4181 | Steps: 4 | Val loss: 2724912.7500 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=159478)[0m top1: 0.2523320895522388
[2m[36m(func pid=159478)[0m top5: 0.7406716417910447
[2m[36m(func pid=159478)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=159478)[0m f1_macro: 0.233830131734388
[2m[36m(func pid=159478)[0m f1_weighted: 0.285138953097596
[2m[36m(func pid=159478)[0m f1_per_class: [0.482, 0.292, 0.095, 0.324, 0.207, 0.202, 0.311, 0.196, 0.126, 0.104]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.17024253731343283
[2m[36m(func pid=160131)[0m top5: 0.7714552238805971
[2m[36m(func pid=160131)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=160131)[0m f1_macro: 0.10475443796767692
[2m[36m(func pid=160131)[0m f1_weighted: 0.081982613451156
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.366, 0.19, 0.0, 0.068, 0.02, 0.0, 0.222, 0.0, 0.18]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:43 (running for 00:34:20.01)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.247 |      0.385 |                   33 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.468 |      0.234 |                   31 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 14.452 |      0.105 |                   29 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  | 11.449 |      0.011 |                    6 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4118470149253731
[2m[36m(func pid=159021)[0m top5: 0.9239738805970149
[2m[36m(func pid=159021)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=159021)[0m f1_macro: 0.3847496045159258
[2m[36m(func pid=159021)[0m f1_weighted: 0.3955514195834636
[2m[36m(func pid=159021)[0m f1_per_class: [0.584, 0.593, 0.522, 0.553, 0.127, 0.221, 0.201, 0.4, 0.342, 0.304]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6041 | Steps: 4 | Val loss: 5.4641 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 4.6323 | Steps: 4 | Val loss: 6.9527 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3135 | Steps: 4 | Val loss: 1.5608 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=159478)[0m top1: 0.22201492537313433
[2m[36m(func pid=159478)[0m top5: 0.7168843283582089
[2m[36m(func pid=159478)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=159478)[0m f1_macro: 0.23436522699839918
[2m[36m(func pid=159478)[0m f1_weighted: 0.25848483025410085
[2m[36m(func pid=159478)[0m f1_per_class: [0.495, 0.22, 0.16, 0.36, 0.385, 0.207, 0.243, 0.102, 0.105, 0.067]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 7.3260 | Steps: 4 | Val loss: 746978.8125 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=160131)[0m top1: 0.37406716417910446
[2m[36m(func pid=160131)[0m top5: 0.7817164179104478
[2m[36m(func pid=160131)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=160131)[0m f1_macro: 0.10992950685091585
[2m[36m(func pid=160131)[0m f1_weighted: 0.2964913624288492
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.005, 0.073, 0.476, 0.0, 0.0, 0.545, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:48 (running for 00:34:25.32)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.313 |      0.389 |                   34 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.604 |      0.234 |                   32 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.632 |      0.11  |                   30 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  | 10.418 |      0.011 |                    7 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4076492537313433
[2m[36m(func pid=159021)[0m top5: 0.9253731343283582
[2m[36m(func pid=159021)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=159021)[0m f1_macro: 0.38931124477135093
[2m[36m(func pid=159021)[0m f1_weighted: 0.3895886000223224
[2m[36m(func pid=159021)[0m f1_per_class: [0.646, 0.613, 0.48, 0.549, 0.124, 0.209, 0.174, 0.393, 0.335, 0.369]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5398 | Steps: 4 | Val loss: 5.9519 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 8.9594 | Steps: 4 | Val loss: 13.3452 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2188 | Steps: 4 | Val loss: 1.5637 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=159478)[0m top1: 0.1571828358208955
[2m[36m(func pid=159478)[0m top5: 0.7196828358208955
[2m[36m(func pid=159478)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=159478)[0m f1_macro: 0.21037109946257554
[2m[36m(func pid=159478)[0m f1_weighted: 0.1775711890150897
[2m[36m(func pid=159478)[0m f1_per_class: [0.467, 0.212, 0.415, 0.286, 0.286, 0.191, 0.066, 0.016, 0.118, 0.047]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 8.2393 | Steps: 4 | Val loss: 203897.1406 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=160131)[0m top1: 0.26725746268656714
[2m[36m(func pid=160131)[0m top5: 0.7555970149253731
[2m[36m(func pid=160131)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=160131)[0m f1_macro: 0.10746895439834696
[2m[36m(func pid=160131)[0m f1_weighted: 0.24869642217409177
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.056, 0.451, 0.0, 0.254, 0.313, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:53 (running for 00:34:30.81)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.219 |      0.398 |                   35 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.54  |      0.21  |                   33 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  8.959 |      0.107 |                   31 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  7.326 |      0.011 |                    8 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4099813432835821
[2m[36m(func pid=159021)[0m top5: 0.9291044776119403
[2m[36m(func pid=159021)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=159021)[0m f1_macro: 0.39843029133163743
[2m[36m(func pid=159021)[0m f1_weighted: 0.3974346676112961
[2m[36m(func pid=159021)[0m f1_per_class: [0.667, 0.614, 0.511, 0.545, 0.118, 0.223, 0.197, 0.385, 0.324, 0.4]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.29617537313432835
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=165522)[0m f1_macro: 0.047224627224627225
[2m[36m(func pid=165522)[0m f1_weighted: 0.1371901764625645
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.457, 0.015, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4401 | Steps: 4 | Val loss: 6.5382 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 6.4169 | Steps: 4 | Val loss: 54.0979 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.1706 | Steps: 4 | Val loss: 1.5432 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=159478)[0m top1: 0.11007462686567164
[2m[36m(func pid=159478)[0m top5: 0.7341417910447762
[2m[36m(func pid=159478)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=159478)[0m f1_macro: 0.18005847465741082
[2m[36m(func pid=159478)[0m f1_weighted: 0.11599930017734672
[2m[36m(func pid=159478)[0m f1_per_class: [0.509, 0.154, 0.296, 0.144, 0.231, 0.184, 0.009, 0.13, 0.102, 0.039]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 10.7861 | Steps: 4 | Val loss: 153637.8281 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=160131)[0m top1: 0.21828358208955223
[2m[36m(func pid=160131)[0m top5: 0.6786380597014925
[2m[36m(func pid=160131)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=160131)[0m f1_macro: 0.08958702244730483
[2m[36m(func pid=160131)[0m f1_weighted: 0.16917125675665254
[2m[36m(func pid=160131)[0m f1_per_class: [0.121, 0.0, 0.051, 0.508, 0.0, 0.215, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:17:59 (running for 00:34:36.32)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.171 |      0.401 |                   36 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.44  |      0.18  |                   34 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  6.417 |      0.09  |                   32 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  8.239 |      0.047 |                    9 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4230410447761194
[2m[36m(func pid=159021)[0m top5: 0.9323694029850746
[2m[36m(func pid=159021)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=159021)[0m f1_macro: 0.4009534387670045
[2m[36m(func pid=159021)[0m f1_weighted: 0.42710728990979074
[2m[36m(func pid=159021)[0m f1_per_class: [0.674, 0.615, 0.453, 0.531, 0.115, 0.229, 0.309, 0.385, 0.327, 0.371]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4980 | Steps: 4 | Val loss: 6.3908 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 5.3207 | Steps: 4 | Val loss: 538.7504 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3833 | Steps: 4 | Val loss: 1.5382 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=159478)[0m top1: 0.15904850746268656
[2m[36m(func pid=159478)[0m top5: 0.7066231343283582
[2m[36m(func pid=159478)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=159478)[0m f1_macro: 0.24090611791387792
[2m[36m(func pid=159478)[0m f1_weighted: 0.14460727180909877
[2m[36m(func pid=159478)[0m f1_per_class: [0.432, 0.362, 0.381, 0.045, 0.45, 0.226, 0.012, 0.414, 0.039, 0.048]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 9.5036 | Steps: 4 | Val loss: 98251.4375 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=160131)[0m top1: 0.10914179104477612
[2m[36m(func pid=160131)[0m top5: 0.5261194029850746
[2m[36m(func pid=160131)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=160131)[0m f1_macro: 0.07190636172416202
[2m[36m(func pid=160131)[0m f1_weighted: 0.09502146110701408
[2m[36m(func pid=160131)[0m f1_per_class: [0.051, 0.0, 0.0, 0.233, 0.0, 0.069, 0.0, 0.366, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:04 (running for 00:34:41.79)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.383 |      0.405 |                   37 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.498 |      0.241 |                   35 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  5.321 |      0.072 |                   33 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  | 10.786 |      0.011 |                   10 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.43236940298507465
[2m[36m(func pid=159021)[0m top5: 0.9351679104477612
[2m[36m(func pid=159021)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=159021)[0m f1_macro: 0.4049888239659495
[2m[36m(func pid=159021)[0m f1_weighted: 0.4390968941967644
[2m[36m(func pid=159021)[0m f1_per_class: [0.646, 0.621, 0.444, 0.546, 0.111, 0.217, 0.339, 0.377, 0.307, 0.441]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4361 | Steps: 4 | Val loss: 6.5866 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 13.5954 | Steps: 4 | Val loss: 826.0792 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.1261 | Steps: 4 | Val loss: 1.5012 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=159478)[0m top1: 0.17397388059701493
[2m[36m(func pid=159478)[0m top5: 0.6693097014925373
[2m[36m(func pid=159478)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=159478)[0m f1_macro: 0.19813088778737187
[2m[36m(func pid=159478)[0m f1_weighted: 0.15311387083299152
[2m[36m(func pid=159478)[0m f1_per_class: [0.309, 0.442, 0.119, 0.023, 0.328, 0.231, 0.03, 0.415, 0.036, 0.048]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 8.3817 | Steps: 4 | Val loss: 25666.6875 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=160131)[0m top1: 0.06809701492537314
[2m[36m(func pid=160131)[0m top5: 0.4542910447761194
[2m[36m(func pid=160131)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=160131)[0m f1_macro: 0.05272954481671745
[2m[36m(func pid=160131)[0m f1_weighted: 0.043968786547937795
[2m[36m(func pid=160131)[0m f1_per_class: [0.105, 0.0, 0.0, 0.087, 0.0, 0.0, 0.0, 0.298, 0.0, 0.038]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:10 (running for 00:34:47.09)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.126 |      0.403 |                   38 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.436 |      0.198 |                   36 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 13.595 |      0.053 |                   34 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  9.504 |      0.011 |                   11 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4454291044776119
[2m[36m(func pid=159021)[0m top5: 0.9370335820895522
[2m[36m(func pid=159021)[0m f1_micro: 0.4454291044776119
[2m[36m(func pid=159021)[0m f1_macro: 0.40270077116950426
[2m[36m(func pid=159021)[0m f1_weighted: 0.45823153813736833
[2m[36m(func pid=159021)[0m f1_per_class: [0.653, 0.618, 0.407, 0.552, 0.117, 0.216, 0.403, 0.381, 0.299, 0.381]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=165522)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8057 | Steps: 4 | Val loss: 6.5139 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 14.9639 | Steps: 4 | Val loss: 1006.5008 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1171 | Steps: 4 | Val loss: 1.5113 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 5.6947 | Steps: 4 | Val loss: 5057.8511 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=159478)[0m top1: 0.2080223880597015
[2m[36m(func pid=159478)[0m top5: 0.6553171641791045
[2m[36m(func pid=159478)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=159478)[0m f1_macro: 0.17124572980727354
[2m[36m(func pid=159478)[0m f1_weighted: 0.2014525985046242
[2m[36m(func pid=159478)[0m f1_per_class: [0.261, 0.505, 0.128, 0.017, 0.235, 0.178, 0.257, 0.076, 0.0, 0.056]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.05317164179104478
[2m[36m(func pid=160131)[0m top5: 0.5736940298507462
[2m[36m(func pid=160131)[0m f1_micro: 0.05317164179104478
[2m[36m(func pid=160131)[0m f1_macro: 0.07076416818302064
[2m[36m(func pid=160131)[0m f1_weighted: 0.036401356843161474
[2m[36m(func pid=160131)[0m f1_per_class: [0.117, 0.0, 0.0, 0.013, 0.011, 0.0, 0.0, 0.493, 0.04, 0.034]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.447294776119403
[2m[36m(func pid=159021)[0m top5: 0.9398320895522388
[2m[36m(func pid=159021)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=159021)[0m f1_macro: 0.40125826856431707
[2m[36m(func pid=159021)[0m f1_weighted: 0.45935792660319275
[2m[36m(func pid=159021)[0m f1_per_class: [0.647, 0.624, 0.429, 0.547, 0.118, 0.214, 0.412, 0.372, 0.292, 0.358]
[2m[36m(func pid=159021)[0m 
== Status ==
Current time: 2024-01-07 14:18:15 (running for 00:34:52.70)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.117 |      0.401 |                   39 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.806 |      0.171 |                   37 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 14.964 |      0.071 |                   35 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  8.382 |      0.011 |                   12 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165522)[0m top1: 0.05970149253731343
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=165522)[0m f1_macro: 0.01228667305848514
[2m[36m(func pid=165522)[0m f1_weighted: 0.009965684222702406
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012, 0.111, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.2396 | Steps: 4 | Val loss: 6.5974 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 8.5562 | Steps: 4 | Val loss: 3502.0315 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2366 | Steps: 4 | Val loss: 1.5158 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 5.2695 | Steps: 4 | Val loss: 5976.2500 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=159478)[0m top1: 0.19636194029850745
[2m[36m(func pid=159478)[0m top5: 0.6161380597014925
[2m[36m(func pid=159478)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=159478)[0m f1_macro: 0.15736661438141658
[2m[36m(func pid=159478)[0m f1_weighted: 0.19964037248927588
[2m[36m(func pid=159478)[0m f1_per_class: [0.178, 0.547, 0.073, 0.033, 0.101, 0.073, 0.232, 0.178, 0.087, 0.071]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.036380597014925374
[2m[36m(func pid=160131)[0m top5: 0.597481343283582
[2m[36m(func pid=160131)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=160131)[0m f1_macro: 0.02190811106127729
[2m[36m(func pid=160131)[0m f1_weighted: 0.026841786767517792
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.148, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.024, 0.032]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:21 (running for 00:34:58.29)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.237 |      0.399 |                   40 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  3.24  |      0.157 |                   38 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  8.556 |      0.022 |                   36 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  5.695 |      0.012 |                   13 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.44402985074626866
[2m[36m(func pid=159021)[0m top5: 0.9393656716417911
[2m[36m(func pid=159021)[0m f1_micro: 0.44402985074626866
[2m[36m(func pid=159021)[0m f1_macro: 0.3994565130115354
[2m[36m(func pid=159021)[0m f1_weighted: 0.46164836016537836
[2m[36m(func pid=159021)[0m f1_per_class: [0.624, 0.618, 0.407, 0.542, 0.11, 0.218, 0.427, 0.375, 0.292, 0.381]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.2980410447761194
[2m[36m(func pid=165522)[0m top5: 0.5149253731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=165522)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=165522)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.4280 | Steps: 4 | Val loss: 7.7381 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 10.5427 | Steps: 4 | Val loss: 8261.0342 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2305 | Steps: 4 | Val loss: 1.4558 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7894 | Steps: 4 | Val loss: 5379.1729 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=159478)[0m top1: 0.16417910447761194
[2m[36m(func pid=159478)[0m top5: 0.5261194029850746
[2m[36m(func pid=159478)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=159478)[0m f1_macro: 0.14158823652936964
[2m[36m(func pid=159478)[0m f1_weighted: 0.12884600603999927
[2m[36m(func pid=159478)[0m f1_per_class: [0.19, 0.51, 0.073, 0.036, 0.088, 0.031, 0.0, 0.33, 0.073, 0.084]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m top1: 0.04197761194029851
[2m[36m(func pid=160131)[0m top5: 0.6096082089552238
[2m[36m(func pid=160131)[0m f1_micro: 0.04197761194029851
[2m[36m(func pid=160131)[0m f1_macro: 0.02845237915881984
[2m[36m(func pid=160131)[0m f1_weighted: 0.03726831988831736
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051, 0.029]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:26 (running for 00:35:03.74)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.231 |      0.407 |                   41 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.428 |      0.142 |                   39 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 10.543 |      0.028 |                   37 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  5.27  |      0.046 |                   14 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.45802238805970147
[2m[36m(func pid=159021)[0m top5: 0.9482276119402985
[2m[36m(func pid=159021)[0m f1_micro: 0.45802238805970147
[2m[36m(func pid=159021)[0m f1_macro: 0.40710257179267806
[2m[36m(func pid=159021)[0m f1_weighted: 0.47643012547653857
[2m[36m(func pid=159021)[0m f1_per_class: [0.66, 0.613, 0.436, 0.566, 0.115, 0.228, 0.455, 0.357, 0.288, 0.353]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.05783582089552239
[2m[36m(func pid=165522)[0m top5: 0.5144589552238806
[2m[36m(func pid=165522)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=165522)[0m f1_macro: 0.010939567710630791
[2m[36m(func pid=165522)[0m f1_weighted: 0.006326988787864822
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.1568 | Steps: 4 | Val loss: 6.7741 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 10.9387 | Steps: 4 | Val loss: 10758.1260 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2379 | Steps: 4 | Val loss: 1.4529 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=159478)[0m top1: 0.228544776119403
[2m[36m(func pid=159478)[0m top5: 0.6035447761194029
[2m[36m(func pid=159478)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=159478)[0m f1_macro: 0.1853115742294416
[2m[36m(func pid=159478)[0m f1_weighted: 0.21937644038744877
[2m[36m(func pid=159478)[0m f1_per_class: [0.204, 0.509, 0.067, 0.359, 0.154, 0.047, 0.0, 0.288, 0.058, 0.167]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 4.4444 | Steps: 4 | Val loss: 4612.6104 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=160131)[0m top1: 0.03451492537313433
[2m[36m(func pid=160131)[0m top5: 0.5466417910447762
[2m[36m(func pid=160131)[0m f1_micro: 0.03451492537313433
[2m[36m(func pid=160131)[0m f1_macro: 0.025562303084522925
[2m[36m(func pid=160131)[0m f1_weighted: 0.03220300813699599
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012, 0.05, 0.021]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:32 (running for 00:35:09.20)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.238 |      0.415 |                   42 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  3.157 |      0.185 |                   40 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 10.939 |      0.026 |                   38 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.789 |      0.011 |                   15 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.46595149253731344
[2m[36m(func pid=159021)[0m top5: 0.9472947761194029
[2m[36m(func pid=159021)[0m f1_micro: 0.46595149253731344
[2m[36m(func pid=159021)[0m f1_macro: 0.4149845217174062
[2m[36m(func pid=159021)[0m f1_weighted: 0.4843293211050175
[2m[36m(func pid=159021)[0m f1_per_class: [0.653, 0.612, 0.48, 0.576, 0.118, 0.233, 0.468, 0.365, 0.297, 0.348]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.17583955223880596
[2m[36m(func pid=165522)[0m top5: 0.5144589552238806
[2m[36m(func pid=165522)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=165522)[0m f1_macro: 0.058542050099726704
[2m[36m(func pid=165522)[0m f1_weighted: 0.17240220795791744
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.577, 0.009, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6660 | Steps: 4 | Val loss: 7.5893 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 9.3008 | Steps: 4 | Val loss: 4508.4741 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3659 | Steps: 4 | Val loss: 1.4933 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=159478)[0m top1: 0.16744402985074627
[2m[36m(func pid=159478)[0m top5: 0.6637126865671642
[2m[36m(func pid=159478)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=159478)[0m f1_macro: 0.16615062346426884
[2m[36m(func pid=159478)[0m f1_weighted: 0.20285145649652367
[2m[36m(func pid=159478)[0m f1_per_class: [0.009, 0.416, 0.033, 0.311, 0.023, 0.096, 0.006, 0.455, 0.064, 0.25]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.7863 | Steps: 4 | Val loss: 4937.3071 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=160131)[0m top1: 0.013992537313432836
[2m[36m(func pid=160131)[0m top5: 0.6077425373134329
[2m[36m(func pid=160131)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=160131)[0m f1_macro: 0.0051527255649153215
[2m[36m(func pid=160131)[0m f1_weighted: 0.005740142360630039
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017, 0.006, 0.0, 0.028]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:37 (running for 00:35:14.59)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.366 |      0.408 |                   43 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.666 |      0.166 |                   41 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  9.301 |      0.005 |                   39 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.444 |      0.059 |                   16 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.45988805970149255
[2m[36m(func pid=159021)[0m top5: 0.9402985074626866
[2m[36m(func pid=159021)[0m f1_micro: 0.45988805970149255
[2m[36m(func pid=159021)[0m f1_macro: 0.4083821603693332
[2m[36m(func pid=159021)[0m f1_weighted: 0.4715735694109659
[2m[36m(func pid=159021)[0m f1_per_class: [0.687, 0.626, 0.407, 0.573, 0.117, 0.2, 0.424, 0.404, 0.304, 0.343]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.2849813432835821
[2m[36m(func pid=165522)[0m top5: 0.5153917910447762
[2m[36m(func pid=165522)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=165522)[0m f1_macro: 0.05001235375336095
[2m[36m(func pid=165522)[0m f1_weighted: 0.14773526497662115
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.493, 0.0, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7022 | Steps: 4 | Val loss: 9.4342 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 6.8494 | Steps: 4 | Val loss: 936.9002 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2554 | Steps: 4 | Val loss: 1.4917 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=159478)[0m top1: 0.1310634328358209
[2m[36m(func pid=159478)[0m top5: 0.6539179104477612
[2m[36m(func pid=159478)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=159478)[0m f1_macro: 0.11755499215522577
[2m[36m(func pid=159478)[0m f1_weighted: 0.17796150004442043
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.392, 0.026, 0.192, 0.021, 0.206, 0.095, 0.0, 0.081, 0.162]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 5.2547 | Steps: 4 | Val loss: 5509.0356 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=160131)[0m top1: 0.018656716417910446
[2m[36m(func pid=160131)[0m top5: 0.5569029850746269
[2m[36m(func pid=160131)[0m f1_micro: 0.018656716417910446
[2m[36m(func pid=160131)[0m f1_macro: 0.009005967795487254
[2m[36m(func pid=160131)[0m f1_weighted: 0.01343533905321782
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.042, 0.008, 0.0, 0.029]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:43 (running for 00:35:20.04)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.255 |      0.399 |                   44 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.702 |      0.118 |                   42 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  6.849 |      0.009 |                   40 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.786 |      0.05  |                   17 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.46222014925373134
[2m[36m(func pid=159021)[0m top5: 0.9416977611940298
[2m[36m(func pid=159021)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=159021)[0m f1_macro: 0.39886700821788507
[2m[36m(func pid=159021)[0m f1_weighted: 0.47372369674760856
[2m[36m(func pid=159021)[0m f1_per_class: [0.641, 0.631, 0.375, 0.564, 0.121, 0.196, 0.444, 0.403, 0.289, 0.324]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.2947761194029851
[2m[36m(func pid=165522)[0m top5: 0.519589552238806
[2m[36m(func pid=165522)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=165522)[0m f1_macro: 0.05062391880982586
[2m[36m(func pid=165522)[0m f1_weighted: 0.14372449251837915
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.477, 0.0, 0.023, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.9106 | Steps: 4 | Val loss: 8.0419 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 4.6230 | Steps: 4 | Val loss: 623.9600 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1019 | Steps: 4 | Val loss: 1.4748 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=159478)[0m top1: 0.15625
[2m[36m(func pid=159478)[0m top5: 0.691231343283582
[2m[36m(func pid=159478)[0m f1_micro: 0.15625
[2m[36m(func pid=159478)[0m f1_macro: 0.1315355781145641
[2m[36m(func pid=159478)[0m f1_weighted: 0.20856848888691343
[2m[36m(func pid=159478)[0m f1_per_class: [0.01, 0.372, 0.03, 0.215, 0.071, 0.249, 0.172, 0.0, 0.08, 0.116]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.8322 | Steps: 4 | Val loss: 3870.5200 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=160131)[0m top1: 0.27052238805970147
[2m[36m(func pid=160131)[0m top5: 0.5326492537313433
[2m[36m(func pid=160131)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=160131)[0m f1_macro: 0.08102170478561645
[2m[36m(func pid=160131)[0m f1_weighted: 0.16728975231075222
[2m[36m(func pid=160131)[0m f1_per_class: [0.068, 0.0, 0.182, 0.0, 0.0, 0.0, 0.551, 0.009, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:48 (running for 00:35:25.40)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.102 |      0.398 |                   45 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.911 |      0.132 |                   43 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.623 |      0.081 |                   41 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  5.255 |      0.051 |                   18 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.46408582089552236
[2m[36m(func pid=159021)[0m top5: 0.9449626865671642
[2m[36m(func pid=159021)[0m f1_micro: 0.46408582089552236
[2m[36m(func pid=159021)[0m f1_macro: 0.39769249543764346
[2m[36m(func pid=159021)[0m f1_weighted: 0.4768965222554248
[2m[36m(func pid=159021)[0m f1_per_class: [0.66, 0.619, 0.333, 0.577, 0.113, 0.211, 0.443, 0.401, 0.289, 0.329]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.28451492537313433
[2m[36m(func pid=165522)[0m top5: 0.5317164179104478
[2m[36m(func pid=165522)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=165522)[0m f1_macro: 0.05683729392860911
[2m[36m(func pid=165522)[0m f1_weighted: 0.14924788057640617
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.495, 0.0, 0.045, 0.029]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7602 | Steps: 4 | Val loss: 6.5976 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 9.1193 | Steps: 4 | Val loss: 467.4815 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2494 | Steps: 4 | Val loss: 1.4794 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=159478)[0m top1: 0.20615671641791045
[2m[36m(func pid=159478)[0m top5: 0.7103544776119403
[2m[36m(func pid=159478)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=159478)[0m f1_macro: 0.1603034555237274
[2m[36m(func pid=159478)[0m f1_weighted: 0.23794563975007824
[2m[36m(func pid=159478)[0m f1_per_class: [0.069, 0.411, 0.042, 0.287, 0.133, 0.225, 0.177, 0.0, 0.141, 0.118]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.8933 | Steps: 4 | Val loss: 3249.5867 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=160131)[0m top1: 0.06856343283582089
[2m[36m(func pid=160131)[0m top5: 0.6958955223880597
[2m[36m(func pid=160131)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=160131)[0m f1_macro: 0.022122289037341196
[2m[36m(func pid=160131)[0m f1_weighted: 0.010327603641770513
[2m[36m(func pid=160131)[0m f1_per_class: [0.066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.155, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:53 (running for 00:35:30.78)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.249 |      0.392 |                   46 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.76  |      0.16  |                   44 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  9.119 |      0.022 |                   42 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.832 |      0.057 |                   19 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.45149253731343286
[2m[36m(func pid=159021)[0m top5: 0.9505597014925373
[2m[36m(func pid=159021)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=159021)[0m f1_macro: 0.39223886829625093
[2m[36m(func pid=159021)[0m f1_weighted: 0.47164267052710734
[2m[36m(func pid=159021)[0m f1_per_class: [0.635, 0.609, 0.324, 0.577, 0.119, 0.224, 0.438, 0.364, 0.254, 0.378]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.2751865671641791
[2m[36m(func pid=165522)[0m top5: 0.5578358208955224
[2m[36m(func pid=165522)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=165522)[0m f1_macro: 0.06269487163243344
[2m[36m(func pid=165522)[0m f1_weighted: 0.15818522452776987
[2m[36m(func pid=165522)[0m f1_per_class: [0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.525, 0.0, 0.018, 0.055]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.6085 | Steps: 4 | Val loss: 6.4774 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 9.0308 | Steps: 4 | Val loss: 369.2145 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0542 | Steps: 4 | Val loss: 1.4378 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=159478)[0m top1: 0.2775186567164179
[2m[36m(func pid=159478)[0m top5: 0.7667910447761194
[2m[36m(func pid=159478)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=159478)[0m f1_macro: 0.24342350716308508
[2m[36m(func pid=159478)[0m f1_weighted: 0.2800812573490899
[2m[36m(func pid=159478)[0m f1_per_class: [0.203, 0.442, 0.348, 0.372, 0.227, 0.184, 0.178, 0.195, 0.167, 0.117]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.4963 | Steps: 4 | Val loss: 3220.7100 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=160131)[0m top1: 0.09235074626865672
[2m[36m(func pid=160131)[0m top5: 0.7164179104477612
[2m[36m(func pid=160131)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=160131)[0m f1_macro: 0.03796150200354716
[2m[36m(func pid=160131)[0m f1_weighted: 0.049477073812859186
[2m[36m(func pid=160131)[0m f1_per_class: [0.067, 0.0, 0.0, 0.136, 0.0, 0.0, 0.0, 0.176, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:18:59 (running for 00:35:36.29)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.054 |      0.401 |                   47 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.609 |      0.243 |                   45 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  9.031 |      0.038 |                   43 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.893 |      0.063 |                   20 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4673507462686567
[2m[36m(func pid=159021)[0m top5: 0.9505597014925373
[2m[36m(func pid=159021)[0m f1_micro: 0.4673507462686567
[2m[36m(func pid=159021)[0m f1_macro: 0.40097884278336915
[2m[36m(func pid=159021)[0m f1_weighted: 0.47692728088957304
[2m[36m(func pid=159021)[0m f1_per_class: [0.624, 0.626, 0.333, 0.583, 0.153, 0.232, 0.43, 0.391, 0.28, 0.357]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.5441 | Steps: 4 | Val loss: 6.8636 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=165522)[0m top1: 0.2733208955223881
[2m[36m(func pid=165522)[0m top5: 0.5909514925373134
[2m[36m(func pid=165522)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=165522)[0m f1_macro: 0.06670548707210391
[2m[36m(func pid=165522)[0m f1_weighted: 0.16005515396780975
[2m[36m(func pid=165522)[0m f1_per_class: [0.09, 0.0, 0.0, 0.003, 0.0, 0.0, 0.526, 0.0, 0.0, 0.048]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 10.2633 | Steps: 4 | Val loss: 255.1856 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1868 | Steps: 4 | Val loss: 1.4858 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=159478)[0m top1: 0.31949626865671643
[2m[36m(func pid=159478)[0m top5: 0.8143656716417911
[2m[36m(func pid=159478)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=159478)[0m f1_macro: 0.30740072974342536
[2m[36m(func pid=159478)[0m f1_weighted: 0.32737523057690343
[2m[36m(func pid=159478)[0m f1_per_class: [0.271, 0.401, 0.786, 0.546, 0.25, 0.047, 0.224, 0.276, 0.122, 0.152]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 3.2052 | Steps: 4 | Val loss: 2926.3750 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=160131)[0m top1: 0.4361007462686567
[2m[36m(func pid=160131)[0m top5: 0.699160447761194
[2m[36m(func pid=160131)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=160131)[0m f1_macro: 0.11517161407330594
[2m[36m(func pid=160131)[0m f1_weighted: 0.32468953526513555
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.541, 0.0, 0.008, 0.577, 0.0, 0.026, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:19:04 (running for 00:35:41.71)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.187 |      0.393 |                   48 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.544 |      0.307 |                   46 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 10.263 |      0.115 |                   44 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.496 |      0.067 |                   21 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.45149253731343286
[2m[36m(func pid=159021)[0m top5: 0.9444962686567164
[2m[36m(func pid=159021)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=159021)[0m f1_macro: 0.3926053487898934
[2m[36m(func pid=159021)[0m f1_weighted: 0.4511761879434217
[2m[36m(func pid=159021)[0m f1_per_class: [0.618, 0.634, 0.312, 0.58, 0.153, 0.231, 0.34, 0.393, 0.289, 0.375]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3732 | Steps: 4 | Val loss: 9.4236 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=165522)[0m top1: 0.26725746268656714
[2m[36m(func pid=165522)[0m top5: 0.6333955223880597
[2m[36m(func pid=165522)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=165522)[0m f1_macro: 0.0628729055042067
[2m[36m(func pid=165522)[0m f1_weighted: 0.16087844002390914
[2m[36m(func pid=165522)[0m f1_per_class: [0.091, 0.0, 0.0, 0.003, 0.0, 0.0, 0.529, 0.005, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 5.7510 | Steps: 4 | Val loss: 205.5512 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0396 | Steps: 4 | Val loss: 1.4492 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=159478)[0m top1: 0.3138992537313433
[2m[36m(func pid=159478)[0m top5: 0.824160447761194
[2m[36m(func pid=159478)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=159478)[0m f1_macro: 0.22178228469919659
[2m[36m(func pid=159478)[0m f1_weighted: 0.2996736931459451
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.254, 0.471, 0.59, 0.273, 0.0, 0.248, 0.136, 0.088, 0.159]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.7811 | Steps: 4 | Val loss: 2855.0178 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=160131)[0m top1: 0.3180970149253731
[2m[36m(func pid=160131)[0m top5: 0.7061567164179104
[2m[36m(func pid=160131)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=160131)[0m f1_macro: 0.10416676742680811
[2m[36m(func pid=160131)[0m f1_weighted: 0.24545278228043962
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.553, 0.0, 0.262, 0.203, 0.0, 0.024, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:19:10 (running for 00:35:47.15)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.04  |      0.399 |                   49 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.373 |      0.222 |                   47 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  5.751 |      0.104 |                   45 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.205 |      0.063 |                   22 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.45848880597014924
[2m[36m(func pid=159021)[0m top5: 0.9552238805970149
[2m[36m(func pid=159021)[0m f1_micro: 0.45848880597014924
[2m[36m(func pid=159021)[0m f1_macro: 0.3987717735466269
[2m[36m(func pid=159021)[0m f1_weighted: 0.4643890051490829
[2m[36m(func pid=159021)[0m f1_per_class: [0.609, 0.618, 0.333, 0.597, 0.156, 0.259, 0.37, 0.385, 0.277, 0.384]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2991 | Steps: 4 | Val loss: 9.5593 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=165522)[0m top1: 0.28125
[2m[36m(func pid=165522)[0m top5: 0.6739738805970149
[2m[36m(func pid=165522)[0m f1_micro: 0.28125
[2m[36m(func pid=165522)[0m f1_macro: 0.07410120464449747
[2m[36m(func pid=165522)[0m f1_weighted: 0.18113141977460012
[2m[36m(func pid=165522)[0m f1_per_class: [0.086, 0.113, 0.0, 0.003, 0.0, 0.0, 0.532, 0.006, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 4.9294 | Steps: 4 | Val loss: 327.7238 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1181 | Steps: 4 | Val loss: 1.4557 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=159478)[0m top1: 0.31949626865671643
[2m[36m(func pid=159478)[0m top5: 0.8106343283582089
[2m[36m(func pid=159478)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=159478)[0m f1_macro: 0.2446299081625579
[2m[36m(func pid=159478)[0m f1_weighted: 0.32269800166865253
[2m[36m(func pid=159478)[0m f1_per_class: [0.041, 0.29, 0.692, 0.558, 0.194, 0.0, 0.347, 0.024, 0.1, 0.2]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.7510 | Steps: 4 | Val loss: 2397.5278 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=160131)[0m top1: 0.21548507462686567
[2m[36m(func pid=160131)[0m top5: 0.7080223880597015
[2m[36m(func pid=160131)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=160131)[0m f1_macro: 0.06221618375299354
[2m[36m(func pid=160131)[0m f1_weighted: 0.15516307893552736
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.538, 0.0, 0.034, 0.0, 0.0, 0.025, 0.025]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:19:15 (running for 00:35:52.43)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.118 |      0.394 |                   50 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.299 |      0.245 |                   48 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.929 |      0.062 |                   46 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.781 |      0.074 |                   23 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4519589552238806
[2m[36m(func pid=159021)[0m top5: 0.9566231343283582
[2m[36m(func pid=159021)[0m f1_micro: 0.4519589552238806
[2m[36m(func pid=159021)[0m f1_macro: 0.39448999546001906
[2m[36m(func pid=159021)[0m f1_weighted: 0.45365144066124913
[2m[36m(func pid=159021)[0m f1_per_class: [0.63, 0.58, 0.353, 0.623, 0.159, 0.261, 0.331, 0.384, 0.268, 0.356]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4948 | Steps: 4 | Val loss: 10.9119 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=165522)[0m top1: 0.29244402985074625
[2m[36m(func pid=165522)[0m top5: 0.7126865671641791
[2m[36m(func pid=165522)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=165522)[0m f1_macro: 0.08371914334405514
[2m[36m(func pid=165522)[0m f1_weighted: 0.18681442664023928
[2m[36m(func pid=165522)[0m f1_per_class: [0.079, 0.136, 0.0, 0.007, 0.0, 0.0, 0.531, 0.015, 0.0, 0.069]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 5.4376 | Steps: 4 | Val loss: 333.7374 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.1243 | Steps: 4 | Val loss: 1.4537 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=159478)[0m top1: 0.23087686567164178
[2m[36m(func pid=159478)[0m top5: 0.7649253731343284
[2m[36m(func pid=159478)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=159478)[0m f1_macro: 0.14056568523827753
[2m[36m(func pid=159478)[0m f1_weighted: 0.25072804732982357
[2m[36m(func pid=159478)[0m f1_per_class: [0.078, 0.207, 0.139, 0.305, 0.0, 0.0, 0.409, 0.0, 0.117, 0.15]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 4.0615 | Steps: 4 | Val loss: 2410.7163 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=160131)[0m top1: 0.1609141791044776
[2m[36m(func pid=160131)[0m top5: 0.6996268656716418
[2m[36m(func pid=160131)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=160131)[0m f1_macro: 0.05810177006519377
[2m[36m(func pid=160131)[0m f1_weighted: 0.13639841199946337
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.462, 0.0, 0.05, 0.0, 0.0, 0.042, 0.027]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:19:20 (running for 00:35:57.59)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.118 |      0.394 |                   50 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  2.495 |      0.141 |                   49 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  5.438 |      0.058 |                   47 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.751 |      0.084 |                   24 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4542910447761194
[2m[36m(func pid=159021)[0m top5: 0.9561567164179104
[2m[36m(func pid=159021)[0m f1_micro: 0.4542910447761194
[2m[36m(func pid=159021)[0m f1_macro: 0.39379974378800653
[2m[36m(func pid=159021)[0m f1_weighted: 0.45477406939470943
[2m[36m(func pid=159021)[0m f1_per_class: [0.574, 0.572, 0.353, 0.623, 0.156, 0.274, 0.335, 0.385, 0.292, 0.375]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.2873134328358209
[2m[36m(func pid=165522)[0m top5: 0.6861007462686567
[2m[36m(func pid=165522)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=165522)[0m f1_macro: 0.07788523319151913
[2m[36m(func pid=165522)[0m f1_weighted: 0.18678194225104627
[2m[36m(func pid=165522)[0m f1_per_class: [0.087, 0.126, 0.0, 0.0, 0.0, 0.014, 0.541, 0.011, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6361 | Steps: 4 | Val loss: 13.8923 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 3.7302 | Steps: 4 | Val loss: 191.4954 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1705 | Steps: 4 | Val loss: 1.4311 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=159478)[0m top1: 0.23274253731343283
[2m[36m(func pid=159478)[0m top5: 0.7999067164179104
[2m[36m(func pid=159478)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=159478)[0m f1_macro: 0.16128887523871052
[2m[36m(func pid=159478)[0m f1_weighted: 0.25683171618856665
[2m[36m(func pid=159478)[0m f1_per_class: [0.077, 0.143, 0.111, 0.315, 0.0, 0.0, 0.388, 0.373, 0.12, 0.086]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 4.7313 | Steps: 4 | Val loss: 3481.1375 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:19:26 (running for 00:36:03.07)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.124 |      0.394 |                   51 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.636 |      0.161 |                   50 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.73  |      0.037 |                   48 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.062 |      0.078 |                   25 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.07882462686567164
[2m[36m(func pid=160131)[0m top5: 0.6693097014925373
[2m[36m(func pid=160131)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=160131)[0m f1_macro: 0.03712916461440265
[2m[36m(func pid=160131)[0m f1_weighted: 0.024279782865236185
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.003, 0.0, 0.063, 0.0, 0.246, 0.059, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4594216417910448
[2m[36m(func pid=159021)[0m top5: 0.9580223880597015
[2m[36m(func pid=159021)[0m f1_micro: 0.4594216417910448
[2m[36m(func pid=159021)[0m f1_macro: 0.3983599323194965
[2m[36m(func pid=159021)[0m f1_weighted: 0.46266742376532044
[2m[36m(func pid=159021)[0m f1_per_class: [0.589, 0.571, 0.358, 0.617, 0.168, 0.291, 0.36, 0.397, 0.282, 0.352]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.8078 | Steps: 4 | Val loss: 16.7472 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=165522)[0m top1: 0.28125
[2m[36m(func pid=165522)[0m top5: 0.6632462686567164
[2m[36m(func pid=165522)[0m f1_micro: 0.28125
[2m[36m(func pid=165522)[0m f1_macro: 0.09074730743537943
[2m[36m(func pid=165522)[0m f1_weighted: 0.19479108695713218
[2m[36m(func pid=165522)[0m f1_per_class: [0.081, 0.105, 0.0, 0.0, 0.0, 0.02, 0.568, 0.012, 0.066, 0.056]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 4.1810 | Steps: 4 | Val loss: 167.9737 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1162 | Steps: 4 | Val loss: 1.3872 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=159478)[0m top1: 0.17397388059701493
[2m[36m(func pid=159478)[0m top5: 0.8031716417910447
[2m[36m(func pid=159478)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=159478)[0m f1_macro: 0.12118668605881813
[2m[36m(func pid=159478)[0m f1_weighted: 0.1766080164918998
[2m[36m(func pid=159478)[0m f1_per_class: [0.085, 0.11, 0.0, 0.234, 0.0, 0.0, 0.212, 0.378, 0.161, 0.032]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 4.4298 | Steps: 4 | Val loss: 3663.9971 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=160131)[0m top1: 0.07742537313432836
[2m[36m(func pid=160131)[0m top5: 0.6520522388059702
[2m[36m(func pid=160131)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=160131)[0m f1_macro: 0.035298456705042405
[2m[36m(func pid=160131)[0m f1_weighted: 0.020861802668793505
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0.0, 0.259, 0.06, 0.0]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:19:31 (running for 00:36:08.49)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.17  |      0.398 |                   52 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.808 |      0.121 |                   51 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.181 |      0.035 |                   49 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.731 |      0.091 |                   26 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.46968283582089554
[2m[36m(func pid=159021)[0m top5: 0.9617537313432836
[2m[36m(func pid=159021)[0m f1_micro: 0.46968283582089554
[2m[36m(func pid=159021)[0m f1_macro: 0.4057514776652197
[2m[36m(func pid=159021)[0m f1_weighted: 0.47615063364806537
[2m[36m(func pid=159021)[0m f1_per_class: [0.579, 0.534, 0.429, 0.627, 0.206, 0.308, 0.418, 0.365, 0.261, 0.33]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.28031716417910446
[2m[36m(func pid=165522)[0m top5: 0.6735074626865671
[2m[36m(func pid=165522)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=165522)[0m f1_macro: 0.08911521728969543
[2m[36m(func pid=165522)[0m f1_weighted: 0.2007224667467294
[2m[36m(func pid=165522)[0m f1_per_class: [0.086, 0.129, 0.0, 0.0, 0.0, 0.027, 0.575, 0.007, 0.047, 0.02]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.9100 | Steps: 4 | Val loss: 16.7639 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 6.2486 | Steps: 4 | Val loss: 158.4949 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0456 | Steps: 4 | Val loss: 1.4539 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=159478)[0m top1: 0.14458955223880596
[2m[36m(func pid=159478)[0m top5: 0.8264925373134329
[2m[36m(func pid=159478)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=159478)[0m f1_macro: 0.17317881543719577
[2m[36m(func pid=159478)[0m f1_weighted: 0.1468679103251286
[2m[36m(func pid=159478)[0m f1_per_class: [0.286, 0.082, 0.485, 0.184, 0.0, 0.0, 0.16, 0.318, 0.194, 0.024]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.4320 | Steps: 4 | Val loss: 2864.9636 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 14:19:37 (running for 00:36:14.04)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.116 |      0.406 |                   53 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.91  |      0.173 |                   52 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  6.249 |      0.036 |                   50 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.43  |      0.089 |                   27 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.07276119402985075
[2m[36m(func pid=160131)[0m top5: 0.699160447761194
[2m[36m(func pid=160131)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=160131)[0m f1_macro: 0.03618245349239776
[2m[36m(func pid=160131)[0m f1_weighted: 0.021163872451243928
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.005, 0.035, 0.0, 0.263, 0.059, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4552238805970149
[2m[36m(func pid=159021)[0m top5: 0.9510261194029851
[2m[36m(func pid=159021)[0m f1_micro: 0.4552238805970149
[2m[36m(func pid=159021)[0m f1_macro: 0.40893357238850864
[2m[36m(func pid=159021)[0m f1_weighted: 0.4643442963816349
[2m[36m(func pid=159021)[0m f1_per_class: [0.596, 0.542, 0.5, 0.624, 0.201, 0.297, 0.377, 0.375, 0.27, 0.308]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.1433 | Steps: 4 | Val loss: 14.5599 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=165522)[0m top1: 0.32136194029850745
[2m[36m(func pid=165522)[0m top5: 0.6833022388059702
[2m[36m(func pid=165522)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=165522)[0m f1_macro: 0.1140075012625322
[2m[36m(func pid=165522)[0m f1_weighted: 0.23750711301620506
[2m[36m(func pid=165522)[0m f1_per_class: [0.102, 0.337, 0.0, 0.0, 0.0, 0.049, 0.567, 0.0, 0.085, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.9314 | Steps: 4 | Val loss: 101.5797 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0597 | Steps: 4 | Val loss: 1.4726 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=159478)[0m top1: 0.10494402985074627
[2m[36m(func pid=159478)[0m top5: 0.7994402985074627
[2m[36m(func pid=159478)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=159478)[0m f1_macro: 0.0957039833836492
[2m[36m(func pid=159478)[0m f1_weighted: 0.10818253765913684
[2m[36m(func pid=159478)[0m f1_per_class: [0.111, 0.077, 0.102, 0.066, 0.0, 0.095, 0.158, 0.188, 0.133, 0.027]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.8543 | Steps: 4 | Val loss: 2090.8809 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:19:42 (running for 00:36:19.69)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.046 |      0.409 |                   54 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.143 |      0.096 |                   53 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.931 |      0.091 |                   51 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.432 |      0.114 |                   28 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.2178171641791045
[2m[36m(func pid=160131)[0m top5: 0.7882462686567164
[2m[36m(func pid=160131)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=160131)[0m f1_macro: 0.09096864275548987
[2m[36m(func pid=160131)[0m f1_weighted: 0.10549026831634248
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.419, 0.0, 0.0, 0.0, 0.113, 0.003, 0.286, 0.088, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4519589552238806
[2m[36m(func pid=159021)[0m top5: 0.9514925373134329
[2m[36m(func pid=159021)[0m f1_micro: 0.4519589552238806
[2m[36m(func pid=159021)[0m f1_macro: 0.4084819993359471
[2m[36m(func pid=159021)[0m f1_weighted: 0.46880736920915334
[2m[36m(func pid=159021)[0m f1_per_class: [0.602, 0.48, 0.5, 0.627, 0.212, 0.32, 0.416, 0.38, 0.255, 0.294]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4309 | Steps: 4 | Val loss: 15.3240 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=165522)[0m top1: 0.3302238805970149
[2m[36m(func pid=165522)[0m top5: 0.6879664179104478
[2m[36m(func pid=165522)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=165522)[0m f1_macro: 0.11586111497277292
[2m[36m(func pid=165522)[0m f1_weighted: 0.24010679116728562
[2m[36m(func pid=165522)[0m f1_per_class: [0.093, 0.35, 0.0, 0.007, 0.0, 0.054, 0.559, 0.011, 0.085, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.6554 | Steps: 4 | Val loss: 34.0757 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0570 | Steps: 4 | Val loss: 1.4656 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=159478)[0m top1: 0.09981343283582089
[2m[36m(func pid=159478)[0m top5: 0.804570895522388
[2m[36m(func pid=159478)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=159478)[0m f1_macro: 0.062297678845153484
[2m[36m(func pid=159478)[0m f1_weighted: 0.0902274605633139
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.037, 0.076, 0.016, 0.0, 0.078, 0.2, 0.161, 0.024, 0.031]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.8489 | Steps: 4 | Val loss: 2043.4244 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:19:48 (running for 00:36:25.25)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.06  |      0.408 |                   55 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.431 |      0.062 |                   54 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.655 |      0.126 |                   52 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.854 |      0.116 |                   29 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.2677238805970149
[2m[36m(func pid=160131)[0m top5: 0.8120335820895522
[2m[36m(func pid=160131)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=160131)[0m f1_macro: 0.12627106118805445
[2m[36m(func pid=160131)[0m f1_weighted: 0.1816150086358903
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.411, 0.0, 0.0, 0.0, 0.192, 0.22, 0.352, 0.087, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.447294776119403
[2m[36m(func pid=159021)[0m top5: 0.9547574626865671
[2m[36m(func pid=159021)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=159021)[0m f1_macro: 0.411645224210811
[2m[36m(func pid=159021)[0m f1_weighted: 0.46899928424040777
[2m[36m(func pid=159021)[0m f1_per_class: [0.636, 0.482, 0.49, 0.624, 0.23, 0.304, 0.427, 0.356, 0.236, 0.333]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.6442 | Steps: 4 | Val loss: 17.7854 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=165522)[0m top1: 0.341884328358209
[2m[36m(func pid=165522)[0m top5: 0.7178171641791045
[2m[36m(func pid=165522)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=165522)[0m f1_macro: 0.10468002035733008
[2m[36m(func pid=165522)[0m f1_weighted: 0.23532440952077327
[2m[36m(func pid=165522)[0m f1_per_class: [0.037, 0.373, 0.0, 0.01, 0.0, 0.0, 0.554, 0.0, 0.072, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 5.3432 | Steps: 4 | Val loss: 41.9879 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1418 | Steps: 4 | Val loss: 1.4747 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=159478)[0m top1: 0.11753731343283583
[2m[36m(func pid=159478)[0m top5: 0.8180970149253731
[2m[36m(func pid=159478)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=159478)[0m f1_macro: 0.09347357305703108
[2m[36m(func pid=159478)[0m f1_weighted: 0.10531347926360136
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.052, 0.375, 0.0, 0.0, 0.016, 0.272, 0.187, 0.0, 0.032]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.0379 | Steps: 4 | Val loss: 1293.8450 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 14:19:53 (running for 00:36:30.71)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.057 |      0.412 |                   56 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.644 |      0.093 |                   55 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  5.343 |      0.123 |                   53 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.849 |      0.105 |                   30 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.34328358208955223
[2m[36m(func pid=160131)[0m top5: 0.8311567164179104
[2m[36m(func pid=160131)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=160131)[0m f1_macro: 0.12251644954557431
[2m[36m(func pid=160131)[0m f1_weighted: 0.26120175295074494
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.404, 0.0, 0.0, 0.0, 0.211, 0.556, 0.0, 0.054, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.44636194029850745
[2m[36m(func pid=159021)[0m top5: 0.9566231343283582
[2m[36m(func pid=159021)[0m f1_micro: 0.44636194029850745
[2m[36m(func pid=159021)[0m f1_macro: 0.4096687213109454
[2m[36m(func pid=159021)[0m f1_weighted: 0.4662386082356686
[2m[36m(func pid=159021)[0m f1_per_class: [0.64, 0.441, 0.48, 0.632, 0.252, 0.307, 0.432, 0.361, 0.23, 0.322]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3414 | Steps: 4 | Val loss: 13.4813 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=165522)[0m top1: 0.3260261194029851
[2m[36m(func pid=165522)[0m top5: 0.7509328358208955
[2m[36m(func pid=165522)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=165522)[0m f1_macro: 0.1151446396641421
[2m[36m(func pid=165522)[0m f1_weighted: 0.25192255945193237
[2m[36m(func pid=165522)[0m f1_per_class: [0.033, 0.41, 0.0, 0.026, 0.0, 0.0, 0.572, 0.01, 0.057, 0.044]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.9600 | Steps: 4 | Val loss: 54.5497 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0589 | Steps: 4 | Val loss: 1.5189 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=159478)[0m top1: 0.13899253731343283
[2m[36m(func pid=159478)[0m top5: 0.8027052238805971
[2m[36m(func pid=159478)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=159478)[0m f1_macro: 0.1184552977454337
[2m[36m(func pid=159478)[0m f1_weighted: 0.13530249216302123
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.198, 0.267, 0.025, 0.214, 0.008, 0.27, 0.156, 0.0, 0.046]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.3562 | Steps: 4 | Val loss: 995.4269 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 14:19:59 (running for 00:36:36.18)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.142 |      0.41  |                   57 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.341 |      0.118 |                   56 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.96  |      0.125 |                   54 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.038 |      0.115 |                   31 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.3362873134328358
[2m[36m(func pid=160131)[0m top5: 0.8260261194029851
[2m[36m(func pid=160131)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=160131)[0m f1_macro: 0.1251344708407843
[2m[36m(func pid=160131)[0m f1_weighted: 0.2596522624364578
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.4, 0.0, 0.0, 0.0, 0.259, 0.539, 0.0, 0.0, 0.054]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.43843283582089554
[2m[36m(func pid=159021)[0m top5: 0.9510261194029851
[2m[36m(func pid=159021)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=159021)[0m f1_macro: 0.40461812324507973
[2m[36m(func pid=159021)[0m f1_weighted: 0.46039144682108624
[2m[36m(func pid=159021)[0m f1_per_class: [0.653, 0.401, 0.48, 0.619, 0.243, 0.312, 0.45, 0.345, 0.221, 0.323]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.8767 | Steps: 4 | Val loss: 9.2361 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=165522)[0m top1: 0.21222014925373134
[2m[36m(func pid=165522)[0m top5: 0.7644589552238806
[2m[36m(func pid=165522)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=165522)[0m f1_macro: 0.06932711248974932
[2m[36m(func pid=165522)[0m f1_weighted: 0.1834999988565083
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.011, 0.018, 0.007, 0.0, 0.0, 0.599, 0.009, 0.0, 0.05]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 4.7928 | Steps: 4 | Val loss: 45.9828 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1205 | Steps: 4 | Val loss: 1.5292 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=159478)[0m top1: 0.21361940298507462
[2m[36m(func pid=159478)[0m top5: 0.8115671641791045
[2m[36m(func pid=159478)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=159478)[0m f1_macro: 0.18980712076514616
[2m[36m(func pid=159478)[0m f1_weighted: 0.2281996829514935
[2m[36m(func pid=159478)[0m f1_per_class: [0.044, 0.254, 0.476, 0.208, 0.227, 0.008, 0.362, 0.162, 0.042, 0.114]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 8.1356 | Steps: 4 | Val loss: 1050.0538 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:20:04 (running for 00:36:41.64)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.059 |      0.405 |                   58 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.877 |      0.19  |                   57 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.793 |      0.077 |                   55 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.356 |      0.069 |                   32 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.20988805970149255
[2m[36m(func pid=160131)[0m top5: 0.8027052238805971
[2m[36m(func pid=160131)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=160131)[0m f1_macro: 0.07682103270594277
[2m[36m(func pid=160131)[0m f1_weighted: 0.12877946127068376
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.39, 0.0, 0.003, 0.0, 0.196, 0.125, 0.0, 0.028, 0.027]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4388992537313433
[2m[36m(func pid=159021)[0m top5: 0.9468283582089553
[2m[36m(func pid=159021)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=159021)[0m f1_macro: 0.4103314246714006
[2m[36m(func pid=159021)[0m f1_weighted: 0.4578977496443096
[2m[36m(func pid=159021)[0m f1_per_class: [0.66, 0.354, 0.545, 0.63, 0.275, 0.297, 0.462, 0.343, 0.217, 0.321]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.2739 | Steps: 4 | Val loss: 6.5671 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=165522)[0m top1: 0.24580223880597016
[2m[36m(func pid=165522)[0m top5: 0.7611940298507462
[2m[36m(func pid=165522)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=165522)[0m f1_macro: 0.06497939861832042
[2m[36m(func pid=165522)[0m f1_weighted: 0.17322229088696428
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.019, 0.0, 0.0, 0.016, 0.572, 0.006, 0.0, 0.037]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.5534 | Steps: 4 | Val loss: 49.7974 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=159478)[0m top1: 0.2751865671641791
[2m[36m(func pid=159478)[0m top5: 0.8059701492537313
[2m[36m(func pid=159478)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=159478)[0m f1_macro: 0.21430615993917518
[2m[36m(func pid=159478)[0m f1_weighted: 0.2816633098526454
[2m[36m(func pid=159478)[0m f1_per_class: [0.203, 0.225, 0.318, 0.313, 0.2, 0.016, 0.437, 0.208, 0.059, 0.163]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.1256 | Steps: 4 | Val loss: 1.5827 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.9480 | Steps: 4 | Val loss: 471.1858 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:20:10 (running for 00:36:47.09)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.12  |      0.41  |                   59 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.274 |      0.214 |                   58 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.553 |      0.061 |                   56 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  8.136 |      0.065 |                   33 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.18983208955223882
[2m[36m(func pid=160131)[0m top5: 0.8092350746268657
[2m[36m(func pid=160131)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=160131)[0m f1_macro: 0.06086539187125482
[2m[36m(func pid=160131)[0m f1_weighted: 0.09061068874682512
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.381, 0.0, 0.01, 0.0, 0.193, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4244402985074627
[2m[36m(func pid=159021)[0m top5: 0.9435634328358209
[2m[36m(func pid=159021)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=159021)[0m f1_macro: 0.4005217590707586
[2m[36m(func pid=159021)[0m f1_weighted: 0.4484229509876263
[2m[36m(func pid=159021)[0m f1_per_class: [0.646, 0.351, 0.533, 0.595, 0.239, 0.287, 0.471, 0.34, 0.214, 0.33]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6440 | Steps: 4 | Val loss: 6.6050 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=165522)[0m top1: 0.22667910447761194
[2m[36m(func pid=165522)[0m top5: 0.7747201492537313
[2m[36m(func pid=165522)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=165522)[0m f1_macro: 0.0695771440996945
[2m[36m(func pid=165522)[0m f1_weighted: 0.17819156775062187
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.018, 0.003, 0.0, 0.031, 0.577, 0.021, 0.0, 0.046]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.0518 | Steps: 4 | Val loss: 35.1940 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=159478)[0m top1: 0.279384328358209
[2m[36m(func pid=159478)[0m top5: 0.8013059701492538
[2m[36m(func pid=159478)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=159478)[0m f1_macro: 0.20118516565238315
[2m[36m(func pid=159478)[0m f1_weighted: 0.27948397372526634
[2m[36m(func pid=159478)[0m f1_per_class: [0.135, 0.152, 0.103, 0.214, 0.133, 0.054, 0.539, 0.283, 0.065, 0.333]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0339 | Steps: 4 | Val loss: 1.5512 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5793 | Steps: 4 | Val loss: 164.8156 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:20:15 (running for 00:36:52.67)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.126 |      0.401 |                   60 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.644 |      0.201 |                   59 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.052 |      0.09  |                   57 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.948 |      0.07  |                   34 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.22994402985074627
[2m[36m(func pid=160131)[0m top5: 0.7672574626865671
[2m[36m(func pid=160131)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=160131)[0m f1_macro: 0.09009081934719342
[2m[36m(func pid=160131)[0m f1_weighted: 0.10348381413721781
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.368, 0.0, 0.01, 0.0, 0.127, 0.0, 0.396, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4351679104477612
[2m[36m(func pid=159021)[0m top5: 0.9440298507462687
[2m[36m(func pid=159021)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=159021)[0m f1_macro: 0.39476221118025384
[2m[36m(func pid=159021)[0m f1_weighted: 0.4483438646945074
[2m[36m(func pid=159021)[0m f1_per_class: [0.653, 0.293, 0.49, 0.623, 0.25, 0.309, 0.473, 0.332, 0.199, 0.326]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3980 | Steps: 4 | Val loss: 7.4676 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=165522)[0m top1: 0.1525186567164179
[2m[36m(func pid=165522)[0m top5: 0.7747201492537313
[2m[36m(func pid=165522)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=165522)[0m f1_macro: 0.07851912875605538
[2m[36m(func pid=165522)[0m f1_weighted: 0.16695753974598818
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.005, 0.017, 0.003, 0.0, 0.193, 0.468, 0.051, 0.0, 0.047]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.26072761194029853
[2m[36m(func pid=159478)[0m top5: 0.8166977611940298
[2m[36m(func pid=159478)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=159478)[0m f1_macro: 0.20637072277233562
[2m[36m(func pid=159478)[0m f1_weighted: 0.266082890758545
[2m[36m(func pid=159478)[0m f1_per_class: [0.109, 0.116, 0.127, 0.164, 0.2, 0.069, 0.543, 0.357, 0.044, 0.333]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.8116 | Steps: 4 | Val loss: 51.5837 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0335 | Steps: 4 | Val loss: 1.4873 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.9111 | Steps: 4 | Val loss: 96.4325 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 14:20:21 (running for 00:36:58.06)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.034 |      0.395 |                   61 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.398 |      0.206 |                   60 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.812 |      0.091 |                   58 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.579 |      0.079 |                   35 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.23087686567164178
[2m[36m(func pid=160131)[0m top5: 0.7863805970149254
[2m[36m(func pid=160131)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=160131)[0m f1_macro: 0.09059208205086443
[2m[36m(func pid=160131)[0m f1_weighted: 0.10431211030164539
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.372, 0.0, 0.007, 0.0, 0.141, 0.0, 0.387, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.9146 | Steps: 4 | Val loss: 7.0104 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=159021)[0m top1: 0.4552238805970149
[2m[36m(func pid=159021)[0m top5: 0.9449626865671642
[2m[36m(func pid=159021)[0m f1_micro: 0.4552238805970149
[2m[36m(func pid=159021)[0m f1_macro: 0.4045001773851398
[2m[36m(func pid=159021)[0m f1_weighted: 0.4651009339890181
[2m[36m(func pid=159021)[0m f1_per_class: [0.653, 0.31, 0.471, 0.635, 0.265, 0.32, 0.497, 0.358, 0.222, 0.314]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.10121268656716417
[2m[36m(func pid=165522)[0m top5: 0.7681902985074627
[2m[36m(func pid=165522)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=165522)[0m f1_macro: 0.06796381241334468
[2m[36m(func pid=165522)[0m f1_weighted: 0.1143850546198156
[2m[36m(func pid=165522)[0m f1_per_class: [0.036, 0.005, 0.0, 0.013, 0.016, 0.202, 0.269, 0.092, 0.0, 0.047]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.22807835820895522
[2m[36m(func pid=159478)[0m top5: 0.8549440298507462
[2m[36m(func pid=159478)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=159478)[0m f1_macro: 0.2011484881484376
[2m[36m(func pid=159478)[0m f1_weighted: 0.2574897624894092
[2m[36m(func pid=159478)[0m f1_per_class: [0.092, 0.171, 0.059, 0.103, 0.152, 0.243, 0.465, 0.424, 0.07, 0.233]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.4999 | Steps: 4 | Val loss: 58.1969 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.1237 | Steps: 4 | Val loss: 1.5125 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.1534 | Steps: 4 | Val loss: 78.1052 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:20:26 (running for 00:37:03.59)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.033 |      0.405 |                   62 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.915 |      0.201 |                   61 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.5   |      0.101 |                   59 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.911 |      0.068 |                   36 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.2294776119402985
[2m[36m(func pid=160131)[0m top5: 0.7504664179104478
[2m[36m(func pid=160131)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=160131)[0m f1_macro: 0.10099183458386685
[2m[36m(func pid=160131)[0m f1_weighted: 0.1098628329513166
[2m[36m(func pid=160131)[0m f1_per_class: [0.071, 0.379, 0.0, 0.007, 0.0, 0.165, 0.0, 0.388, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.5621 | Steps: 4 | Val loss: 7.4951 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=159021)[0m top1: 0.45149253731343286
[2m[36m(func pid=159021)[0m top5: 0.9430970149253731
[2m[36m(func pid=159021)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=159021)[0m f1_macro: 0.394073247684137
[2m[36m(func pid=159021)[0m f1_weighted: 0.45297541291279353
[2m[36m(func pid=159021)[0m f1_per_class: [0.654, 0.277, 0.444, 0.655, 0.241, 0.312, 0.46, 0.355, 0.235, 0.308]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.10354477611940298
[2m[36m(func pid=165522)[0m top5: 0.7737873134328358
[2m[36m(func pid=165522)[0m f1_micro: 0.10354477611940298
[2m[36m(func pid=165522)[0m f1_macro: 0.07328317998291223
[2m[36m(func pid=165522)[0m f1_weighted: 0.0973854090154945
[2m[36m(func pid=165522)[0m f1_per_class: [0.057, 0.0, 0.0, 0.007, 0.015, 0.239, 0.189, 0.17, 0.0, 0.056]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.20242537313432835
[2m[36m(func pid=159478)[0m top5: 0.8227611940298507
[2m[36m(func pid=159478)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=159478)[0m f1_macro: 0.20106592284455999
[2m[36m(func pid=159478)[0m f1_weighted: 0.21773417108184107
[2m[36m(func pid=159478)[0m f1_per_class: [0.11, 0.252, 0.326, 0.105, 0.134, 0.169, 0.32, 0.346, 0.098, 0.149]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 7.8806 | Steps: 4 | Val loss: 63.5221 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0400 | Steps: 4 | Val loss: 1.4227 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.2745 | Steps: 4 | Val loss: 70.0288 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:20:32 (running for 00:37:09.22)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.124 |      0.394 |                   63 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.562 |      0.201 |                   62 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  7.881 |      0.081 |                   60 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.153 |      0.073 |                   37 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.1166044776119403
[2m[36m(func pid=160131)[0m top5: 0.7458022388059702
[2m[36m(func pid=160131)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=160131)[0m f1_macro: 0.08116397596459649
[2m[36m(func pid=160131)[0m f1_weighted: 0.07475670836502668
[2m[36m(func pid=160131)[0m f1_per_class: [0.061, 0.14, 0.0, 0.01, 0.0, 0.211, 0.0, 0.39, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.480410447761194
[2m[36m(func pid=159021)[0m top5: 0.9575559701492538
[2m[36m(func pid=159021)[0m f1_micro: 0.480410447761194
[2m[36m(func pid=159021)[0m f1_macro: 0.41108480904665645
[2m[36m(func pid=159021)[0m f1_weighted: 0.4872134831026956
[2m[36m(func pid=159021)[0m f1_per_class: [0.636, 0.405, 0.436, 0.662, 0.253, 0.332, 0.487, 0.357, 0.248, 0.296]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 3.9548 | Steps: 4 | Val loss: 7.8237 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=165522)[0m top1: 0.12173507462686567
[2m[36m(func pid=165522)[0m top5: 0.7336753731343284
[2m[36m(func pid=165522)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=165522)[0m f1_macro: 0.07683695102807882
[2m[36m(func pid=165522)[0m f1_weighted: 0.09071513131561605
[2m[36m(func pid=165522)[0m f1_per_class: [0.071, 0.0, 0.0, 0.0, 0.017, 0.277, 0.142, 0.261, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.197294776119403
[2m[36m(func pid=159478)[0m top5: 0.7817164179104478
[2m[36m(func pid=159478)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=159478)[0m f1_macro: 0.1694919106280913
[2m[36m(func pid=159478)[0m f1_weighted: 0.19702862205565216
[2m[36m(func pid=159478)[0m f1_per_class: [0.135, 0.387, 0.089, 0.075, 0.081, 0.14, 0.216, 0.332, 0.145, 0.094]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 5.6544 | Steps: 4 | Val loss: 49.8189 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1482 | Steps: 4 | Val loss: 1.4100 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 4.2075 | Steps: 4 | Val loss: 61.6477 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:20:37 (running for 00:37:14.72)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.04  |      0.411 |                   64 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  3.955 |      0.169 |                   63 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  5.654 |      0.078 |                   61 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.274 |      0.077 |                   38 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.10820895522388059
[2m[36m(func pid=160131)[0m top5: 0.7439365671641791
[2m[36m(func pid=160131)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=160131)[0m f1_macro: 0.07830739392825777
[2m[36m(func pid=160131)[0m f1_weighted: 0.0690575027758294
[2m[36m(func pid=160131)[0m f1_per_class: [0.061, 0.108, 0.0, 0.007, 0.0, 0.216, 0.0, 0.391, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.48134328358208955
[2m[36m(func pid=159021)[0m top5: 0.9598880597014925
[2m[36m(func pid=159021)[0m f1_micro: 0.48134328358208955
[2m[36m(func pid=159021)[0m f1_macro: 0.42767530179697266
[2m[36m(func pid=159021)[0m f1_weighted: 0.5020953438470478
[2m[36m(func pid=159021)[0m f1_per_class: [0.646, 0.473, 0.522, 0.627, 0.255, 0.342, 0.527, 0.341, 0.247, 0.296]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.2268 | Steps: 4 | Val loss: 7.1739 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=165522)[0m top1: 0.1394589552238806
[2m[36m(func pid=165522)[0m top5: 0.6819029850746269
[2m[36m(func pid=165522)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=165522)[0m f1_macro: 0.08312437015727937
[2m[36m(func pid=165522)[0m f1_weighted: 0.1011195725581206
[2m[36m(func pid=165522)[0m f1_per_class: [0.093, 0.0, 0.0, 0.007, 0.024, 0.264, 0.172, 0.271, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.2560634328358209
[2m[36m(func pid=159478)[0m top5: 0.7691231343283582
[2m[36m(func pid=159478)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=159478)[0m f1_macro: 0.19413834556562964
[2m[36m(func pid=159478)[0m f1_weighted: 0.25987670065688456
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.451, 0.122, 0.084, 0.081, 0.231, 0.35, 0.362, 0.135, 0.126]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0231 | Steps: 4 | Val loss: 1.3971 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 4.8836 | Steps: 4 | Val loss: 33.9851 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.3335 | Steps: 4 | Val loss: 29.3694 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:20:43 (running for 00:37:20.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.023 |      0.435 |                   66 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.227 |      0.194 |                   64 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  5.654 |      0.078 |                   61 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.208 |      0.083 |                   39 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4864738805970149
[2m[36m(func pid=159021)[0m top5: 0.9589552238805971
[2m[36m(func pid=159021)[0m f1_micro: 0.4864738805970149
[2m[36m(func pid=159021)[0m f1_macro: 0.43517680610228
[2m[36m(func pid=159021)[0m f1_weighted: 0.5041561261068039
[2m[36m(func pid=159021)[0m f1_per_class: [0.646, 0.522, 0.522, 0.635, 0.273, 0.351, 0.489, 0.362, 0.253, 0.299]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=160131)[0m top1: 0.11240671641791045
[2m[36m(func pid=160131)[0m top5: 0.7490671641791045
[2m[36m(func pid=160131)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=160131)[0m f1_macro: 0.08174428192118861
[2m[36m(func pid=160131)[0m f1_weighted: 0.07575712560754784
[2m[36m(func pid=160131)[0m f1_per_class: [0.061, 0.084, 0.0, 0.026, 0.0, 0.246, 0.006, 0.395, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6625 | Steps: 4 | Val loss: 8.8673 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=165522)[0m top1: 0.1394589552238806
[2m[36m(func pid=165522)[0m top5: 0.5984141791044776
[2m[36m(func pid=165522)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=165522)[0m f1_macro: 0.07859708546420338
[2m[36m(func pid=165522)[0m f1_weighted: 0.09363888082808948
[2m[36m(func pid=165522)[0m f1_per_class: [0.077, 0.0, 0.0, 0.01, 0.0, 0.24, 0.149, 0.288, 0.021, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.197294776119403
[2m[36m(func pid=159478)[0m top5: 0.7602611940298507
[2m[36m(func pid=159478)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=159478)[0m f1_macro: 0.15712815716450418
[2m[36m(func pid=159478)[0m f1_weighted: 0.17207919093380183
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.418, 0.097, 0.047, 0.148, 0.191, 0.138, 0.298, 0.109, 0.126]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2470 | Steps: 4 | Val loss: 1.4138 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 3.2634 | Steps: 4 | Val loss: 8.7487 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 4.1545 | Steps: 4 | Val loss: 26.3068 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 14:20:48 (running for 00:37:25.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.247 |      0.429 |                   67 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.663 |      0.157 |                   65 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  4.884 |      0.082 |                   62 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.333 |      0.079 |                   40 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.4780783582089552
[2m[36m(func pid=159021)[0m top5: 0.9575559701492538
[2m[36m(func pid=159021)[0m f1_micro: 0.4780783582089552
[2m[36m(func pid=159021)[0m f1_macro: 0.42926136764685213
[2m[36m(func pid=159021)[0m f1_weighted: 0.49090049034638883
[2m[36m(func pid=159021)[0m f1_per_class: [0.667, 0.524, 0.48, 0.637, 0.245, 0.327, 0.452, 0.347, 0.25, 0.364]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.3446 | Steps: 4 | Val loss: 9.2993 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=160131)[0m top1: 0.11100746268656717
[2m[36m(func pid=160131)[0m top5: 0.742070895522388
[2m[36m(func pid=160131)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=160131)[0m f1_macro: 0.08728025514453067
[2m[36m(func pid=160131)[0m f1_weighted: 0.09015418274260263
[2m[36m(func pid=160131)[0m f1_per_class: [0.06, 0.031, 0.0, 0.003, 0.0, 0.24, 0.1, 0.438, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=165522)[0m top1: 0.13152985074626866
[2m[36m(func pid=165522)[0m top5: 0.5932835820895522
[2m[36m(func pid=165522)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=165522)[0m f1_macro: 0.07331067948647893
[2m[36m(func pid=165522)[0m f1_weighted: 0.08674413325245948
[2m[36m(func pid=165522)[0m f1_per_class: [0.082, 0.0, 0.0, 0.0, 0.0, 0.227, 0.146, 0.262, 0.017, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.17583955223880596
[2m[36m(func pid=159478)[0m top5: 0.7714552238805971
[2m[36m(func pid=159478)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=159478)[0m f1_macro: 0.16140997603783702
[2m[36m(func pid=159478)[0m f1_weighted: 0.17892871958245704
[2m[36m(func pid=159478)[0m f1_per_class: [0.0, 0.402, 0.24, 0.082, 0.174, 0.277, 0.144, 0.085, 0.101, 0.11]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0552 | Steps: 4 | Val loss: 1.3571 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.9576 | Steps: 4 | Val loss: 10.7588 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.1407 | Steps: 4 | Val loss: 19.1467 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:20:54 (running for 00:37:31.03)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.247 |      0.429 |                   67 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.345 |      0.161 |                   66 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  2.958 |      0.078 |                   64 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.154 |      0.073 |                   41 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.14132462686567165
[2m[36m(func pid=160131)[0m top5: 0.7448694029850746
[2m[36m(func pid=160131)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=160131)[0m f1_macro: 0.07828122389223101
[2m[36m(func pid=160131)[0m f1_weighted: 0.16352117999953084
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.005, 0.018, 0.003, 0.0, 0.259, 0.43, 0.068, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.5837 | Steps: 4 | Val loss: 9.0824 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=159021)[0m top1: 0.49906716417910446
[2m[36m(func pid=159021)[0m top5: 0.9617537313432836
[2m[36m(func pid=159021)[0m f1_micro: 0.49906716417910446
[2m[36m(func pid=159021)[0m f1_macro: 0.44216648612887777
[2m[36m(func pid=159021)[0m f1_weighted: 0.5104708512232312
[2m[36m(func pid=159021)[0m f1_per_class: [0.66, 0.539, 0.5, 0.638, 0.277, 0.334, 0.502, 0.345, 0.279, 0.348]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.11380597014925373
[2m[36m(func pid=165522)[0m top5: 0.5872201492537313
[2m[36m(func pid=165522)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=165522)[0m f1_macro: 0.0678642570295673
[2m[36m(func pid=165522)[0m f1_weighted: 0.07737341383432962
[2m[36m(func pid=165522)[0m f1_per_class: [0.077, 0.005, 0.0, 0.0, 0.0, 0.187, 0.128, 0.247, 0.035, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.19636194029850745
[2m[36m(func pid=159478)[0m top5: 0.7943097014925373
[2m[36m(func pid=159478)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=159478)[0m f1_macro: 0.19495028607330617
[2m[36m(func pid=159478)[0m f1_weighted: 0.21682600610737235
[2m[36m(func pid=159478)[0m f1_per_class: [0.108, 0.389, 0.45, 0.149, 0.0, 0.35, 0.18, 0.083, 0.096, 0.144]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 3.2614 | Steps: 4 | Val loss: 19.5469 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1428 | Steps: 4 | Val loss: 1.3776 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.9278 | Steps: 4 | Val loss: 27.2415 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:20:59 (running for 00:37:36.49)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.055 |      0.442 |                   68 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  2.584 |      0.195 |                   67 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.261 |      0.073 |                   65 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.141 |      0.068 |                   42 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.8144 | Steps: 4 | Val loss: 8.4884 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=160131)[0m top1: 0.14598880597014927
[2m[36m(func pid=160131)[0m top5: 0.7481343283582089
[2m[36m(func pid=160131)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=160131)[0m f1_macro: 0.0727792318084984
[2m[36m(func pid=160131)[0m f1_weighted: 0.1647180804636689
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.018, 0.003, 0.0, 0.255, 0.451, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.5004664179104478
[2m[36m(func pid=159021)[0m top5: 0.960820895522388
[2m[36m(func pid=159021)[0m f1_micro: 0.5004664179104478
[2m[36m(func pid=159021)[0m f1_macro: 0.44380013286228703
[2m[36m(func pid=159021)[0m f1_weighted: 0.5121293511038632
[2m[36m(func pid=159021)[0m f1_per_class: [0.646, 0.57, 0.545, 0.637, 0.226, 0.339, 0.485, 0.377, 0.276, 0.337]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.07835820895522388
[2m[36m(func pid=165522)[0m top5: 0.5932835820895522
[2m[36m(func pid=165522)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=165522)[0m f1_macro: 0.04592595886617856
[2m[36m(func pid=165522)[0m f1_weighted: 0.04480557197611586
[2m[36m(func pid=165522)[0m f1_per_class: [0.069, 0.0, 0.0, 0.0, 0.0, 0.023, 0.091, 0.2, 0.058, 0.019]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.197294776119403
[2m[36m(func pid=159478)[0m top5: 0.7957089552238806
[2m[36m(func pid=159478)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=159478)[0m f1_macro: 0.22252266076385746
[2m[36m(func pid=159478)[0m f1_weighted: 0.2107934240730235
[2m[36m(func pid=159478)[0m f1_per_class: [0.142, 0.335, 0.571, 0.107, 0.095, 0.327, 0.215, 0.156, 0.11, 0.165]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.4323 | Steps: 4 | Val loss: 25.5455 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0383 | Steps: 4 | Val loss: 1.4023 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7428 | Steps: 4 | Val loss: 21.5093 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6217 | Steps: 4 | Val loss: 9.0402 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:21:05 (running for 00:37:42.10)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.143 |      0.444 |                   69 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.814 |      0.223 |                   68 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.432 |      0.076 |                   66 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.928 |      0.046 |                   43 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.15578358208955223
[2m[36m(func pid=160131)[0m top5: 0.6725746268656716
[2m[36m(func pid=160131)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=160131)[0m f1_macro: 0.07605883599921621
[2m[36m(func pid=160131)[0m f1_weighted: 0.17261753365715557
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.019, 0.003, 0.0, 0.264, 0.474, 0.0, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.48927238805970147
[2m[36m(func pid=159021)[0m top5: 0.9575559701492538
[2m[36m(func pid=159021)[0m f1_micro: 0.48927238805970147
[2m[36m(func pid=159021)[0m f1_macro: 0.43567501618789245
[2m[36m(func pid=159021)[0m f1_weighted: 0.4999211672811477
[2m[36m(func pid=159021)[0m f1_per_class: [0.653, 0.532, 0.533, 0.637, 0.222, 0.296, 0.485, 0.37, 0.258, 0.371]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.07042910447761194
[2m[36m(func pid=165522)[0m top5: 0.7252798507462687
[2m[36m(func pid=165522)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=165522)[0m f1_macro: 0.03988046641381353
[2m[36m(func pid=165522)[0m f1_weighted: 0.03617532283412655
[2m[36m(func pid=165522)[0m f1_per_class: [0.063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.073, 0.2, 0.042, 0.02]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.14972014925373134
[2m[36m(func pid=159478)[0m top5: 0.7472014925373134
[2m[36m(func pid=159478)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=159478)[0m f1_macro: 0.14322806816319938
[2m[36m(func pid=159478)[0m f1_weighted: 0.16612970155302573
[2m[36m(func pid=159478)[0m f1_per_class: [0.142, 0.267, 0.0, 0.103, 0.222, 0.215, 0.172, 0.11, 0.097, 0.103]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 7.6613 | Steps: 4 | Val loss: 32.9944 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0994 | Steps: 4 | Val loss: 1.4365 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6922 | Steps: 4 | Val loss: 19.6121 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.8032 | Steps: 4 | Val loss: 8.1827 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=160131)[0m top1: 0.15345149253731344
[2m[36m(func pid=160131)[0m top5: 0.6268656716417911
[2m[36m(func pid=160131)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=160131)[0m f1_macro: 0.07966844432791031
[2m[36m(func pid=160131)[0m f1_weighted: 0.1711612363841162
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.02, 0.003, 0.0, 0.281, 0.462, 0.0, 0.0, 0.03]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:21:10 (running for 00:37:47.59)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.038 |      0.436 |                   70 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.622 |      0.143 |                   69 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  7.661 |      0.08  |                   67 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.743 |      0.04  |                   44 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.480410447761194
[2m[36m(func pid=159021)[0m top5: 0.957089552238806
[2m[36m(func pid=159021)[0m f1_micro: 0.480410447761194
[2m[36m(func pid=159021)[0m f1_macro: 0.43970892478786555
[2m[36m(func pid=159021)[0m f1_weighted: 0.487046947327748
[2m[36m(func pid=159021)[0m f1_per_class: [0.66, 0.568, 0.545, 0.623, 0.218, 0.308, 0.424, 0.38, 0.264, 0.405]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.12919776119402984
[2m[36m(func pid=165522)[0m top5: 0.7397388059701493
[2m[36m(func pid=165522)[0m f1_micro: 0.12919776119402984
[2m[36m(func pid=165522)[0m f1_macro: 0.06293810290698124
[2m[36m(func pid=165522)[0m f1_weighted: 0.1347971878190171
[2m[36m(func pid=165522)[0m f1_per_class: [0.059, 0.03, 0.0, 0.0, 0.0, 0.0, 0.411, 0.085, 0.019, 0.025]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.18190298507462688
[2m[36m(func pid=159478)[0m top5: 0.7234141791044776
[2m[36m(func pid=159478)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=159478)[0m f1_macro: 0.17632830517692868
[2m[36m(func pid=159478)[0m f1_weighted: 0.2080160152741335
[2m[36m(func pid=159478)[0m f1_per_class: [0.171, 0.376, 0.0, 0.208, 0.356, 0.139, 0.167, 0.155, 0.107, 0.084]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 9.0496 | Steps: 4 | Val loss: 37.3704 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0136 | Steps: 4 | Val loss: 1.4612 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.5887 | Steps: 4 | Val loss: 19.8722 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6157 | Steps: 4 | Val loss: 7.2309 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 14:21:16 (running for 00:37:53.03)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.099 |      0.44  |                   71 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.803 |      0.176 |                   70 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  9.05  |      0.037 |                   68 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.692 |      0.063 |                   45 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.46595149253731344
[2m[36m(func pid=159021)[0m top5: 0.9561567164179104
[2m[36m(func pid=159021)[0m f1_micro: 0.46595149253731344
[2m[36m(func pid=159021)[0m f1_macro: 0.4274120263331507
[2m[36m(func pid=159021)[0m f1_weighted: 0.47620379437434796
[2m[36m(func pid=159021)[0m f1_per_class: [0.603, 0.577, 0.545, 0.604, 0.196, 0.288, 0.415, 0.37, 0.257, 0.419]
[2m[36m(func pid=160131)[0m top1: 0.051305970149253734
[2m[36m(func pid=160131)[0m top5: 0.6357276119402985
[2m[36m(func pid=160131)[0m f1_micro: 0.051305970149253734
[2m[36m(func pid=160131)[0m f1_macro: 0.0374526278104969
[2m[36m(func pid=160131)[0m f1_weighted: 0.04573473441550472
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.0, 0.286, 0.042, 0.0, 0.0, 0.025]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.15764925373134328
[2m[36m(func pid=165522)[0m top5: 0.7541977611940298
[2m[36m(func pid=165522)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=165522)[0m f1_macro: 0.09464658096289782
[2m[36m(func pid=165522)[0m f1_weighted: 0.17468806358097186
[2m[36m(func pid=165522)[0m f1_per_class: [0.043, 0.113, 0.0, 0.0, 0.047, 0.08, 0.46, 0.076, 0.087, 0.04]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.25
[2m[36m(func pid=159478)[0m top5: 0.7555970149253731
[2m[36m(func pid=159478)[0m f1_micro: 0.25
[2m[36m(func pid=159478)[0m f1_macro: 0.1923454219939773
[2m[36m(func pid=159478)[0m f1_weighted: 0.26131185607025464
[2m[36m(func pid=159478)[0m f1_per_class: [0.188, 0.42, 0.0, 0.381, 0.217, 0.035, 0.174, 0.27, 0.148, 0.091]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0143 | Steps: 4 | Val loss: 1.5266 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 14.0797 | Steps: 4 | Val loss: 24.3142 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7245 | Steps: 4 | Val loss: 15.1630 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.9185 | Steps: 4 | Val loss: 6.9429 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=159021)[0m top1: 0.4556902985074627
[2m[36m(func pid=159021)[0m top5: 0.9482276119402985
[2m[36m(func pid=159021)[0m f1_micro: 0.4556902985074627
[2m[36m(func pid=159021)[0m f1_macro: 0.42387216824903745
[2m[36m(func pid=159021)[0m f1_weighted: 0.46356867227122533
[2m[36m(func pid=159021)[0m f1_per_class: [0.623, 0.585, 0.6, 0.594, 0.175, 0.261, 0.388, 0.357, 0.261, 0.395]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=160131)[0m top1: 0.04664179104477612
[2m[36m(func pid=160131)[0m top5: 0.6338619402985075
[2m[36m(func pid=160131)[0m f1_micro: 0.04664179104477612
[2m[36m(func pid=160131)[0m f1_macro: 0.037759295569273994
[2m[36m(func pid=160131)[0m f1_weighted: 0.04206390907852078
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.021, 0.007, 0.0, 0.268, 0.025, 0.031, 0.0, 0.026]
== Status ==
Current time: 2024-01-07 14:21:21 (running for 00:37:58.65)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.014 |      0.424 |                   73 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.616 |      0.192 |                   71 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  9.05  |      0.037 |                   68 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.589 |      0.095 |                   46 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=165522)[0m top1: 0.1357276119402985
[2m[36m(func pid=165522)[0m top5: 0.7630597014925373
[2m[36m(func pid=165522)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=165522)[0m f1_macro: 0.08544119845624855
[2m[36m(func pid=165522)[0m f1_weighted: 0.1606064628548262
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.1, 0.023, 0.003, 0.021, 0.106, 0.409, 0.095, 0.068, 0.029]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.27611940298507465
[2m[36m(func pid=159478)[0m top5: 0.7817164179104478
[2m[36m(func pid=159478)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=159478)[0m f1_macro: 0.23797986005535793
[2m[36m(func pid=159478)[0m f1_weighted: 0.2866905857725021
[2m[36m(func pid=159478)[0m f1_per_class: [0.135, 0.427, 0.4, 0.383, 0.198, 0.0, 0.249, 0.327, 0.16, 0.1]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.9662 | Steps: 4 | Val loss: 12.8559 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0289 | Steps: 4 | Val loss: 1.5540 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 3.5810 | Steps: 4 | Val loss: 26.6637 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3690 | Steps: 4 | Val loss: 7.1080 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:21:27 (running for 00:38:04.14)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.014 |      0.424 |                   73 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.919 |      0.238 |                   72 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  2.966 |      0.065 |                   70 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.725 |      0.085 |                   47 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m top1: 0.08861940298507463
[2m[36m(func pid=160131)[0m top5: 0.6222014925373134
[2m[36m(func pid=160131)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=160131)[0m f1_macro: 0.06518098346651142
[2m[36m(func pid=160131)[0m f1_weighted: 0.05786919851008225
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.02, 0.007, 0.0, 0.258, 0.022, 0.346, 0.0, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4552238805970149
[2m[36m(func pid=159021)[0m top5: 0.9458955223880597
[2m[36m(func pid=159021)[0m f1_micro: 0.4552238805970149
[2m[36m(func pid=159021)[0m f1_macro: 0.4266351771833944
[2m[36m(func pid=159021)[0m f1_weighted: 0.4630121604624221
[2m[36m(func pid=159021)[0m f1_per_class: [0.579, 0.601, 0.585, 0.587, 0.206, 0.251, 0.387, 0.356, 0.273, 0.442]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.166044776119403
[2m[36m(func pid=165522)[0m top5: 0.7770522388059702
[2m[36m(func pid=165522)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=165522)[0m f1_macro: 0.0828057228487915
[2m[36m(func pid=165522)[0m f1_weighted: 0.1787635013586881
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.043, 0.024, 0.007, 0.0, 0.156, 0.492, 0.056, 0.05, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.27705223880597013
[2m[36m(func pid=159478)[0m top5: 0.7761194029850746
[2m[36m(func pid=159478)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=159478)[0m f1_macro: 0.25039110749914284
[2m[36m(func pid=159478)[0m f1_weighted: 0.2665810889258279
[2m[36m(func pid=159478)[0m f1_per_class: [0.085, 0.386, 0.519, 0.42, 0.306, 0.0, 0.168, 0.327, 0.155, 0.139]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.3386 | Steps: 4 | Val loss: 10.6303 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0357 | Steps: 4 | Val loss: 1.5526 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.8534 | Steps: 4 | Val loss: 13.8387 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.0421 | Steps: 4 | Val loss: 7.5783 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=160131)[0m top1: 0.08255597014925373
[2m[36m(func pid=160131)[0m top5: 0.6105410447761194
[2m[36m(func pid=160131)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=160131)[0m f1_macro: 0.06460315519733453
[2m[36m(func pid=160131)[0m f1_weighted: 0.05247426270415279
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.019, 0.003, 0.0, 0.23, 0.012, 0.368, 0.0, 0.013]
[2m[36m(func pid=160131)[0m 
== Status ==
Current time: 2024-01-07 14:21:32 (running for 00:38:09.68)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.029 |      0.427 |                   74 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  0.369 |      0.25  |                   73 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.339 |      0.065 |                   71 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.581 |      0.083 |                   48 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.44869402985074625
[2m[36m(func pid=159021)[0m top5: 0.9482276119402985
[2m[36m(func pid=159021)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=159021)[0m f1_macro: 0.41901161376115004
[2m[36m(func pid=159021)[0m f1_weighted: 0.45272764219551437
[2m[36m(func pid=159021)[0m f1_per_class: [0.611, 0.583, 0.545, 0.591, 0.201, 0.243, 0.363, 0.352, 0.265, 0.435]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.0853544776119403
[2m[36m(func pid=165522)[0m top5: 0.7691231343283582
[2m[36m(func pid=165522)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=165522)[0m f1_macro: 0.06479464061851442
[2m[36m(func pid=165522)[0m f1_weighted: 0.06742392695183566
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.082, 0.021, 0.0, 0.0, 0.159, 0.06, 0.257, 0.068, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.23787313432835822
[2m[36m(func pid=159478)[0m top5: 0.7472014925373134
[2m[36m(func pid=159478)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=159478)[0m f1_macro: 0.2238218814871426
[2m[36m(func pid=159478)[0m f1_weighted: 0.2316007784560418
[2m[36m(func pid=159478)[0m f1_per_class: [0.056, 0.382, 0.562, 0.408, 0.262, 0.0, 0.095, 0.164, 0.192, 0.117]
[2m[36m(func pid=159478)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0513 | Steps: 4 | Val loss: 1.5545 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.6308 | Steps: 4 | Val loss: 8.5291 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7819 | Steps: 4 | Val loss: 16.0050 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=159478)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.0571 | Steps: 4 | Val loss: 8.5377 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=159021)[0m top1: 0.4542910447761194
[2m[36m(func pid=159021)[0m top5: 0.9463619402985075
[2m[36m(func pid=159021)[0m f1_micro: 0.4542910447761194
[2m[36m(func pid=159021)[0m f1_macro: 0.42234963120813934
[2m[36m(func pid=159021)[0m f1_weighted: 0.46028156480979115
[2m[36m(func pid=159021)[0m f1_per_class: [0.602, 0.593, 0.522, 0.585, 0.205, 0.261, 0.38, 0.354, 0.274, 0.447]
== Status ==
Current time: 2024-01-07 14:21:38 (running for 00:38:15.20)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.051 |      0.422 |                   76 |
| train_5806f_00017 | RUNNING    | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.042 |      0.224 |                   74 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.339 |      0.065 |                   71 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.853 |      0.065 |                   49 |
| train_5806f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=160131)[0m top1: 0.07649253731343283
[2m[36m(func pid=160131)[0m top5: 0.6012126865671642
[2m[36m(func pid=160131)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=160131)[0m f1_macro: 0.06630317363419072
[2m[36m(func pid=160131)[0m f1_weighted: 0.045783449597154226
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.019, 0.0, 0.0, 0.179, 0.003, 0.376, 0.073, 0.014]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=165522)[0m top1: 0.10074626865671642
[2m[36m(func pid=165522)[0m top5: 0.7667910447761194
[2m[36m(func pid=165522)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=165522)[0m f1_macro: 0.07680066488619566
[2m[36m(func pid=165522)[0m f1_weighted: 0.0821385011685667
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.116, 0.017, 0.0, 0.0, 0.154, 0.078, 0.321, 0.082, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159478)[0m top1: 0.14878731343283583
[2m[36m(func pid=159478)[0m top5: 0.7495335820895522
[2m[36m(func pid=159478)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=159478)[0m f1_macro: 0.14260381655122512
[2m[36m(func pid=159478)[0m f1_weighted: 0.1706469659066012
[2m[36m(func pid=159478)[0m f1_per_class: [0.066, 0.37, 0.138, 0.199, 0.267, 0.0, 0.134, 0.069, 0.063, 0.12]
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0554 | Steps: 4 | Val loss: 1.5053 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 11.0147 | Steps: 4 | Val loss: 14.2111 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.9546 | Steps: 4 | Val loss: 14.3226 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=159021)[0m top1: 0.4701492537313433
[2m[36m(func pid=159021)[0m top5: 0.9524253731343284
[2m[36m(func pid=159021)[0m f1_micro: 0.47014925373134325
[2m[36m(func pid=159021)[0m f1_macro: 0.4382561410376141
[2m[36m(func pid=159021)[0m f1_weighted: 0.484925378947714
[2m[36m(func pid=159021)[0m f1_per_class: [0.629, 0.602, 0.6, 0.579, 0.212, 0.278, 0.455, 0.36, 0.258, 0.41]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=160131)[0m top1: 0.07975746268656717
[2m[36m(func pid=160131)[0m top5: 0.628731343283582
[2m[36m(func pid=160131)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=160131)[0m f1_macro: 0.0629071153440492
[2m[36m(func pid=160131)[0m f1_weighted: 0.04824670521991684
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.02, 0.003, 0.0, 0.191, 0.012, 0.337, 0.065, 0.0]
[2m[36m(func pid=165522)[0m top1: 0.09841417910447761
[2m[36m(func pid=165522)[0m top5: 0.7555970149253731
[2m[36m(func pid=165522)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=165522)[0m f1_macro: 0.07164344444889147
[2m[36m(func pid=165522)[0m f1_weighted: 0.08169838100311969
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.158, 0.01, 0.0, 0.0, 0.078, 0.082, 0.334, 0.054, 0.0]
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0259 | Steps: 4 | Val loss: 1.5390 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:21:43 (running for 00:38:20.68)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35025
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.055 |      0.438 |                   77 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.631 |      0.066 |                   72 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.782 |      0.077 |                   50 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=177931)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=177931)[0m Configuration completed!
[2m[36m(func pid=177931)[0m New optimizer parameters:
[2m[36m(func pid=177931)[0m SGD (
[2m[36m(func pid=177931)[0m Parameter Group 0
[2m[36m(func pid=177931)[0m     dampening: 0
[2m[36m(func pid=177931)[0m     differentiable: False
[2m[36m(func pid=177931)[0m     foreach: None
[2m[36m(func pid=177931)[0m     lr: 0.0001
[2m[36m(func pid=177931)[0m     maximize: False
[2m[36m(func pid=177931)[0m     momentum: 0.9
[2m[36m(func pid=177931)[0m     nesterov: False
[2m[36m(func pid=177931)[0m     weight_decay: 1e-05
[2m[36m(func pid=177931)[0m )
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:21:49 (running for 00:38:26.07)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35025
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.026 |      0.428 |                   78 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  | 11.015 |      0.063 |                   73 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.955 |      0.072 |                   51 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.45988805970149255
[2m[36m(func pid=159021)[0m top5: 0.9486940298507462
[2m[36m(func pid=159021)[0m f1_micro: 0.45988805970149255
[2m[36m(func pid=159021)[0m f1_macro: 0.42830658976668107
[2m[36m(func pid=159021)[0m f1_weighted: 0.4760048987094366
[2m[36m(func pid=159021)[0m f1_per_class: [0.596, 0.584, 0.558, 0.57, 0.217, 0.27, 0.45, 0.36, 0.256, 0.421]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.9701 | Steps: 4 | Val loss: 8.1995 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.8451 | Steps: 4 | Val loss: 12.1230 | Batch size: 32 | lr: 0.1 | Duration: 3.32s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0529 | Steps: 4 | Val loss: 1.5326 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1160 | Steps: 4 | Val loss: 2.5390 | Batch size: 32 | lr: 0.0001 | Duration: 4.76s
[2m[36m(func pid=160131)[0m top1: 0.08162313432835822
[2m[36m(func pid=160131)[0m top5: 0.613339552238806
[2m[36m(func pid=160131)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=160131)[0m f1_macro: 0.06374267218380683
[2m[36m(func pid=160131)[0m f1_weighted: 0.05645418191458819
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.02, 0.0, 0.0, 0.072, 0.078, 0.379, 0.088, 0.0]
[2m[36m(func pid=160131)[0m 
[2m[36m(func pid=165522)[0m top1: 0.09468283582089553
[2m[36m(func pid=165522)[0m top5: 0.7509328358208955
[2m[36m(func pid=165522)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=165522)[0m f1_macro: 0.06507703291392805
[2m[36m(func pid=165522)[0m f1_weighted: 0.07320950857701143
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.156, 0.0, 0.01, 0.0, 0.059, 0.056, 0.32, 0.05, 0.0]
[2m[36m(func pid=165522)[0m 
== Status ==
Current time: 2024-01-07 14:21:54 (running for 00:38:31.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35025
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.053 |      0.453 |                   79 |
| train_5806f_00018 | RUNNING    | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  2.97  |      0.064 |                   74 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.845 |      0.065 |                   52 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m top1: 0.47388059701492535
[2m[36m(func pid=159021)[0m top5: 0.9505597014925373
[2m[36m(func pid=159021)[0m f1_micro: 0.47388059701492535
[2m[36m(func pid=159021)[0m f1_macro: 0.4533280643103457
[2m[36m(func pid=159021)[0m f1_weighted: 0.49362543823475774
[2m[36m(func pid=159021)[0m f1_per_class: [0.641, 0.602, 0.649, 0.558, 0.22, 0.27, 0.505, 0.345, 0.27, 0.474]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=177931)[0m top1: 0.06343283582089553
[2m[36m(func pid=177931)[0m top5: 0.48367537313432835
[2m[36m(func pid=177931)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=177931)[0m f1_macro: 0.032531052897215564
[2m[36m(func pid=177931)[0m f1_weighted: 0.03436901382079696
[2m[36m(func pid=177931)[0m f1_per_class: [0.055, 0.01, 0.0, 0.08, 0.0, 0.019, 0.0, 0.102, 0.024, 0.036]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=160131)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 3.1537 | Steps: 4 | Val loss: 6.9220 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 3.4201 | Steps: 4 | Val loss: 8.3290 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.1671 | Steps: 4 | Val loss: 1.5388 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.2213 | Steps: 4 | Val loss: 2.5615 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=160131)[0m top1: 0.10261194029850747
[2m[36m(func pid=160131)[0m top5: 0.6277985074626866
[2m[36m(func pid=160131)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=160131)[0m f1_macro: 0.06908126787736175
[2m[36m(func pid=160131)[0m f1_weighted: 0.08828190433473282
[2m[36m(func pid=160131)[0m f1_per_class: [0.0, 0.0, 0.021, 0.003, 0.0, 0.0, 0.21, 0.384, 0.072, 0.0]
[2m[36m(func pid=165522)[0m top1: 0.14925373134328357
[2m[36m(func pid=165522)[0m top5: 0.7276119402985075
[2m[36m(func pid=165522)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=165522)[0m f1_macro: 0.10665307306679823
[2m[36m(func pid=165522)[0m f1_weighted: 0.14290565458676194
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.244, 0.0, 0.098, 0.0, 0.164, 0.102, 0.392, 0.047, 0.02]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159021)[0m top1: 0.47947761194029853
[2m[36m(func pid=159021)[0m top5: 0.9500932835820896
[2m[36m(func pid=159021)[0m f1_micro: 0.47947761194029853
[2m[36m(func pid=159021)[0m f1_macro: 0.45185110003120654
[2m[36m(func pid=159021)[0m f1_weighted: 0.4981806159012009
[2m[36m(func pid=159021)[0m f1_per_class: [0.653, 0.615, 0.667, 0.55, 0.207, 0.255, 0.528, 0.329, 0.281, 0.434]
[2m[36m(func pid=177931)[0m top1: 0.06343283582089553
[2m[36m(func pid=177931)[0m top5: 0.4701492537313433
[2m[36m(func pid=177931)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=177931)[0m f1_macro: 0.034309675321739376
[2m[36m(func pid=177931)[0m f1_weighted: 0.03652975522743059
[2m[36m(func pid=177931)[0m f1_per_class: [0.046, 0.019, 0.0, 0.082, 0.0, 0.019, 0.0, 0.104, 0.024, 0.05]
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.0606 | Steps: 4 | Val loss: 7.2021 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=165522)[0m top1: 0.17210820895522388
[2m[36m(func pid=165522)[0m top5: 0.7276119402985075
[2m[36m(func pid=165522)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=165522)[0m f1_macro: 0.10478171732959204
[2m[36m(func pid=165522)[0m f1_weighted: 0.13203141976752625
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.368, 0.0, 0.05, 0.035, 0.134, 0.056, 0.382, 0.0, 0.022]
== Status ==
Current time: 2024-01-07 14:21:59 (running for 00:38:36.67)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.053 |      0.453 |                   79 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.42  |      0.107 |                   53 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  3.116 |      0.033 |                    1 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=178896)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=178896)[0m Configuration completed!
[2m[36m(func pid=178896)[0m New optimizer parameters:
[2m[36m(func pid=178896)[0m SGD (
[2m[36m(func pid=178896)[0m Parameter Group 0
[2m[36m(func pid=178896)[0m     dampening: 0
[2m[36m(func pid=178896)[0m     differentiable: False
[2m[36m(func pid=178896)[0m     foreach: None
[2m[36m(func pid=178896)[0m     lr: 0.001
[2m[36m(func pid=178896)[0m     maximize: False
[2m[36m(func pid=178896)[0m     momentum: 0.9
[2m[36m(func pid=178896)[0m     nesterov: False
[2m[36m(func pid=178896)[0m     weight_decay: 1e-05
[2m[36m(func pid=178896)[0m )
[2m[36m(func pid=178896)[0m 
== Status ==
Current time: 2024-01-07 14:22:07 (running for 00:38:44.56)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.053 |      0.453 |                   79 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.061 |      0.105 |                   54 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  3.116 |      0.033 |                    1 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6888 | Steps: 4 | Val loss: 7.1239 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1147 | Steps: 4 | Val loss: 2.5675 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0308 | Steps: 4 | Val loss: 1.4925 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0717 | Steps: 4 | Val loss: 2.4619 | Batch size: 32 | lr: 0.001 | Duration: 4.98s
== Status ==
Current time: 2024-01-07 14:22:12 (running for 00:38:49.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.167 |      0.452 |                   80 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.061 |      0.105 |                   54 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  3.221 |      0.034 |                    2 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.06436567164179105
[2m[36m(func pid=177931)[0m top5: 0.45382462686567165
[2m[36m(func pid=177931)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=177931)[0m f1_macro: 0.03647896930109203
[2m[36m(func pid=177931)[0m f1_weighted: 0.04301165409752691
[2m[36m(func pid=177931)[0m f1_per_class: [0.037, 0.058, 0.0, 0.087, 0.0, 0.007, 0.0, 0.103, 0.022, 0.052]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=159021)[0m top1: 0.488339552238806
[2m[36m(func pid=159021)[0m top5: 0.9510261194029851
[2m[36m(func pid=159021)[0m f1_micro: 0.488339552238806
[2m[36m(func pid=159021)[0m f1_macro: 0.45503524401429274
[2m[36m(func pid=159021)[0m f1_weighted: 0.5034993047683511
[2m[36m(func pid=159021)[0m f1_per_class: [0.617, 0.616, 0.649, 0.571, 0.231, 0.249, 0.527, 0.334, 0.291, 0.466]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.1296641791044776
[2m[36m(func pid=165522)[0m top5: 0.7313432835820896
[2m[36m(func pid=165522)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=165522)[0m f1_macro: 0.08979787068508058
[2m[36m(func pid=165522)[0m f1_weighted: 0.11113332468782308
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.225, 0.0, 0.104, 0.032, 0.155, 0.018, 0.339, 0.0, 0.024]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.07602611940298508
[2m[36m(func pid=178896)[0m top5: 0.478544776119403
[2m[36m(func pid=178896)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=178896)[0m f1_macro: 0.05851489796228695
[2m[36m(func pid=178896)[0m f1_weighted: 0.05548854208310205
[2m[36m(func pid=178896)[0m f1_per_class: [0.167, 0.072, 0.0, 0.1, 0.0, 0.027, 0.0, 0.108, 0.054, 0.058]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9318 | Steps: 4 | Val loss: 2.5658 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0243 | Steps: 4 | Val loss: 1.5191 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5983 | Steps: 4 | Val loss: 6.1083 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7144 | Steps: 4 | Val loss: 2.4136 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=177931)[0m top1: 0.07136194029850747
[2m[36m(func pid=177931)[0m top5: 0.4351679104477612
[2m[36m(func pid=177931)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=177931)[0m f1_macro: 0.049854217804949655
[2m[36m(func pid=177931)[0m f1_weighted: 0.05582417563006565
[2m[36m(func pid=177931)[0m f1_per_class: [0.067, 0.068, 0.0, 0.115, 0.0, 0.012, 0.0, 0.11, 0.073, 0.055]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:22:18 (running for 00:38:55.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.031 |      0.455 |                   81 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.689 |      0.09  |                   55 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.932 |      0.05  |                    4 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  3.072 |      0.059 |                    1 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165522)[0m top1: 0.25279850746268656
[2m[36m(func pid=165522)[0m top5: 0.7425373134328358
[2m[36m(func pid=165522)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=165522)[0m f1_macro: 0.13033785672640563
[2m[36m(func pid=165522)[0m f1_weighted: 0.21040957442730962
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.158, 0.0, 0.44, 0.026, 0.283, 0.025, 0.356, 0.0, 0.016]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4855410447761194
[2m[36m(func pid=159021)[0m top5: 0.9463619402985075
[2m[36m(func pid=159021)[0m f1_micro: 0.4855410447761194
[2m[36m(func pid=159021)[0m f1_macro: 0.4410628001701526
[2m[36m(func pid=159021)[0m f1_weighted: 0.4953455596455702
[2m[36m(func pid=159021)[0m f1_per_class: [0.576, 0.63, 0.585, 0.559, 0.187, 0.233, 0.509, 0.36, 0.291, 0.48]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=178896)[0m top1: 0.08582089552238806
[2m[36m(func pid=178896)[0m top5: 0.47574626865671643
[2m[36m(func pid=178896)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=178896)[0m f1_macro: 0.0986079431298542
[2m[36m(func pid=178896)[0m f1_weighted: 0.08012698176501817
[2m[36m(func pid=178896)[0m f1_per_class: [0.147, 0.175, 0.25, 0.099, 0.0, 0.032, 0.009, 0.141, 0.085, 0.049]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8394 | Steps: 4 | Val loss: 2.5509 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.0676 | Steps: 4 | Val loss: 5.9972 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0193 | Steps: 4 | Val loss: 1.5175 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5563 | Steps: 4 | Val loss: 2.3997 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:22:23 (running for 00:39:00.69)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.024 |      0.441 |                   82 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.598 |      0.13  |                   56 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.839 |      0.054 |                    5 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  2.714 |      0.099 |                    2 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.07369402985074627
[2m[36m(func pid=177931)[0m top5: 0.42024253731343286
[2m[36m(func pid=177931)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=177931)[0m f1_macro: 0.0539488877420274
[2m[36m(func pid=177931)[0m f1_weighted: 0.07096879009291461
[2m[36m(func pid=177931)[0m f1_per_class: [0.045, 0.093, 0.0, 0.147, 0.0, 0.023, 0.003, 0.109, 0.079, 0.04]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=165522)[0m top1: 0.12826492537313433
[2m[36m(func pid=165522)[0m top5: 0.7546641791044776
[2m[36m(func pid=165522)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=165522)[0m f1_macro: 0.08894024961045247
[2m[36m(func pid=165522)[0m f1_weighted: 0.10045150417017829
[2m[36m(func pid=165522)[0m f1_per_class: [0.058, 0.043, 0.0, 0.026, 0.0, 0.295, 0.103, 0.348, 0.0, 0.017]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159021)[0m top1: 0.48367537313432835
[2m[36m(func pid=159021)[0m top5: 0.9477611940298507
[2m[36m(func pid=159021)[0m f1_micro: 0.4836753731343283
[2m[36m(func pid=159021)[0m f1_macro: 0.4449322259012331
[2m[36m(func pid=159021)[0m f1_weighted: 0.4889141602353514
[2m[36m(func pid=159021)[0m f1_per_class: [0.553, 0.634, 0.615, 0.573, 0.181, 0.22, 0.475, 0.358, 0.303, 0.537]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=178896)[0m top1: 0.07975746268656717
[2m[36m(func pid=178896)[0m top5: 0.5051305970149254
[2m[36m(func pid=178896)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=178896)[0m f1_macro: 0.11552133311783833
[2m[36m(func pid=178896)[0m f1_weighted: 0.07902762258533447
[2m[36m(func pid=178896)[0m f1_per_class: [0.128, 0.086, 0.412, 0.079, 0.019, 0.012, 0.062, 0.234, 0.093, 0.03]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8165 | Steps: 4 | Val loss: 2.5162 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6151 | Steps: 4 | Val loss: 4.9043 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0170 | Steps: 4 | Val loss: 1.5253 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.2691 | Steps: 4 | Val loss: 2.2605 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:22:29 (running for 00:39:06.12)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.019 |      0.445 |                   83 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.068 |      0.089 |                   57 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.816 |      0.059 |                    6 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  2.556 |      0.116 |                    3 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.08069029850746269
[2m[36m(func pid=177931)[0m top5: 0.4281716417910448
[2m[36m(func pid=177931)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=177931)[0m f1_macro: 0.0588164837536266
[2m[36m(func pid=177931)[0m f1_weighted: 0.08174426007801532
[2m[36m(func pid=177931)[0m f1_per_class: [0.052, 0.107, 0.0, 0.176, 0.0, 0.033, 0.0, 0.118, 0.07, 0.033]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=165522)[0m top1: 0.13386194029850745
[2m[36m(func pid=165522)[0m top5: 0.7672574626865671
[2m[36m(func pid=165522)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=165522)[0m f1_macro: 0.08935204295791202
[2m[36m(func pid=165522)[0m f1_weighted: 0.1275096974609211
[2m[36m(func pid=165522)[0m f1_per_class: [0.057, 0.016, 0.0, 0.003, 0.0, 0.299, 0.249, 0.245, 0.0, 0.025]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=159021)[0m top1: 0.4846082089552239
[2m[36m(func pid=159021)[0m top5: 0.9486940298507462
[2m[36m(func pid=159021)[0m f1_micro: 0.4846082089552239
[2m[36m(func pid=159021)[0m f1_macro: 0.44030898229551296
[2m[36m(func pid=159021)[0m f1_weighted: 0.4948712748371823
[2m[36m(func pid=159021)[0m f1_per_class: [0.556, 0.62, 0.558, 0.582, 0.201, 0.23, 0.492, 0.362, 0.294, 0.508]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=178896)[0m top1: 0.13059701492537312
[2m[36m(func pid=178896)[0m top5: 0.6245335820895522
[2m[36m(func pid=178896)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=178896)[0m f1_macro: 0.1298488697598646
[2m[36m(func pid=178896)[0m f1_weighted: 0.1406386977662275
[2m[36m(func pid=178896)[0m f1_per_class: [0.109, 0.11, 0.256, 0.171, 0.044, 0.04, 0.153, 0.277, 0.09, 0.047]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8110 | Steps: 4 | Val loss: 2.4883 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 4.9251 | Steps: 4 | Val loss: 4.1204 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0557 | Steps: 4 | Val loss: 1.5244 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:22:34 (running for 00:39:11.29)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.017 |      0.44  |                   84 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.615 |      0.089 |                   58 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.811 |      0.069 |                    7 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  2.269 |      0.13  |                    4 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.08722014925373134
[2m[36m(func pid=177931)[0m top5: 0.43703358208955223
[2m[36m(func pid=177931)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=177931)[0m f1_macro: 0.06853947376829143
[2m[36m(func pid=177931)[0m f1_weighted: 0.08886872583138718
[2m[36m(func pid=177931)[0m f1_per_class: [0.069, 0.128, 0.033, 0.178, 0.0, 0.038, 0.003, 0.133, 0.073, 0.032]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.7347 | Steps: 4 | Val loss: 2.0722 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=159021)[0m top1: 0.4743470149253731
[2m[36m(func pid=159021)[0m top5: 0.9519589552238806
[2m[36m(func pid=159021)[0m f1_micro: 0.4743470149253731
[2m[36m(func pid=159021)[0m f1_macro: 0.4384625687012469
[2m[36m(func pid=159021)[0m f1_weighted: 0.486881062877725
[2m[36m(func pid=159021)[0m f1_per_class: [0.596, 0.607, 0.558, 0.593, 0.196, 0.225, 0.468, 0.333, 0.274, 0.533]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.11473880597014925
[2m[36m(func pid=165522)[0m top5: 0.769589552238806
[2m[36m(func pid=165522)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=165522)[0m f1_macro: 0.09485181041043017
[2m[36m(func pid=165522)[0m f1_weighted: 0.10756203794040121
[2m[36m(func pid=165522)[0m f1_per_class: [0.059, 0.016, 0.0, 0.003, 0.0, 0.287, 0.162, 0.329, 0.071, 0.022]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.24673507462686567
[2m[36m(func pid=178896)[0m top5: 0.7416044776119403
[2m[36m(func pid=178896)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=178896)[0m f1_macro: 0.20933493778473636
[2m[36m(func pid=178896)[0m f1_weighted: 0.19279433255325343
[2m[36m(func pid=178896)[0m f1_per_class: [0.267, 0.375, 0.367, 0.203, 0.0, 0.035, 0.108, 0.35, 0.139, 0.25]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6198 | Steps: 4 | Val loss: 2.4572 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0072 | Steps: 4 | Val loss: 1.5193 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.7914 | Steps: 4 | Val loss: 6.5558 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:22:40 (running for 00:39:17.13)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.056 |      0.438 |                   85 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  4.925 |      0.095 |                   59 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.62  |      0.07  |                    8 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  1.735 |      0.209 |                    5 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.08488805970149253
[2m[36m(func pid=177931)[0m top5: 0.4449626865671642
[2m[36m(func pid=177931)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=177931)[0m f1_macro: 0.07008189861170545
[2m[36m(func pid=177931)[0m f1_weighted: 0.0892712603561645
[2m[36m(func pid=177931)[0m f1_per_class: [0.065, 0.134, 0.028, 0.163, 0.0, 0.059, 0.006, 0.135, 0.068, 0.041]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4978 | Steps: 4 | Val loss: 1.9424 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
[2m[36m(func pid=159021)[0m top1: 0.4822761194029851
[2m[36m(func pid=159021)[0m top5: 0.9472947761194029
[2m[36m(func pid=159021)[0m f1_micro: 0.4822761194029851
[2m[36m(func pid=159021)[0m f1_macro: 0.43553897794728036
[2m[36m(func pid=159021)[0m f1_weighted: 0.4881748071775401
[2m[36m(func pid=159021)[0m f1_per_class: [0.557, 0.622, 0.533, 0.569, 0.174, 0.228, 0.477, 0.392, 0.287, 0.516]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.08115671641791045
[2m[36m(func pid=165522)[0m top5: 0.6478544776119403
[2m[36m(func pid=165522)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=165522)[0m f1_macro: 0.07018940624826821
[2m[36m(func pid=165522)[0m f1_weighted: 0.047283209314509474
[2m[36m(func pid=165522)[0m f1_per_class: [0.055, 0.0, 0.0, 0.016, 0.0, 0.093, 0.015, 0.384, 0.12, 0.018]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.29524253731343286
[2m[36m(func pid=178896)[0m top5: 0.8073694029850746
[2m[36m(func pid=178896)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=178896)[0m f1_macro: 0.25476427460506224
[2m[36m(func pid=178896)[0m f1_weighted: 0.2287395149032864
[2m[36m(func pid=178896)[0m f1_per_class: [0.33, 0.475, 0.537, 0.382, 0.077, 0.008, 0.015, 0.268, 0.162, 0.294]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5974 | Steps: 4 | Val loss: 2.4311 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0076 | Steps: 4 | Val loss: 1.5171 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.0382 | Steps: 4 | Val loss: 8.1891 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:22:45 (running for 00:39:22.42)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.007 |      0.436 |                   86 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.791 |      0.07  |                   60 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.597 |      0.076 |                    9 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  1.498 |      0.255 |                    6 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.08908582089552239
[2m[36m(func pid=177931)[0m top5: 0.4519589552238806
[2m[36m(func pid=177931)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=177931)[0m f1_macro: 0.0761885267867532
[2m[36m(func pid=177931)[0m f1_weighted: 0.09780324915132475
[2m[36m(func pid=177931)[0m f1_per_class: [0.072, 0.134, 0.046, 0.179, 0.004, 0.056, 0.018, 0.144, 0.07, 0.037]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.2057 | Steps: 4 | Val loss: 1.8882 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=159021)[0m top1: 0.4916044776119403
[2m[36m(func pid=159021)[0m top5: 0.9482276119402985
[2m[36m(func pid=159021)[0m f1_micro: 0.4916044776119403
[2m[36m(func pid=159021)[0m f1_macro: 0.44127201372574537
[2m[36m(func pid=159021)[0m f1_weighted: 0.4962470368366389
[2m[36m(func pid=159021)[0m f1_per_class: [0.534, 0.63, 0.558, 0.582, 0.177, 0.216, 0.49, 0.386, 0.325, 0.514]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.08162313432835822
[2m[36m(func pid=165522)[0m top5: 0.6305970149253731
[2m[36m(func pid=165522)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=165522)[0m f1_macro: 0.06089780940163292
[2m[36m(func pid=165522)[0m f1_weighted: 0.04069912178081196
[2m[36m(func pid=165522)[0m f1_per_class: [0.058, 0.005, 0.0, 0.038, 0.0, 0.008, 0.006, 0.383, 0.089, 0.021]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.31902985074626866
[2m[36m(func pid=178896)[0m top5: 0.8278917910447762
[2m[36m(func pid=178896)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=178896)[0m f1_macro: 0.2578086599826979
[2m[36m(func pid=178896)[0m f1_weighted: 0.25150761108738734
[2m[36m(func pid=178896)[0m f1_per_class: [0.357, 0.399, 0.429, 0.501, 0.286, 0.015, 0.03, 0.277, 0.038, 0.246]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5210 | Steps: 4 | Val loss: 2.3825 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.2629 | Steps: 4 | Val loss: 5.0629 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0161 | Steps: 4 | Val loss: 1.5051 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:22:50 (running for 00:39:27.87)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.008 |      0.441 |                   87 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.038 |      0.061 |                   61 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.521 |      0.085 |                   10 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  1.206 |      0.258 |                    7 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.10307835820895522
[2m[36m(func pid=177931)[0m top5: 0.4916044776119403
[2m[36m(func pid=177931)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=177931)[0m f1_macro: 0.08538856937297282
[2m[36m(func pid=177931)[0m f1_weighted: 0.11238503992298272
[2m[36m(func pid=177931)[0m f1_per_class: [0.082, 0.149, 0.065, 0.206, 0.004, 0.046, 0.036, 0.148, 0.065, 0.053]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.0242 | Steps: 4 | Val loss: 1.8601 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=159021)[0m top1: 0.5
[2m[36m(func pid=159021)[0m top5: 0.945429104477612
[2m[36m(func pid=159021)[0m f1_micro: 0.5
[2m[36m(func pid=159021)[0m f1_macro: 0.44460602469460103
[2m[36m(func pid=159021)[0m f1_weighted: 0.511341304470769
[2m[36m(func pid=159021)[0m f1_per_class: [0.53, 0.632, 0.545, 0.586, 0.187, 0.228, 0.535, 0.389, 0.291, 0.523]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.08675373134328358
[2m[36m(func pid=165522)[0m top5: 0.5694962686567164
[2m[36m(func pid=165522)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=165522)[0m f1_macro: 0.07137410695457234
[2m[36m(func pid=165522)[0m f1_weighted: 0.06028916178425034
[2m[36m(func pid=165522)[0m f1_per_class: [0.062, 0.0, 0.0, 0.094, 0.0, 0.024, 0.006, 0.454, 0.058, 0.017]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.310634328358209
[2m[36m(func pid=178896)[0m top5: 0.8568097014925373
[2m[36m(func pid=178896)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=178896)[0m f1_macro: 0.2279913294962645
[2m[36m(func pid=178896)[0m f1_weighted: 0.2808247937594497
[2m[36m(func pid=178896)[0m f1_per_class: [0.301, 0.284, 0.192, 0.502, 0.156, 0.07, 0.177, 0.303, 0.089, 0.207]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4809 | Steps: 4 | Val loss: 2.3621 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0109 | Steps: 4 | Val loss: 1.4777 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.8194 | Steps: 4 | Val loss: 3.5246 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:22:56 (running for 00:39:33.33)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.016 |      0.445 |                   88 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.263 |      0.071 |                   62 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.481 |      0.091 |                   11 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  1.024 |      0.228 |                    8 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.10680970149253731
[2m[36m(func pid=177931)[0m top5: 0.511660447761194
[2m[36m(func pid=177931)[0m f1_micro: 0.10680970149253732
[2m[36m(func pid=177931)[0m f1_macro: 0.0912379173308322
[2m[36m(func pid=177931)[0m f1_weighted: 0.11883267667260275
[2m[36m(func pid=177931)[0m f1_per_class: [0.091, 0.13, 0.09, 0.22, 0.012, 0.056, 0.05, 0.147, 0.068, 0.048]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9317 | Steps: 4 | Val loss: 1.8313 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=159021)[0m top1: 0.5046641791044776
[2m[36m(func pid=159021)[0m top5: 0.945429104477612
[2m[36m(func pid=159021)[0m f1_micro: 0.5046641791044776
[2m[36m(func pid=159021)[0m f1_macro: 0.45020336958681123
[2m[36m(func pid=159021)[0m f1_weighted: 0.5144685133083121
[2m[36m(func pid=159021)[0m f1_per_class: [0.579, 0.627, 0.545, 0.572, 0.2, 0.251, 0.552, 0.365, 0.311, 0.5]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.09001865671641791
[2m[36m(func pid=165522)[0m top5: 0.6926305970149254
[2m[36m(func pid=165522)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=165522)[0m f1_macro: 0.0822979074172466
[2m[36m(func pid=165522)[0m f1_weighted: 0.06922716363523271
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.02, 0.051, 0.0, 0.227, 0.006, 0.434, 0.051, 0.034]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.32649253731343286
[2m[36m(func pid=178896)[0m top5: 0.8628731343283582
[2m[36m(func pid=178896)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=178896)[0m f1_macro: 0.2453237716905566
[2m[36m(func pid=178896)[0m f1_weighted: 0.31075913971548297
[2m[36m(func pid=178896)[0m f1_per_class: [0.386, 0.19, 0.22, 0.556, 0.1, 0.062, 0.272, 0.306, 0.143, 0.218]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.3742 | Steps: 4 | Val loss: 2.3389 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0096 | Steps: 4 | Val loss: 1.5307 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.9525 | Steps: 4 | Val loss: 3.3626 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:23:01 (running for 00:39:38.61)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.011 |      0.45  |                   89 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.819 |      0.082 |                   63 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.374 |      0.102 |                   12 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.932 |      0.245 |                    9 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.11940298507462686
[2m[36m(func pid=177931)[0m top5: 0.53125
[2m[36m(func pid=177931)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=177931)[0m f1_macro: 0.10163970616610349
[2m[36m(func pid=177931)[0m f1_weighted: 0.13208630456187526
[2m[36m(func pid=177931)[0m f1_per_class: [0.104, 0.167, 0.125, 0.227, 0.013, 0.06, 0.063, 0.153, 0.054, 0.05]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7219 | Steps: 4 | Val loss: 1.8596 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=159021)[0m top1: 0.4822761194029851
[2m[36m(func pid=159021)[0m top5: 0.9435634328358209
[2m[36m(func pid=159021)[0m f1_micro: 0.4822761194029851
[2m[36m(func pid=159021)[0m f1_macro: 0.43476505100458507
[2m[36m(func pid=159021)[0m f1_weighted: 0.48952351902610397
[2m[36m(func pid=159021)[0m f1_per_class: [0.543, 0.632, 0.571, 0.561, 0.177, 0.234, 0.486, 0.356, 0.319, 0.468]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.09281716417910447
[2m[36m(func pid=165522)[0m top5: 0.7327425373134329
[2m[36m(func pid=165522)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=165522)[0m f1_macro: 0.07249727629347921
[2m[36m(func pid=165522)[0m f1_weighted: 0.06304794736886289
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.005, 0.021, 0.019, 0.0, 0.283, 0.018, 0.318, 0.0, 0.062]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.2873134328358209
[2m[36m(func pid=178896)[0m top5: 0.8717350746268657
[2m[36m(func pid=178896)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=178896)[0m f1_macro: 0.24474072540425523
[2m[36m(func pid=178896)[0m f1_weighted: 0.29546382664550863
[2m[36m(func pid=178896)[0m f1_per_class: [0.466, 0.198, 0.173, 0.509, 0.085, 0.129, 0.226, 0.323, 0.163, 0.176]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.3469 | Steps: 4 | Val loss: 2.3051 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0255 | Steps: 4 | Val loss: 1.5437 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6839 | Steps: 4 | Val loss: 3.9394 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:23:07 (running for 00:39:44.15)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.01  |      0.435 |                   90 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.952 |      0.072 |                   64 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.347 |      0.112 |                   13 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.722 |      0.245 |                   10 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.13712686567164178
[2m[36m(func pid=177931)[0m top5: 0.5620335820895522
[2m[36m(func pid=177931)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=177931)[0m f1_macro: 0.11246711376359539
[2m[36m(func pid=177931)[0m f1_weighted: 0.14884792482570564
[2m[36m(func pid=177931)[0m f1_per_class: [0.125, 0.192, 0.11, 0.241, 0.016, 0.057, 0.087, 0.177, 0.061, 0.059]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8360 | Steps: 4 | Val loss: 1.8053 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=159021)[0m top1: 0.48134328358208955
[2m[36m(func pid=159021)[0m top5: 0.9402985074626866
[2m[36m(func pid=159021)[0m f1_micro: 0.48134328358208955
[2m[36m(func pid=159021)[0m f1_macro: 0.4385122894797947
[2m[36m(func pid=159021)[0m f1_weighted: 0.4840495514241264
[2m[36m(func pid=159021)[0m f1_per_class: [0.538, 0.643, 0.545, 0.548, 0.168, 0.239, 0.465, 0.384, 0.317, 0.537]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.14272388059701493
[2m[36m(func pid=165522)[0m top5: 0.7999067164179104
[2m[36m(func pid=165522)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=165522)[0m f1_macro: 0.08233951268115745
[2m[36m(func pid=165522)[0m f1_weighted: 0.1548575894884453
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.005, 0.02, 0.01, 0.0, 0.3, 0.369, 0.119, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=178896)[0m top1: 0.30223880597014924
[2m[36m(func pid=178896)[0m top5: 0.8917910447761194
[2m[36m(func pid=178896)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=178896)[0m f1_macro: 0.27522749575572825
[2m[36m(func pid=178896)[0m f1_weighted: 0.3019395230407021
[2m[36m(func pid=178896)[0m f1_per_class: [0.481, 0.242, 0.293, 0.53, 0.128, 0.228, 0.157, 0.346, 0.136, 0.211]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2756 | Steps: 4 | Val loss: 2.2958 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0074 | Steps: 4 | Val loss: 1.5763 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.8644 | Steps: 4 | Val loss: 3.3921 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:23:12 (running for 00:39:49.50)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.026 |      0.439 |                   91 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.684 |      0.082 |                   65 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.276 |      0.116 |                   14 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.836 |      0.275 |                   11 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.13992537313432835
[2m[36m(func pid=177931)[0m top5: 0.5676305970149254
[2m[36m(func pid=177931)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=177931)[0m f1_macro: 0.11590596926572694
[2m[36m(func pid=177931)[0m f1_weighted: 0.15019399388572321
[2m[36m(func pid=177931)[0m f1_per_class: [0.142, 0.211, 0.117, 0.23, 0.023, 0.064, 0.089, 0.162, 0.058, 0.063]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5060 | Steps: 4 | Val loss: 1.7420 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=159021)[0m top1: 0.4780783582089552
[2m[36m(func pid=159021)[0m top5: 0.9388992537313433
[2m[36m(func pid=159021)[0m f1_micro: 0.4780783582089552
[2m[36m(func pid=159021)[0m f1_macro: 0.4255861964093377
[2m[36m(func pid=159021)[0m f1_weighted: 0.4807422487539392
[2m[36m(func pid=159021)[0m f1_per_class: [0.538, 0.636, 0.522, 0.57, 0.162, 0.214, 0.448, 0.399, 0.317, 0.45]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.20615671641791045
[2m[36m(func pid=165522)[0m top5: 0.8367537313432836
[2m[36m(func pid=165522)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=165522)[0m f1_macro: 0.07196817160723425
[2m[36m(func pid=165522)[0m f1_weighted: 0.18898536658183548
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.005, 0.02, 0.007, 0.0, 0.102, 0.585, 0.0, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.2240 | Steps: 4 | Val loss: 2.2785 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=178896)[0m top1: 0.34421641791044777
[2m[36m(func pid=178896)[0m top5: 0.90625
[2m[36m(func pid=178896)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=178896)[0m f1_macro: 0.3400042212274853
[2m[36m(func pid=178896)[0m f1_weighted: 0.3304976257571984
[2m[36m(func pid=178896)[0m f1_per_class: [0.515, 0.512, 0.462, 0.446, 0.195, 0.292, 0.128, 0.38, 0.205, 0.267]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0106 | Steps: 4 | Val loss: 1.5612 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.9783 | Steps: 4 | Val loss: 2.5665 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:23:17 (running for 00:39:54.81)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.007 |      0.426 |                   92 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.864 |      0.072 |                   66 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.224 |      0.122 |                   15 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.506 |      0.34  |                   12 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.14365671641791045
[2m[36m(func pid=177931)[0m top5: 0.5914179104477612
[2m[36m(func pid=177931)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=177931)[0m f1_macro: 0.12233915980222403
[2m[36m(func pid=177931)[0m f1_weighted: 0.1585297172155007
[2m[36m(func pid=177931)[0m f1_per_class: [0.141, 0.208, 0.126, 0.209, 0.021, 0.064, 0.132, 0.192, 0.068, 0.064]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4869 | Steps: 4 | Val loss: 1.7202 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=159021)[0m top1: 0.47201492537313433
[2m[36m(func pid=159021)[0m top5: 0.9388992537313433
[2m[36m(func pid=159021)[0m f1_micro: 0.47201492537313433
[2m[36m(func pid=159021)[0m f1_macro: 0.4187880188412418
[2m[36m(func pid=159021)[0m f1_weighted: 0.47966843104987883
[2m[36m(func pid=159021)[0m f1_per_class: [0.538, 0.621, 0.522, 0.554, 0.168, 0.215, 0.478, 0.349, 0.319, 0.424]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.19496268656716417
[2m[36m(func pid=165522)[0m top5: 0.8292910447761194
[2m[36m(func pid=165522)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=165522)[0m f1_macro: 0.10852629204183799
[2m[36m(func pid=165522)[0m f1_weighted: 0.19486302172510545
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.026, 0.02, 0.0, 0.0, 0.07, 0.526, 0.444, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.2032 | Steps: 4 | Val loss: 2.2536 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=178896)[0m top1: 0.3558768656716418
[2m[36m(func pid=178896)[0m top5: 0.9020522388059702
[2m[36m(func pid=178896)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=178896)[0m f1_macro: 0.35961182508493433
[2m[36m(func pid=178896)[0m f1_weighted: 0.3423754142595953
[2m[36m(func pid=178896)[0m f1_per_class: [0.306, 0.549, 0.579, 0.401, 0.212, 0.293, 0.192, 0.35, 0.253, 0.462]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.1604 | Steps: 4 | Val loss: 1.5231 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 3.4767 | Steps: 4 | Val loss: 3.2010 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 14:23:23 (running for 00:40:00.35)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.011 |      0.419 |                   93 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.978 |      0.109 |                   67 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.203 |      0.133 |                   16 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.487 |      0.36  |                   13 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.1599813432835821
[2m[36m(func pid=177931)[0m top5: 0.6245335820895522
[2m[36m(func pid=177931)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=177931)[0m f1_macro: 0.13303719547775236
[2m[36m(func pid=177931)[0m f1_weighted: 0.17499165582772847
[2m[36m(func pid=177931)[0m f1_per_class: [0.152, 0.216, 0.125, 0.241, 0.022, 0.089, 0.139, 0.207, 0.064, 0.074]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5843 | Steps: 4 | Val loss: 1.7003 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=159021)[0m top1: 0.47901119402985076
[2m[36m(func pid=159021)[0m top5: 0.9458955223880597
[2m[36m(func pid=159021)[0m f1_micro: 0.47901119402985076
[2m[36m(func pid=159021)[0m f1_macro: 0.409346755579826
[2m[36m(func pid=159021)[0m f1_weighted: 0.4854333322587003
[2m[36m(func pid=159021)[0m f1_per_class: [0.483, 0.622, 0.462, 0.564, 0.156, 0.199, 0.496, 0.354, 0.335, 0.424]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.322294776119403
[2m[36m(func pid=165522)[0m top5: 0.6553171641791045
[2m[36m(func pid=165522)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=165522)[0m f1_macro: 0.12901883775428322
[2m[36m(func pid=165522)[0m f1_weighted: 0.22887078718412338
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.415, 0.0, 0.0, 0.0, 0.007, 0.443, 0.425, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.1707 | Steps: 4 | Val loss: 2.2483 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=178896)[0m top1: 0.3833955223880597
[2m[36m(func pid=178896)[0m top5: 0.8955223880597015
[2m[36m(func pid=178896)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=178896)[0m f1_macro: 0.3742429999264597
[2m[36m(func pid=178896)[0m f1_weighted: 0.3769168511561135
[2m[36m(func pid=178896)[0m f1_per_class: [0.318, 0.544, 0.71, 0.422, 0.165, 0.266, 0.301, 0.341, 0.262, 0.412]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0150 | Steps: 4 | Val loss: 1.5898 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.7882 | Steps: 4 | Val loss: 2.7181 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:23:29 (running for 00:40:05.90)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.16  |      0.409 |                   94 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  3.477 |      0.129 |                   68 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.171 |      0.129 |                   17 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.584 |      0.374 |                   14 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.15485074626865672
[2m[36m(func pid=177931)[0m top5: 0.6231343283582089
[2m[36m(func pid=177931)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=177931)[0m f1_macro: 0.128946334081052
[2m[36m(func pid=177931)[0m f1_weighted: 0.17096146402086307
[2m[36m(func pid=177931)[0m f1_per_class: [0.144, 0.186, 0.112, 0.229, 0.024, 0.076, 0.158, 0.203, 0.078, 0.078]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3340 | Steps: 4 | Val loss: 1.6764 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=159021)[0m top1: 0.4664179104477612
[2m[36m(func pid=159021)[0m top5: 0.9398320895522388
[2m[36m(func pid=159021)[0m f1_micro: 0.4664179104477612
[2m[36m(func pid=159021)[0m f1_macro: 0.40559712465911124
[2m[36m(func pid=159021)[0m f1_weighted: 0.47398356599918584
[2m[36m(func pid=159021)[0m f1_per_class: [0.507, 0.611, 0.471, 0.552, 0.141, 0.182, 0.476, 0.376, 0.339, 0.4]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.33955223880597013
[2m[36m(func pid=165522)[0m top5: 0.6473880597014925
[2m[36m(func pid=165522)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=165522)[0m f1_macro: 0.1474056871769905
[2m[36m(func pid=165522)[0m f1_weighted: 0.25250081689859843
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.408, 0.0, 0.0, 0.0, 0.077, 0.487, 0.477, 0.026, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.1188 | Steps: 4 | Val loss: 2.2216 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=178896)[0m top1: 0.38899253731343286
[2m[36m(func pid=178896)[0m top5: 0.8819962686567164
[2m[36m(func pid=178896)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=178896)[0m f1_macro: 0.36057927061099604
[2m[36m(func pid=178896)[0m f1_weighted: 0.38696329872238494
[2m[36m(func pid=178896)[0m f1_per_class: [0.37, 0.545, 0.629, 0.484, 0.115, 0.183, 0.301, 0.377, 0.3, 0.302]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0140 | Steps: 4 | Val loss: 1.6181 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.8589 | Steps: 4 | Val loss: 2.9915 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:23:34 (running for 00:40:11.29)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.015 |      0.406 |                   95 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.788 |      0.147 |                   69 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.119 |      0.135 |                   18 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.334 |      0.361 |                   15 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.16791044776119404
[2m[36m(func pid=177931)[0m top5: 0.6501865671641791
[2m[36m(func pid=177931)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=177931)[0m f1_macro: 0.13470965070978597
[2m[36m(func pid=177931)[0m f1_weighted: 0.17888456204021289
[2m[36m(func pid=177931)[0m f1_per_class: [0.157, 0.219, 0.094, 0.237, 0.032, 0.078, 0.154, 0.214, 0.082, 0.078]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7465 | Steps: 4 | Val loss: 1.6511 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=159021)[0m top1: 0.46548507462686567
[2m[36m(func pid=159021)[0m top5: 0.9305037313432836
[2m[36m(func pid=159021)[0m f1_micro: 0.4654850746268657
[2m[36m(func pid=159021)[0m f1_macro: 0.4104250524240058
[2m[36m(func pid=159021)[0m f1_weighted: 0.46784883484757234
[2m[36m(func pid=159021)[0m f1_per_class: [0.53, 0.615, 0.511, 0.55, 0.16, 0.186, 0.454, 0.369, 0.329, 0.4]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.29617537313432835
[2m[36m(func pid=165522)[0m top5: 0.6497201492537313
[2m[36m(func pid=165522)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=165522)[0m f1_macro: 0.14384222130513022
[2m[36m(func pid=165522)[0m f1_weighted: 0.2169711554627043
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.412, 0.0, 0.007, 0.0, 0.286, 0.288, 0.447, 0.0, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1438 | Steps: 4 | Val loss: 2.2118 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=178896)[0m top1: 0.41277985074626866
[2m[36m(func pid=178896)[0m top5: 0.8880597014925373
[2m[36m(func pid=178896)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=178896)[0m f1_macro: 0.37779461418408894
[2m[36m(func pid=178896)[0m f1_weighted: 0.3993881540586805
[2m[36m(func pid=178896)[0m f1_per_class: [0.508, 0.541, 0.667, 0.557, 0.098, 0.128, 0.284, 0.398, 0.313, 0.286]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0544 | Steps: 4 | Val loss: 1.7028 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6024 | Steps: 4 | Val loss: 2.4713 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 14:23:39 (running for 00:40:16.76)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.014 |      0.41  |                   96 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.859 |      0.144 |                   70 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.144 |      0.135 |                   19 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.746 |      0.378 |                   16 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.16791044776119404
[2m[36m(func pid=177931)[0m top5: 0.6595149253731343
[2m[36m(func pid=177931)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=177931)[0m f1_macro: 0.134871353199368
[2m[36m(func pid=177931)[0m f1_weighted: 0.1785889934280828
[2m[36m(func pid=177931)[0m f1_per_class: [0.155, 0.211, 0.103, 0.242, 0.035, 0.081, 0.155, 0.206, 0.071, 0.09]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3859 | Steps: 4 | Val loss: 1.7139 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=159021)[0m top1: 0.44869402985074625
[2m[36m(func pid=159021)[0m top5: 0.9267723880597015
[2m[36m(func pid=159021)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=159021)[0m f1_macro: 0.4054329493459166
[2m[36m(func pid=159021)[0m f1_weighted: 0.45187737241611553
[2m[36m(func pid=159021)[0m f1_per_class: [0.5, 0.621, 0.533, 0.544, 0.141, 0.195, 0.4, 0.387, 0.299, 0.435]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.2574626865671642
[2m[36m(func pid=165522)[0m top5: 0.8097014925373134
[2m[36m(func pid=165522)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=165522)[0m f1_macro: 0.13642141658728965
[2m[36m(func pid=165522)[0m f1_weighted: 0.15674846209715954
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.409, 0.0, 0.019, 0.027, 0.289, 0.054, 0.545, 0.0, 0.022]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.0925 | Steps: 4 | Val loss: 2.2073 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=178896)[0m top1: 0.3670708955223881
[2m[36m(func pid=178896)[0m top5: 0.8969216417910447
[2m[36m(func pid=178896)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=178896)[0m f1_macro: 0.36539117301885216
[2m[36m(func pid=178896)[0m f1_weighted: 0.3638101746935263
[2m[36m(func pid=178896)[0m f1_per_class: [0.6, 0.369, 0.71, 0.565, 0.126, 0.168, 0.249, 0.385, 0.185, 0.296]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0216 | Steps: 4 | Val loss: 1.6748 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6977 | Steps: 4 | Val loss: 2.5538 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:23:45 (running for 00:40:22.13)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.054 |      0.405 |                   97 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.602 |      0.136 |                   71 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.092 |      0.131 |                   20 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.386 |      0.365 |                   17 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.16184701492537312
[2m[36m(func pid=177931)[0m top5: 0.6618470149253731
[2m[36m(func pid=177931)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=177931)[0m f1_macro: 0.13094143685441195
[2m[36m(func pid=177931)[0m f1_weighted: 0.17500182246275944
[2m[36m(func pid=177931)[0m f1_per_class: [0.139, 0.202, 0.102, 0.221, 0.041, 0.085, 0.168, 0.2, 0.077, 0.075]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4843 | Steps: 4 | Val loss: 1.7934 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=159021)[0m top1: 0.46175373134328357
[2m[36m(func pid=159021)[0m top5: 0.929570895522388
[2m[36m(func pid=159021)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=159021)[0m f1_macro: 0.4156738268039987
[2m[36m(func pid=159021)[0m f1_weighted: 0.4663239462835383
[2m[36m(func pid=159021)[0m f1_per_class: [0.526, 0.631, 0.6, 0.545, 0.14, 0.178, 0.448, 0.368, 0.311, 0.409]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.10867537313432836
[2m[36m(func pid=165522)[0m top5: 0.5755597014925373
[2m[36m(func pid=165522)[0m f1_micro: 0.10867537313432836
[2m[36m(func pid=165522)[0m f1_macro: 0.1026965161743048
[2m[36m(func pid=165522)[0m f1_weighted: 0.08328666746646511
[2m[36m(func pid=165522)[0m f1_per_class: [0.063, 0.016, 0.0, 0.025, 0.029, 0.291, 0.022, 0.543, 0.025, 0.013]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.0217 | Steps: 4 | Val loss: 2.2148 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=178896)[0m top1: 0.310634328358209
[2m[36m(func pid=178896)[0m top5: 0.8959888059701493
[2m[36m(func pid=178896)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=178896)[0m f1_macro: 0.3465815804207548
[2m[36m(func pid=178896)[0m f1_weighted: 0.3269023492885436
[2m[36m(func pid=178896)[0m f1_per_class: [0.609, 0.384, 0.611, 0.496, 0.131, 0.194, 0.177, 0.372, 0.143, 0.349]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0478 | Steps: 4 | Val loss: 1.6608 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.7515 | Steps: 4 | Val loss: 2.6161 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=177931)[0m top1: 0.15345149253731344
[2m[36m(func pid=177931)[0m top5: 0.6590485074626866
[2m[36m(func pid=177931)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=177931)[0m f1_macro: 0.13030115987652863
[2m[36m(func pid=177931)[0m f1_weighted: 0.16762148812782096
[2m[36m(func pid=177931)[0m f1_per_class: [0.123, 0.19, 0.11, 0.205, 0.055, 0.091, 0.161, 0.209, 0.086, 0.074]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:23:50 (running for 00:40:27.51)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.022 |      0.416 |                   98 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.698 |      0.103 |                   72 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.022 |      0.13  |                   21 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.484 |      0.347 |                   18 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2131 | Steps: 4 | Val loss: 1.7824 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=159021)[0m top1: 0.46781716417910446
[2m[36m(func pid=159021)[0m top5: 0.9319029850746269
[2m[36m(func pid=159021)[0m f1_micro: 0.46781716417910446
[2m[36m(func pid=159021)[0m f1_macro: 0.42302882037640455
[2m[36m(func pid=159021)[0m f1_weighted: 0.47060992391391504
[2m[36m(func pid=159021)[0m f1_per_class: [0.511, 0.635, 0.558, 0.542, 0.138, 0.176, 0.459, 0.39, 0.292, 0.529]
[2m[36m(func pid=159021)[0m 
[2m[36m(func pid=165522)[0m top1: 0.08815298507462686
[2m[36m(func pid=165522)[0m top5: 0.7723880597014925
[2m[36m(func pid=165522)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=165522)[0m f1_macro: 0.08759837247584831
[2m[36m(func pid=165522)[0m f1_weighted: 0.05812310001909907
[2m[36m(func pid=165522)[0m f1_per_class: [0.063, 0.0, 0.0, 0.007, 0.0, 0.207, 0.0, 0.485, 0.09, 0.024]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.9912 | Steps: 4 | Val loss: 2.1940 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=178896)[0m top1: 0.31203358208955223
[2m[36m(func pid=178896)[0m top5: 0.8908582089552238
[2m[36m(func pid=178896)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=178896)[0m f1_macro: 0.33935730523308627
[2m[36m(func pid=178896)[0m f1_weighted: 0.300078759271202
[2m[36m(func pid=178896)[0m f1_per_class: [0.585, 0.491, 0.55, 0.372, 0.145, 0.195, 0.138, 0.373, 0.18, 0.364]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=159021)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0091 | Steps: 4 | Val loss: 1.6425 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6285 | Steps: 4 | Val loss: 2.6367 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 14:23:56 (running for 00:40:32.95)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00016 | RUNNING    | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.048 |      0.423 |                   99 |
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.752 |      0.088 |                   73 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.991 |      0.138 |                   22 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.213 |      0.339 |                   19 |
| train_5806f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.16417910447761194
[2m[36m(func pid=177931)[0m top5: 0.6823694029850746
[2m[36m(func pid=177931)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=177931)[0m f1_macro: 0.13800271402169254
[2m[36m(func pid=177931)[0m f1_weighted: 0.17621341162191673
[2m[36m(func pid=177931)[0m f1_per_class: [0.129, 0.194, 0.12, 0.249, 0.068, 0.103, 0.14, 0.223, 0.068, 0.086]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3703 | Steps: 4 | Val loss: 1.7814 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=159021)[0m top1: 0.46408582089552236
[2m[36m(func pid=159021)[0m top5: 0.9323694029850746
[2m[36m(func pid=159021)[0m f1_micro: 0.46408582089552236
[2m[36m(func pid=159021)[0m f1_macro: 0.41065515633185257
[2m[36m(func pid=159021)[0m f1_weighted: 0.47282131484905443
[2m[36m(func pid=159021)[0m f1_per_class: [0.507, 0.634, 0.533, 0.527, 0.138, 0.185, 0.493, 0.344, 0.259, 0.486]
[2m[36m(func pid=165522)[0m top1: 0.09375
[2m[36m(func pid=165522)[0m top5: 0.5690298507462687
[2m[36m(func pid=165522)[0m f1_micro: 0.09375
[2m[36m(func pid=165522)[0m f1_macro: 0.07575913156797902
[2m[36m(func pid=165522)[0m f1_weighted: 0.05086051976575546
[2m[36m(func pid=165522)[0m f1_per_class: [0.062, 0.0, 0.0, 0.0, 0.0, 0.205, 0.0, 0.401, 0.09, 0.0]
[2m[36m(func pid=165522)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0134 | Steps: 4 | Val loss: 2.1651 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=178896)[0m top1: 0.3204291044776119
[2m[36m(func pid=178896)[0m top5: 0.8894589552238806
[2m[36m(func pid=178896)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=178896)[0m f1_macro: 0.3430416063915792
[2m[36m(func pid=178896)[0m f1_weighted: 0.2904093334094966
[2m[36m(func pid=178896)[0m f1_per_class: [0.615, 0.498, 0.522, 0.28, 0.165, 0.225, 0.169, 0.379, 0.237, 0.341]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=165522)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6061 | Steps: 4 | Val loss: 2.6122 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=177931)[0m top1: 0.1865671641791045
[2m[36m(func pid=177931)[0m top5: 0.7075559701492538
[2m[36m(func pid=177931)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=177931)[0m f1_macro: 0.15650055989237918
[2m[36m(func pid=177931)[0m f1_weighted: 0.2066353015445033
[2m[36m(func pid=177931)[0m f1_per_class: [0.147, 0.207, 0.158, 0.282, 0.063, 0.122, 0.19, 0.232, 0.096, 0.069]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3356 | Steps: 4 | Val loss: 1.7348 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=165522)[0m top1: 0.08908582089552239
[2m[36m(func pid=165522)[0m top5: 0.39505597014925375
[2m[36m(func pid=165522)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=165522)[0m f1_macro: 0.06623250968795438
[2m[36m(func pid=165522)[0m f1_weighted: 0.04285884533833058
[2m[36m(func pid=165522)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.143, 0.0, 0.41, 0.073, 0.037]
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8529 | Steps: 4 | Val loss: 2.1462 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=178896)[0m top1: 0.345615671641791
[2m[36m(func pid=178896)[0m top5: 0.902518656716418
[2m[36m(func pid=178896)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=178896)[0m f1_macro: 0.34937710826303603
[2m[36m(func pid=178896)[0m f1_weighted: 0.3171390529845303
[2m[36m(func pid=178896)[0m f1_per_class: [0.535, 0.508, 0.512, 0.297, 0.14, 0.238, 0.225, 0.395, 0.335, 0.309]
== Status ==
Current time: 2024-01-07 14:24:01 (running for 00:40:38.12)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.34550000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00019 | RUNNING    | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.628 |      0.076 |                   74 |
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.013 |      0.157 |                   23 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.37  |      0.343 |                   20 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 14:24:06 (running for 00:40:43.58)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 3 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  2.013 |      0.157 |                   23 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.37  |      0.343 |                   20 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.19542910447761194
[2m[36m(func pid=177931)[0m top5: 0.7192164179104478
[2m[36m(func pid=177931)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=177931)[0m f1_macro: 0.1597409486067942
[2m[36m(func pid=177931)[0m f1_weighted: 0.21698341509093283
[2m[36m(func pid=177931)[0m f1_per_class: [0.149, 0.195, 0.149, 0.315, 0.053, 0.12, 0.2, 0.239, 0.091, 0.086]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=183948)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=183948)[0m Configuration completed!
[2m[36m(func pid=183948)[0m New optimizer parameters:
[2m[36m(func pid=183948)[0m SGD (
[2m[36m(func pid=183948)[0m Parameter Group 0
[2m[36m(func pid=183948)[0m     dampening: 0
[2m[36m(func pid=183948)[0m     differentiable: False
[2m[36m(func pid=183948)[0m     foreach: None
[2m[36m(func pid=183948)[0m     lr: 0.01
[2m[36m(func pid=183948)[0m     maximize: False
[2m[36m(func pid=183948)[0m     momentum: 0.9
[2m[36m(func pid=183948)[0m     nesterov: False
[2m[36m(func pid=183948)[0m     weight_decay: 1e-05
[2m[36m(func pid=183948)[0m )
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1888 | Steps: 4 | Val loss: 1.7368 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.6904 | Steps: 4 | Val loss: 2.1516 | Batch size: 32 | lr: 0.0001 | Duration: 3.29s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9257 | Steps: 4 | Val loss: 2.4455 | Batch size: 32 | lr: 0.01 | Duration: 5.08s
[2m[36m(func pid=178896)[0m top1: 0.35027985074626866
[2m[36m(func pid=178896)[0m top5: 0.8969216417910447
[2m[36m(func pid=178896)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=178896)[0m f1_macro: 0.3346237619795914
[2m[36m(func pid=178896)[0m f1_weighted: 0.32074668837811016
[2m[36m(func pid=178896)[0m f1_per_class: [0.394, 0.527, 0.4, 0.334, 0.182, 0.269, 0.189, 0.406, 0.311, 0.333]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m top1: 0.19263059701492538
[2m[36m(func pid=177931)[0m top5: 0.7094216417910447
[2m[36m(func pid=177931)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=177931)[0m f1_macro: 0.16019202653787878
[2m[36m(func pid=177931)[0m f1_weighted: 0.2109126304536151
[2m[36m(func pid=177931)[0m f1_per_class: [0.171, 0.207, 0.144, 0.32, 0.047, 0.122, 0.163, 0.26, 0.088, 0.08]
[2m[36m(func pid=183948)[0m top1: 0.06763059701492537
[2m[36m(func pid=183948)[0m top5: 0.5093283582089553
[2m[36m(func pid=183948)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=183948)[0m f1_macro: 0.08285505785535743
[2m[36m(func pid=183948)[0m f1_weighted: 0.03975979332043569
[2m[36m(func pid=183948)[0m f1_per_class: [0.111, 0.0, 0.4, 0.003, 0.047, 0.134, 0.056, 0.0, 0.047, 0.031]
== Status ==
Current time: 2024-01-07 14:24:12 (running for 00:40:49.25)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.853 |      0.16  |                   24 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.189 |      0.335 |                   22 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=184567)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=184567)[0m Configuration completed!
[2m[36m(func pid=184567)[0m New optimizer parameters:
[2m[36m(func pid=184567)[0m SGD (
[2m[36m(func pid=184567)[0m Parameter Group 0
[2m[36m(func pid=184567)[0m     dampening: 0
[2m[36m(func pid=184567)[0m     differentiable: False
[2m[36m(func pid=184567)[0m     foreach: None
[2m[36m(func pid=184567)[0m     lr: 0.1
[2m[36m(func pid=184567)[0m     maximize: False
[2m[36m(func pid=184567)[0m     momentum: 0.9
[2m[36m(func pid=184567)[0m     nesterov: False
[2m[36m(func pid=184567)[0m     weight_decay: 1e-05
[2m[36m(func pid=184567)[0m )
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3649 | Steps: 4 | Val loss: 1.7678 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=178896)[0m top1: 0.3516791044776119
[2m[36m(func pid=178896)[0m top5: 0.8810634328358209
[2m[36m(func pid=178896)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=178896)[0m f1_macro: 0.3154358229767308
[2m[36m(func pid=178896)[0m f1_weighted: 0.3332779869215711
[2m[36m(func pid=178896)[0m f1_per_class: [0.365, 0.52, 0.265, 0.399, 0.169, 0.278, 0.188, 0.368, 0.283, 0.318]
[2m[36m(func pid=178896)[0m 
== Status ==
Current time: 2024-01-07 14:24:17 (running for 00:40:54.87)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.69  |      0.16  |                   25 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.365 |      0.315 |                   23 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.926 |      0.083 |                    1 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.7635 | Steps: 4 | Val loss: 2.1485 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.8202 | Steps: 4 | Val loss: 1.8711 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.5146 | Steps: 4 | Val loss: 6486.5454 | Batch size: 32 | lr: 0.1 | Duration: 4.97s
[2m[36m(func pid=183948)[0m top1: 0.3138992537313433
[2m[36m(func pid=183948)[0m top5: 0.8577425373134329
[2m[36m(func pid=183948)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=183948)[0m f1_macro: 0.27877493541420717
[2m[36m(func pid=183948)[0m f1_weighted: 0.2873273572096531
[2m[36m(func pid=183948)[0m f1_per_class: [0.461, 0.398, 0.289, 0.539, 0.082, 0.177, 0.024, 0.349, 0.167, 0.303]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.20382462686567165
[2m[36m(func pid=177931)[0m top5: 0.7122201492537313
[2m[36m(func pid=177931)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=177931)[0m f1_macro: 0.17186608076918647
[2m[36m(func pid=177931)[0m f1_weighted: 0.22033725794829398
[2m[36m(func pid=177931)[0m f1_per_class: [0.206, 0.243, 0.165, 0.328, 0.064, 0.134, 0.157, 0.262, 0.086, 0.072]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1938 | Steps: 4 | Val loss: 1.6838 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=184567)[0m top1: 0.006063432835820896
[2m[36m(func pid=184567)[0m top5: 0.5093283582089553
[2m[36m(func pid=184567)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=184567)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=184567)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:24:23 (running for 00:41:00.52)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.764 |      0.172 |                   26 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.194 |      0.325 |                   24 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.82  |      0.279 |                    2 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  4.515 |      0.001 |                    1 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.36800373134328357
[2m[36m(func pid=178896)[0m top5: 0.9104477611940298
[2m[36m(func pid=178896)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=178896)[0m f1_macro: 0.3245600659068499
[2m[36m(func pid=178896)[0m f1_weighted: 0.3602396925361732
[2m[36m(func pid=178896)[0m f1_per_class: [0.364, 0.547, 0.245, 0.398, 0.23, 0.308, 0.265, 0.326, 0.229, 0.333]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0587 | Steps: 4 | Val loss: 1.7069 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.7197 | Steps: 4 | Val loss: 2.1565 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 13.2553 | Steps: 4 | Val loss: 663263117312.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=177931)[0m top1: 0.19869402985074627
[2m[36m(func pid=177931)[0m top5: 0.7066231343283582
[2m[36m(func pid=177931)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=177931)[0m f1_macro: 0.16706857730871927
[2m[36m(func pid=177931)[0m f1_weighted: 0.21858516921515947
[2m[36m(func pid=177931)[0m f1_per_class: [0.185, 0.22, 0.18, 0.333, 0.059, 0.113, 0.171, 0.263, 0.079, 0.068]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.3460820895522388
[2m[36m(func pid=183948)[0m top5: 0.9160447761194029
[2m[36m(func pid=183948)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=183948)[0m f1_macro: 0.26829144789800996
[2m[36m(func pid=183948)[0m f1_weighted: 0.3143574020576662
[2m[36m(func pid=183948)[0m f1_per_class: [0.575, 0.406, 0.23, 0.608, 0.267, 0.156, 0.082, 0.228, 0.13, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.1655 | Steps: 4 | Val loss: 1.5944 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=184567)[0m top1: 0.03311567164179104
[2m[36m(func pid=184567)[0m top5: 0.5149253731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=184567)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=184567)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:24:29 (running for 00:41:05.93)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.72  |      0.167 |                   27 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.166 |      0.338 |                   25 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.059 |      0.268 |                    3 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 13.255 |      0.006 |                    2 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4048507462686567
[2m[36m(func pid=178896)[0m top5: 0.9281716417910447
[2m[36m(func pid=178896)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=178896)[0m f1_macro: 0.33848720007163535
[2m[36m(func pid=178896)[0m f1_weighted: 0.3996540279610136
[2m[36m(func pid=178896)[0m f1_per_class: [0.411, 0.569, 0.263, 0.511, 0.217, 0.324, 0.282, 0.293, 0.17, 0.346]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6406 | Steps: 4 | Val loss: 2.1434 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6589 | Steps: 4 | Val loss: 2.2284 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 10.4855 | Steps: 4 | Val loss: 6147239936.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=177931)[0m top1: 0.20662313432835822
[2m[36m(func pid=177931)[0m top5: 0.7117537313432836
[2m[36m(func pid=177931)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=177931)[0m f1_macro: 0.17309911850468931
[2m[36m(func pid=177931)[0m f1_weighted: 0.2230373854467445
[2m[36m(func pid=177931)[0m f1_per_class: [0.209, 0.234, 0.208, 0.36, 0.054, 0.103, 0.155, 0.251, 0.089, 0.067]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.23694029850746268
[2m[36m(func pid=183948)[0m top5: 0.8544776119402985
[2m[36m(func pid=183948)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=183948)[0m f1_macro: 0.2481520730706733
[2m[36m(func pid=183948)[0m f1_weighted: 0.24110836637082003
[2m[36m(func pid=183948)[0m f1_per_class: [0.593, 0.016, 0.277, 0.493, 0.078, 0.185, 0.142, 0.257, 0.143, 0.298]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1596 | Steps: 4 | Val loss: 1.5164 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=184567)[0m top1: 0.05783582089552239
[2m[36m(func pid=184567)[0m top5: 0.5149253731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=184567)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=184567)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:24:34 (running for 00:41:11.36)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.641 |      0.173 |                   28 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.16  |      0.343 |                   26 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.659 |      0.248 |                    4 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 10.486 |      0.011 |                    3 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.42677238805970147
[2m[36m(func pid=178896)[0m top5: 0.9388992537313433
[2m[36m(func pid=178896)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=178896)[0m f1_macro: 0.3434444968560357
[2m[36m(func pid=178896)[0m f1_weighted: 0.4300489988590355
[2m[36m(func pid=178896)[0m f1_per_class: [0.435, 0.559, 0.283, 0.553, 0.204, 0.326, 0.358, 0.237, 0.186, 0.294]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.6643 | Steps: 4 | Val loss: 2.1355 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5319 | Steps: 4 | Val loss: 3.1496 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 13.4136 | Steps: 4 | Val loss: 33815788.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=177931)[0m top1: 0.20988805970149255
[2m[36m(func pid=177931)[0m top5: 0.7210820895522388
[2m[36m(func pid=177931)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=177931)[0m f1_macro: 0.1778974341273793
[2m[36m(func pid=177931)[0m f1_weighted: 0.22395262758937654
[2m[36m(func pid=177931)[0m f1_per_class: [0.206, 0.24, 0.218, 0.354, 0.064, 0.114, 0.154, 0.268, 0.087, 0.076]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2254 | Steps: 4 | Val loss: 1.4873 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=183948)[0m top1: 0.22434701492537312
[2m[36m(func pid=183948)[0m top5: 0.7630597014925373
[2m[36m(func pid=183948)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=183948)[0m f1_macro: 0.28918012370945334
[2m[36m(func pid=183948)[0m f1_weighted: 0.16645807938876772
[2m[36m(func pid=183948)[0m f1_per_class: [0.598, 0.446, 0.759, 0.087, 0.069, 0.114, 0.016, 0.31, 0.312, 0.183]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.05783582089552239
[2m[36m(func pid=184567)[0m top5: 0.5149253731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=184567)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=184567)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:24:39 (running for 00:41:16.76)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.664 |      0.178 |                   29 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.225 |      0.358 |                   27 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.532 |      0.289 |                    5 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 13.414 |      0.011 |                    4 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.44029850746268656
[2m[36m(func pid=178896)[0m top5: 0.9440298507462687
[2m[36m(func pid=178896)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=178896)[0m f1_macro: 0.3579751038925583
[2m[36m(func pid=178896)[0m f1_weighted: 0.452645915284883
[2m[36m(func pid=178896)[0m f1_per_class: [0.413, 0.553, 0.325, 0.569, 0.195, 0.324, 0.409, 0.325, 0.173, 0.294]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.7524 | Steps: 4 | Val loss: 2.1321 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.3467 | Steps: 4 | Val loss: 4.2311 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 14.4324 | Steps: 4 | Val loss: 1174572.1250 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=177931)[0m top1: 0.21875
[2m[36m(func pid=177931)[0m top5: 0.7103544776119403
[2m[36m(func pid=177931)[0m f1_micro: 0.21875
[2m[36m(func pid=177931)[0m f1_macro: 0.17673206959415494
[2m[36m(func pid=177931)[0m f1_weighted: 0.22472154844144046
[2m[36m(func pid=177931)[0m f1_per_class: [0.204, 0.245, 0.179, 0.375, 0.063, 0.096, 0.136, 0.29, 0.08, 0.099]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1950 | Steps: 4 | Val loss: 1.4977 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=183948)[0m top1: 0.17024253731343283
[2m[36m(func pid=183948)[0m top5: 0.6972947761194029
[2m[36m(func pid=183948)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=183948)[0m f1_macro: 0.21767037048912236
[2m[36m(func pid=183948)[0m f1_weighted: 0.13769200545617225
[2m[36m(func pid=183948)[0m f1_per_class: [0.423, 0.33, 0.526, 0.117, 0.086, 0.0, 0.022, 0.398, 0.14, 0.135]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.05783582089552239
[2m[36m(func pid=184567)[0m top5: 0.5149253731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=184567)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=184567)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:24:45 (running for 00:41:22.25)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.752 |      0.177 |                   30 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.195 |      0.361 |                   28 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.347 |      0.218 |                    6 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 14.432 |      0.011 |                    5 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4361007462686567
[2m[36m(func pid=178896)[0m top5: 0.9402985074626866
[2m[36m(func pid=178896)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=178896)[0m f1_macro: 0.3611916131450859
[2m[36m(func pid=178896)[0m f1_weighted: 0.4512548903281129
[2m[36m(func pid=178896)[0m f1_per_class: [0.372, 0.488, 0.453, 0.58, 0.178, 0.298, 0.431, 0.359, 0.245, 0.208]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.5565 | Steps: 4 | Val loss: 2.0969 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5178 | Steps: 4 | Val loss: 4.1052 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 23.6416 | Steps: 4 | Val loss: 2182856.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=177931)[0m top1: 0.23227611940298507
[2m[36m(func pid=177931)[0m top5: 0.7332089552238806
[2m[36m(func pid=177931)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=177931)[0m f1_macro: 0.18519765692946397
[2m[36m(func pid=177931)[0m f1_weighted: 0.23461026462339743
[2m[36m(func pid=177931)[0m f1_per_class: [0.208, 0.239, 0.232, 0.408, 0.081, 0.094, 0.147, 0.261, 0.086, 0.098]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2716 | Steps: 4 | Val loss: 1.5243 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=183948)[0m top1: 0.2574626865671642
[2m[36m(func pid=183948)[0m top5: 0.6968283582089553
[2m[36m(func pid=183948)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=183948)[0m f1_macro: 0.1792980166009142
[2m[36m(func pid=183948)[0m f1_weighted: 0.21028784325318794
[2m[36m(func pid=183948)[0m f1_per_class: [0.244, 0.239, 0.262, 0.482, 0.136, 0.0, 0.056, 0.075, 0.116, 0.183]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.2980410447761194
[2m[36m(func pid=184567)[0m top5: 0.5153917910447762
[2m[36m(func pid=184567)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=184567)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=184567)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:24:50 (running for 00:41:27.63)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.556 |      0.185 |                   31 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.272 |      0.359 |                   29 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.518 |      0.179 |                    7 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 23.642 |      0.046 |                    6 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4244402985074627
[2m[36m(func pid=178896)[0m top5: 0.9375
[2m[36m(func pid=178896)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=178896)[0m f1_macro: 0.3588919905352706
[2m[36m(func pid=178896)[0m f1_weighted: 0.4123428589398353
[2m[36m(func pid=178896)[0m f1_per_class: [0.437, 0.477, 0.522, 0.566, 0.182, 0.252, 0.323, 0.427, 0.202, 0.2]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4964 | Steps: 4 | Val loss: 2.0863 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4315 | Steps: 4 | Val loss: 4.4341 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 15.7120 | Steps: 4 | Val loss: 418524.9062 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=177931)[0m top1: 0.2392723880597015
[2m[36m(func pid=177931)[0m top5: 0.7406716417910447
[2m[36m(func pid=177931)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=177931)[0m f1_macro: 0.19089903304619174
[2m[36m(func pid=177931)[0m f1_weighted: 0.23742448835896637
[2m[36m(func pid=177931)[0m f1_per_class: [0.215, 0.247, 0.237, 0.41, 0.083, 0.094, 0.146, 0.263, 0.101, 0.114]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1464 | Steps: 4 | Val loss: 1.5427 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=183948)[0m top1: 0.26399253731343286
[2m[36m(func pid=183948)[0m top5: 0.7322761194029851
[2m[36m(func pid=183948)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=183948)[0m f1_macro: 0.1961518103302157
[2m[36m(func pid=183948)[0m f1_weighted: 0.2407332463289421
[2m[36m(func pid=183948)[0m f1_per_class: [0.21, 0.376, 0.045, 0.552, 0.16, 0.039, 0.012, 0.0, 0.068, 0.5]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.0625
[2m[36m(func pid=184567)[0m top5: 0.6343283582089553
[2m[36m(func pid=184567)[0m f1_micro: 0.0625
[2m[36m(func pid=184567)[0m f1_macro: 0.019082004944704044
[2m[36m(func pid=184567)[0m f1_weighted: 0.007936091205506842
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.131, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:24:56 (running for 00:41:33.10)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.496 |      0.191 |                   32 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.146 |      0.377 |                   30 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.431 |      0.196 |                    8 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 15.712 |      0.019 |                    7 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.42677238805970147
[2m[36m(func pid=178896)[0m top5: 0.9333022388059702
[2m[36m(func pid=178896)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=178896)[0m f1_macro: 0.37701178548739855
[2m[36m(func pid=178896)[0m f1_weighted: 0.41420557092176474
[2m[36m(func pid=178896)[0m f1_per_class: [0.524, 0.48, 0.533, 0.562, 0.19, 0.233, 0.324, 0.408, 0.306, 0.209]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.4490 | Steps: 4 | Val loss: 2.0838 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.2416 | Steps: 4 | Val loss: 3.6965 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 13.5903 | Steps: 4 | Val loss: 20564.9648 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=177931)[0m top1: 0.23367537313432835
[2m[36m(func pid=177931)[0m top5: 0.7495335820895522
[2m[36m(func pid=177931)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=177931)[0m f1_macro: 0.19290963780703957
[2m[36m(func pid=177931)[0m f1_weighted: 0.23993917371287596
[2m[36m(func pid=177931)[0m f1_per_class: [0.23, 0.269, 0.229, 0.387, 0.08, 0.101, 0.161, 0.257, 0.109, 0.107]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.25652985074626866
[2m[36m(func pid=183948)[0m top5: 0.8138992537313433
[2m[36m(func pid=183948)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=183948)[0m f1_macro: 0.2374399932118342
[2m[36m(func pid=183948)[0m f1_weighted: 0.30034224008191823
[2m[36m(func pid=183948)[0m f1_per_class: [0.38, 0.409, 0.173, 0.466, 0.224, 0.232, 0.179, 0.093, 0.082, 0.138]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1261 | Steps: 4 | Val loss: 1.5619 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=184567)[0m top1: 0.07042910447761194
[2m[36m(func pid=184567)[0m top5: 0.4832089552238806
[2m[36m(func pid=184567)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=184567)[0m f1_macro: 0.023231011396310362
[2m[36m(func pid=184567)[0m f1_weighted: 0.010739649706219462
[2m[36m(func pid=184567)[0m f1_per_class: [0.072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:25:01 (running for 00:41:38.74)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.449 |      0.193 |                   33 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.126 |      0.404 |                   31 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.242 |      0.237 |                    9 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 13.59  |      0.023 |                    8 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.43050373134328357
[2m[36m(func pid=178896)[0m top5: 0.9309701492537313
[2m[36m(func pid=178896)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=178896)[0m f1_macro: 0.4039916060975647
[2m[36m(func pid=178896)[0m f1_weighted: 0.40983059376974557
[2m[36m(func pid=178896)[0m f1_per_class: [0.653, 0.539, 0.632, 0.555, 0.203, 0.248, 0.266, 0.415, 0.271, 0.258]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.4608 | Steps: 4 | Val loss: 2.0473 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.2483 | Steps: 4 | Val loss: 4.5022 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 20.8957 | Steps: 4 | Val loss: 12057620.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=177931)[0m top1: 0.2555970149253731
[2m[36m(func pid=177931)[0m top5: 0.7700559701492538
[2m[36m(func pid=177931)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=177931)[0m f1_macro: 0.20526884735623926
[2m[36m(func pid=177931)[0m f1_weighted: 0.2650701121250953
[2m[36m(func pid=177931)[0m f1_per_class: [0.251, 0.32, 0.214, 0.397, 0.088, 0.093, 0.208, 0.251, 0.118, 0.114]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3349 | Steps: 4 | Val loss: 1.5394 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=183948)[0m top1: 0.14319029850746268
[2m[36m(func pid=183948)[0m top5: 0.8120335820895522
[2m[36m(func pid=183948)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=183948)[0m f1_macro: 0.21623806059136505
[2m[36m(func pid=183948)[0m f1_weighted: 0.16938929300865932
[2m[36m(func pid=183948)[0m f1_per_class: [0.543, 0.169, 0.115, 0.168, 0.119, 0.164, 0.122, 0.322, 0.079, 0.361]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.05783582089552239
[2m[36m(func pid=184567)[0m top5: 0.5149253731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=184567)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=184567)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:25:07 (running for 00:41:44.32)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.461 |      0.205 |                   34 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.335 |      0.398 |                   32 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.248 |      0.216 |                   10 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 20.896 |      0.011 |                    9 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.43050373134328357
[2m[36m(func pid=178896)[0m top5: 0.9356343283582089
[2m[36m(func pid=178896)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=178896)[0m f1_macro: 0.3980777228584227
[2m[36m(func pid=178896)[0m f1_weighted: 0.41591294968536946
[2m[36m(func pid=178896)[0m f1_per_class: [0.638, 0.556, 0.55, 0.546, 0.203, 0.271, 0.282, 0.404, 0.256, 0.275]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.4836 | Steps: 4 | Val loss: 2.0523 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 32.1388 | Steps: 4 | Val loss: 1718708.3750 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5730 | Steps: 4 | Val loss: 6.8653 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=177931)[0m top1: 0.25466417910447764
[2m[36m(func pid=177931)[0m top5: 0.7719216417910447
[2m[36m(func pid=177931)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=177931)[0m f1_macro: 0.20685995613097266
[2m[36m(func pid=177931)[0m f1_weighted: 0.2645178815437336
[2m[36m(func pid=177931)[0m f1_per_class: [0.259, 0.352, 0.195, 0.366, 0.087, 0.108, 0.209, 0.253, 0.121, 0.119]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=184567)[0m top1: 0.05783582089552239
[2m[36m(func pid=184567)[0m top5: 0.5149253731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=184567)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=184567)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.08815298507462686
[2m[36m(func pid=183948)[0m top5: 0.773320895522388
[2m[36m(func pid=183948)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=183948)[0m f1_macro: 0.17984239381745123
[2m[36m(func pid=183948)[0m f1_weighted: 0.0988161720165625
[2m[36m(func pid=183948)[0m f1_per_class: [0.548, 0.011, 0.023, 0.01, 0.161, 0.268, 0.087, 0.335, 0.072, 0.283]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1039 | Steps: 4 | Val loss: 1.5482 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.4506 | Steps: 4 | Val loss: 2.0304 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:25:12 (running for 00:41:49.84)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.484 |      0.207 |                   35 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.104 |      0.399 |                   33 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.573 |      0.18  |                   11 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 32.139 |      0.011 |                   10 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.42490671641791045
[2m[36m(func pid=178896)[0m top5: 0.9281716417910447
[2m[36m(func pid=178896)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=178896)[0m f1_macro: 0.3993347676122415
[2m[36m(func pid=178896)[0m f1_weighted: 0.4191286464933813
[2m[36m(func pid=178896)[0m f1_per_class: [0.589, 0.542, 0.524, 0.537, 0.209, 0.278, 0.3, 0.454, 0.253, 0.308]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 19.9119 | Steps: 4 | Val loss: 85062.6875 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.0233 | Steps: 4 | Val loss: 9.7461 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=177931)[0m top1: 0.2635261194029851
[2m[36m(func pid=177931)[0m top5: 0.7877798507462687
[2m[36m(func pid=177931)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=177931)[0m f1_macro: 0.21226298683117997
[2m[36m(func pid=177931)[0m f1_weighted: 0.2686287792918823
[2m[36m(func pid=177931)[0m f1_per_class: [0.28, 0.365, 0.207, 0.387, 0.08, 0.088, 0.2, 0.26, 0.118, 0.138]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=184567)[0m top1: 0.04710820895522388
[2m[36m(func pid=184567)[0m top5: 0.5163246268656716
[2m[36m(func pid=184567)[0m f1_micro: 0.04710820895522388
[2m[36m(func pid=184567)[0m f1_macro: 0.018013668870982946
[2m[36m(func pid=184567)[0m f1_weighted: 0.042026440405724924
[2m[36m(func pid=184567)[0m f1_per_class: [0.024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.139, 0.0, 0.0, 0.017]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2221 | Steps: 4 | Val loss: 1.5665 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=183948)[0m top1: 0.014925373134328358
[2m[36m(func pid=183948)[0m top5: 0.6735074626865671
[2m[36m(func pid=183948)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=183948)[0m f1_macro: 0.04196506845010162
[2m[36m(func pid=183948)[0m f1_weighted: 0.005039037488489793
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.0, 0.016, 0.0, 0.1, 0.0, 0.0, 0.0, 0.031, 0.273]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.4953 | Steps: 4 | Val loss: 2.0263 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 10.4726 | Steps: 4 | Val loss: 6109.6528 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:25:18 (running for 00:41:55.38)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.451 |      0.212 |                   36 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.222 |      0.396 |                   34 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.023 |      0.042 |                   12 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 19.912 |      0.018 |                   11 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.41044776119402987
[2m[36m(func pid=178896)[0m top5: 0.9309701492537313
[2m[36m(func pid=178896)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=178896)[0m f1_macro: 0.3955869564946174
[2m[36m(func pid=178896)[0m f1_weighted: 0.4053121939430799
[2m[36m(func pid=178896)[0m f1_per_class: [0.6, 0.524, 0.55, 0.543, 0.19, 0.292, 0.262, 0.396, 0.24, 0.359]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.1479 | Steps: 4 | Val loss: 5.8300 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=184567)[0m top1: 0.05783582089552239
[2m[36m(func pid=184567)[0m top5: 0.5149253731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=184567)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=184567)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.26399253731343286
[2m[36m(func pid=177931)[0m top5: 0.7882462686567164
[2m[36m(func pid=177931)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=177931)[0m f1_macro: 0.2141893627798946
[2m[36m(func pid=177931)[0m f1_weighted: 0.26865491544413883
[2m[36m(func pid=177931)[0m f1_per_class: [0.297, 0.367, 0.229, 0.382, 0.089, 0.082, 0.204, 0.27, 0.107, 0.115]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2166 | Steps: 4 | Val loss: 1.4472 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=183948)[0m top1: 0.24720149253731344
[2m[36m(func pid=183948)[0m top5: 0.7234141791044776
[2m[36m(func pid=183948)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=183948)[0m f1_macro: 0.1438586307348858
[2m[36m(func pid=183948)[0m f1_weighted: 0.2047235148913484
[2m[36m(func pid=183948)[0m f1_per_class: [0.016, 0.331, 0.123, 0.497, 0.153, 0.0, 0.006, 0.0, 0.058, 0.254]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 5.8024 | Steps: 4 | Val loss: 824.3560 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.5129 | Steps: 4 | Val loss: 2.0231 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:25:23 (running for 00:42:00.86)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.495 |      0.214 |                   37 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.217 |      0.422 |                   35 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.148 |      0.144 |                   13 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 10.473 |      0.011 |                   12 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4650186567164179
[2m[36m(func pid=178896)[0m top5: 0.9496268656716418
[2m[36m(func pid=178896)[0m f1_micro: 0.46501865671641784
[2m[36m(func pid=178896)[0m f1_macro: 0.42231641804500375
[2m[36m(func pid=178896)[0m f1_weighted: 0.4769289817484358
[2m[36m(func pid=178896)[0m f1_per_class: [0.559, 0.572, 0.5, 0.526, 0.191, 0.3, 0.489, 0.376, 0.265, 0.444]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.6628 | Steps: 4 | Val loss: 13.1022 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=184567)[0m top1: 0.2980410447761194
[2m[36m(func pid=184567)[0m top5: 0.6058768656716418
[2m[36m(func pid=184567)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=184567)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=184567)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2653917910447761
[2m[36m(func pid=177931)[0m top5: 0.7831156716417911
[2m[36m(func pid=177931)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=177931)[0m f1_macro: 0.2204034969522135
[2m[36m(func pid=177931)[0m f1_weighted: 0.26555267408179967
[2m[36m(func pid=177931)[0m f1_per_class: [0.319, 0.363, 0.27, 0.4, 0.087, 0.111, 0.166, 0.275, 0.109, 0.106]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.06436567164179105
[2m[36m(func pid=183948)[0m top5: 0.6044776119402985
[2m[36m(func pid=183948)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=183948)[0m f1_macro: 0.08755741067404448
[2m[36m(func pid=183948)[0m f1_weighted: 0.06703249148845523
[2m[36m(func pid=183948)[0m f1_per_class: [0.062, 0.19, 0.035, 0.019, 0.116, 0.032, 0.033, 0.162, 0.05, 0.176]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0614 | Steps: 4 | Val loss: 1.4961 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 7.6606 | Steps: 4 | Val loss: 127.6363 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:25:29 (running for 00:42:06.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.513 |      0.22  |                   38 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.061 |      0.41  |                   36 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.663 |      0.088 |                   14 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  5.802 |      0.046 |                   13 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.4435 | Steps: 4 | Val loss: 2.0175 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=178896)[0m top1: 0.46781716417910446
[2m[36m(func pid=178896)[0m top5: 0.9328358208955224
[2m[36m(func pid=178896)[0m f1_micro: 0.46781716417910446
[2m[36m(func pid=178896)[0m f1_macro: 0.4104916012499647
[2m[36m(func pid=178896)[0m f1_weighted: 0.47822331620809716
[2m[36m(func pid=178896)[0m f1_per_class: [0.564, 0.584, 0.436, 0.487, 0.171, 0.255, 0.549, 0.338, 0.253, 0.467]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.0005 | Steps: 4 | Val loss: 17.2758 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=184567)[0m top1: 0.1982276119402985
[2m[36m(func pid=184567)[0m top5: 0.6222014925373134
[2m[36m(func pid=184567)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=184567)[0m f1_macro: 0.06462666858134018
[2m[36m(func pid=184567)[0m f1_weighted: 0.10559559709296254
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.288, 0.339, 0.0, 0.019]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2691231343283582
[2m[36m(func pid=177931)[0m top5: 0.7933768656716418
[2m[36m(func pid=177931)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=177931)[0m f1_macro: 0.22515851864109876
[2m[36m(func pid=177931)[0m f1_weighted: 0.27288668545064426
[2m[36m(func pid=177931)[0m f1_per_class: [0.309, 0.362, 0.312, 0.417, 0.086, 0.114, 0.175, 0.261, 0.112, 0.104]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2143 | Steps: 4 | Val loss: 1.4946 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=183948)[0m top1: 0.07276119402985075
[2m[36m(func pid=183948)[0m top5: 0.6590485074626866
[2m[36m(func pid=183948)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=183948)[0m f1_macro: 0.07150605599295816
[2m[36m(func pid=183948)[0m f1_weighted: 0.07529407656956268
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.068, 0.043, 0.099, 0.114, 0.221, 0.006, 0.092, 0.07, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.8030 | Steps: 4 | Val loss: 21.8217 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:25:34 (running for 00:42:11.67)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.444 |      0.225 |                   39 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.214 |      0.397 |                   37 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1     |      0.072 |                   15 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  7.661 |      0.065 |                   14 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.2711 | Steps: 4 | Val loss: 2.0001 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=178896)[0m top1: 0.47574626865671643
[2m[36m(func pid=178896)[0m top5: 0.9328358208955224
[2m[36m(func pid=178896)[0m f1_micro: 0.47574626865671643
[2m[36m(func pid=178896)[0m f1_macro: 0.3972635428932031
[2m[36m(func pid=178896)[0m f1_weighted: 0.4841931580978045
[2m[36m(func pid=178896)[0m f1_per_class: [0.569, 0.587, 0.304, 0.487, 0.168, 0.206, 0.593, 0.317, 0.241, 0.5]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8272 | Steps: 4 | Val loss: 7.4202 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=184567)[0m top1: 0.2896455223880597
[2m[36m(func pid=184567)[0m top5: 0.6091417910447762
[2m[36m(func pid=184567)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=184567)[0m f1_macro: 0.04519650655021834
[2m[36m(func pid=184567)[0m f1_weighted: 0.13470414032457798
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.452, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2719216417910448
[2m[36m(func pid=177931)[0m top5: 0.8120335820895522
[2m[36m(func pid=177931)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=177931)[0m f1_macro: 0.22876037673837457
[2m[36m(func pid=177931)[0m f1_weighted: 0.2725728014756985
[2m[36m(func pid=177931)[0m f1_per_class: [0.319, 0.351, 0.312, 0.433, 0.105, 0.123, 0.161, 0.258, 0.11, 0.115]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.1752 | Steps: 4 | Val loss: 1.6211 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=183948)[0m top1: 0.19309701492537312
[2m[36m(func pid=183948)[0m top5: 0.8530783582089553
[2m[36m(func pid=183948)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=183948)[0m f1_macro: 0.16590038134015464
[2m[36m(func pid=183948)[0m f1_weighted: 0.20226109172633755
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.267, 0.062, 0.388, 0.417, 0.113, 0.042, 0.286, 0.086, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 5.8846 | Steps: 4 | Val loss: 47.0746 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:25:40 (running for 00:42:17.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.271 |      0.229 |                   40 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.175 |      0.364 |                   38 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.827 |      0.166 |                   16 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.803 |      0.045 |                   15 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4300373134328358
[2m[36m(func pid=178896)[0m top5: 0.9151119402985075
[2m[36m(func pid=178896)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=178896)[0m f1_macro: 0.3636811579443592
[2m[36m(func pid=178896)[0m f1_weighted: 0.44631820232202635
[2m[36m(func pid=178896)[0m f1_per_class: [0.547, 0.576, 0.234, 0.457, 0.126, 0.124, 0.539, 0.314, 0.23, 0.491]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5051 | Steps: 4 | Val loss: 1.9790 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7645 | Steps: 4 | Val loss: 7.0865 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=184567)[0m top1: 0.29850746268656714
[2m[36m(func pid=184567)[0m top5: 0.488339552238806
[2m[36m(func pid=184567)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=184567)[0m f1_macro: 0.048397581814909316
[2m[36m(func pid=184567)[0m f1_weighted: 0.13910047648492457
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.465, 0.0, 0.019, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2681902985074627
[2m[36m(func pid=177931)[0m top5: 0.824160447761194
[2m[36m(func pid=177931)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=177931)[0m f1_macro: 0.22949325094202572
[2m[36m(func pid=177931)[0m f1_weighted: 0.2740702520648388
[2m[36m(func pid=177931)[0m f1_per_class: [0.366, 0.328, 0.276, 0.44, 0.097, 0.133, 0.169, 0.241, 0.129, 0.118]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.18050373134328357
[2m[36m(func pid=183948)[0m top5: 0.7658582089552238
[2m[36m(func pid=183948)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=183948)[0m f1_macro: 0.13223884954254117
[2m[36m(func pid=183948)[0m f1_weighted: 0.20364829466150303
[2m[36m(func pid=183948)[0m f1_per_class: [0.126, 0.118, 0.0, 0.323, 0.111, 0.0, 0.239, 0.257, 0.091, 0.058]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1247 | Steps: 4 | Val loss: 1.6698 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 8.5511 | Steps: 4 | Val loss: 13.4632 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=178896)[0m top1: 0.41138059701492535
[2m[36m(func pid=178896)[0m top5: 0.9146455223880597
[2m[36m(func pid=178896)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=178896)[0m f1_macro: 0.3593741539444299
[2m[36m(func pid=178896)[0m f1_weighted: 0.4275987172306788
[2m[36m(func pid=178896)[0m f1_per_class: [0.582, 0.575, 0.265, 0.466, 0.096, 0.09, 0.47, 0.35, 0.263, 0.438]
[2m[36m(func pid=178896)[0m 
== Status ==
Current time: 2024-01-07 14:25:45 (running for 00:42:22.62)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.505 |      0.229 |                   41 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.125 |      0.359 |                   39 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  0.765 |      0.132 |                   17 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  5.885 |      0.048 |                   16 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.3096 | Steps: 4 | Val loss: 1.9591 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4659 | Steps: 4 | Val loss: 9.9872 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=184567)[0m top1: 0.07416044776119403
[2m[36m(func pid=184567)[0m top5: 0.511660447761194
[2m[36m(func pid=184567)[0m f1_micro: 0.07416044776119403
[2m[36m(func pid=184567)[0m f1_macro: 0.05924535722866494
[2m[36m(func pid=184567)[0m f1_weighted: 0.06402236245742017
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.018, 0.0, 0.13, 0.425, 0.019, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.29011194029850745
[2m[36m(func pid=177931)[0m top5: 0.8339552238805971
[2m[36m(func pid=177931)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=177931)[0m f1_macro: 0.2395752970103346
[2m[36m(func pid=177931)[0m f1_weighted: 0.2860862073785196
[2m[36m(func pid=177931)[0m f1_per_class: [0.36, 0.333, 0.296, 0.487, 0.125, 0.137, 0.161, 0.24, 0.105, 0.151]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1022 | Steps: 4 | Val loss: 1.7171 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=183948)[0m top1: 0.17723880597014927
[2m[36m(func pid=183948)[0m top5: 0.5844216417910447
[2m[36m(func pid=183948)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=183948)[0m f1_macro: 0.14143417727158447
[2m[36m(func pid=183948)[0m f1_weighted: 0.19444740707876526
[2m[36m(func pid=183948)[0m f1_per_class: [0.193, 0.081, 0.085, 0.0, 0.0, 0.258, 0.438, 0.193, 0.133, 0.033]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.3598 | Steps: 4 | Val loss: 3.3340 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:25:50 (running for 00:42:27.87)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.31  |      0.24  |                   42 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.102 |      0.346 |                   40 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.466 |      0.141 |                   18 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  8.551 |      0.059 |                   17 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.3917910447761194
[2m[36m(func pid=178896)[0m top5: 0.9081156716417911
[2m[36m(func pid=178896)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=178896)[0m f1_macro: 0.3463320549656802
[2m[36m(func pid=178896)[0m f1_weighted: 0.40885005018077214
[2m[36m(func pid=178896)[0m f1_per_class: [0.61, 0.547, 0.333, 0.475, 0.089, 0.095, 0.416, 0.35, 0.256, 0.291]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.2595 | Steps: 4 | Val loss: 1.9385 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.0276 | Steps: 4 | Val loss: 8.0135 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=184567)[0m top1: 0.036380597014925374
[2m[36m(func pid=184567)[0m top5: 0.5657649253731343
[2m[36m(func pid=184567)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=184567)[0m f1_macro: 0.04952338560161514
[2m[36m(func pid=184567)[0m f1_weighted: 0.027812642445346144
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.479, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3031716417910448
[2m[36m(func pid=177931)[0m top5: 0.8465485074626866
[2m[36m(func pid=177931)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=177931)[0m f1_macro: 0.25137058708351645
[2m[36m(func pid=177931)[0m f1_weighted: 0.303486989790596
[2m[36m(func pid=177931)[0m f1_per_class: [0.342, 0.329, 0.316, 0.5, 0.145, 0.156, 0.199, 0.251, 0.119, 0.157]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0697 | Steps: 4 | Val loss: 1.7426 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=183948)[0m top1: 0.24486940298507462
[2m[36m(func pid=183948)[0m top5: 0.5592350746268657
[2m[36m(func pid=183948)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=183948)[0m f1_macro: 0.18526685701627935
[2m[36m(func pid=183948)[0m f1_weighted: 0.22057833126789808
[2m[36m(func pid=183948)[0m f1_per_class: [0.394, 0.135, 0.552, 0.0, 0.04, 0.008, 0.61, 0.0, 0.073, 0.041]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.1257 | Steps: 4 | Val loss: 7.9476 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:25:56 (running for 00:42:33.52)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.259 |      0.251 |                   43 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.07  |      0.337 |                   41 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.028 |      0.185 |                   19 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.36  |      0.05  |                   18 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.3675373134328358
[2m[36m(func pid=178896)[0m top5: 0.9067164179104478
[2m[36m(func pid=178896)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=178896)[0m f1_macro: 0.33653435808871157
[2m[36m(func pid=178896)[0m f1_weighted: 0.37901915682665943
[2m[36m(func pid=178896)[0m f1_per_class: [0.598, 0.547, 0.312, 0.466, 0.092, 0.137, 0.309, 0.345, 0.269, 0.291]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1288 | Steps: 4 | Val loss: 1.9392 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.5778 | Steps: 4 | Val loss: 4.8095 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=184567)[0m top1: 0.044309701492537316
[2m[36m(func pid=184567)[0m top5: 0.585820895522388
[2m[36m(func pid=184567)[0m f1_micro: 0.044309701492537316
[2m[36m(func pid=184567)[0m f1_macro: 0.021746821733412414
[2m[36m(func pid=184567)[0m f1_weighted: 0.060108732299717534
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.201, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.300839552238806
[2m[36m(func pid=177931)[0m top5: 0.8418843283582089
[2m[36m(func pid=177931)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=177931)[0m f1_macro: 0.24561193099763717
[2m[36m(func pid=177931)[0m f1_weighted: 0.30497405227684604
[2m[36m(func pid=177931)[0m f1_per_class: [0.359, 0.327, 0.304, 0.499, 0.11, 0.147, 0.211, 0.25, 0.115, 0.135]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0820 | Steps: 4 | Val loss: 1.7845 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=183948)[0m top1: 0.1958955223880597
[2m[36m(func pid=183948)[0m top5: 0.7010261194029851
[2m[36m(func pid=183948)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=183948)[0m f1_macro: 0.15570797558170596
[2m[36m(func pid=183948)[0m f1_weighted: 0.19020616730661952
[2m[36m(func pid=183948)[0m f1_per_class: [0.479, 0.093, 0.113, 0.306, 0.035, 0.0, 0.21, 0.255, 0.0, 0.066]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 4.3713 | Steps: 4 | Val loss: 3.3419 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:26:02 (running for 00:42:38.94)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.129 |      0.246 |                   44 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.082 |      0.331 |                   42 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.578 |      0.156 |                   20 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  4.126 |      0.022 |                   19 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.3516791044776119
[2m[36m(func pid=178896)[0m top5: 0.9053171641791045
[2m[36m(func pid=178896)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=178896)[0m f1_macro: 0.33112895035745954
[2m[36m(func pid=178896)[0m f1_weighted: 0.35414432071053875
[2m[36m(func pid=178896)[0m f1_per_class: [0.595, 0.527, 0.348, 0.483, 0.098, 0.163, 0.211, 0.35, 0.259, 0.278]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.2803 | Steps: 4 | Val loss: 1.9702 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2296 | Steps: 4 | Val loss: 10.4807 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=184567)[0m top1: 0.15111940298507462
[2m[36m(func pid=184567)[0m top5: 0.804570895522388
[2m[36m(func pid=184567)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=184567)[0m f1_macro: 0.07546479149052157
[2m[36m(func pid=184567)[0m f1_weighted: 0.07883871069702002
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.218, 0.095, 0.441, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2849813432835821
[2m[36m(func pid=177931)[0m top5: 0.8213619402985075
[2m[36m(func pid=177931)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=177931)[0m f1_macro: 0.2290168468900756
[2m[36m(func pid=177931)[0m f1_weighted: 0.28522930994103596
[2m[36m(func pid=177931)[0m f1_per_class: [0.341, 0.3, 0.197, 0.475, 0.12, 0.166, 0.176, 0.26, 0.123, 0.133]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0600 | Steps: 4 | Val loss: 1.7638 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=183948)[0m top1: 0.12546641791044777
[2m[36m(func pid=183948)[0m top5: 0.7248134328358209
[2m[36m(func pid=183948)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=183948)[0m f1_macro: 0.11317019730030176
[2m[36m(func pid=183948)[0m f1_weighted: 0.16096480004333263
[2m[36m(func pid=183948)[0m f1_per_class: [0.156, 0.262, 0.023, 0.098, 0.022, 0.0, 0.235, 0.242, 0.0, 0.093]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 5.3007 | Steps: 4 | Val loss: 3.1677 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:26:07 (running for 00:42:44.43)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.28  |      0.229 |                   45 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.06  |      0.338 |                   43 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.23  |      0.113 |                   21 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  4.371 |      0.075 |                   20 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.3628731343283582
[2m[36m(func pid=178896)[0m top5: 0.9011194029850746
[2m[36m(func pid=178896)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=178896)[0m f1_macro: 0.33792107418985895
[2m[36m(func pid=178896)[0m f1_weighted: 0.364317475294719
[2m[36m(func pid=178896)[0m f1_per_class: [0.61, 0.549, 0.333, 0.488, 0.106, 0.176, 0.22, 0.362, 0.253, 0.281]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1556 | Steps: 4 | Val loss: 1.9909 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 3.0168 | Steps: 4 | Val loss: 29.7883 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=184567)[0m top1: 0.35867537313432835
[2m[36m(func pid=184567)[0m top5: 0.8250932835820896
[2m[36m(func pid=184567)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=184567)[0m f1_macro: 0.1489332159808067
[2m[36m(func pid=184567)[0m f1_weighted: 0.2648743910388028
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.423, 0.0, 0.0, 0.0, 0.082, 0.523, 0.46, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.27052238805970147
[2m[36m(func pid=177931)[0m top5: 0.8083022388059702
[2m[36m(func pid=177931)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=177931)[0m f1_macro: 0.21713493168329195
[2m[36m(func pid=177931)[0m f1_weighted: 0.2717777482950717
[2m[36m(func pid=177931)[0m f1_per_class: [0.341, 0.25, 0.202, 0.478, 0.085, 0.126, 0.171, 0.268, 0.131, 0.121]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0384 | Steps: 4 | Val loss: 1.6764 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=183948)[0m top1: 0.026119402985074626
[2m[36m(func pid=183948)[0m top5: 0.6124067164179104
[2m[36m(func pid=183948)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=183948)[0m f1_macro: 0.018843691589323596
[2m[36m(func pid=183948)[0m f1_weighted: 0.026234854689701978
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.016, 0.015, 0.01, 0.0, 0.024, 0.052, 0.0, 0.071, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.6652 | Steps: 4 | Val loss: 3.6193 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:26:12 (running for 00:42:49.86)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.156 |      0.217 |                   46 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.038 |      0.35  |                   44 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.017 |      0.019 |                   22 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  5.301 |      0.149 |                   21 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.38572761194029853
[2m[36m(func pid=178896)[0m top5: 0.9137126865671642
[2m[36m(func pid=178896)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=178896)[0m f1_macro: 0.3498870887045326
[2m[36m(func pid=178896)[0m f1_weighted: 0.3952066979492172
[2m[36m(func pid=178896)[0m f1_per_class: [0.615, 0.554, 0.296, 0.5, 0.126, 0.217, 0.292, 0.38, 0.252, 0.267]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.2022 | Steps: 4 | Val loss: 1.9785 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.8114 | Steps: 4 | Val loss: 110.4389 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=184567)[0m top1: 0.27005597014925375
[2m[36m(func pid=184567)[0m top5: 0.835820895522388
[2m[36m(func pid=184567)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=184567)[0m f1_macro: 0.12456985844191049
[2m[36m(func pid=184567)[0m f1_weighted: 0.23010217959902482
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.261, 0.034, 0.0, 0.0, 0.0, 0.541, 0.409, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2728544776119403
[2m[36m(func pid=177931)[0m top5: 0.8218283582089553
[2m[36m(func pid=177931)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=177931)[0m f1_macro: 0.22054619721493712
[2m[36m(func pid=177931)[0m f1_weighted: 0.28700834371141964
[2m[36m(func pid=177931)[0m f1_per_class: [0.341, 0.28, 0.166, 0.46, 0.089, 0.14, 0.218, 0.264, 0.133, 0.115]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0537 | Steps: 4 | Val loss: 1.6511 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=183948)[0m top1: 0.010261194029850746
[2m[36m(func pid=183948)[0m top5: 0.302705223880597
[2m[36m(func pid=183948)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=183948)[0m f1_macro: 0.011153645500827739
[2m[36m(func pid=183948)[0m f1_weighted: 0.002696044454605578
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.004, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.037, 0.056]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 14.4052 | Steps: 4 | Val loss: 5.3596 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 14:26:18 (running for 00:42:55.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.202 |      0.221 |                   47 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.054 |      0.355 |                   45 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.811 |      0.011 |                   23 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  4.665 |      0.125 |                   22 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.3941231343283582
[2m[36m(func pid=178896)[0m top5: 0.9165111940298507
[2m[36m(func pid=178896)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=178896)[0m f1_macro: 0.3548436724907777
[2m[36m(func pid=178896)[0m f1_weighted: 0.40922652269266835
[2m[36m(func pid=178896)[0m f1_per_class: [0.593, 0.542, 0.313, 0.5, 0.13, 0.207, 0.347, 0.392, 0.253, 0.271]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1765 | Steps: 4 | Val loss: 2.0007 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.3437 | Steps: 4 | Val loss: 30.5946 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=184567)[0m top1: 0.271455223880597
[2m[36m(func pid=184567)[0m top5: 0.5354477611940298
[2m[36m(func pid=184567)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=184567)[0m f1_macro: 0.050454802747281015
[2m[36m(func pid=184567)[0m f1_weighted: 0.14766793049581234
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.495, 0.0, 0.0, 0.009]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.25699626865671643
[2m[36m(func pid=177931)[0m top5: 0.808768656716418
[2m[36m(func pid=177931)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=177931)[0m f1_macro: 0.21266932037018998
[2m[36m(func pid=177931)[0m f1_weighted: 0.27345230416714156
[2m[36m(func pid=177931)[0m f1_per_class: [0.301, 0.263, 0.163, 0.437, 0.073, 0.15, 0.2, 0.261, 0.162, 0.117]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1194 | Steps: 4 | Val loss: 1.6012 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=183948)[0m top1: 0.05037313432835821
[2m[36m(func pid=183948)[0m top5: 0.5065298507462687
[2m[36m(func pid=183948)[0m f1_micro: 0.05037313432835821
[2m[36m(func pid=183948)[0m f1_macro: 0.04355883648677239
[2m[36m(func pid=183948)[0m f1_weighted: 0.06524403156813338
[2m[36m(func pid=183948)[0m f1_per_class: [0.056, 0.189, 0.018, 0.104, 0.0, 0.0, 0.0, 0.035, 0.0, 0.034]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 10.1981 | Steps: 4 | Val loss: 3.2454 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:26:23 (running for 00:43:00.87)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.177 |      0.213 |                   48 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.119 |      0.379 |                   46 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.344 |      0.044 |                   24 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 14.405 |      0.05  |                   23 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.41044776119402987
[2m[36m(func pid=178896)[0m top5: 0.929570895522388
[2m[36m(func pid=178896)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=178896)[0m f1_macro: 0.37924552287474333
[2m[36m(func pid=178896)[0m f1_weighted: 0.43308250713744495
[2m[36m(func pid=178896)[0m f1_per_class: [0.635, 0.541, 0.436, 0.502, 0.149, 0.246, 0.411, 0.366, 0.255, 0.252]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1287 | Steps: 4 | Val loss: 1.9932 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 4.1599 | Steps: 4 | Val loss: 8.9973 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=184567)[0m top1: 0.03404850746268657
[2m[36m(func pid=184567)[0m top5: 0.4533582089552239
[2m[36m(func pid=184567)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=184567)[0m f1_macro: 0.045339655251347914
[2m[36m(func pid=184567)[0m f1_weighted: 0.026601804329784994
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.423, 0.0, 0.024]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2537313432835821
[2m[36m(func pid=177931)[0m top5: 0.8092350746268657
[2m[36m(func pid=177931)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=177931)[0m f1_macro: 0.21358567407501386
[2m[36m(func pid=177931)[0m f1_weighted: 0.27631975492759864
[2m[36m(func pid=177931)[0m f1_per_class: [0.296, 0.291, 0.179, 0.394, 0.068, 0.146, 0.235, 0.283, 0.129, 0.115]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1175 | Steps: 4 | Val loss: 1.6271 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=183948)[0m top1: 0.28638059701492535
[2m[36m(func pid=183948)[0m top5: 0.644589552238806
[2m[36m(func pid=183948)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=183948)[0m f1_macro: 0.13658505956881709
[2m[36m(func pid=183948)[0m f1_weighted: 0.23799407102475362
[2m[36m(func pid=183948)[0m f1_per_class: [0.119, 0.496, 0.176, 0.0, 0.0, 0.0, 0.498, 0.0, 0.0, 0.077]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.8783 | Steps: 4 | Val loss: 2.8938 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=178896)[0m top1: 0.3983208955223881
[2m[36m(func pid=178896)[0m top5: 0.9230410447761194
[2m[36m(func pid=178896)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=178896)[0m f1_macro: 0.3742335289201844
[2m[36m(func pid=178896)[0m f1_weighted: 0.4156188838364529
[2m[36m(func pid=178896)[0m f1_per_class: [0.594, 0.556, 0.453, 0.475, 0.154, 0.251, 0.37, 0.358, 0.251, 0.281]
[2m[36m(func pid=178896)[0m 
== Status ==
Current time: 2024-01-07 14:26:29 (running for 00:43:06.31)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.129 |      0.214 |                   49 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.118 |      0.374 |                   47 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  4.16  |      0.137 |                   25 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  | 10.198 |      0.045 |                   24 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2400 | Steps: 4 | Val loss: 1.9767 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1218 | Steps: 4 | Val loss: 22.3517 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=184567)[0m top1: 0.006996268656716418
[2m[36m(func pid=184567)[0m top5: 0.384794776119403
[2m[36m(func pid=184567)[0m f1_micro: 0.006996268656716418
[2m[36m(func pid=184567)[0m f1_macro: 0.0031373974008207933
[2m[36m(func pid=184567)[0m f1_weighted: 0.0019053367046011395
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.003, 0.016, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.2775186567164179
[2m[36m(func pid=177931)[0m top5: 0.820429104477612
[2m[36m(func pid=177931)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=177931)[0m f1_macro: 0.23464749586873776
[2m[36m(func pid=177931)[0m f1_weighted: 0.30007957988277606
[2m[36m(func pid=177931)[0m f1_per_class: [0.31, 0.346, 0.218, 0.405, 0.064, 0.13, 0.269, 0.3, 0.166, 0.138]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0278 | Steps: 4 | Val loss: 1.6322 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=183948)[0m top1: 0.19542910447761194
[2m[36m(func pid=183948)[0m top5: 0.5965485074626866
[2m[36m(func pid=183948)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=183948)[0m f1_macro: 0.10711343626902217
[2m[36m(func pid=183948)[0m f1_weighted: 0.13838777634834448
[2m[36m(func pid=183948)[0m f1_per_class: [0.076, 0.391, 0.0, 0.0, 0.0, 0.298, 0.09, 0.129, 0.0, 0.087]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.6111 | Steps: 4 | Val loss: 2.9926 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:26:34 (running for 00:43:11.66)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.24  |      0.235 |                   50 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.028 |      0.371 |                   48 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.122 |      0.107 |                   26 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.878 |      0.003 |                   25 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4025186567164179
[2m[36m(func pid=178896)[0m top5: 0.9244402985074627
[2m[36m(func pid=178896)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=178896)[0m f1_macro: 0.3705090349212611
[2m[36m(func pid=178896)[0m f1_weighted: 0.40554131158128826
[2m[36m(func pid=178896)[0m f1_per_class: [0.623, 0.583, 0.296, 0.426, 0.158, 0.236, 0.363, 0.381, 0.28, 0.36]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1308 | Steps: 4 | Val loss: 1.9489 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3902 | Steps: 4 | Val loss: 73.5684 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=184567)[0m top1: 0.027052238805970148
[2m[36m(func pid=184567)[0m top5: 0.5657649253731343
[2m[36m(func pid=184567)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=184567)[0m f1_macro: 0.013476672677046678
[2m[36m(func pid=184567)[0m f1_weighted: 0.03637599212838288
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.29151119402985076
[2m[36m(func pid=177931)[0m top5: 0.8283582089552238
[2m[36m(func pid=177931)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=177931)[0m f1_macro: 0.24253723567551355
[2m[36m(func pid=177931)[0m f1_weighted: 0.3054960529519152
[2m[36m(func pid=177931)[0m f1_per_class: [0.316, 0.373, 0.226, 0.447, 0.068, 0.128, 0.231, 0.294, 0.174, 0.168]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.1403 | Steps: 4 | Val loss: 1.6650 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=183948)[0m top1: 0.05223880597014925
[2m[36m(func pid=183948)[0m top5: 0.6091417910447762
[2m[36m(func pid=183948)[0m f1_micro: 0.05223880597014925
[2m[36m(func pid=183948)[0m f1_macro: 0.05198989779256251
[2m[36m(func pid=183948)[0m f1_weighted: 0.05312009436801497
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.093, 0.023, 0.0, 0.049, 0.081, 0.065, 0.123, 0.0, 0.087]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 4.3132 | Steps: 4 | Val loss: 2.6232 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=178896)[0m top1: 0.396455223880597
[2m[36m(func pid=178896)[0m top5: 0.9235074626865671
[2m[36m(func pid=178896)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=178896)[0m f1_macro: 0.3770065870631152
[2m[36m(func pid=178896)[0m f1_weighted: 0.38909504185969307
[2m[36m(func pid=178896)[0m f1_per_class: [0.627, 0.597, 0.295, 0.425, 0.162, 0.232, 0.298, 0.358, 0.32, 0.455]
[2m[36m(func pid=178896)[0m 
== Status ==
Current time: 2024-01-07 14:26:40 (running for 00:43:17.17)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.131 |      0.243 |                   51 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.14  |      0.377 |                   49 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.39  |      0.052 |                   27 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.611 |      0.013 |                   26 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0268 | Steps: 4 | Val loss: 1.9524 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0130 | Steps: 4 | Val loss: 14.9052 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=184567)[0m top1: 0.021455223880597014
[2m[36m(func pid=184567)[0m top5: 0.5643656716417911
[2m[36m(func pid=184567)[0m f1_micro: 0.021455223880597014
[2m[36m(func pid=184567)[0m f1_macro: 0.024928124680996998
[2m[36m(func pid=184567)[0m f1_weighted: 0.027970898118493657
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.059, 0.177, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1167 | Steps: 4 | Val loss: 1.6511 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=177931)[0m top1: 0.2943097014925373
[2m[36m(func pid=177931)[0m top5: 0.8213619402985075
[2m[36m(func pid=177931)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=177931)[0m f1_macro: 0.24704988488447333
[2m[36m(func pid=177931)[0m f1_weighted: 0.3042957393763093
[2m[36m(func pid=177931)[0m f1_per_class: [0.337, 0.416, 0.245, 0.433, 0.071, 0.138, 0.21, 0.301, 0.16, 0.159]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.16837686567164178
[2m[36m(func pid=183948)[0m top5: 0.6506529850746269
[2m[36m(func pid=183948)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=183948)[0m f1_macro: 0.12278283615430344
[2m[36m(func pid=183948)[0m f1_weighted: 0.12254248988994665
[2m[36m(func pid=183948)[0m f1_per_class: [0.082, 0.51, 0.034, 0.007, 0.013, 0.0, 0.033, 0.315, 0.0, 0.234]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8476 | Steps: 4 | Val loss: 2.9834 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 14:26:45 (running for 00:43:22.29)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.027 |      0.247 |                   52 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.117 |      0.361 |                   50 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.013 |      0.123 |                   28 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  4.313 |      0.025 |                   27 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4025186567164179
[2m[36m(func pid=178896)[0m top5: 0.9221082089552238
[2m[36m(func pid=178896)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=178896)[0m f1_macro: 0.36063803646937836
[2m[36m(func pid=178896)[0m f1_weighted: 0.40701773086861415
[2m[36m(func pid=178896)[0m f1_per_class: [0.621, 0.586, 0.22, 0.44, 0.154, 0.22, 0.36, 0.37, 0.323, 0.312]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0015 | Steps: 4 | Val loss: 1.9254 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 5.0798 | Steps: 4 | Val loss: 10.9951 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=184567)[0m top1: 0.20055970149253732
[2m[36m(func pid=184567)[0m top5: 0.6627798507462687
[2m[36m(func pid=184567)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=184567)[0m f1_macro: 0.06809171788408312
[2m[36m(func pid=184567)[0m f1_weighted: 0.12782024539757378
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.306, 0.0, 0.153, 0.0, 0.014, 0.078, 0.129, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0720 | Steps: 4 | Val loss: 1.6813 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=183948)[0m top1: 0.1875
[2m[36m(func pid=183948)[0m top5: 0.7243470149253731
[2m[36m(func pid=183948)[0m f1_micro: 0.1875
[2m[36m(func pid=183948)[0m f1_macro: 0.13682580531752203
[2m[36m(func pid=183948)[0m f1_weighted: 0.13363217356727247
[2m[36m(func pid=183948)[0m f1_per_class: [0.09, 0.489, 0.052, 0.076, 0.0, 0.0, 0.009, 0.313, 0.064, 0.276]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3045708955223881
[2m[36m(func pid=177931)[0m top5: 0.8395522388059702
[2m[36m(func pid=177931)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=177931)[0m f1_macro: 0.2571074615208285
[2m[36m(func pid=177931)[0m f1_weighted: 0.31301808951504667
[2m[36m(func pid=177931)[0m f1_per_class: [0.333, 0.415, 0.308, 0.44, 0.09, 0.136, 0.233, 0.293, 0.178, 0.146]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7566 | Steps: 4 | Val loss: 2.3468 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:26:50 (running for 00:43:27.86)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.002 |      0.257 |                   53 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.072 |      0.351 |                   51 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  5.08  |      0.137 |                   29 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.848 |      0.068 |                   28 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.3908582089552239
[2m[36m(func pid=178896)[0m top5: 0.9169776119402985
[2m[36m(func pid=178896)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=178896)[0m f1_macro: 0.3508923056621294
[2m[36m(func pid=178896)[0m f1_weighted: 0.39734529937116037
[2m[36m(func pid=178896)[0m f1_per_class: [0.614, 0.584, 0.161, 0.43, 0.169, 0.256, 0.328, 0.371, 0.31, 0.286]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0145 | Steps: 4 | Val loss: 3.9901 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=184567)[0m top1: 0.05317164179104478
[2m[36m(func pid=184567)[0m top5: 0.6534514925373134
[2m[36m(func pid=184567)[0m f1_micro: 0.05317164179104478
[2m[36m(func pid=184567)[0m f1_macro: 0.0359104316892474
[2m[36m(func pid=184567)[0m f1_weighted: 0.038314954313435144
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.078, 0.205, 0.068, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0229 | Steps: 4 | Val loss: 1.9008 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1137 | Steps: 4 | Val loss: 1.6385 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=183948)[0m top1: 0.23087686567164178
[2m[36m(func pid=183948)[0m top5: 0.7621268656716418
[2m[36m(func pid=183948)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=183948)[0m f1_macro: 0.24090781038290648
[2m[36m(func pid=183948)[0m f1_weighted: 0.2074224622512402
[2m[36m(func pid=183948)[0m f1_per_class: [0.13, 0.347, 0.409, 0.34, 0.087, 0.188, 0.003, 0.227, 0.166, 0.512]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3185634328358209
[2m[36m(func pid=177931)[0m top5: 0.8540111940298507
[2m[36m(func pid=177931)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=177931)[0m f1_macro: 0.27213031863612147
[2m[36m(func pid=177931)[0m f1_weighted: 0.3266536646799569
[2m[36m(func pid=177931)[0m f1_per_class: [0.328, 0.449, 0.338, 0.443, 0.101, 0.169, 0.242, 0.284, 0.188, 0.178]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.2057 | Steps: 4 | Val loss: 2.3794 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 14:26:56 (running for 00:43:33.35)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.023 |      0.272 |                   54 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.114 |      0.358 |                   52 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.014 |      0.241 |                   30 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.757 |      0.036 |                   29 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.3978544776119403
[2m[36m(func pid=178896)[0m top5: 0.9272388059701493
[2m[36m(func pid=178896)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=178896)[0m f1_macro: 0.3582629653441146
[2m[36m(func pid=178896)[0m f1_weighted: 0.40699170756767405
[2m[36m(func pid=178896)[0m f1_per_class: [0.627, 0.594, 0.17, 0.44, 0.161, 0.245, 0.349, 0.362, 0.303, 0.33]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m top1: 0.07276119402985075
[2m[36m(func pid=184567)[0m top5: 0.6730410447761194
[2m[36m(func pid=184567)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=184567)[0m f1_macro: 0.04649917608592749
[2m[36m(func pid=184567)[0m f1_weighted: 0.06794572981169293
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.178, 0.218, 0.069, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.9267 | Steps: 4 | Val loss: 4.8204 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.8702 | Steps: 4 | Val loss: 1.8943 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1187 | Steps: 4 | Val loss: 1.5399 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=183948)[0m top1: 0.12173507462686567
[2m[36m(func pid=183948)[0m top5: 0.6655783582089553
[2m[36m(func pid=183948)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=183948)[0m f1_macro: 0.0931789559686039
[2m[36m(func pid=183948)[0m f1_weighted: 0.08678189063566671
[2m[36m(func pid=183948)[0m f1_per_class: [0.097, 0.032, 0.0, 0.155, 0.084, 0.206, 0.0, 0.092, 0.164, 0.103]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3199626865671642
[2m[36m(func pid=177931)[0m top5: 0.8563432835820896
[2m[36m(func pid=177931)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=177931)[0m f1_macro: 0.2798030813281502
[2m[36m(func pid=177931)[0m f1_weighted: 0.32688846682876194
[2m[36m(func pid=177931)[0m f1_per_class: [0.377, 0.466, 0.367, 0.434, 0.09, 0.167, 0.238, 0.292, 0.173, 0.194]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.0017 | Steps: 4 | Val loss: 2.2842 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:27:01 (running for 00:43:38.76)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.87  |      0.28  |                   55 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.119 |      0.387 |                   53 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.927 |      0.093 |                   31 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.206 |      0.046 |                   30 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4300373134328358
[2m[36m(func pid=178896)[0m top5: 0.9402985074626866
[2m[36m(func pid=178896)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=178896)[0m f1_macro: 0.3869399410775044
[2m[36m(func pid=178896)[0m f1_weighted: 0.43636253795498703
[2m[36m(func pid=178896)[0m f1_per_class: [0.614, 0.597, 0.329, 0.501, 0.215, 0.289, 0.375, 0.334, 0.302, 0.313]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m top1: 0.0648320895522388
[2m[36m(func pid=184567)[0m top5: 0.6688432835820896
[2m[36m(func pid=184567)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=184567)[0m f1_macro: 0.05679404736525777
[2m[36m(func pid=184567)[0m f1_weighted: 0.04947161632810638
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.076, 0.422, 0.069, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.8962 | Steps: 4 | Val loss: 4.9819 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8991 | Steps: 4 | Val loss: 1.9010 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.1162 | Steps: 4 | Val loss: 1.5411 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=183948)[0m top1: 0.29244402985074625
[2m[36m(func pid=183948)[0m top5: 0.7915111940298507
[2m[36m(func pid=183948)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=183948)[0m f1_macro: 0.22600874772773044
[2m[36m(func pid=183948)[0m f1_weighted: 0.27565265586853854
[2m[36m(func pid=183948)[0m f1_per_class: [0.107, 0.325, 0.421, 0.126, 0.153, 0.0, 0.568, 0.0, 0.135, 0.426]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7886 | Steps: 4 | Val loss: 2.4009 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=177931)[0m top1: 0.314365671641791
[2m[36m(func pid=177931)[0m top5: 0.8586753731343284
[2m[36m(func pid=177931)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=177931)[0m f1_macro: 0.28107215842537925
[2m[36m(func pid=177931)[0m f1_weighted: 0.32110101457811613
[2m[36m(func pid=177931)[0m f1_per_class: [0.373, 0.447, 0.407, 0.454, 0.085, 0.177, 0.206, 0.299, 0.17, 0.193]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:27:07 (running for 00:43:44.10)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.899 |      0.281 |                   56 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.116 |      0.394 |                   54 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.896 |      0.226 |                   32 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.002 |      0.057 |                   31 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4388992537313433
[2m[36m(func pid=178896)[0m top5: 0.9347014925373134
[2m[36m(func pid=178896)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=178896)[0m f1_macro: 0.39407346387311154
[2m[36m(func pid=178896)[0m f1_weighted: 0.4432824675933477
[2m[36m(func pid=178896)[0m f1_per_class: [0.64, 0.602, 0.347, 0.526, 0.173, 0.273, 0.368, 0.367, 0.321, 0.325]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m top1: 0.05223880597014925
[2m[36m(func pid=184567)[0m top5: 0.6716417910447762
[2m[36m(func pid=184567)[0m f1_micro: 0.05223880597014925
[2m[36m(func pid=184567)[0m f1_macro: 0.04749220453653975
[2m[36m(func pid=184567)[0m f1_weighted: 0.02807152920516061
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.003, 0.372, 0.071, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4703 | Steps: 4 | Val loss: 2.6272 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.0172 | Steps: 4 | Val loss: 1.8678 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.1106 | Steps: 4 | Val loss: 1.5756 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.5715 | Steps: 4 | Val loss: 2.3305 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183948)[0m top1: 0.408115671641791
[2m[36m(func pid=183948)[0m top5: 0.8997201492537313
[2m[36m(func pid=183948)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=183948)[0m f1_macro: 0.28932654837342564
[2m[36m(func pid=183948)[0m f1_weighted: 0.3529069329441683
[2m[36m(func pid=183948)[0m f1_per_class: [0.27, 0.523, 0.265, 0.171, 0.156, 0.0, 0.6, 0.311, 0.147, 0.451]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.32742537313432835
[2m[36m(func pid=177931)[0m top5: 0.8666044776119403
[2m[36m(func pid=177931)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=177931)[0m f1_macro: 0.2926389961257283
[2m[36m(func pid=177931)[0m f1_weighted: 0.33479400764463924
[2m[36m(func pid=177931)[0m f1_per_class: [0.381, 0.449, 0.44, 0.459, 0.1, 0.171, 0.245, 0.304, 0.177, 0.2]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:27:12 (running for 00:43:49.55)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.017 |      0.293 |                   57 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.111 |      0.389 |                   55 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.47  |      0.289 |                   33 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.789 |      0.047 |                   32 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.42257462686567165
[2m[36m(func pid=178896)[0m top5: 0.9230410447761194
[2m[36m(func pid=178896)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=178896)[0m f1_macro: 0.3889193476310358
[2m[36m(func pid=178896)[0m f1_weighted: 0.4327454673948691
[2m[36m(func pid=178896)[0m f1_per_class: [0.674, 0.595, 0.407, 0.51, 0.135, 0.23, 0.37, 0.366, 0.291, 0.313]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m top1: 0.13619402985074627
[2m[36m(func pid=184567)[0m top5: 0.523320895522388
[2m[36m(func pid=184567)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=184567)[0m f1_macro: 0.061122211122211126
[2m[36m(func pid=184567)[0m f1_weighted: 0.058061817302722155
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.212, 0.045, 0.354, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.8455 | Steps: 4 | Val loss: 10.2039 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8546 | Steps: 4 | Val loss: 1.8585 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0655 | Steps: 4 | Val loss: 1.6289 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.0529 | Steps: 4 | Val loss: 2.2618 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=183948)[0m top1: 0.17957089552238806
[2m[36m(func pid=183948)[0m top5: 0.8731343283582089
[2m[36m(func pid=183948)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=183948)[0m f1_macro: 0.13013111376064296
[2m[36m(func pid=183948)[0m f1_weighted: 0.15537673044304678
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.118, 0.077, 0.393, 0.051, 0.0, 0.012, 0.256, 0.077, 0.318]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.32276119402985076
[2m[36m(func pid=177931)[0m top5: 0.8745335820895522
[2m[36m(func pid=177931)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=177931)[0m f1_macro: 0.29538403626954146
[2m[36m(func pid=177931)[0m f1_weighted: 0.3355124966647871
[2m[36m(func pid=177931)[0m f1_per_class: [0.371, 0.412, 0.5, 0.465, 0.099, 0.164, 0.265, 0.313, 0.169, 0.197]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:27:18 (running for 00:43:55.03)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.855 |      0.295 |                   58 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.066 |      0.392 |                   56 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.846 |      0.13  |                   34 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.572 |      0.061 |                   33 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=178896)[0m top1: 0.4207089552238806
[2m[36m(func pid=178896)[0m top5: 0.9132462686567164
[2m[36m(func pid=178896)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=178896)[0m f1_macro: 0.3916998457592273
[2m[36m(func pid=178896)[0m f1_weighted: 0.4249585243369211
[2m[36m(func pid=178896)[0m f1_per_class: [0.667, 0.592, 0.453, 0.524, 0.111, 0.206, 0.333, 0.411, 0.284, 0.337]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m top1: 0.15485074626865672
[2m[36m(func pid=184567)[0m top5: 0.5461753731343284
[2m[36m(func pid=184567)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=184567)[0m f1_macro: 0.07648016948042813
[2m[36m(func pid=184567)[0m f1_weighted: 0.08875104460157365
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.217, 0.134, 0.414, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.3174 | Steps: 4 | Val loss: 5.6299 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.9658 | Steps: 4 | Val loss: 1.8692 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1832 | Steps: 4 | Val loss: 1.7113 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5729 | Steps: 4 | Val loss: 2.5026 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=177931)[0m top1: 0.31902985074626866
[2m[36m(func pid=177931)[0m top5: 0.871268656716418
[2m[36m(func pid=177931)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=177931)[0m f1_macro: 0.2945145981443745
[2m[36m(func pid=177931)[0m f1_weighted: 0.33411796465179644
[2m[36m(func pid=177931)[0m f1_per_class: [0.38, 0.452, 0.478, 0.426, 0.085, 0.161, 0.273, 0.314, 0.177, 0.198]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.2416044776119403
[2m[36m(func pid=183948)[0m top5: 0.8722014925373134
[2m[36m(func pid=183948)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=183948)[0m f1_macro: 0.22125397659106016
[2m[36m(func pid=183948)[0m f1_weighted: 0.2426672860899653
[2m[36m(func pid=183948)[0m f1_per_class: [0.167, 0.293, 0.116, 0.426, 0.069, 0.213, 0.024, 0.421, 0.33, 0.153]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:27:23 (running for 00:44:00.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.966 |      0.295 |                   59 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.066 |      0.392 |                   56 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.317 |      0.221 |                   35 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.573 |      0.077 |                   35 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.1525186567164179
[2m[36m(func pid=184567)[0m top5: 0.6702425373134329
[2m[36m(func pid=184567)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=184567)[0m f1_macro: 0.07672158744527166
[2m[36m(func pid=184567)[0m f1_weighted: 0.08321839380822836
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.216, 0.111, 0.44, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m top1: 0.39972014925373134
[2m[36m(func pid=178896)[0m top5: 0.9067164179104478
[2m[36m(func pid=178896)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=178896)[0m f1_macro: 0.3819586143849461
[2m[36m(func pid=178896)[0m f1_weighted: 0.39768169414289467
[2m[36m(func pid=178896)[0m f1_per_class: [0.638, 0.581, 0.471, 0.513, 0.104, 0.193, 0.259, 0.443, 0.278, 0.34]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.5639 | Steps: 4 | Val loss: 7.2789 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.9051 | Steps: 4 | Val loss: 1.8777 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.9078 | Steps: 4 | Val loss: 2.4816 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2814 | Steps: 4 | Val loss: 1.6789 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=183948)[0m top1: 0.3199626865671642
[2m[36m(func pid=183948)[0m top5: 0.8465485074626866
[2m[36m(func pid=183948)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=183948)[0m f1_macro: 0.1854423751771632
[2m[36m(func pid=183948)[0m f1_weighted: 0.3131252231386703
[2m[36m(func pid=183948)[0m f1_per_class: [0.164, 0.125, 0.0, 0.33, 0.165, 0.146, 0.565, 0.0, 0.273, 0.087]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.31669776119402987
[2m[36m(func pid=177931)[0m top5: 0.8642723880597015
[2m[36m(func pid=177931)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=177931)[0m f1_macro: 0.2958420145749374
[2m[36m(func pid=177931)[0m f1_weighted: 0.33071522743397186
[2m[36m(func pid=177931)[0m f1_per_class: [0.406, 0.413, 0.478, 0.466, 0.075, 0.159, 0.243, 0.33, 0.162, 0.224]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:27:28 (running for 00:44:05.69)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.905 |      0.296 |                   60 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.183 |      0.382 |                   57 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.564 |      0.185 |                   36 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.908 |      0.071 |                   36 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.15671641791044777
[2m[36m(func pid=184567)[0m top5: 0.6725746268656716
[2m[36m(func pid=184567)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=184567)[0m f1_macro: 0.07058277647514039
[2m[36m(func pid=184567)[0m f1_weighted: 0.0958443507712314
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.218, 0.178, 0.31, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m top1: 0.4048507462686567
[2m[36m(func pid=178896)[0m top5: 0.9132462686567164
[2m[36m(func pid=178896)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=178896)[0m f1_macro: 0.36883569870469013
[2m[36m(func pid=178896)[0m f1_weighted: 0.40476986105178214
[2m[36m(func pid=178896)[0m f1_per_class: [0.659, 0.592, 0.304, 0.513, 0.108, 0.167, 0.289, 0.44, 0.266, 0.35]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.1263 | Steps: 4 | Val loss: 7.4659 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8992 | Steps: 4 | Val loss: 1.8859 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8015 | Steps: 4 | Val loss: 2.3567 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0761 | Steps: 4 | Val loss: 1.5261 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=177931)[0m top1: 0.314365671641791
[2m[36m(func pid=177931)[0m top5: 0.8572761194029851
[2m[36m(func pid=177931)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=177931)[0m f1_macro: 0.287891445726971
[2m[36m(func pid=177931)[0m f1_weighted: 0.327534361535101
[2m[36m(func pid=177931)[0m f1_per_class: [0.374, 0.391, 0.478, 0.46, 0.081, 0.136, 0.26, 0.347, 0.164, 0.188]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=183948)[0m top1: 0.3138992537313433
[2m[36m(func pid=183948)[0m top5: 0.8269589552238806
[2m[36m(func pid=183948)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=183948)[0m f1_macro: 0.19775708493588934
[2m[36m(func pid=183948)[0m f1_weighted: 0.3169582681855737
[2m[36m(func pid=183948)[0m f1_per_class: [0.122, 0.114, 0.068, 0.336, 0.24, 0.155, 0.564, 0.119, 0.151, 0.109]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:27:34 (running for 00:44:11.05)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.899 |      0.288 |                   61 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.281 |      0.369 |                   58 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.126 |      0.198 |                   37 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.801 |      0.042 |                   37 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.14412313432835822
[2m[36m(func pid=184567)[0m top5: 0.6595149253731343
[2m[36m(func pid=184567)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=184567)[0m f1_macro: 0.0416104839801513
[2m[36m(func pid=184567)[0m f1_weighted: 0.07716457006223101
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.214, 0.171, 0.031, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m top1: 0.4608208955223881
[2m[36m(func pid=178896)[0m top5: 0.9328358208955224
[2m[36m(func pid=178896)[0m f1_micro: 0.4608208955223881
[2m[36m(func pid=178896)[0m f1_macro: 0.3854995468250382
[2m[36m(func pid=178896)[0m f1_weighted: 0.4834382390738572
[2m[36m(func pid=178896)[0m f1_per_class: [0.652, 0.571, 0.271, 0.564, 0.104, 0.174, 0.53, 0.372, 0.267, 0.351]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6817 | Steps: 4 | Val loss: 33.8457 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.9548 | Steps: 4 | Val loss: 1.8723 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6695 | Steps: 4 | Val loss: 2.3511 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.1209 | Steps: 4 | Val loss: 1.5422 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=183948)[0m top1: 0.16557835820895522
[2m[36m(func pid=183948)[0m top5: 0.7658582089552238
[2m[36m(func pid=183948)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=183948)[0m f1_macro: 0.14651750397742408
[2m[36m(func pid=183948)[0m f1_weighted: 0.21200162462384034
[2m[36m(func pid=183948)[0m f1_per_class: [0.087, 0.142, 0.025, 0.303, 0.25, 0.09, 0.266, 0.095, 0.089, 0.119]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.32322761194029853
[2m[36m(func pid=177931)[0m top5: 0.8619402985074627
[2m[36m(func pid=177931)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=177931)[0m f1_macro: 0.2892998532947396
[2m[36m(func pid=177931)[0m f1_weighted: 0.32990072254537184
[2m[36m(func pid=177931)[0m f1_per_class: [0.383, 0.37, 0.478, 0.503, 0.084, 0.13, 0.242, 0.342, 0.163, 0.198]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:27:39 (running for 00:44:16.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.955 |      0.289 |                   62 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.076 |      0.385 |                   59 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.682 |      0.147 |                   38 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.67  |      0.059 |                   38 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.14272388059701493
[2m[36m(func pid=184567)[0m top5: 0.6651119402985075
[2m[36m(func pid=184567)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=184567)[0m f1_macro: 0.05896705837875484
[2m[36m(func pid=184567)[0m f1_weighted: 0.07543378412013577
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.213, 0.122, 0.255, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m top1: 0.46408582089552236
[2m[36m(func pid=178896)[0m top5: 0.9253731343283582
[2m[36m(func pid=178896)[0m f1_micro: 0.46408582089552236
[2m[36m(func pid=178896)[0m f1_macro: 0.37914633354468413
[2m[36m(func pid=178896)[0m f1_weighted: 0.48429495437194775
[2m[36m(func pid=178896)[0m f1_per_class: [0.638, 0.567, 0.263, 0.543, 0.092, 0.093, 0.588, 0.362, 0.249, 0.396]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 4.0426 | Steps: 4 | Val loss: 11.6146 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0116 | Steps: 4 | Val loss: 1.8675 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.9400 | Steps: 4 | Val loss: 2.3636 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0741 | Steps: 4 | Val loss: 1.5083 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=183948)[0m top1: 0.11333955223880597
[2m[36m(func pid=183948)[0m top5: 0.7406716417910447
[2m[36m(func pid=183948)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=183948)[0m f1_macro: 0.1126573226328196
[2m[36m(func pid=183948)[0m f1_weighted: 0.14378677352397837
[2m[36m(func pid=183948)[0m f1_per_class: [0.087, 0.271, 0.0, 0.213, 0.184, 0.094, 0.064, 0.0, 0.102, 0.112]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.32369402985074625
[2m[36m(func pid=177931)[0m top5: 0.8572761194029851
[2m[36m(func pid=177931)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=177931)[0m f1_macro: 0.29232093384720187
[2m[36m(func pid=177931)[0m f1_weighted: 0.3289018067961927
[2m[36m(func pid=177931)[0m f1_per_class: [0.391, 0.413, 0.5, 0.473, 0.091, 0.13, 0.244, 0.326, 0.178, 0.178]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:27:44 (running for 00:44:21.78)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  1.012 |      0.292 |                   63 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.121 |      0.379 |                   60 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  4.043 |      0.113 |                   39 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.94  |      0.07  |                   39 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.14225746268656717
[2m[36m(func pid=184567)[0m top5: 0.6847014925373134
[2m[36m(func pid=184567)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=184567)[0m f1_macro: 0.069944006282222
[2m[36m(func pid=184567)[0m f1_weighted: 0.052576657580682064
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.215, 0.0, 0.485, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m top1: 0.47154850746268656
[2m[36m(func pid=178896)[0m top5: 0.9309701492537313
[2m[36m(func pid=178896)[0m f1_micro: 0.47154850746268656
[2m[36m(func pid=178896)[0m f1_macro: 0.3947680197483056
[2m[36m(func pid=178896)[0m f1_weighted: 0.48860287283896114
[2m[36m(func pid=178896)[0m f1_per_class: [0.652, 0.569, 0.338, 0.571, 0.093, 0.08, 0.564, 0.423, 0.266, 0.391]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8747 | Steps: 4 | Val loss: 1.8721 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1198 | Steps: 4 | Val loss: 12.7984 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3530 | Steps: 4 | Val loss: 2.3109 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0336 | Steps: 4 | Val loss: 1.5325 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=183948)[0m top1: 0.10867537313432836
[2m[36m(func pid=183948)[0m top5: 0.5088619402985075
[2m[36m(func pid=183948)[0m f1_micro: 0.10867537313432836
[2m[36m(func pid=183948)[0m f1_macro: 0.08787723130586814
[2m[36m(func pid=183948)[0m f1_weighted: 0.1350243135026605
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.396, 0.003, 0.193, 0.094, 0.081, 0.0, 0.0, 0.084, 0.028]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3269589552238806
[2m[36m(func pid=177931)[0m top5: 0.8493470149253731
[2m[36m(func pid=177931)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=177931)[0m f1_macro: 0.29265684661478636
[2m[36m(func pid=177931)[0m f1_weighted: 0.3317665548144126
[2m[36m(func pid=177931)[0m f1_per_class: [0.409, 0.451, 0.449, 0.465, 0.09, 0.127, 0.237, 0.333, 0.179, 0.186]
[2m[36m(func pid=177931)[0m 
== Status ==
Current time: 2024-01-07 14:27:50 (running for 00:44:27.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.875 |      0.293 |                   64 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.074 |      0.395 |                   61 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.12  |      0.088 |                   40 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.353 |      0.065 |                   40 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.13619402985074627
[2m[36m(func pid=184567)[0m top5: 0.5321828358208955
[2m[36m(func pid=184567)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=184567)[0m f1_macro: 0.06491852674146611
[2m[36m(func pid=184567)[0m f1_weighted: 0.049557085129007465
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.213, 0.0, 0.436, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m top1: 0.4664179104477612
[2m[36m(func pid=178896)[0m top5: 0.9309701492537313
[2m[36m(func pid=178896)[0m f1_micro: 0.4664179104477612
[2m[36m(func pid=178896)[0m f1_macro: 0.3918078204577365
[2m[36m(func pid=178896)[0m f1_weighted: 0.4852788038121829
[2m[36m(func pid=178896)[0m f1_per_class: [0.645, 0.578, 0.342, 0.582, 0.086, 0.074, 0.541, 0.42, 0.27, 0.379]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 6.6348 | Steps: 4 | Val loss: 5.0892 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7421 | Steps: 4 | Val loss: 1.8467 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.0334 | Steps: 4 | Val loss: 2.3327 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0325 | Steps: 4 | Val loss: 1.5679 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=183948)[0m top1: 0.259794776119403
[2m[36m(func pid=183948)[0m top5: 0.7271455223880597
[2m[36m(func pid=183948)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=183948)[0m f1_macro: 0.14828929481949046
[2m[36m(func pid=183948)[0m f1_weighted: 0.22060021704751262
[2m[36m(func pid=183948)[0m f1_per_class: [0.08, 0.362, 0.023, 0.448, 0.0, 0.0, 0.0, 0.508, 0.062, 0.0]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:27:55 (running for 00:44:32.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.742 |      0.293 |                   65 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.034 |      0.392 |                   62 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  6.635 |      0.148 |                   41 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.353 |      0.065 |                   40 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=177931)[0m top1: 0.333955223880597
[2m[36m(func pid=177931)[0m top5: 0.8642723880597015
[2m[36m(func pid=177931)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=177931)[0m f1_macro: 0.29284104465846894
[2m[36m(func pid=177931)[0m f1_weighted: 0.3387818130565946
[2m[36m(func pid=177931)[0m f1_per_class: [0.435, 0.465, 0.423, 0.468, 0.081, 0.107, 0.257, 0.325, 0.192, 0.175]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=184567)[0m top1: 0.1259328358208955
[2m[36m(func pid=184567)[0m top5: 0.539179104477612
[2m[36m(func pid=184567)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=184567)[0m f1_macro: 0.0967944270544061
[2m[36m(func pid=184567)[0m f1_weighted: 0.07095461982915993
[2m[36m(func pid=184567)[0m f1_per_class: [0.074, 0.0, 0.0, 0.0, 0.0, 0.295, 0.012, 0.509, 0.077, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=178896)[0m top1: 0.45475746268656714
[2m[36m(func pid=178896)[0m top5: 0.9328358208955224
[2m[36m(func pid=178896)[0m f1_micro: 0.45475746268656714
[2m[36m(func pid=178896)[0m f1_macro: 0.3914542301750986
[2m[36m(func pid=178896)[0m f1_weighted: 0.47806036628215626
[2m[36m(func pid=178896)[0m f1_per_class: [0.681, 0.575, 0.351, 0.584, 0.083, 0.085, 0.51, 0.432, 0.264, 0.349]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.8475 | Steps: 4 | Val loss: 7.6875 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.8237 | Steps: 4 | Val loss: 1.8207 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.1550 | Steps: 4 | Val loss: 2.4261 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1466 | Steps: 4 | Val loss: 1.5630 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:28:00 (running for 00:44:37.28)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.742 |      0.293 |                   65 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.032 |      0.391 |                   63 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  5.848 |      0.078 |                   42 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.033 |      0.097 |                   41 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m top1: 0.2271455223880597
[2m[36m(func pid=183948)[0m top5: 0.8661380597014925
[2m[36m(func pid=183948)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=183948)[0m f1_macro: 0.07791166786588402
[2m[36m(func pid=183948)[0m f1_weighted: 0.10887290927726645
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.411, 0.0, 0.006, 0.0, 0.0, 0.067, 0.267, 0.027, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.11380597014925373
[2m[36m(func pid=184567)[0m top5: 0.6548507462686567
[2m[36m(func pid=184567)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=184567)[0m f1_macro: 0.09312889318196181
[2m[36m(func pid=184567)[0m f1_weighted: 0.1276697290587852
[2m[36m(func pid=184567)[0m f1_per_class: [0.053, 0.0, 0.0, 0.0, 0.0, 0.2, 0.269, 0.41, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3451492537313433
[2m[36m(func pid=177931)[0m top5: 0.8708022388059702
[2m[36m(func pid=177931)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=177931)[0m f1_macro: 0.3111980576716666
[2m[36m(func pid=177931)[0m f1_weighted: 0.3493594637296762
[2m[36m(func pid=177931)[0m f1_per_class: [0.485, 0.447, 0.55, 0.495, 0.09, 0.11, 0.277, 0.309, 0.162, 0.187]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.4528917910447761
[2m[36m(func pid=178896)[0m top5: 0.929570895522388
[2m[36m(func pid=178896)[0m f1_micro: 0.4528917910447761
[2m[36m(func pid=178896)[0m f1_macro: 0.38978444211543006
[2m[36m(func pid=178896)[0m f1_weighted: 0.4770396134345061
[2m[36m(func pid=178896)[0m f1_per_class: [0.689, 0.551, 0.347, 0.607, 0.078, 0.089, 0.5, 0.422, 0.255, 0.361]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5813 | Steps: 4 | Val loss: 7.9885 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6637 | Steps: 4 | Val loss: 2.4780 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7340 | Steps: 4 | Val loss: 1.8049 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1125 | Steps: 4 | Val loss: 1.4875 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=183948)[0m top1: 0.19309701492537312
[2m[36m(func pid=183948)[0m top5: 0.8726679104477612
[2m[36m(func pid=183948)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=183948)[0m f1_macro: 0.09807457930357315
[2m[36m(func pid=183948)[0m f1_weighted: 0.11743385105745023
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.514, 0.075, 0.003, 0.0, 0.016, 0.031, 0.211, 0.13, 0.0]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:28:05 (running for 00:44:42.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.824 |      0.311 |                   66 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.147 |      0.39  |                   64 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.581 |      0.098 |                   43 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.155 |      0.093 |                   42 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.23880597014925373
[2m[36m(func pid=184567)[0m top5: 0.3694029850746269
[2m[36m(func pid=184567)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=184567)[0m f1_macro: 0.06825117324469462
[2m[36m(func pid=184567)[0m f1_weighted: 0.15618013239323736
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.345, 0.0, 0.0, 0.0, 0.0, 0.322, 0.016, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3591417910447761
[2m[36m(func pid=177931)[0m top5: 0.8805970149253731
[2m[36m(func pid=177931)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=177931)[0m f1_macro: 0.3164344913639968
[2m[36m(func pid=177931)[0m f1_weighted: 0.3654639552431601
[2m[36m(func pid=177931)[0m f1_per_class: [0.427, 0.462, 0.579, 0.489, 0.092, 0.108, 0.329, 0.315, 0.176, 0.187]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.4762126865671642
[2m[36m(func pid=178896)[0m top5: 0.9398320895522388
[2m[36m(func pid=178896)[0m f1_micro: 0.4762126865671642
[2m[36m(func pid=178896)[0m f1_macro: 0.40155316739499874
[2m[36m(func pid=178896)[0m f1_weighted: 0.502147449467043
[2m[36m(func pid=178896)[0m f1_per_class: [0.626, 0.559, 0.414, 0.586, 0.089, 0.151, 0.578, 0.412, 0.289, 0.311]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.2856 | Steps: 4 | Val loss: 2.3278 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.2657 | Steps: 4 | Val loss: 7.5547 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7609 | Steps: 4 | Val loss: 1.7655 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0494 | Steps: 4 | Val loss: 1.4767 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=184567)[0m top1: 0.24067164179104478
[2m[36m(func pid=184567)[0m top5: 0.3628731343283582
[2m[36m(func pid=184567)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=184567)[0m f1_macro: 0.08522917335668759
[2m[36m(func pid=184567)[0m f1_weighted: 0.16189085607577353
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.346, 0.0, 0.0, 0.0, 0.0, 0.306, 0.187, 0.0, 0.013]
== Status ==
Current time: 2024-01-07 14:28:11 (running for 00:44:48.17)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.734 |      0.316 |                   67 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.112 |      0.402 |                   65 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.581 |      0.098 |                   43 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.286 |      0.085 |                   44 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m top1: 0.15625
[2m[36m(func pid=183948)[0m top5: 0.7392723880597015
[2m[36m(func pid=183948)[0m f1_micro: 0.15625
[2m[36m(func pid=183948)[0m f1_macro: 0.11702390615456418
[2m[36m(func pid=183948)[0m f1_weighted: 0.11701060763443878
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.386, 0.037, 0.003, 0.0, 0.008, 0.103, 0.192, 0.072, 0.369]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3810634328358209
[2m[36m(func pid=177931)[0m top5: 0.8992537313432836
[2m[36m(func pid=177931)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=177931)[0m f1_macro: 0.31790456228896957
[2m[36m(func pid=177931)[0m f1_weighted: 0.3923354889470562
[2m[36m(func pid=177931)[0m f1_per_class: [0.411, 0.44, 0.537, 0.513, 0.101, 0.091, 0.421, 0.294, 0.187, 0.186]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.47154850746268656
[2m[36m(func pid=178896)[0m top5: 0.9416977611940298
[2m[36m(func pid=178896)[0m f1_micro: 0.47154850746268656
[2m[36m(func pid=178896)[0m f1_macro: 0.39918833322909714
[2m[36m(func pid=178896)[0m f1_weighted: 0.4989798552441476
[2m[36m(func pid=178896)[0m f1_per_class: [0.66, 0.56, 0.329, 0.597, 0.092, 0.185, 0.542, 0.401, 0.299, 0.326]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6401 | Steps: 4 | Val loss: 2.6869 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 4.1434 | Steps: 4 | Val loss: 6.6879 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7164 | Steps: 4 | Val loss: 1.7722 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1446 | Steps: 4 | Val loss: 1.4599 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:28:16 (running for 00:44:53.49)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.761 |      0.318 |                   68 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.049 |      0.399 |                   66 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.266 |      0.117 |                   44 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.64  |      0.086 |                   45 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.19962686567164178
[2m[36m(func pid=184567)[0m top5: 0.3530783582089552
[2m[36m(func pid=184567)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=184567)[0m f1_macro: 0.0864606120547546
[2m[36m(func pid=184567)[0m f1_weighted: 0.11969011095044269
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.38, 0.0, 0.0, 0.0, 0.0, 0.112, 0.358, 0.0, 0.014]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.19496268656716417
[2m[36m(func pid=183948)[0m top5: 0.6497201492537313
[2m[36m(func pid=183948)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=183948)[0m f1_macro: 0.14627343072513008
[2m[36m(func pid=183948)[0m f1_weighted: 0.17586051209783482
[2m[36m(func pid=183948)[0m f1_per_class: [0.405, 0.432, 0.053, 0.013, 0.022, 0.0, 0.246, 0.245, 0.046, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.376865671641791
[2m[36m(func pid=177931)[0m top5: 0.8964552238805971
[2m[36m(func pid=177931)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=177931)[0m f1_macro: 0.3115811343778795
[2m[36m(func pid=177931)[0m f1_weighted: 0.39561602655907563
[2m[36m(func pid=177931)[0m f1_per_class: [0.434, 0.458, 0.478, 0.497, 0.098, 0.098, 0.436, 0.29, 0.177, 0.149]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.46222014925373134
[2m[36m(func pid=178896)[0m top5: 0.9440298507462687
[2m[36m(func pid=178896)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=178896)[0m f1_macro: 0.398036491427391
[2m[36m(func pid=178896)[0m f1_weighted: 0.4870361483531004
[2m[36m(func pid=178896)[0m f1_per_class: [0.66, 0.538, 0.338, 0.583, 0.12, 0.216, 0.52, 0.385, 0.288, 0.333]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3895 | Steps: 4 | Val loss: 2.8410 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.3541 | Steps: 4 | Val loss: 6.4743 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.8511 | Steps: 4 | Val loss: 1.7352 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0238 | Steps: 4 | Val loss: 1.5003 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:28:21 (running for 00:44:58.80)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.716 |      0.312 |                   69 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.145 |      0.398 |                   67 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  4.143 |      0.146 |                   45 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.39  |      0.06  |                   46 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.058768656716417914
[2m[36m(func pid=184567)[0m top5: 0.5307835820895522
[2m[36m(func pid=184567)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=184567)[0m f1_macro: 0.060116627701379974
[2m[36m(func pid=184567)[0m f1_weighted: 0.07115799226166543
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.156, 0.422, 0.0, 0.023]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.291044776119403
[2m[36m(func pid=183948)[0m top5: 0.6571828358208955
[2m[36m(func pid=183948)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=183948)[0m f1_macro: 0.11441896318999464
[2m[36m(func pid=183948)[0m f1_weighted: 0.2170334749953655
[2m[36m(func pid=183948)[0m f1_per_class: [0.146, 0.053, 0.113, 0.095, 0.019, 0.0, 0.58, 0.062, 0.02, 0.056]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3941231343283582
[2m[36m(func pid=177931)[0m top5: 0.9085820895522388
[2m[36m(func pid=177931)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=177931)[0m f1_macro: 0.3298298213979164
[2m[36m(func pid=177931)[0m f1_weighted: 0.4115078401776846
[2m[36m(func pid=177931)[0m f1_per_class: [0.459, 0.472, 0.5, 0.502, 0.114, 0.138, 0.455, 0.298, 0.189, 0.171]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.45009328358208955
[2m[36m(func pid=178896)[0m top5: 0.9370335820895522
[2m[36m(func pid=178896)[0m f1_micro: 0.45009328358208955
[2m[36m(func pid=178896)[0m f1_macro: 0.3800118942367382
[2m[36m(func pid=178896)[0m f1_weighted: 0.4801040831071667
[2m[36m(func pid=178896)[0m f1_per_class: [0.569, 0.534, 0.283, 0.566, 0.121, 0.263, 0.508, 0.374, 0.293, 0.29]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.6878 | Steps: 4 | Val loss: 3.9413 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.2329 | Steps: 4 | Val loss: 5.6944 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.8698 | Steps: 4 | Val loss: 1.7433 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0309 | Steps: 4 | Val loss: 1.5401 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:28:27 (running for 00:45:04.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.851 |      0.33  |                   70 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.024 |      0.38  |                   68 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.354 |      0.114 |                   46 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.688 |      0.111 |                   47 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.25513059701492535
[2m[36m(func pid=184567)[0m top5: 0.48134328358208955
[2m[36m(func pid=184567)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=184567)[0m f1_macro: 0.11112352960946428
[2m[36m(func pid=184567)[0m f1_weighted: 0.16783946936082345
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.352, 0.0, 0.0, 0.0, 0.0, 0.268, 0.474, 0.0, 0.019]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.11054104477611941
[2m[36m(func pid=183948)[0m top5: 0.7094216417910447
[2m[36m(func pid=183948)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=183948)[0m f1_macro: 0.0903387359206237
[2m[36m(func pid=183948)[0m f1_weighted: 0.10191267196225437
[2m[36m(func pid=183948)[0m f1_per_class: [0.266, 0.047, 0.004, 0.154, 0.045, 0.044, 0.09, 0.196, 0.058, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.39132462686567165
[2m[36m(func pid=177931)[0m top5: 0.909981343283582
[2m[36m(func pid=177931)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=177931)[0m f1_macro: 0.3226776871822454
[2m[36m(func pid=177931)[0m f1_weighted: 0.4077643843977992
[2m[36m(func pid=177931)[0m f1_per_class: [0.446, 0.468, 0.468, 0.488, 0.138, 0.146, 0.462, 0.267, 0.184, 0.157]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.43656716417910446
[2m[36m(func pid=178896)[0m top5: 0.9281716417910447
[2m[36m(func pid=178896)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=178896)[0m f1_macro: 0.3743450597660267
[2m[36m(func pid=178896)[0m f1_weighted: 0.464267174163727
[2m[36m(func pid=178896)[0m f1_per_class: [0.562, 0.541, 0.31, 0.563, 0.117, 0.232, 0.464, 0.389, 0.284, 0.283]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.9967 | Steps: 4 | Val loss: 2.7313 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.7301 | Steps: 4 | Val loss: 6.2598 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6279 | Steps: 4 | Val loss: 1.7848 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0294 | Steps: 4 | Val loss: 1.5406 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:28:32 (running for 00:45:09.55)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.87  |      0.323 |                   71 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.031 |      0.374 |                   69 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.233 |      0.09  |                   47 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.997 |      0.112 |                   48 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.26119402985074625
[2m[36m(func pid=184567)[0m top5: 0.7061567164179104
[2m[36m(func pid=184567)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=184567)[0m f1_macro: 0.11238663326009454
[2m[36m(func pid=184567)[0m f1_weighted: 0.18282206766196307
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.339, 0.0, 0.0, 0.0, 0.03, 0.322, 0.432, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.14132462686567165
[2m[36m(func pid=183948)[0m top5: 0.6142723880597015
[2m[36m(func pid=183948)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=183948)[0m f1_macro: 0.12221654487522486
[2m[36m(func pid=183948)[0m f1_weighted: 0.13340374945483516
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.0, 0.049, 0.007, 0.053, 0.322, 0.24, 0.292, 0.116, 0.143]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.37033582089552236
[2m[36m(func pid=177931)[0m top5: 0.898320895522388
[2m[36m(func pid=177931)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=177931)[0m f1_macro: 0.30974367957557664
[2m[36m(func pid=177931)[0m f1_weighted: 0.39225551094107286
[2m[36m(func pid=177931)[0m f1_per_class: [0.42, 0.458, 0.444, 0.468, 0.11, 0.174, 0.427, 0.271, 0.186, 0.138]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.43330223880597013
[2m[36m(func pid=178896)[0m top5: 0.9314365671641791
[2m[36m(func pid=178896)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=178896)[0m f1_macro: 0.36427629128363287
[2m[36m(func pid=178896)[0m f1_weighted: 0.4539732557528173
[2m[36m(func pid=178896)[0m f1_per_class: [0.461, 0.541, 0.31, 0.558, 0.135, 0.225, 0.441, 0.396, 0.28, 0.296]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.8223 | Steps: 4 | Val loss: 2.8195 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.9879 | Steps: 4 | Val loss: 12.3876 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9480 | Steps: 4 | Val loss: 1.8119 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1271 | Steps: 4 | Val loss: 1.5894 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:28:37 (running for 00:45:14.86)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.628 |      0.31  |                   72 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.029 |      0.364 |                   70 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.73  |      0.122 |                   48 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.822 |      0.075 |                   49 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.24673507462686567
[2m[36m(func pid=184567)[0m top5: 0.6791044776119403
[2m[36m(func pid=184567)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=184567)[0m f1_macro: 0.07468029737561158
[2m[36m(func pid=184567)[0m f1_weighted: 0.16821801597621025
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.339, 0.0, 0.0, 0.0, 0.064, 0.344, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.028917910447761194
[2m[36m(func pid=183948)[0m top5: 0.5125932835820896
[2m[36m(func pid=183948)[0m f1_micro: 0.028917910447761194
[2m[36m(func pid=183948)[0m f1_macro: 0.0325679198221591
[2m[36m(func pid=183948)[0m f1_weighted: 0.02489395301305438
[2m[36m(func pid=183948)[0m f1_per_class: [0.04, 0.117, 0.036, 0.0, 0.025, 0.0, 0.0, 0.0, 0.108, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=177931)[0m top1: 0.35027985074626866
[2m[36m(func pid=177931)[0m top5: 0.8922574626865671
[2m[36m(func pid=177931)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=177931)[0m f1_macro: 0.29475988385255836
[2m[36m(func pid=177931)[0m f1_weighted: 0.37336433858155627
[2m[36m(func pid=177931)[0m f1_per_class: [0.38, 0.427, 0.414, 0.466, 0.103, 0.166, 0.391, 0.266, 0.192, 0.143]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=178896)[0m top1: 0.41324626865671643
[2m[36m(func pid=178896)[0m top5: 0.9277052238805971
[2m[36m(func pid=178896)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=178896)[0m f1_macro: 0.3596186695964976
[2m[36m(func pid=178896)[0m f1_weighted: 0.4296417837574039
[2m[36m(func pid=178896)[0m f1_per_class: [0.454, 0.539, 0.31, 0.555, 0.123, 0.243, 0.354, 0.39, 0.31, 0.319]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.8129 | Steps: 4 | Val loss: 2.8035 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.9279 | Steps: 4 | Val loss: 8.2559 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3237 | Steps: 4 | Val loss: 1.7222 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6607 | Steps: 4 | Val loss: 1.8033 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 14:28:43 (running for 00:45:20.14)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.948 |      0.295 |                   73 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.127 |      0.36  |                   71 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.988 |      0.033 |                   49 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.813 |      0.049 |                   50 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.10261194029850747
[2m[36m(func pid=184567)[0m top5: 0.6375932835820896
[2m[36m(func pid=184567)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=184567)[0m f1_macro: 0.04915021994296832
[2m[36m(func pid=184567)[0m f1_weighted: 0.12829932058660432
[2m[36m(func pid=184567)[0m f1_per_class: [0.011, 0.008, 0.018, 0.0, 0.0, 0.05, 0.406, 0.0, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.12686567164179105
[2m[36m(func pid=183948)[0m top5: 0.49113805970149255
[2m[36m(func pid=183948)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=183948)[0m f1_macro: 0.09545936820798699
[2m[36m(func pid=183948)[0m f1_weighted: 0.09097141820017426
[2m[36m(func pid=183948)[0m f1_per_class: [0.069, 0.375, 0.044, 0.0, 0.0, 0.0, 0.0, 0.378, 0.088, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m top1: 0.3829291044776119
[2m[36m(func pid=178896)[0m top5: 0.9225746268656716
[2m[36m(func pid=178896)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=178896)[0m f1_macro: 0.33579564779765625
[2m[36m(func pid=178896)[0m f1_weighted: 0.3954533661644803
[2m[36m(func pid=178896)[0m f1_per_class: [0.365, 0.504, 0.371, 0.564, 0.149, 0.229, 0.28, 0.32, 0.265, 0.31]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m top1: 0.3558768656716418
[2m[36m(func pid=177931)[0m top5: 0.8941231343283582
[2m[36m(func pid=177931)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=177931)[0m f1_macro: 0.29012328376460317
[2m[36m(func pid=177931)[0m f1_weighted: 0.3794565667800839
[2m[36m(func pid=177931)[0m f1_per_class: [0.337, 0.429, 0.369, 0.466, 0.118, 0.187, 0.409, 0.25, 0.187, 0.149]
[2m[36m(func pid=177931)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.1698 | Steps: 4 | Val loss: 2.1657 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3193 | Steps: 4 | Val loss: 5.1743 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0436 | Steps: 4 | Val loss: 1.7644 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=177931)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9942 | Steps: 4 | Val loss: 1.8205 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=184567)[0m top1: 0.11473880597014925
[2m[36m(func pid=184567)[0m top5: 0.5223880597014925
[2m[36m(func pid=184567)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=184567)[0m f1_macro: 0.08548419241311553
[2m[36m(func pid=184567)[0m f1_weighted: 0.13233167985398464
[2m[36m(func pid=184567)[0m f1_per_class: [0.05, 0.0, 0.0, 0.0, 0.0, 0.047, 0.342, 0.416, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:28:48 (running for 00:45:25.67)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00020 | RUNNING    | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.661 |      0.29  |                   74 |
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.324 |      0.336 |                   72 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.928 |      0.095 |                   50 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  3.17  |      0.085 |                   51 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m top1: 0.15345149253731344
[2m[36m(func pid=183948)[0m top5: 0.6044776119402985
[2m[36m(func pid=183948)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=183948)[0m f1_macro: 0.148499578028751
[2m[36m(func pid=183948)[0m f1_weighted: 0.12718883284556223
[2m[36m(func pid=183948)[0m f1_per_class: [0.132, 0.409, 0.058, 0.0, 0.021, 0.0, 0.062, 0.481, 0.164, 0.158]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m top1: 0.376865671641791
[2m[36m(func pid=178896)[0m top5: 0.9235074626865671
[2m[36m(func pid=178896)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=178896)[0m f1_macro: 0.3297882508577559
[2m[36m(func pid=178896)[0m f1_weighted: 0.38130276405058366
[2m[36m(func pid=178896)[0m f1_per_class: [0.385, 0.569, 0.361, 0.539, 0.151, 0.182, 0.242, 0.282, 0.264, 0.321]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=177931)[0m top1: 0.35027985074626866
[2m[36m(func pid=177931)[0m top5: 0.8903917910447762
[2m[36m(func pid=177931)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=177931)[0m f1_macro: 0.2834119954298427
[2m[36m(func pid=177931)[0m f1_weighted: 0.3781858852959741
[2m[36m(func pid=177931)[0m f1_per_class: [0.319, 0.413, 0.329, 0.46, 0.119, 0.188, 0.419, 0.265, 0.188, 0.134]
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4770 | Steps: 4 | Val loss: 2.3322 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 4.0765 | Steps: 4 | Val loss: 4.0471 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0507 | Steps: 4 | Val loss: 1.7874 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:28:54 (running for 00:45:31.01)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.336
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.044 |      0.33  |                   73 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.319 |      0.148 |                   51 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.477 |      0.073 |                   52 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.07462686567164178
[2m[36m(func pid=184567)[0m top5: 0.5736940298507462
[2m[36m(func pid=184567)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=184567)[0m f1_macro: 0.07293439401795751
[2m[36m(func pid=184567)[0m f1_weighted: 0.06435442567314097
[2m[36m(func pid=184567)[0m f1_per_class: [0.049, 0.0, 0.0, 0.0, 0.0, 0.11, 0.074, 0.496, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.24486940298507462
[2m[36m(func pid=183948)[0m top5: 0.5909514925373134
[2m[36m(func pid=183948)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=183948)[0m f1_macro: 0.1986490091181246
[2m[36m(func pid=183948)[0m f1_weighted: 0.21312042128011463
[2m[36m(func pid=183948)[0m f1_per_class: [0.397, 0.46, 0.253, 0.0, 0.047, 0.322, 0.273, 0.0, 0.138, 0.097]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m top1: 0.37220149253731344
[2m[36m(func pid=178896)[0m top5: 0.9225746268656716
[2m[36m(func pid=178896)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=178896)[0m f1_macro: 0.3289301960324332
[2m[36m(func pid=178896)[0m f1_weighted: 0.3703294379658436
[2m[36m(func pid=178896)[0m f1_per_class: [0.383, 0.58, 0.342, 0.534, 0.214, 0.201, 0.199, 0.267, 0.254, 0.314]
[2m[36m(func pid=178896)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7756 | Steps: 4 | Val loss: 2.4502 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3874 | Steps: 4 | Val loss: 2.6688 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=178896)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0298 | Steps: 4 | Val loss: 1.8128 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=184567)[0m top1: 0.0830223880597015
[2m[36m(func pid=184567)[0m top5: 0.5834888059701493
[2m[36m(func pid=184567)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=184567)[0m f1_macro: 0.07305362286673459
[2m[36m(func pid=184567)[0m f1_weighted: 0.04829514439326277
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.141, 0.0, 0.513, 0.077, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:28:59 (running for 00:45:36.58)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.336
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00021 | RUNNING    | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.051 |      0.329 |                   74 |
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  4.077 |      0.199 |                   52 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.776 |      0.073 |                   53 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m top1: 0.2943097014925373
[2m[36m(func pid=183948)[0m top5: 0.8913246268656716
[2m[36m(func pid=183948)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=183948)[0m f1_macro: 0.20099841865487628
[2m[36m(func pid=183948)[0m f1_weighted: 0.31366976881882513
[2m[36m(func pid=183948)[0m f1_per_class: [0.478, 0.466, 0.0, 0.207, 0.059, 0.04, 0.52, 0.0, 0.145, 0.095]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=178896)[0m top1: 0.3656716417910448
[2m[36m(func pid=178896)[0m top5: 0.9043843283582089
[2m[36m(func pid=178896)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=178896)[0m f1_macro: 0.33003788291966457
[2m[36m(func pid=178896)[0m f1_weighted: 0.35691001379850795
[2m[36m(func pid=178896)[0m f1_per_class: [0.444, 0.586, 0.28, 0.487, 0.198, 0.198, 0.184, 0.267, 0.346, 0.31]
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.7795 | Steps: 4 | Val loss: 2.3974 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.7286 | Steps: 4 | Val loss: 3.9390 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 14:29:04 (running for 00:45:41.69)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.387 |      0.201 |                   53 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.779 |      0.071 |                   54 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.08582089552238806
[2m[36m(func pid=184567)[0m top5: 0.5778917910447762
[2m[36m(func pid=184567)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=184567)[0m f1_macro: 0.071382806906774
[2m[36m(func pid=184567)[0m f1_weighted: 0.050686572432449034
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.199, 0.0, 0.439, 0.075, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.4300373134328358
[2m[36m(func pid=183948)[0m top5: 0.8736007462686567
[2m[36m(func pid=183948)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=183948)[0m f1_macro: 0.21148953457989478
[2m[36m(func pid=183948)[0m f1_weighted: 0.37262245495841634
[2m[36m(func pid=183948)[0m f1_per_class: [0.24, 0.032, 0.0, 0.57, 0.114, 0.016, 0.614, 0.21, 0.085, 0.235]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6510 | Steps: 4 | Val loss: 2.2181 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2966 | Steps: 4 | Val loss: 3.6662 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 14:29:10 (running for 00:45:47.07)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.729 |      0.211 |                   54 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.651 |      0.084 |                   55 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.16044776119402984
[2m[36m(func pid=184567)[0m top5: 0.6497201492537313
[2m[36m(func pid=184567)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=184567)[0m f1_macro: 0.0843302889312523
[2m[36m(func pid=184567)[0m f1_weighted: 0.0911750681531394
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.221, 0.125, 0.498, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.38759328358208955
[2m[36m(func pid=183948)[0m top5: 0.8605410447761194
[2m[36m(func pid=183948)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=183948)[0m f1_macro: 0.2843412129131391
[2m[36m(func pid=183948)[0m f1_weighted: 0.3730682014699785
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.0, 0.571, 0.56, 0.294, 0.345, 0.5, 0.32, 0.061, 0.191]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5427 | Steps: 4 | Val loss: 2.0460 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.0891 | Steps: 4 | Val loss: 4.6154 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 14:29:15 (running for 00:45:52.39)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.297 |      0.284 |                   55 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.543 |      0.147 |                   56 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.40718283582089554
[2m[36m(func pid=184567)[0m top5: 0.6277985074626866
[2m[36m(func pid=184567)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=184567)[0m f1_macro: 0.1473350967781091
[2m[36m(func pid=184567)[0m f1_weighted: 0.3404436261958897
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.542, 0.0, 0.166, 0.532, 0.156, 0.077, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.3302238805970149
[2m[36m(func pid=183948)[0m top5: 0.7957089552238806
[2m[36m(func pid=183948)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=183948)[0m f1_macro: 0.2330215951821414
[2m[36m(func pid=183948)[0m f1_weighted: 0.3452346728982605
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.241, 0.154, 0.355, 0.246, 0.342, 0.485, 0.201, 0.133, 0.172]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.5471 | Steps: 4 | Val loss: 2.0762 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:29:20 (running for 00:45:57.72)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.089 |      0.233 |                   56 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.547 |      0.112 |                   57 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.40111940298507465
[2m[36m(func pid=184567)[0m top5: 0.5755597014925373
[2m[36m(func pid=184567)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=184567)[0m f1_macro: 0.11231756936545423
[2m[36m(func pid=184567)[0m f1_weighted: 0.30193566031405483
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.519, 0.0, 0.0, 0.517, 0.046, 0.0, 0.041]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.5213 | Steps: 4 | Val loss: 3.5647 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=183948)[0m top1: 0.24860074626865672
[2m[36m(func pid=183948)[0m top5: 0.7835820895522388
[2m[36m(func pid=183948)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=183948)[0m f1_macro: 0.17976429307372505
[2m[36m(func pid=183948)[0m f1_weighted: 0.25926175958618775
[2m[36m(func pid=183948)[0m f1_per_class: [0.043, 0.211, 0.0, 0.323, 0.138, 0.244, 0.265, 0.297, 0.171, 0.106]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.4847 | Steps: 4 | Val loss: 2.1016 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 14:29:26 (running for 00:46:03.30)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.521 |      0.18  |                   57 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.485 |      0.145 |                   58 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.38619402985074625
[2m[36m(func pid=184567)[0m top5: 0.5904850746268657
[2m[36m(func pid=184567)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=184567)[0m f1_macro: 0.1453709241053777
[2m[36m(func pid=184567)[0m f1_weighted: 0.3187599213742261
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.542, 0.024, 0.0, 0.486, 0.384, 0.0, 0.017]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.3147 | Steps: 4 | Val loss: 6.9574 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=183948)[0m top1: 0.12779850746268656
[2m[36m(func pid=183948)[0m top5: 0.6478544776119403
[2m[36m(func pid=183948)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=183948)[0m f1_macro: 0.08035125221039005
[2m[36m(func pid=183948)[0m f1_weighted: 0.09464702488917466
[2m[36m(func pid=183948)[0m f1_per_class: [0.175, 0.441, 0.0, 0.034, 0.0, 0.007, 0.003, 0.0, 0.112, 0.032]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.9498 | Steps: 4 | Val loss: 2.2370 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 14:29:31 (running for 00:46:08.87)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.315 |      0.08  |                   58 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.95  |      0.148 |                   59 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.35074626865671643
[2m[36m(func pid=184567)[0m top5: 0.6012126865671642
[2m[36m(func pid=184567)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=184567)[0m f1_macro: 0.14824223922735205
[2m[36m(func pid=184567)[0m f1_weighted: 0.28587006298326895
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.542, 0.0, 0.246, 0.278, 0.402, 0.015, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 5.9142 | Steps: 4 | Val loss: 47.3710 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=183948)[0m top1: 0.055970149253731345
[2m[36m(func pid=183948)[0m top5: 0.6226679104477612
[2m[36m(func pid=183948)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=183948)[0m f1_macro: 0.0337705981148691
[2m[36m(func pid=183948)[0m f1_weighted: 0.041917149738050385
[2m[36m(func pid=183948)[0m f1_per_class: [0.067, 0.217, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6807 | Steps: 4 | Val loss: 2.1600 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 14:29:37 (running for 00:46:14.38)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  5.914 |      0.034 |                   59 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.681 |      0.142 |                   60 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.3875 | Steps: 4 | Val loss: 11.9241 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=184567)[0m top1: 0.302705223880597
[2m[36m(func pid=184567)[0m top5: 0.6254664179104478
[2m[36m(func pid=184567)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=184567)[0m f1_macro: 0.14234381755741352
[2m[36m(func pid=184567)[0m f1_weighted: 0.23019018043576162
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.541, 0.0, 0.264, 0.062, 0.487, 0.069, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m top1: 0.016791044776119403
[2m[36m(func pid=183948)[0m top5: 0.574160447761194
[2m[36m(func pid=183948)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=183948)[0m f1_macro: 0.007642189411891843
[2m[36m(func pid=183948)[0m f1_weighted: 0.004139945954037419
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.01, 0.025, 0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.4683 | Steps: 4 | Val loss: 2.1421 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=184567)[0m top1: 0.2826492537313433
[2m[36m(func pid=184567)[0m top5: 0.648320895522388
[2m[36m(func pid=184567)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=184567)[0m f1_macro: 0.14201047714124104
[2m[36m(func pid=184567)[0m f1_weighted: 0.2123004771234192
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.0, 0.524, 0.0, 0.284, 0.0, 0.544, 0.068, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:29:42 (running for 00:46:19.77)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.387 |      0.008 |                   60 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.468 |      0.142 |                   61 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 4.0347 | Steps: 4 | Val loss: 73.3088 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=183948)[0m top1: 0.07555970149253731
[2m[36m(func pid=183948)[0m top5: 0.5760261194029851
[2m[36m(func pid=183948)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=183948)[0m f1_macro: 0.0518949685160374
[2m[36m(func pid=183948)[0m f1_weighted: 0.017906863210254555
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.0, 0.037, 0.01, 0.175, 0.0, 0.0, 0.22, 0.0, 0.077]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.5190 | Steps: 4 | Val loss: 2.2112 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 14:29:48 (running for 00:46:24.90)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  4.035 |      0.052 |                   61 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.519 |      0.089 |                   62 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.09561567164179105
[2m[36m(func pid=184567)[0m top5: 0.7574626865671642
[2m[36m(func pid=184567)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=184567)[0m f1_macro: 0.08931860947171331
[2m[36m(func pid=184567)[0m f1_weighted: 0.06422625706508488
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.0, 0.275, 0.0, 0.52, 0.076, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.2861 | Steps: 4 | Val loss: 66.2408 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=183948)[0m top1: 0.08955223880597014
[2m[36m(func pid=183948)[0m top5: 0.5419776119402985
[2m[36m(func pid=183948)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=183948)[0m f1_macro: 0.07434148163701935
[2m[36m(func pid=183948)[0m f1_weighted: 0.02780173759727883
[2m[36m(func pid=183948)[0m f1_per_class: [0.241, 0.016, 0.067, 0.013, 0.06, 0.0, 0.0, 0.237, 0.027, 0.082]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.7356 | Steps: 4 | Val loss: 2.4439 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:29:53 (running for 00:46:30.49)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.286 |      0.074 |                   62 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.736 |      0.08  |                   63 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.08069029850746269
[2m[36m(func pid=184567)[0m top5: 0.7565298507462687
[2m[36m(func pid=184567)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=184567)[0m f1_macro: 0.07976576006521736
[2m[36m(func pid=184567)[0m f1_weighted: 0.05494195522196193
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.02, 0.217, 0.003, 0.45, 0.087, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6603 | Steps: 4 | Val loss: 43.4992 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=183948)[0m top1: 0.2019589552238806
[2m[36m(func pid=183948)[0m top5: 0.6100746268656716
[2m[36m(func pid=183948)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=183948)[0m f1_macro: 0.16288956156913206
[2m[36m(func pid=183948)[0m f1_weighted: 0.15002120218326012
[2m[36m(func pid=183948)[0m f1_per_class: [0.224, 0.508, 0.081, 0.074, 0.218, 0.112, 0.012, 0.245, 0.133, 0.022]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.5403 | Steps: 4 | Val loss: 2.4147 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:29:59 (running for 00:46:36.07)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.66  |      0.163 |                   63 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.54  |      0.082 |                   64 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.09235074626865672
[2m[36m(func pid=184567)[0m top5: 0.7234141791044776
[2m[36m(func pid=184567)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=184567)[0m f1_macro: 0.08152002412264531
[2m[36m(func pid=184567)[0m f1_weighted: 0.11111565019633134
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.02, 0.0, 0.007, 0.0, 0.272, 0.516, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2668 | Steps: 4 | Val loss: 10.0131 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=183948)[0m top1: 0.22527985074626866
[2m[36m(func pid=183948)[0m top5: 0.6021455223880597
[2m[36m(func pid=183948)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=183948)[0m f1_macro: 0.1798622185081234
[2m[36m(func pid=183948)[0m f1_weighted: 0.169224430811437
[2m[36m(func pid=183948)[0m f1_per_class: [0.126, 0.51, 0.163, 0.042, 0.143, 0.333, 0.006, 0.386, 0.09, 0.0]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.9682 | Steps: 4 | Val loss: 2.2425 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 14:30:04 (running for 00:46:41.57)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.267 |      0.18  |                   64 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.968 |      0.062 |                   65 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.06716417910447761
[2m[36m(func pid=184567)[0m top5: 0.7238805970149254
[2m[36m(func pid=184567)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=184567)[0m f1_macro: 0.061681633564189084
[2m[36m(func pid=184567)[0m f1_weighted: 0.0904339270028211
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.022, 0.0, 0.013, 0.0, 0.235, 0.346, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2470 | Steps: 4 | Val loss: 4.5968 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.4489 | Steps: 4 | Val loss: 2.0271 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=183948)[0m top1: 0.19496268656716417
[2m[36m(func pid=183948)[0m top5: 0.6711753731343284
[2m[36m(func pid=183948)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=183948)[0m f1_macro: 0.20645798363719892
[2m[36m(func pid=183948)[0m f1_weighted: 0.17020110595393959
[2m[36m(func pid=183948)[0m f1_per_class: [0.286, 0.385, 0.024, 0.052, 0.198, 0.232, 0.076, 0.448, 0.129, 0.235]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:30:10 (running for 00:46:46.92)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.247 |      0.206 |                   65 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.449 |      0.09  |                   66 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.16277985074626866
[2m[36m(func pid=184567)[0m top5: 0.784981343283582
[2m[36m(func pid=184567)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=184567)[0m f1_macro: 0.08991049950417154
[2m[36m(func pid=184567)[0m f1_weighted: 0.17373402766242393
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.023, 0.0, 0.026, 0.0, 0.517, 0.333, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.7987 | Steps: 4 | Val loss: 4.6431 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.5601 | Steps: 4 | Val loss: 2.0047 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=183948)[0m top1: 0.15904850746268656
[2m[36m(func pid=183948)[0m top5: 0.7593283582089553
[2m[36m(func pid=183948)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=183948)[0m f1_macro: 0.1123757315149541
[2m[36m(func pid=183948)[0m f1_weighted: 0.14961288668801714
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.123, 0.024, 0.274, 0.078, 0.0, 0.096, 0.282, 0.154, 0.093]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.197294776119403
[2m[36m(func pid=184567)[0m top5: 0.7901119402985075
[2m[36m(func pid=184567)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=184567)[0m f1_macro: 0.0810992889012809
[2m[36m(func pid=184567)[0m f1_weighted: 0.18731937881029503
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.022, 0.0, 0.043, 0.173, 0.558, 0.016, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:30:15 (running for 00:46:52.52)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.799 |      0.112 |                   66 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.56  |      0.081 |                   67 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.8888 | Steps: 4 | Val loss: 6.1861 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.4405 | Steps: 4 | Val loss: 2.1617 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=183948)[0m top1: 0.24300373134328357
[2m[36m(func pid=183948)[0m top5: 0.8558768656716418
[2m[36m(func pid=183948)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=183948)[0m f1_macro: 0.12372413520922172
[2m[36m(func pid=183948)[0m f1_weighted: 0.21610042508713276
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.172, 0.0, 0.373, 0.065, 0.0, 0.233, 0.104, 0.141, 0.148]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:30:21 (running for 00:46:57.95)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.889 |      0.124 |                   67 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.441 |      0.082 |                   68 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.11567164179104478
[2m[36m(func pid=184567)[0m top5: 0.753731343283582
[2m[36m(func pid=184567)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=184567)[0m f1_macro: 0.08241250184727186
[2m[36m(func pid=184567)[0m f1_weighted: 0.06884508963113695
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.022, 0.0, 0.0, 0.317, 0.018, 0.466, 0.0, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.8769 | Steps: 4 | Val loss: 16.8218 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.5141 | Steps: 4 | Val loss: 2.2252 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183948)[0m top1: 0.34841417910447764
[2m[36m(func pid=183948)[0m top5: 0.8055037313432836
[2m[36m(func pid=183948)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=183948)[0m f1_macro: 0.14547203448945614
[2m[36m(func pid=183948)[0m f1_weighted: 0.3223657654989404
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.216, 0.06, 0.348, 0.08, 0.0, 0.619, 0.0, 0.063, 0.069]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.10541044776119403
[2m[36m(func pid=184567)[0m top5: 0.7639925373134329
[2m[36m(func pid=184567)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=184567)[0m f1_macro: 0.08586570639173642
[2m[36m(func pid=184567)[0m f1_weighted: 0.06704544122904228
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.023, 0.0, 0.0, 0.301, 0.009, 0.511, 0.0, 0.014]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:30:26 (running for 00:47:03.21)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.877 |      0.145 |                   68 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.514 |      0.086 |                   69 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0018 | Steps: 4 | Val loss: 11.7892 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.5640 | Steps: 4 | Val loss: 2.1967 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=183948)[0m top1: 0.3069029850746269
[2m[36m(func pid=183948)[0m top5: 0.7658582089552238
[2m[36m(func pid=183948)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=183948)[0m f1_macro: 0.13597430056637
[2m[36m(func pid=183948)[0m f1_weighted: 0.2497773340267444
[2m[36m(func pid=183948)[0m f1_per_class: [0.0, 0.265, 0.044, 0.056, 0.072, 0.016, 0.6, 0.016, 0.144, 0.148]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:30:31 (running for 00:47:08.66)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.002 |      0.136 |                   69 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.564 |      0.085 |                   70 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.08675373134328358
[2m[36m(func pid=184567)[0m top5: 0.773320895522388
[2m[36m(func pid=184567)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=184567)[0m f1_macro: 0.08513227217597925
[2m[36m(func pid=184567)[0m f1_weighted: 0.06555624575966357
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.026, 0.0, 0.0, 0.277, 0.012, 0.515, 0.0, 0.02]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.0407 | Steps: 4 | Val loss: 10.2496 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.7588 | Steps: 4 | Val loss: 2.1140 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=183948)[0m top1: 0.3148320895522388
[2m[36m(func pid=183948)[0m top5: 0.777518656716418
[2m[36m(func pid=183948)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=183948)[0m f1_macro: 0.14810578450175574
[2m[36m(func pid=183948)[0m f1_weighted: 0.2452438233213061
[2m[36m(func pid=183948)[0m f1_per_class: [0.09, 0.257, 0.067, 0.003, 0.055, 0.105, 0.604, 0.0, 0.085, 0.214]
[2m[36m(func pid=183948)[0m 
[2m[36m(func pid=184567)[0m top1: 0.12546641791044777
[2m[36m(func pid=184567)[0m top5: 0.7583955223880597
[2m[36m(func pid=184567)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=184567)[0m f1_macro: 0.11052585486326434
[2m[36m(func pid=184567)[0m f1_weighted: 0.1513328826736271
[2m[36m(func pid=184567)[0m f1_per_class: [0.0, 0.0, 0.023, 0.124, 0.0, 0.265, 0.203, 0.398, 0.08, 0.012]
[2m[36m(func pid=184567)[0m 
== Status ==
Current time: 2024-01-07 14:30:37 (running for 00:47:14.13)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.041 |      0.148 |                   70 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.759 |      0.111 |                   71 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3945 | Steps: 4 | Val loss: 10.2273 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.5222 | Steps: 4 | Val loss: 2.1304 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=183948)[0m top1: 0.355410447761194
[2m[36m(func pid=183948)[0m top5: 0.8540111940298507
[2m[36m(func pid=183948)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=183948)[0m f1_macro: 0.20488600258099857
[2m[36m(func pid=183948)[0m f1_weighted: 0.3072519545925439
[2m[36m(func pid=183948)[0m f1_per_class: [0.162, 0.329, 0.135, 0.141, 0.061, 0.137, 0.621, 0.0, 0.026, 0.438]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:30:42 (running for 00:47:19.58)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.395 |      0.205 |                   71 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.522 |      0.105 |                   72 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.13619402985074627
[2m[36m(func pid=184567)[0m top5: 0.7299440298507462
[2m[36m(func pid=184567)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=184567)[0m f1_macro: 0.10549482581382248
[2m[36m(func pid=184567)[0m f1_weighted: 0.16285716812641535
[2m[36m(func pid=184567)[0m f1_per_class: [0.065, 0.0, 0.0, 0.195, 0.0, 0.097, 0.24, 0.386, 0.073, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.1802 | Steps: 4 | Val loss: 2.0359 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5080 | Steps: 4 | Val loss: 2.0457 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=183948)[0m top1: 0.32322761194029853
[2m[36m(func pid=183948)[0m top5: 0.8833955223880597
[2m[36m(func pid=183948)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=183948)[0m f1_macro: 0.2855243154088625
[2m[36m(func pid=183948)[0m f1_weighted: 0.36231001834189913
[2m[36m(func pid=183948)[0m f1_per_class: [0.219, 0.396, 0.471, 0.306, 0.056, 0.253, 0.501, 0.26, 0.157, 0.237]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:30:48 (running for 00:47:25.32)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  3.18  |      0.286 |                   72 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.508 |      0.095 |                   73 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.1417910447761194
[2m[36m(func pid=184567)[0m top5: 0.78125
[2m[36m(func pid=184567)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=184567)[0m f1_macro: 0.09451839432508005
[2m[36m(func pid=184567)[0m f1_weighted: 0.16787903174838092
[2m[36m(func pid=184567)[0m f1_per_class: [0.068, 0.0, 0.0, 0.191, 0.0, 0.086, 0.294, 0.228, 0.078, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2477 | Steps: 4 | Val loss: 2.1752 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3877 | Steps: 4 | Val loss: 1.9759 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=183948)[0m top1: 0.36380597014925375
[2m[36m(func pid=183948)[0m top5: 0.8708022388059702
[2m[36m(func pid=183948)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=183948)[0m f1_macro: 0.324681290585257
[2m[36m(func pid=183948)[0m f1_weighted: 0.35788861590160614
[2m[36m(func pid=183948)[0m f1_per_class: [0.299, 0.481, 0.353, 0.248, 0.138, 0.369, 0.403, 0.423, 0.183, 0.35]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:30:53 (running for 00:47:30.58)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.248 |      0.325 |                   73 |
| train_5806f_00023 | RUNNING    | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.388 |      0.1   |                   74 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.15764925373134328
[2m[36m(func pid=184567)[0m top5: 0.8194962686567164
[2m[36m(func pid=184567)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=184567)[0m f1_macro: 0.09953856023838337
[2m[36m(func pid=184567)[0m f1_weighted: 0.1662194692198146
[2m[36m(func pid=184567)[0m f1_per_class: [0.068, 0.0, 0.0, 0.172, 0.0, 0.294, 0.242, 0.158, 0.062, 0.0]
[2m[36m(func pid=184567)[0m 
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1350 | Steps: 4 | Val loss: 2.6949 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=184567)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.4697 | Steps: 4 | Val loss: 1.9185 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=183948)[0m top1: 0.3125
[2m[36m(func pid=183948)[0m top5: 0.8372201492537313
[2m[36m(func pid=183948)[0m f1_micro: 0.3125
[2m[36m(func pid=183948)[0m f1_macro: 0.30965509220125625
[2m[36m(func pid=183948)[0m f1_weighted: 0.24922541246836674
[2m[36m(func pid=183948)[0m f1_per_class: [0.395, 0.478, 0.343, 0.22, 0.286, 0.354, 0.062, 0.414, 0.186, 0.358]
[2m[36m(func pid=183948)[0m 
== Status ==
Current time: 2024-01-07 14:30:59 (running for 00:47:36.14)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.333
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00022 | RUNNING    | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  2.135 |      0.31  |                   74 |
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
| train_5806f_00018 | TERMINATED | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.154 |      0.069 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=184567)[0m top1: 0.2271455223880597
[2m[36m(func pid=184567)[0m top5: 0.8111007462686567
[2m[36m(func pid=184567)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=184567)[0m f1_macro: 0.1372475677266004
[2m[36m(func pid=184567)[0m f1_weighted: 0.22274550187683206
[2m[36m(func pid=184567)[0m f1_per_class: [0.084, 0.0, 0.0, 0.273, 0.0, 0.349, 0.281, 0.347, 0.039, 0.0]
[2m[36m(func pid=183948)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.7925 | Steps: 4 | Val loss: 2.1654 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:31:02 (running for 00:47:39.74)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.3315
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5806f_00000 | TERMINATED | 192.168.7.53:78294  | 0.0001 |       0.99 |         0      |  0.018 |      0.386 |                  100 |
| train_5806f_00001 | TERMINATED | 192.168.7.53:78677  | 0.001  |       0.99 |         0      |  1.257 |      0.159 |                   75 |
| train_5806f_00002 | TERMINATED | 192.168.7.53:79093  | 0.01   |       0.99 |         0      |  2.927 |      0.112 |                  100 |
| train_5806f_00003 | TERMINATED | 192.168.7.53:79525  | 0.1    |       0.99 |         0      |  5.236 |      0.071 |                   75 |
| train_5806f_00004 | TERMINATED | 192.168.7.53:97361  | 0.0001 |       0.9  |         0      |  0.682 |      0.336 |                   75 |
| train_5806f_00005 | TERMINATED | 192.168.7.53:97446  | 0.001  |       0.9  |         0      |  0.016 |      0.402 |                  100 |
| train_5806f_00006 | TERMINATED | 192.168.7.53:103465 | 0.01   |       0.9  |         0      |  2.337 |      0.173 |                   75 |
| train_5806f_00007 | TERMINATED | 192.168.7.53:103554 | 0.1    |       0.9  |         0      |  2.411 |      0.067 |                   75 |
| train_5806f_00008 | TERMINATED | 192.168.7.53:116143 | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.414 |                  100 |
| train_5806f_00009 | TERMINATED | 192.168.7.53:121898 | 0.001  |       0.99 |         0.0001 |  1.489 |      0.164 |                   75 |
| train_5806f_00010 | TERMINATED | 192.168.7.53:122441 | 0.01   |       0.99 |         0.0001 |  2.678 |      0.077 |                   75 |
| train_5806f_00011 | TERMINATED | 192.168.7.53:122462 | 0.1    |       0.99 |         0.0001 |  3.054 |      0.099 |                   75 |
| train_5806f_00012 | TERMINATED | 192.168.7.53:140272 | 0.0001 |       0.9  |         0.0001 |  0.688 |      0.303 |                   75 |
| train_5806f_00013 | TERMINATED | 192.168.7.53:140840 | 0.001  |       0.9  |         0.0001 |  0.014 |      0.403 |                  100 |
| train_5806f_00014 | TERMINATED | 192.168.7.53:141385 | 0.01   |       0.9  |         0.0001 |  1.4   |      0.186 |                   75 |
| train_5806f_00015 | TERMINATED | 192.168.7.53:141940 | 0.1    |       0.9  |         0.0001 |  2.307 |      0.084 |                   75 |
| train_5806f_00016 | TERMINATED | 192.168.7.53:159021 | 0.0001 |       0.99 |         1e-05  |  0.009 |      0.411 |                  100 |
| train_5806f_00017 | TERMINATED | 192.168.7.53:159478 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.143 |                   75 |
| train_5806f_00018 | TERMINATED | 192.168.7.53:160131 | 0.01   |       0.99 |         1e-05  |  3.154 |      0.069 |                   75 |
| train_5806f_00019 | TERMINATED | 192.168.7.53:165522 | 0.1    |       0.99 |         1e-05  |  2.606 |      0.066 |                   75 |
| train_5806f_00020 | TERMINATED | 192.168.7.53:177931 | 0.0001 |       0.9  |         1e-05  |  0.994 |      0.283 |                   75 |
| train_5806f_00021 | TERMINATED | 192.168.7.53:178896 | 0.001  |       0.9  |         1e-05  |  0.03  |      0.33  |                   75 |
| train_5806f_00022 | TERMINATED | 192.168.7.53:183948 | 0.01   |       0.9  |         1e-05  |  1.793 |      0.245 |                   75 |
| train_5806f_00023 | TERMINATED | 192.168.7.53:184567 | 0.1    |       0.9  |         1e-05  |  2.47  |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 14:31:02,863	INFO tune.py:798 -- Total run time: 2860.84 seconds (2859.72 seconds for the tuning loop).
[2m[36m(func pid=183948)[0m top1: 0.2537313432835821
[2m[36m(func pid=183948)[0m top5: 0.8479477611940298
[2m[36m(func pid=183948)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=183948)[0m f1_macro: 0.2454977954791256
[2m[36m(func pid=183948)[0m f1_weighted: 0.21712444551448173
[2m[36m(func pid=183948)[0m f1_per_class: [0.31, 0.476, 0.556, 0.259, 0.117, 0.243, 0.043, 0.06, 0.171, 0.22]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341349.1 ON aap04 CANCELLED AT 2024-01-07T14:31:10 ***
srun: error: aap04: task 0: Exited with exit code 1

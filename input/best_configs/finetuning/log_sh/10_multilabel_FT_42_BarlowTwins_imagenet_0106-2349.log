IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 19:48:55,424	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 19:48:55,425	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 19:48:57,942	SUCC scripts.py:747 -- --------------------
2024-01-07 19:48:57,942	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 19:48:57,943	SUCC scripts.py:749 -- --------------------
2024-01-07 19:48:57,943	INFO scripts.py:751 -- Next steps
2024-01-07 19:48:57,943	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 19:48:57,943	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 19:48:57,943	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 19:48:57,943	INFO scripts.py:773 -- import ray
2024-01-07 19:48:57,943	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 19:48:57,943	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 19:48:57,943	INFO scripts.py:791 --   ray status
2024-01-07 19:48:57,943	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 19:48:57,943	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 19:48:57,943	INFO scripts.py:810 --   ray stop
2024-01-07 19:48:57,943	INFO scripts.py:891 -- --block
2024-01-07 19:48:57,943	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 19:48:57,943	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              12519440275699033841
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f8bc17eb100>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          BarlowTwins
task_name:           multilabel
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   FT
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1


Model resnet18 with pretrained weights using BarlowTwins SSL
Model loaded from snapshot_BarlowTwins_resnet18_bd=False_iw=random.pt
Model name:        BarlowTwins
Backbone name:     resnet18
Hidden layer dim.: 256
Output layer dim.: 128
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Fine-tuning adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 19:49:41,193	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 19:49:41,219	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 19:50:04,313	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 19:50:04 (running for 00:00:22.24)
Memory usage on this node: 13.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |
| train_84a75_00001 | PENDING  |                     | 0.001  |       0.99 |         0      |
| train_84a75_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_84a75_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=182810)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=182810)[0m Configuration completed!
[2m[36m(func pid=182810)[0m New optimizer parameters:
[2m[36m(func pid=182810)[0m SGD (
[2m[36m(func pid=182810)[0m Parameter Group 0
[2m[36m(func pid=182810)[0m     dampening: 0
[2m[36m(func pid=182810)[0m     differentiable: False
[2m[36m(func pid=182810)[0m     foreach: None
[2m[36m(func pid=182810)[0m     lr: 0.0001
[2m[36m(func pid=182810)[0m     maximize: False
[2m[36m(func pid=182810)[0m     momentum: 0.99
[2m[36m(func pid=182810)[0m     nesterov: False
[2m[36m(func pid=182810)[0m     weight_decay: 0
[2m[36m(func pid=182810)[0m )
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8982 | Steps: 4 | Val loss: 0.7048 | Batch size: 32 | lr: 0.0001 | Duration: 4.96s
[2m[36m(func pid=182810)[0m rmse: 0.1827249825000763
[2m[36m(func pid=182810)[0m mae:  0.13450844585895538
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.267, 0.108, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 19:50:14 (running for 00:00:31.86)
Memory usage on this node: 15.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |
| train_84a75_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_84a75_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183191)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=183191)[0m Configuration completed!
[2m[36m(func pid=183191)[0m New optimizer parameters:
[2m[36m(func pid=183191)[0m SGD (
[2m[36m(func pid=183191)[0m Parameter Group 0
[2m[36m(func pid=183191)[0m     dampening: 0
[2m[36m(func pid=183191)[0m     differentiable: False
[2m[36m(func pid=183191)[0m     foreach: None
[2m[36m(func pid=183191)[0m     lr: 0.001
[2m[36m(func pid=183191)[0m     maximize: False
[2m[36m(func pid=183191)[0m     momentum: 0.99
[2m[36m(func pid=183191)[0m     nesterov: False
[2m[36m(func pid=183191)[0m     weight_decay: 0
[2m[36m(func pid=183191)[0m )
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8912 | Steps: 4 | Val loss: 0.6932 | Batch size: 32 | lr: 0.001 | Duration: 4.82s
[2m[36m(func pid=183191)[0m rmse: 0.18256530165672302
[2m[36m(func pid=183191)[0m mae:  0.13434827327728271
[2m[36m(func pid=183191)[0m rmse_per_class: [0.116, 0.267, 0.108, 0.339, 0.112, 0.19, 0.294, 0.144, 0.143, 0.113]
== Status ==
Current time: 2024-01-07 19:50:22 (running for 00:00:40.60)
Memory usage on this node: 18.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |
| train_84a75_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183612)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=183612)[0m Configuration completed!
[2m[36m(func pid=183612)[0m New optimizer parameters:
[2m[36m(func pid=183612)[0m SGD (
[2m[36m(func pid=183612)[0m Parameter Group 0
[2m[36m(func pid=183612)[0m     dampening: 0
[2m[36m(func pid=183612)[0m     differentiable: False
[2m[36m(func pid=183612)[0m     foreach: None
[2m[36m(func pid=183612)[0m     lr: 0.01
[2m[36m(func pid=183612)[0m     maximize: False
[2m[36m(func pid=183612)[0m     momentum: 0.99
[2m[36m(func pid=183612)[0m     nesterov: False
[2m[36m(func pid=183612)[0m     weight_decay: 0
[2m[36m(func pid=183612)[0m )
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8653 | Steps: 4 | Val loss: 0.6263 | Batch size: 32 | lr: 0.01 | Duration: 4.82s
[2m[36m(func pid=183612)[0m rmse: 0.18235450983047485
[2m[36m(func pid=183612)[0m mae:  0.13408400118350983
[2m[36m(func pid=183612)[0m rmse_per_class: [0.117, 0.267, 0.109, 0.339, 0.111, 0.191, 0.295, 0.143, 0.142, 0.11]
== Status ==
Current time: 2024-01-07 19:50:31 (running for 00:00:49.50)
Memory usage on this node: 20.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=184036)[0m Dataloader to compute accuracy: val

[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=184036)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=184036)[0m Configuration completed!
[2m[36m(func pid=184036)[0m New optimizer parameters:
[2m[36m(func pid=184036)[0m SGD (
[2m[36m(func pid=184036)[0m Parameter Group 0
[2m[36m(func pid=184036)[0m     dampening: 0
[2m[36m(func pid=184036)[0m     differentiable: False
[2m[36m(func pid=184036)[0m     foreach: None
[2m[36m(func pid=184036)[0m     lr: 0.1
[2m[36m(func pid=184036)[0m     maximize: False
[2m[36m(func pid=184036)[0m     momentum: 0.99
[2m[36m(func pid=184036)[0m     nesterov: False
[2m[36m(func pid=184036)[0m     weight_decay: 0
[2m[36m(func pid=184036)[0m )
[2m[36m(func pid=184036)[0m 
== Status ==
Current time: 2024-01-07 19:50:39 (running for 00:00:57.36)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |        |        |                      |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |        |        |                      |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.865 |  0.182 |                    1 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |        |        |                      |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8722 | Steps: 4 | Val loss: 0.6653 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7048 | Steps: 4 | Val loss: 0.4642 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8970 | Steps: 4 | Val loss: 0.6955 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6859 | Steps: 4 | Val loss: 0.3314 | Batch size: 32 | lr: 0.1 | Duration: 4.75s
== Status ==
Current time: 2024-01-07 19:50:44 (running for 00:01:02.39)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.898 |  0.183 |                    1 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.891 |  0.183 |                    1 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.865 |  0.182 |                    1 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |        |        |                      |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.1818656474351883
[2m[36m(func pid=183191)[0m mae:  0.13376489281654358
[2m[36m(func pid=183191)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1821976602077484
[2m[36m(func pid=182810)[0m mae:  0.13410647213459015
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.18025285005569458
[2m[36m(func pid=183612)[0m mae:  0.13212524354457855
[2m[36m(func pid=183612)[0m rmse_per_class: [0.115, 0.266, 0.105, 0.333, 0.109, 0.192, 0.293, 0.142, 0.14, 0.107]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.177068829536438
[2m[36m(func pid=184036)[0m mae:  0.1294773519039154
[2m[36m(func pid=184036)[0m rmse_per_class: [0.114, 0.263, 0.108, 0.334, 0.089, 0.189, 0.282, 0.138, 0.143, 0.111]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8320 | Steps: 4 | Val loss: 0.6267 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8922 | Steps: 4 | Val loss: 0.6883 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5086 | Steps: 4 | Val loss: 0.3504 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4607 | Steps: 4 | Val loss: 0.3758 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 19:50:50 (running for 00:01:08.00)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.897 |  0.182 |                    2 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.832 |  0.181 |                    3 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.705 |  0.18  |                    2 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.686 |  0.177 |                    1 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.18093906342983246
[2m[36m(func pid=183191)[0m mae:  0.13285091519355774
[2m[36m(func pid=183191)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.337, 0.113, 0.19, 0.294, 0.141, 0.14, 0.11]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.18152306973934174
[2m[36m(func pid=182810)[0m mae:  0.133559912443161
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.113, 0.189, 0.295, 0.142, 0.143, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1776312291622162
[2m[36m(func pid=183612)[0m mae:  0.1301768273115158
[2m[36m(func pid=183612)[0m rmse_per_class: [0.117, 0.263, 0.103, 0.331, 0.093, 0.192, 0.287, 0.138, 0.144, 0.108]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16502651572227478
[2m[36m(func pid=184036)[0m mae:  0.11889026314020157
[2m[36m(func pid=184036)[0m rmse_per_class: [0.142, 0.257, 0.065, 0.311, 0.059, 0.188, 0.264, 0.128, 0.136, 0.101]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8849 | Steps: 4 | Val loss: 0.6839 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7796 | Steps: 4 | Val loss: 0.5789 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4135 | Steps: 4 | Val loss: 0.3180 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.6095 | Steps: 4 | Val loss: 0.4543 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:50:55 (running for 00:01:13.37)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.892 |  0.182 |                    3 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.78  |  0.18  |                    4 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.509 |  0.178 |                    3 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.461 |  0.165 |                    2 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.17979761958122253
[2m[36m(func pid=183191)[0m mae:  0.1318378746509552
[2m[36m(func pid=183191)[0m rmse_per_class: [0.115, 0.263, 0.1, 0.335, 0.113, 0.191, 0.293, 0.14, 0.14, 0.108]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.18098893761634827
[2m[36m(func pid=182810)[0m mae:  0.13308770954608917
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.338, 0.113, 0.189, 0.295, 0.141, 0.142, 0.11]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.17643390595912933
[2m[36m(func pid=183612)[0m mae:  0.12909644842147827
[2m[36m(func pid=183612)[0m rmse_per_class: [0.128, 0.263, 0.097, 0.333, 0.081, 0.191, 0.28, 0.133, 0.149, 0.109]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15116794407367706
[2m[36m(func pid=184036)[0m mae:  0.10172031819820404
[2m[36m(func pid=184036)[0m rmse_per_class: [0.129, 0.245, 0.049, 0.278, 0.055, 0.177, 0.224, 0.128, 0.133, 0.095]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8816 | Steps: 4 | Val loss: 0.6759 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4213 | Steps: 4 | Val loss: 0.3348 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7231 | Steps: 4 | Val loss: 0.5296 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5831 | Steps: 4 | Val loss: 0.4603 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 19:51:00 (running for 00:01:18.44)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.882 |  0.181 |                    5 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.78  |  0.18  |                    4 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.414 |  0.176 |                    4 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.609 |  0.151 |                    3 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.18065035343170166
[2m[36m(func pid=182810)[0m mae:  0.13275283575057983
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.338, 0.112, 0.189, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.17239680886268616
[2m[36m(func pid=183612)[0m mae:  0.1255674958229065
[2m[36m(func pid=183612)[0m rmse_per_class: [0.135, 0.254, 0.084, 0.329, 0.072, 0.193, 0.273, 0.13, 0.148, 0.106]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17925617098808289
[2m[36m(func pid=183191)[0m mae:  0.1313798427581787
[2m[36m(func pid=183191)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.335, 0.112, 0.191, 0.292, 0.14, 0.141, 0.108]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17130714654922485
[2m[36m(func pid=184036)[0m mae:  0.10606463998556137
[2m[36m(func pid=184036)[0m rmse_per_class: [0.106, 0.246, 0.049, 0.309, 0.056, 0.196, 0.395, 0.132, 0.138, 0.087]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8648 | Steps: 4 | Val loss: 0.6702 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4625 | Steps: 4 | Val loss: 0.3871 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6575 | Steps: 4 | Val loss: 0.4836 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4726 | Steps: 4 | Val loss: 1.0758 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=182810)[0m rmse: 0.18028798699378967== Status ==
Current time: 2024-01-07 19:51:05 (running for 00:01:23.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.865 |  0.18  |                    6 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.723 |  0.179 |                    5 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.421 |  0.172 |                    5 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.583 |  0.171 |                    4 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=182810)[0m mae:  0.13247844576835632
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.337, 0.111, 0.189, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16811031103134155
[2m[36m(func pid=183612)[0m mae:  0.12152490764856339
[2m[36m(func pid=183612)[0m rmse_per_class: [0.145, 0.241, 0.07, 0.324, 0.064, 0.188, 0.269, 0.13, 0.147, 0.103]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17859934270381927
[2m[36m(func pid=183191)[0m mae:  0.13080927729606628
[2m[36m(func pid=183191)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.334, 0.107, 0.191, 0.292, 0.139, 0.142, 0.108]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.2125907838344574
[2m[36m(func pid=184036)[0m mae:  0.134622722864151
[2m[36m(func pid=184036)[0m rmse_per_class: [0.11, 0.272, 0.049, 0.377, 0.056, 0.213, 0.673, 0.147, 0.14, 0.089]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8550 | Steps: 4 | Val loss: 0.6624 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6045 | Steps: 4 | Val loss: 0.4380 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5137 | Steps: 4 | Val loss: 0.4275 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4082 | Steps: 4 | Val loss: 1.1590 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=182810)[0m rmse: 0.1802673041820526
[2m[36m(func pid=182810)[0m mae:  0.132480651140213
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.338, 0.111, 0.189, 0.294, 0.14, 0.143, 0.11]
[2m[36m(func pid=182810)[0m 
== Status ==
Current time: 2024-01-07 19:51:11 (running for 00:01:28.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.855 |  0.18  |                    7 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.657 |  0.179 |                    6 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.463 |  0.168 |                    6 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.473 |  0.213 |                    5 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16255879402160645
[2m[36m(func pid=183612)[0m mae:  0.11597045511007309
[2m[36m(func pid=183612)[0m rmse_per_class: [0.143, 0.235, 0.058, 0.326, 0.058, 0.179, 0.254, 0.129, 0.145, 0.098]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1782272607088089
[2m[36m(func pid=183191)[0m mae:  0.1305723488330841
[2m[36m(func pid=183191)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.103, 0.19, 0.29, 0.138, 0.144, 0.11]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.21549269556999207
[2m[36m(func pid=184036)[0m mae:  0.13515125215053558
[2m[36m(func pid=184036)[0m rmse_per_class: [0.102, 0.27, 0.049, 0.37, 0.056, 0.226, 0.621, 0.229, 0.14, 0.091]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8484 | Steps: 4 | Val loss: 0.6519 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5525 | Steps: 4 | Val loss: 0.4514 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5373 | Steps: 4 | Val loss: 0.3982 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4149 | Steps: 4 | Val loss: 0.4825 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 19:51:16 (running for 00:01:33.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.848 |  0.18  |                    8 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.605 |  0.178 |                    7 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.514 |  0.163 |                    7 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.408 |  0.215 |                    6 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.18004971742630005
[2m[36m(func pid=182810)[0m mae:  0.1322808414697647
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.262, 0.099, 0.337, 0.112, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1560501903295517
[2m[36m(func pid=183612)[0m mae:  0.1095513254404068
[2m[36m(func pid=183612)[0m rmse_per_class: [0.14, 0.231, 0.047, 0.318, 0.055, 0.17, 0.24, 0.123, 0.144, 0.092]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17811410129070282
[2m[36m(func pid=183191)[0m mae:  0.13058345019817352
[2m[36m(func pid=183191)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.336, 0.098, 0.191, 0.288, 0.138, 0.146, 0.113]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17875030636787415
[2m[36m(func pid=184036)[0m mae:  0.1081826463341713
[2m[36m(func pid=184036)[0m rmse_per_class: [0.103, 0.254, 0.05, 0.337, 0.056, 0.184, 0.413, 0.155, 0.148, 0.087]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8307 | Steps: 4 | Val loss: 0.6408 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5547 | Steps: 4 | Val loss: 0.4605 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4899 | Steps: 4 | Val loss: 0.3688 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3850 | Steps: 4 | Val loss: 0.3839 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 19:51:21 (running for 00:01:39.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.831 |  0.18  |                    9 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.537 |  0.178 |                    8 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.553 |  0.156 |                    8 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.415 |  0.179 |                    7 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.1798340529203415
[2m[36m(func pid=182810)[0m mae:  0.132084459066391
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15137095749378204
[2m[36m(func pid=183612)[0m mae:  0.10452944040298462
[2m[36m(func pid=183612)[0m rmse_per_class: [0.131, 0.228, 0.039, 0.312, 0.054, 0.169, 0.231, 0.119, 0.142, 0.089]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17876595258712769
[2m[36m(func pid=183191)[0m mae:  0.13111117482185364
[2m[36m(func pid=183191)[0m rmse_per_class: [0.119, 0.261, 0.096, 0.337, 0.095, 0.191, 0.287, 0.139, 0.147, 0.117]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17476163804531097
[2m[36m(func pid=184036)[0m mae:  0.10968279838562012
[2m[36m(func pid=184036)[0m rmse_per_class: [0.113, 0.257, 0.118, 0.335, 0.056, 0.16, 0.293, 0.119, 0.204, 0.092]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8146 | Steps: 4 | Val loss: 0.6262 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5492 | Steps: 4 | Val loss: 0.4586 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4593 | Steps: 4 | Val loss: 0.3472 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3555 | Steps: 4 | Val loss: 0.4920 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 19:51:27 (running for 00:01:44.69)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.815 |  0.179 |                   10 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.49  |  0.179 |                    9 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.555 |  0.151 |                    9 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.385 |  0.175 |                    8 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17940732836723328
[2m[36m(func pid=182810)[0m mae:  0.131675586104393
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.336, 0.111, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.14988896250724792
[2m[36m(func pid=183612)[0m mae:  0.1018734946846962
[2m[36m(func pid=183612)[0m rmse_per_class: [0.124, 0.226, 0.036, 0.312, 0.055, 0.17, 0.232, 0.117, 0.141, 0.087]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17825742065906525
[2m[36m(func pid=183191)[0m mae:  0.13069401681423187
[2m[36m(func pid=183191)[0m rmse_per_class: [0.118, 0.261, 0.093, 0.336, 0.093, 0.19, 0.286, 0.138, 0.148, 0.119]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20929774641990662
[2m[36m(func pid=184036)[0m mae:  0.13574640452861786
[2m[36m(func pid=184036)[0m rmse_per_class: [0.138, 0.473, 0.04, 0.366, 0.056, 0.198, 0.298, 0.141, 0.193, 0.19]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8009 | Steps: 4 | Val loss: 0.6139 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5364 | Steps: 4 | Val loss: 0.4326 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4320 | Steps: 4 | Val loss: 0.3322 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3597 | Steps: 4 | Val loss: 0.5012 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 19:51:32 (running for 00:01:50.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.801 |  0.179 |                   11 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.459 |  0.178 |                   10 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.549 |  0.15  |                   10 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.356 |  0.209 |                    9 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.1793222576379776
[2m[36m(func pid=182810)[0m mae:  0.1315998136997223
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15110507607460022
[2m[36m(func pid=183612)[0m mae:  0.10085779428482056
[2m[36m(func pid=183612)[0m rmse_per_class: [0.113, 0.225, 0.04, 0.32, 0.055, 0.17, 0.247, 0.115, 0.14, 0.086]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1780770868062973
[2m[36m(func pid=183191)[0m mae:  0.13052141666412354
[2m[36m(func pid=183191)[0m rmse_per_class: [0.12, 0.261, 0.09, 0.335, 0.092, 0.191, 0.285, 0.138, 0.148, 0.12]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.21411924064159393
[2m[36m(func pid=184036)[0m mae:  0.13585254549980164
[2m[36m(func pid=184036)[0m rmse_per_class: [0.13, 0.439, 0.11, 0.368, 0.056, 0.195, 0.293, 0.142, 0.251, 0.155]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7848 | Steps: 4 | Val loss: 0.6025 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5147 | Steps: 4 | Val loss: 0.3895 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4122 | Steps: 4 | Val loss: 0.3215 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3505 | Steps: 4 | Val loss: 0.4267 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 19:51:37 (running for 00:01:55.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.785 |  0.179 |                   12 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.432 |  0.178 |                   11 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.536 |  0.151 |                   11 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   10 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.1792326271533966
[2m[36m(func pid=182810)[0m mae:  0.13153265416622162
[2m[36m(func pid=182810)[0m rmse_per_class: [0.114, 0.26, 0.098, 0.336, 0.109, 0.19, 0.294, 0.139, 0.142, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15378394722938538
[2m[36m(func pid=183612)[0m mae:  0.10123202949762344
[2m[36m(func pid=183612)[0m rmse_per_class: [0.108, 0.228, 0.043, 0.33, 0.055, 0.17, 0.263, 0.113, 0.142, 0.085]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17715439200401306
[2m[36m(func pid=183191)[0m mae:  0.12968359887599945
[2m[36m(func pid=183191)[0m rmse_per_class: [0.119, 0.261, 0.089, 0.334, 0.089, 0.191, 0.283, 0.137, 0.147, 0.121]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.2063210904598236
[2m[36m(func pid=184036)[0m mae:  0.13169944286346436
[2m[36m(func pid=184036)[0m rmse_per_class: [0.126, 0.281, 0.123, 0.355, 0.056, 0.203, 0.271, 0.143, 0.28, 0.225]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7692 | Steps: 4 | Val loss: 0.5903 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4737 | Steps: 4 | Val loss: 0.3589 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4045 | Steps: 4 | Val loss: 0.3172 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3737 | Steps: 4 | Val loss: 0.3962 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=182810)[0m rmse: 0.17928485572338104
[2m[36m(func pid=182810)[0m mae:  0.13153406977653503
[2m[36m(func pid=182810)[0m rmse_per_class: [0.114, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.143, 0.109]
[2m[36m(func pid=182810)[0m 
== Status ==
Current time: 2024-01-07 19:51:43 (running for 00:02:00.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.769 |  0.179 |                   13 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.412 |  0.177 |                   12 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.515 |  0.154 |                   12 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.351 |  0.206 |                   11 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15821847319602966
[2m[36m(func pid=183612)[0m mae:  0.10269814729690552
[2m[36m(func pid=183612)[0m rmse_per_class: [0.105, 0.232, 0.044, 0.341, 0.056, 0.169, 0.295, 0.113, 0.143, 0.085]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17617832124233246
[2m[36m(func pid=183191)[0m mae:  0.128863126039505
[2m[36m(func pid=183191)[0m rmse_per_class: [0.119, 0.261, 0.087, 0.332, 0.086, 0.191, 0.28, 0.137, 0.147, 0.121]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20264478027820587
[2m[36m(func pid=184036)[0m mae:  0.13169924914836884
[2m[36m(func pid=184036)[0m rmse_per_class: [0.116, 0.239, 0.083, 0.328, 0.061, 0.217, 0.262, 0.145, 0.243, 0.332]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7507 | Steps: 4 | Val loss: 0.5769 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4164 | Steps: 4 | Val loss: 0.3442 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4055 | Steps: 4 | Val loss: 0.3182 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3615 | Steps: 4 | Val loss: 0.5193 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 19:51:48 (running for 00:02:06.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.751 |  0.179 |                   14 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.405 |  0.176 |                   13 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.474 |  0.158 |                   13 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.374 |  0.203 |                   12 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.1792561113834381
[2m[36m(func pid=182810)[0m mae:  0.13155043125152588
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16228915750980377
[2m[36m(func pid=183612)[0m mae:  0.10418180376291275
[2m[36m(func pid=183612)[0m rmse_per_class: [0.102, 0.235, 0.043, 0.349, 0.056, 0.167, 0.33, 0.111, 0.143, 0.086]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1747550368309021
[2m[36m(func pid=183191)[0m mae:  0.1276639997959137
[2m[36m(func pid=183191)[0m rmse_per_class: [0.118, 0.26, 0.084, 0.331, 0.082, 0.191, 0.277, 0.136, 0.148, 0.121]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20970475673675537
[2m[36m(func pid=184036)[0m mae:  0.12724260985851288
[2m[36m(func pid=184036)[0m rmse_per_class: [0.119, 0.392, 0.058, 0.311, 0.08, 0.223, 0.261, 0.16, 0.189, 0.303]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7351 | Steps: 4 | Val loss: 0.5623 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3986 | Steps: 4 | Val loss: 0.3321 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4049 | Steps: 4 | Val loss: 0.3226 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4082 | Steps: 4 | Val loss: 0.4484 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 19:51:53 (running for 00:02:11.62)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.735 |  0.179 |                   15 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.405 |  0.175 |                   14 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.416 |  0.162 |                   14 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.361 |  0.21  |                   13 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17910043895244598
[2m[36m(func pid=182810)[0m mae:  0.1313985288143158
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16342362761497498
[2m[36m(func pid=183612)[0m mae:  0.10416898876428604
[2m[36m(func pid=183612)[0m rmse_per_class: [0.1, 0.235, 0.044, 0.351, 0.056, 0.165, 0.345, 0.108, 0.143, 0.086]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1733173429965973
[2m[36m(func pid=183191)[0m mae:  0.12642619013786316
[2m[36m(func pid=183191)[0m rmse_per_class: [0.117, 0.259, 0.081, 0.33, 0.078, 0.19, 0.274, 0.136, 0.148, 0.12]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.21196594834327698
[2m[36m(func pid=184036)[0m mae:  0.12423722445964813
[2m[36m(func pid=184036)[0m rmse_per_class: [0.139, 0.392, 0.086, 0.37, 0.116, 0.218, 0.229, 0.16, 0.15, 0.261]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7197 | Steps: 4 | Val loss: 0.5475 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4157 | Steps: 4 | Val loss: 0.3291 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3629 | Steps: 4 | Val loss: 0.3424 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3752 | Steps: 4 | Val loss: 0.4024 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 19:51:59 (running for 00:02:16.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.72  |  0.179 |                   16 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.405 |  0.173 |                   15 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.399 |  0.163 |                   15 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.408 |  0.212 |                   14 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17903083562850952
[2m[36m(func pid=182810)[0m mae:  0.1313677281141281
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17256924510002136
[2m[36m(func pid=183191)[0m mae:  0.12575633823871613
[2m[36m(func pid=183191)[0m rmse_per_class: [0.116, 0.261, 0.08, 0.328, 0.075, 0.189, 0.271, 0.134, 0.148, 0.121]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16733118891716003
[2m[36m(func pid=183612)[0m mae:  0.10613834857940674
[2m[36m(func pid=183612)[0m rmse_per_class: [0.103, 0.238, 0.046, 0.358, 0.056, 0.164, 0.368, 0.105, 0.149, 0.086]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.21319136023521423
[2m[36m(func pid=184036)[0m mae:  0.12312909215688705
[2m[36m(func pid=184036)[0m rmse_per_class: [0.133, 0.297, 0.214, 0.361, 0.129, 0.199, 0.235, 0.135, 0.137, 0.292]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4232 | Steps: 4 | Val loss: 0.3353 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7047 | Steps: 4 | Val loss: 0.5329 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3612 | Steps: 4 | Val loss: 0.3509 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3856 | Steps: 4 | Val loss: 0.3777 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 19:52:04 (running for 00:02:22.47)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.72  |  0.179 |                   16 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.423 |  0.171 |                   17 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.363 |  0.167 |                   16 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.375 |  0.213 |                   15 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.1788657009601593
[2m[36m(func pid=182810)[0m mae:  0.13123060762882233
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.335, 0.108, 0.19, 0.293, 0.139, 0.143, 0.108]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.17056819796562195
[2m[36m(func pid=183191)[0m mae:  0.12418530136346817
[2m[36m(func pid=183191)[0m rmse_per_class: [0.115, 0.258, 0.076, 0.327, 0.073, 0.189, 0.268, 0.134, 0.148, 0.118]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16794750094413757
[2m[36m(func pid=183612)[0m mae:  0.10662857443094254
[2m[36m(func pid=183612)[0m rmse_per_class: [0.103, 0.238, 0.046, 0.36, 0.056, 0.164, 0.354, 0.106, 0.169, 0.085]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20551657676696777
[2m[36m(func pid=184036)[0m mae:  0.11764925718307495
[2m[36m(func pid=184036)[0m rmse_per_class: [0.118, 0.263, 0.23, 0.345, 0.119, 0.165, 0.258, 0.255, 0.183, 0.118]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6823 | Steps: 4 | Val loss: 0.5202 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4341 | Steps: 4 | Val loss: 0.3437 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3612 | Steps: 4 | Val loss: 0.3681 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3479 | Steps: 4 | Val loss: 0.4187 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 19:52:09 (running for 00:02:27.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.682 |  0.179 |                   18 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.423 |  0.171 |                   17 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.361 |  0.168 |                   17 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.386 |  0.206 |                   16 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17859503626823425
[2m[36m(func pid=182810)[0m mae:  0.13100630044937134
[2m[36m(func pid=182810)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.107, 0.19, 0.292, 0.139, 0.143, 0.108]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.16903135180473328
[2m[36m(func pid=183191)[0m mae:  0.12279614061117172
[2m[36m(func pid=183191)[0m rmse_per_class: [0.115, 0.257, 0.074, 0.325, 0.07, 0.188, 0.264, 0.132, 0.148, 0.117]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.17110472917556763
[2m[36m(func pid=183612)[0m mae:  0.1080581545829773
[2m[36m(func pid=183612)[0m rmse_per_class: [0.108, 0.238, 0.045, 0.362, 0.056, 0.166, 0.311, 0.114, 0.226, 0.084]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20129235088825226
[2m[36m(func pid=184036)[0m mae:  0.12044855207204819
[2m[36m(func pid=184036)[0m rmse_per_class: [0.155, 0.255, 0.065, 0.379, 0.102, 0.199, 0.251, 0.14, 0.361, 0.107]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6650 | Steps: 4 | Val loss: 0.5051 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4381 | Steps: 4 | Val loss: 0.3520 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4010 | Steps: 4 | Val loss: 0.3570 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3575 | Steps: 4 | Val loss: 0.3802 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=182810)[0m rmse: 0.17864742875099182
[2m[36m(func pid=182810)[0m mae:  0.13105106353759766
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.106, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=182810)[0m 
== Status ==
Current time: 2024-01-07 19:52:15 (running for 00:02:32.91)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.665 |  0.179 |                   19 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.434 |  0.169 |                   18 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.361 |  0.171 |                   18 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.348 |  0.201 |                   17 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16654887795448303
[2m[36m(func pid=183612)[0m mae:  0.10604874789714813
[2m[36m(func pid=183612)[0m rmse_per_class: [0.115, 0.23, 0.032, 0.355, 0.056, 0.167, 0.268, 0.13, 0.229, 0.083]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.16771557927131653
[2m[36m(func pid=183191)[0m mae:  0.12158925831317902
[2m[36m(func pid=183191)[0m rmse_per_class: [0.114, 0.257, 0.072, 0.324, 0.068, 0.187, 0.26, 0.131, 0.149, 0.115]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.19567540287971497
[2m[36m(func pid=184036)[0m mae:  0.1191546693444252
[2m[36m(func pid=184036)[0m rmse_per_class: [0.127, 0.308, 0.121, 0.307, 0.071, 0.217, 0.24, 0.146, 0.211, 0.209]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6494 | Steps: 4 | Val loss: 0.4930 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4597 | Steps: 4 | Val loss: 0.3587 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3569 | Steps: 4 | Val loss: 0.3510 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3339 | Steps: 4 | Val loss: 0.4487 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 19:52:20 (running for 00:02:38.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.649 |  0.179 |                   20 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.438 |  0.168 |                   19 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.401 |  0.167 |                   19 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.358 |  0.196 |                   18 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17873221635818481
[2m[36m(func pid=182810)[0m mae:  0.13110695779323578
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.336, 0.105, 0.19, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.16487808525562286
[2m[36m(func pid=183191)[0m mae:  0.11903456598520279
[2m[36m(func pid=183191)[0m rmse_per_class: [0.111, 0.254, 0.068, 0.322, 0.066, 0.185, 0.254, 0.13, 0.148, 0.112]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16431504487991333
[2m[36m(func pid=183612)[0m mae:  0.10535812377929688
[2m[36m(func pid=183612)[0m rmse_per_class: [0.119, 0.227, 0.025, 0.351, 0.056, 0.169, 0.25, 0.13, 0.232, 0.084]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20468667149543762
[2m[36m(func pid=184036)[0m mae:  0.1245628148317337
[2m[36m(func pid=184036)[0m rmse_per_class: [0.117, 0.411, 0.072, 0.373, 0.065, 0.219, 0.244, 0.166, 0.139, 0.241]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6330 | Steps: 4 | Val loss: 0.4803 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4589 | Steps: 4 | Val loss: 0.3647 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3619 | Steps: 4 | Val loss: 0.3363 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3448 | Steps: 4 | Val loss: 0.4621 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 19:52:25 (running for 00:02:43.51)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.633 |  0.179 |                   21 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.46  |  0.165 |                   20 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.357 |  0.164 |                   20 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.334 |  0.205 |                   19 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.178504079580307
[2m[36m(func pid=182810)[0m mae:  0.13090619444847107
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.105, 0.19, 0.292, 0.139, 0.144, 0.11]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.163344606757164
[2m[36m(func pid=183191)[0m mae:  0.11763862520456314
[2m[36m(func pid=183191)[0m rmse_per_class: [0.11, 0.253, 0.064, 0.319, 0.064, 0.184, 0.251, 0.129, 0.148, 0.11]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1614644080400467
[2m[36m(func pid=183612)[0m mae:  0.10473208129405975
[2m[36m(func pid=183612)[0m rmse_per_class: [0.116, 0.223, 0.024, 0.343, 0.056, 0.17, 0.248, 0.127, 0.222, 0.086]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.21331460773944855
[2m[36m(func pid=184036)[0m mae:  0.1251739114522934
[2m[36m(func pid=184036)[0m rmse_per_class: [0.115, 0.408, 0.044, 0.358, 0.059, 0.211, 0.26, 0.263, 0.139, 0.276]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6151 | Steps: 4 | Val loss: 0.4686 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3640 | Steps: 4 | Val loss: 0.3463 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4678 | Steps: 4 | Val loss: 0.3684 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3465 | Steps: 4 | Val loss: 0.4475 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=182810)[0m rmse: 0.17830322682857513
[2m[36m(func pid=182810)[0m mae:  0.13073039054870605
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.291, 0.139, 0.144, 0.109]
== Status ==
Current time: 2024-01-07 19:52:31 (running for 00:02:48.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.615 |  0.178 |                   22 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.459 |  0.163 |                   21 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.362 |  0.161 |                   21 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.345 |  0.213 |                   20 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.16158123314380646
[2m[36m(func pid=183191)[0m mae:  0.11589445173740387
[2m[36m(func pid=183191)[0m rmse_per_class: [0.109, 0.252, 0.061, 0.315, 0.062, 0.182, 0.249, 0.129, 0.148, 0.109]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16529102623462677
[2m[36m(func pid=183612)[0m mae:  0.1081286072731018
[2m[36m(func pid=183612)[0m rmse_per_class: [0.117, 0.223, 0.024, 0.346, 0.055, 0.172, 0.259, 0.126, 0.237, 0.093]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.21915185451507568
[2m[36m(func pid=184036)[0m mae:  0.13027498126029968
[2m[36m(func pid=184036)[0m rmse_per_class: [0.117, 0.334, 0.087, 0.363, 0.056, 0.199, 0.302, 0.32, 0.153, 0.262]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6012 | Steps: 4 | Val loss: 0.4576 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4688 | Steps: 4 | Val loss: 0.3724 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3561 | Steps: 4 | Val loss: 0.3432 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3341 | Steps: 4 | Val loss: 0.4290 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 19:52:36 (running for 00:02:54.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.601 |  0.178 |                   23 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.468 |  0.162 |                   22 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.364 |  0.165 |                   22 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.346 |  0.219 |                   21 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17817522585391998
[2m[36m(func pid=182810)[0m mae:  0.13061973452568054
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.291, 0.139, 0.144, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.15980462729930878
[2m[36m(func pid=183191)[0m mae:  0.11432597786188126
[2m[36m(func pid=183191)[0m rmse_per_class: [0.108, 0.25, 0.059, 0.313, 0.061, 0.18, 0.246, 0.128, 0.147, 0.106]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16439512372016907
[2m[36m(func pid=183612)[0m mae:  0.10721836984157562
[2m[36m(func pid=183612)[0m rmse_per_class: [0.113, 0.223, 0.026, 0.344, 0.055, 0.172, 0.261, 0.141, 0.215, 0.095]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20620283484458923
[2m[36m(func pid=184036)[0m mae:  0.12324589490890503
[2m[36m(func pid=184036)[0m rmse_per_class: [0.117, 0.326, 0.148, 0.37, 0.055, 0.202, 0.278, 0.181, 0.15, 0.236]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5853 | Steps: 4 | Val loss: 0.4456 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4792 | Steps: 4 | Val loss: 0.3739 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3432 | Steps: 4 | Val loss: 0.3438 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4130 | Steps: 4 | Val loss: 0.4078 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 19:52:41 (running for 00:02:59.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.585 |  0.178 |                   24 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.469 |  0.16  |                   23 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.356 |  0.164 |                   23 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.334 |  0.206 |                   22 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.1780814379453659
[2m[36m(func pid=182810)[0m mae:  0.13055428862571716
[2m[36m(func pid=182810)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.334, 0.103, 0.19, 0.291, 0.139, 0.144, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.15835769474506378
[2m[36m(func pid=183191)[0m mae:  0.112751305103302
[2m[36m(func pid=183191)[0m rmse_per_class: [0.107, 0.251, 0.057, 0.31, 0.059, 0.179, 0.243, 0.129, 0.146, 0.105]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16570620238780975
[2m[36m(func pid=183612)[0m mae:  0.10758869349956512
[2m[36m(func pid=183612)[0m rmse_per_class: [0.113, 0.225, 0.027, 0.343, 0.055, 0.172, 0.266, 0.163, 0.201, 0.093]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.19484099745750427
[2m[36m(func pid=184036)[0m mae:  0.1161467432975769
[2m[36m(func pid=184036)[0m rmse_per_class: [0.125, 0.265, 0.159, 0.353, 0.056, 0.218, 0.244, 0.12, 0.141, 0.266]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5719 | Steps: 4 | Val loss: 0.4339 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4819 | Steps: 4 | Val loss: 0.3771 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3599 | Steps: 4 | Val loss: 0.3505 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3798 | Steps: 4 | Val loss: 0.5318 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 19:52:47 (running for 00:03:04.95)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.572 |  0.178 |                   25 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.479 |  0.158 |                   24 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.343 |  0.166 |                   24 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.413 |  0.195 |                   23 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17790603637695312
[2m[36m(func pid=182810)[0m mae:  0.13041482865810394
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.259, 0.095, 0.334, 0.102, 0.19, 0.291, 0.139, 0.144, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.15658649802207947
[2m[36m(func pid=183191)[0m mae:  0.11110017448663712
[2m[36m(func pid=183191)[0m rmse_per_class: [0.106, 0.249, 0.055, 0.306, 0.058, 0.177, 0.241, 0.127, 0.145, 0.102]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.17016346752643585
[2m[36m(func pid=183612)[0m mae:  0.10952405631542206
[2m[36m(func pid=183612)[0m rmse_per_class: [0.116, 0.227, 0.029, 0.345, 0.054, 0.173, 0.272, 0.193, 0.194, 0.098]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.2046208679676056
[2m[36m(func pid=184036)[0m mae:  0.12448452413082123
[2m[36m(func pid=184036)[0m rmse_per_class: [0.134, 0.266, 0.118, 0.346, 0.061, 0.214, 0.255, 0.105, 0.148, 0.4]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5598 | Steps: 4 | Val loss: 0.4233 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4745 | Steps: 4 | Val loss: 0.3794 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3618 | Steps: 4 | Val loss: 0.3424 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3669 | Steps: 4 | Val loss: 0.6144 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 19:52:52 (running for 00:03:10.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.56  |  0.178 |                   26 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.482 |  0.157 |                   25 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.36  |  0.17  |                   25 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.38  |  0.205 |                   24 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17773963510990143
[2m[36m(func pid=182810)[0m mae:  0.1302492767572403
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.258, 0.095, 0.334, 0.101, 0.19, 0.29, 0.138, 0.144, 0.109]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1555313616991043
[2m[36m(func pid=183191)[0m mae:  0.11013925075531006
[2m[36m(func pid=183191)[0m rmse_per_class: [0.106, 0.249, 0.053, 0.302, 0.057, 0.177, 0.241, 0.126, 0.145, 0.1]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.17070619761943817
[2m[36m(func pid=183612)[0m mae:  0.10941821336746216
[2m[36m(func pid=183612)[0m rmse_per_class: [0.118, 0.227, 0.031, 0.337, 0.054, 0.17, 0.267, 0.217, 0.185, 0.103]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.21329264342784882
[2m[36m(func pid=184036)[0m mae:  0.1345580518245697
[2m[36m(func pid=184036)[0m rmse_per_class: [0.158, 0.25, 0.067, 0.346, 0.069, 0.219, 0.289, 0.127, 0.211, 0.398]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5468 | Steps: 4 | Val loss: 0.4138 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4806 | Steps: 4 | Val loss: 0.3786 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3365 | Steps: 4 | Val loss: 0.3308 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3743 | Steps: 4 | Val loss: 0.3814 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=182810)[0m rmse: 0.17773719131946564
[2m[36m(func pid=182810)[0m mae:  0.1302424818277359
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.259, 0.095, 0.334, 0.1, 0.191, 0.289, 0.138, 0.145, 0.11]
[2m[36m(func pid=182810)[0m 
== Status ==
Current time: 2024-01-07 19:52:58 (running for 00:03:15.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.547 |  0.178 |                   27 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.474 |  0.156 |                   26 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.362 |  0.171 |                   26 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.367 |  0.213 |                   25 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.1537843495607376
[2m[36m(func pid=183191)[0m mae:  0.10857687145471573
[2m[36m(func pid=183191)[0m rmse_per_class: [0.105, 0.248, 0.05, 0.296, 0.057, 0.176, 0.24, 0.126, 0.143, 0.098]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1715763509273529
[2m[36m(func pid=183612)[0m mae:  0.10992525517940521
[2m[36m(func pid=183612)[0m rmse_per_class: [0.114, 0.224, 0.041, 0.321, 0.053, 0.172, 0.266, 0.243, 0.168, 0.113]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1879081428050995
[2m[36m(func pid=184036)[0m mae:  0.11824903637170792
[2m[36m(func pid=184036)[0m rmse_per_class: [0.185, 0.284, 0.05, 0.363, 0.086, 0.197, 0.236, 0.105, 0.199, 0.175]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5346 | Steps: 4 | Val loss: 0.4048 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4773 | Steps: 4 | Val loss: 0.3785 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3448 | Steps: 4 | Val loss: 0.3323 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3647 | Steps: 4 | Val loss: 0.4965 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 19:53:03 (running for 00:03:21.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.535 |  0.178 |                   28 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.481 |  0.154 |                   27 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.336 |  0.172 |                   27 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.374 |  0.188 |                   26 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17758341133594513
[2m[36m(func pid=182810)[0m mae:  0.13011792302131653
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.258, 0.095, 0.334, 0.098, 0.191, 0.289, 0.137, 0.145, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1529364436864853
[2m[36m(func pid=183191)[0m mae:  0.10784339904785156
[2m[36m(func pid=183191)[0m rmse_per_class: [0.104, 0.247, 0.049, 0.292, 0.056, 0.176, 0.241, 0.125, 0.142, 0.098]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.17515411972999573
[2m[36m(func pid=183612)[0m mae:  0.11180196702480316
[2m[36m(func pid=183612)[0m rmse_per_class: [0.122, 0.226, 0.046, 0.32, 0.053, 0.178, 0.272, 0.249, 0.153, 0.134]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20065398514270782
[2m[36m(func pid=184036)[0m mae:  0.12159129232168198
[2m[36m(func pid=184036)[0m rmse_per_class: [0.153, 0.278, 0.059, 0.374, 0.09, 0.202, 0.238, 0.182, 0.199, 0.232]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5193 | Steps: 4 | Val loss: 0.3964 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4820 | Steps: 4 | Val loss: 0.3753 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3602 | Steps: 4 | Val loss: 0.3234 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3898 | Steps: 4 | Val loss: 0.4766 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=183612)[0m rmse: 0.17339977622032166
[2m[36m(func pid=183612)[0m mae:  0.11008652299642563
[2m[36m(func pid=183612)[0m rmse_per_class: [0.119, 0.226, 0.048, 0.318, 0.052, 0.174, 0.272, 0.241, 0.143, 0.14]
[2m[36m(func pid=183612)[0m 
== Status ==
Current time: 2024-01-07 19:53:08 (running for 00:03:26.56)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.535 |  0.178 |                   28 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.477 |  0.153 |                   28 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.36  |  0.173 |                   29 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.365 |  0.201 |                   27 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.1774730086326599
[2m[36m(func pid=182810)[0m mae:  0.1299942433834076
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.258, 0.096, 0.334, 0.098, 0.19, 0.289, 0.137, 0.145, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.15226119756698608
[2m[36m(func pid=183191)[0m mae:  0.10686057806015015
[2m[36m(func pid=183191)[0m rmse_per_class: [0.101, 0.247, 0.047, 0.292, 0.055, 0.177, 0.238, 0.126, 0.142, 0.098]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.200938418507576
[2m[36m(func pid=184036)[0m mae:  0.11867649853229523
[2m[36m(func pid=184036)[0m rmse_per_class: [0.187, 0.277, 0.116, 0.37, 0.083, 0.206, 0.231, 0.236, 0.145, 0.159]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3277 | Steps: 4 | Val loss: 0.3083 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5105 | Steps: 4 | Val loss: 0.3897 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4682 | Steps: 4 | Val loss: 0.3711 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3551 | Steps: 4 | Val loss: 0.4284 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 19:53:13 (running for 00:03:31.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.519 |  0.177 |                   29 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.482 |  0.152 |                   29 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.328 |  0.168 |                   30 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.39  |  0.201 |                   28 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16847816109657288
[2m[36m(func pid=183612)[0m mae:  0.10699834674596786
[2m[36m(func pid=183612)[0m rmse_per_class: [0.103, 0.238, 0.054, 0.31, 0.052, 0.172, 0.27, 0.194, 0.137, 0.156]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1777425855398178
[2m[36m(func pid=182810)[0m mae:  0.13029707968235016
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.259, 0.095, 0.334, 0.098, 0.19, 0.29, 0.137, 0.146, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.15102465450763702
[2m[36m(func pid=183191)[0m mae:  0.10563677549362183
[2m[36m(func pid=183191)[0m rmse_per_class: [0.1, 0.245, 0.045, 0.289, 0.055, 0.176, 0.238, 0.126, 0.14, 0.096]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.20045408606529236
[2m[36m(func pid=184036)[0m mae:  0.11749575287103653
[2m[36m(func pid=184036)[0m rmse_per_class: [0.225, 0.256, 0.128, 0.369, 0.085, 0.19, 0.228, 0.24, 0.154, 0.131]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3144 | Steps: 4 | Val loss: 0.2977 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4636 | Steps: 4 | Val loss: 0.3681 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5037 | Steps: 4 | Val loss: 0.3820 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3390 | Steps: 4 | Val loss: 0.4131 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 19:53:19 (running for 00:03:37.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.504 |  0.178 |                   31 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.468 |  0.151 |                   30 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.328 |  0.168 |                   30 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.355 |  0.2   |                   29 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=182810)[0m rmse: 0.17790552973747253
[2m[36m(func pid=182810)[0m mae:  0.13041424751281738
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.096, 0.334, 0.097, 0.19, 0.289, 0.138, 0.146, 0.112]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1503077745437622
[2m[36m(func pid=183191)[0m mae:  0.10486414283514023
[2m[36m(func pid=183191)[0m rmse_per_class: [0.099, 0.244, 0.044, 0.29, 0.055, 0.176, 0.236, 0.125, 0.14, 0.095]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.16633939743041992
[2m[36m(func pid=183612)[0m mae:  0.10396891832351685
[2m[36m(func pid=183612)[0m rmse_per_class: [0.091, 0.249, 0.065, 0.297, 0.053, 0.174, 0.261, 0.184, 0.135, 0.154]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.2014186829328537
[2m[36m(func pid=184036)[0m mae:  0.12057618051767349
[2m[36m(func pid=184036)[0m rmse_per_class: [0.284, 0.234, 0.14, 0.365, 0.076, 0.185, 0.244, 0.148, 0.214, 0.125]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4556 | Steps: 4 | Val loss: 0.3617 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4893 | Steps: 4 | Val loss: 0.3759 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3140 | Steps: 4 | Val loss: 0.2860 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3587 | Steps: 4 | Val loss: 0.3904 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 19:53:24 (running for 00:03:42.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.504 |  0.178 |                   31 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.456 |  0.149 |                   32 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.314 |  0.166 |                   31 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.339 |  0.201 |                   30 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.1490193009376526
[2m[36m(func pid=183191)[0m mae:  0.10368822515010834
[2m[36m(func pid=183191)[0m rmse_per_class: [0.097, 0.242, 0.042, 0.289, 0.055, 0.174, 0.235, 0.125, 0.14, 0.092]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.177977055311203
[2m[36m(func pid=182810)[0m mae:  0.13052988052368164
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.26, 0.096, 0.335, 0.095, 0.19, 0.288, 0.139, 0.146, 0.113]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15890441834926605
[2m[36m(func pid=183612)[0m mae:  0.09818151593208313
[2m[36m(func pid=183612)[0m rmse_per_class: [0.091, 0.245, 0.077, 0.288, 0.06, 0.163, 0.244, 0.144, 0.132, 0.143]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.19633319973945618
[2m[36m(func pid=184036)[0m mae:  0.11851565539836884
[2m[36m(func pid=184036)[0m rmse_per_class: [0.285, 0.283, 0.079, 0.355, 0.072, 0.189, 0.245, 0.117, 0.22, 0.118]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4480 | Steps: 4 | Val loss: 0.3556 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4797 | Steps: 4 | Val loss: 0.3693 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3182 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3563 | Steps: 4 | Val loss: 0.3790 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 19:53:30 (running for 00:03:47.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.489 |  0.178 |                   32 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.448 |  0.149 |                   33 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.314 |  0.159 |                   32 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.359 |  0.196 |                   31 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.1485084742307663
[2m[36m(func pid=183191)[0m mae:  0.10345053672790527
[2m[36m(func pid=183191)[0m rmse_per_class: [0.097, 0.241, 0.041, 0.287, 0.055, 0.174, 0.234, 0.124, 0.14, 0.093]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17803089320659637
[2m[36m(func pid=182810)[0m mae:  0.1304844319820404
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.259, 0.096, 0.334, 0.095, 0.19, 0.287, 0.139, 0.146, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15517988801002502
[2m[36m(func pid=183612)[0m mae:  0.09386280924081802
[2m[36m(func pid=183612)[0m rmse_per_class: [0.085, 0.247, 0.084, 0.282, 0.07, 0.158, 0.232, 0.123, 0.13, 0.139]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.19135969877243042
[2m[36m(func pid=184036)[0m mae:  0.116024449467659
[2m[36m(func pid=184036)[0m rmse_per_class: [0.212, 0.318, 0.038, 0.348, 0.066, 0.2, 0.244, 0.107, 0.27, 0.11]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4446 | Steps: 4 | Val loss: 0.3488 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4720 | Steps: 4 | Val loss: 0.3641 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3218 | Steps: 4 | Val loss: 0.2772 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3387 | Steps: 4 | Val loss: 0.3502 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 19:53:35 (running for 00:03:53.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.48  |  0.178 |                   33 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.445 |  0.148 |                   34 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.318 |  0.155 |                   33 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.356 |  0.191 |                   32 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.14799153804779053
[2m[36m(func pid=183191)[0m mae:  0.10249022394418716
[2m[36m(func pid=183191)[0m rmse_per_class: [0.095, 0.239, 0.04, 0.288, 0.055, 0.175, 0.234, 0.124, 0.138, 0.091]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17760074138641357
[2m[36m(func pid=182810)[0m mae:  0.13013425469398499
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.259, 0.094, 0.334, 0.095, 0.191, 0.286, 0.138, 0.146, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15327751636505127
[2m[36m(func pid=183612)[0m mae:  0.09133228659629822
[2m[36m(func pid=183612)[0m rmse_per_class: [0.082, 0.241, 0.071, 0.277, 0.09, 0.162, 0.22, 0.121, 0.131, 0.137]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1813822090625763
[2m[36m(func pid=184036)[0m mae:  0.10928314924240112
[2m[36m(func pid=184036)[0m rmse_per_class: [0.191, 0.309, 0.038, 0.349, 0.066, 0.18, 0.231, 0.136, 0.219, 0.094]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4387 | Steps: 4 | Val loss: 0.3412 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3385 | Steps: 4 | Val loss: 0.2757 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4649 | Steps: 4 | Val loss: 0.3596 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3458 | Steps: 4 | Val loss: 0.3470 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=183191)[0m rmse: 0.1475076526403427
[2m[36m(func pid=183191)[0m mae:  0.10177452862262726
[2m[36m(func pid=183191)[0m rmse_per_class: [0.094, 0.238, 0.039, 0.288, 0.054, 0.174, 0.235, 0.124, 0.137, 0.09]
[2m[36m(func pid=183191)[0m 
== Status ==
Current time: 2024-01-07 19:53:40 (running for 00:03:58.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.472 |  0.178 |                   34 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.439 |  0.148 |                   35 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.322 |  0.153 |                   34 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.339 |  0.181 |                   33 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1528715193271637
[2m[36m(func pid=183612)[0m mae:  0.08969710022211075
[2m[36m(func pid=183612)[0m rmse_per_class: [0.084, 0.241, 0.092, 0.278, 0.1, 0.157, 0.212, 0.105, 0.131, 0.128]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17775604128837585
[2m[36m(func pid=182810)[0m mae:  0.13029208779335022
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.26, 0.094, 0.334, 0.094, 0.19, 0.286, 0.138, 0.147, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1847730576992035
[2m[36m(func pid=184036)[0m mae:  0.11095412820577621
[2m[36m(func pid=184036)[0m rmse_per_class: [0.216, 0.291, 0.06, 0.354, 0.063, 0.176, 0.227, 0.18, 0.169, 0.111]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4395 | Steps: 4 | Val loss: 0.3314 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3129 | Steps: 4 | Val loss: 0.2780 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4549 | Steps: 4 | Val loss: 0.3542 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3379 | Steps: 4 | Val loss: 0.3491 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 19:53:46 (running for 00:04:03.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.465 |  0.178 |                   35 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.439 |  0.147 |                   36 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.338 |  0.153 |                   35 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.346 |  0.185 |                   34 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.14695625007152557
[2m[36m(func pid=183191)[0m mae:  0.10105440765619278
[2m[36m(func pid=183191)[0m rmse_per_class: [0.093, 0.238, 0.038, 0.289, 0.054, 0.174, 0.233, 0.123, 0.137, 0.091]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15668292343616486
[2m[36m(func pid=183612)[0m mae:  0.0924939438700676
[2m[36m(func pid=183612)[0m rmse_per_class: [0.106, 0.232, 0.11, 0.285, 0.128, 0.163, 0.214, 0.093, 0.128, 0.109]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17767676711082458
[2m[36m(func pid=182810)[0m mae:  0.13020461797714233
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.26, 0.095, 0.333, 0.093, 0.191, 0.286, 0.138, 0.146, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1927337646484375
[2m[36m(func pid=184036)[0m mae:  0.11495780944824219
[2m[36m(func pid=184036)[0m rmse_per_class: [0.236, 0.278, 0.072, 0.357, 0.076, 0.185, 0.23, 0.187, 0.152, 0.155]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4246 | Steps: 4 | Val loss: 0.3229 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3269 | Steps: 4 | Val loss: 0.2782 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4521 | Steps: 4 | Val loss: 0.3483 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3217 | Steps: 4 | Val loss: 0.3466 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 19:53:51 (running for 00:04:08.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.455 |  0.178 |                   36 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.425 |  0.147 |                   37 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.313 |  0.157 |                   36 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.338 |  0.193 |                   35 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.14701758325099945
[2m[36m(func pid=183191)[0m mae:  0.10051310062408447
[2m[36m(func pid=183191)[0m rmse_per_class: [0.09, 0.239, 0.037, 0.29, 0.055, 0.174, 0.233, 0.124, 0.136, 0.091]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1574171483516693
[2m[36m(func pid=183612)[0m mae:  0.09267742186784744
[2m[36m(func pid=183612)[0m rmse_per_class: [0.106, 0.224, 0.11, 0.289, 0.145, 0.166, 0.209, 0.099, 0.126, 0.1]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17743347585201263
[2m[36m(func pid=182810)[0m mae:  0.1299583911895752
[2m[36m(func pid=182810)[0m rmse_per_class: [0.12, 0.26, 0.095, 0.332, 0.092, 0.191, 0.286, 0.139, 0.146, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.19621847569942474
[2m[36m(func pid=184036)[0m mae:  0.11531277745962143
[2m[36m(func pid=184036)[0m rmse_per_class: [0.219, 0.27, 0.098, 0.354, 0.091, 0.186, 0.231, 0.2, 0.151, 0.163]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4184 | Steps: 4 | Val loss: 0.3163 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3197 | Steps: 4 | Val loss: 0.2765 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3736 | Steps: 4 | Val loss: 0.3318 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4448 | Steps: 4 | Val loss: 0.3429 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 19:53:56 (running for 00:04:14.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.452 |  0.177 |                   37 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.425 |  0.147 |                   37 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.32  |  0.153 |                   38 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.322 |  0.196 |                   36 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15295282006263733
[2m[36m(func pid=183612)[0m mae:  0.09007156640291214
[2m[36m(func pid=183612)[0m rmse_per_class: [0.081, 0.21, 0.081, 0.288, 0.157, 0.165, 0.208, 0.106, 0.127, 0.107]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14661751687526703
[2m[36m(func pid=183191)[0m mae:  0.10013069212436676
[2m[36m(func pid=183191)[0m rmse_per_class: [0.09, 0.238, 0.036, 0.291, 0.054, 0.175, 0.231, 0.122, 0.137, 0.092]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.19060161709785461
[2m[36m(func pid=184036)[0m mae:  0.1125330924987793
[2m[36m(func pid=184036)[0m rmse_per_class: [0.166, 0.248, 0.072, 0.334, 0.106, 0.187, 0.243, 0.239, 0.161, 0.149]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17713381350040436
[2m[36m(func pid=182810)[0m mae:  0.12965744733810425
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.26, 0.095, 0.332, 0.091, 0.19, 0.285, 0.138, 0.146, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3038 | Steps: 4 | Val loss: 0.2870 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3988 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3479 | Steps: 4 | Val loss: 0.3192 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4393 | Steps: 4 | Val loss: 0.3372 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 19:54:01 (running for 00:04:19.50)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.445 |  0.177 |                   38 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.418 |  0.147 |                   38 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.304 |  0.156 |                   39 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.374 |  0.191 |                   37 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15557168424129486
[2m[36m(func pid=183612)[0m mae:  0.0915006473660469
[2m[36m(func pid=183612)[0m rmse_per_class: [0.071, 0.21, 0.049, 0.304, 0.155, 0.164, 0.216, 0.115, 0.163, 0.108]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1463133692741394
[2m[36m(func pid=183191)[0m mae:  0.09985913336277008
[2m[36m(func pid=183191)[0m rmse_per_class: [0.089, 0.237, 0.035, 0.292, 0.054, 0.175, 0.231, 0.122, 0.137, 0.091]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.18231749534606934
[2m[36m(func pid=184036)[0m mae:  0.11107710748910904
[2m[36m(func pid=184036)[0m rmse_per_class: [0.138, 0.237, 0.06, 0.331, 0.105, 0.18, 0.257, 0.168, 0.181, 0.166]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1765502244234085
[2m[36m(func pid=182810)[0m mae:  0.129198357462883
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.259, 0.092, 0.331, 0.092, 0.19, 0.284, 0.138, 0.146, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2847 | Steps: 4 | Val loss: 0.2976 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4118 | Steps: 4 | Val loss: 0.3053 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3427 | Steps: 4 | Val loss: 0.3182 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4367 | Steps: 4 | Val loss: 0.3331 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 19:54:07 (running for 00:04:24.78)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.439 |  0.177 |                   39 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.399 |  0.146 |                   39 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.285 |  0.16  |                   40 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.348 |  0.182 |                   38 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1599988043308258
[2m[36m(func pid=183612)[0m mae:  0.09347479790449142
[2m[36m(func pid=183612)[0m rmse_per_class: [0.072, 0.211, 0.032, 0.32, 0.155, 0.167, 0.222, 0.116, 0.195, 0.109]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14568480849266052
[2m[36m(func pid=183191)[0m mae:  0.0991704910993576
[2m[36m(func pid=183191)[0m rmse_per_class: [0.089, 0.234, 0.034, 0.292, 0.055, 0.174, 0.232, 0.12, 0.136, 0.09]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17703816294670105
[2m[36m(func pid=184036)[0m mae:  0.10869588702917099
[2m[36m(func pid=184036)[0m rmse_per_class: [0.124, 0.23, 0.056, 0.336, 0.111, 0.17, 0.251, 0.177, 0.201, 0.115]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1761883795261383
[2m[36m(func pid=182810)[0m mae:  0.1288105994462967
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.092, 0.331, 0.089, 0.19, 0.284, 0.136, 0.147, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3309 | Steps: 4 | Val loss: 0.3047 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3837 | Steps: 4 | Val loss: 0.2983 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3504 | Steps: 4 | Val loss: 0.3154 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4291 | Steps: 4 | Val loss: 0.3320 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 19:54:12 (running for 00:04:30.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.437 |  0.176 |                   40 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.412 |  0.146 |                   40 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.331 |  0.162 |                   41 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.343 |  0.177 |                   39 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1622323989868164
[2m[36m(func pid=183612)[0m mae:  0.09447480738162994
[2m[36m(func pid=183612)[0m rmse_per_class: [0.075, 0.218, 0.028, 0.321, 0.154, 0.179, 0.237, 0.117, 0.198, 0.097]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14558300375938416
[2m[36m(func pid=183191)[0m mae:  0.09893061220645905
[2m[36m(func pid=183191)[0m rmse_per_class: [0.09, 0.233, 0.034, 0.294, 0.055, 0.174, 0.23, 0.121, 0.136, 0.09]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17420318722724915
[2m[36m(func pid=184036)[0m mae:  0.10690964758396149
[2m[36m(func pid=184036)[0m rmse_per_class: [0.13, 0.222, 0.059, 0.338, 0.107, 0.165, 0.241, 0.203, 0.184, 0.095]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17692679166793823
[2m[36m(func pid=182810)[0m mae:  0.12944379448890686
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.26, 0.094, 0.332, 0.088, 0.19, 0.285, 0.137, 0.147, 0.117]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2856 | Steps: 4 | Val loss: 0.2969 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3843 | Steps: 4 | Val loss: 0.2942 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4235 | Steps: 4 | Val loss: 0.3279 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3430 | Steps: 4 | Val loss: 0.3129 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 19:54:17 (running for 00:04:35.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.429 |  0.177 |                   41 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.384 |  0.146 |                   41 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.286 |  0.157 |                   42 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.35  |  0.174 |                   40 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15717673301696777
[2m[36m(func pid=183612)[0m mae:  0.09203307330608368
[2m[36m(func pid=183612)[0m rmse_per_class: [0.075, 0.215, 0.028, 0.313, 0.144, 0.177, 0.227, 0.124, 0.177, 0.091]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.145534485578537
[2m[36m(func pid=183191)[0m mae:  0.09929174184799194
[2m[36m(func pid=183191)[0m rmse_per_class: [0.091, 0.232, 0.033, 0.297, 0.055, 0.174, 0.229, 0.118, 0.137, 0.091]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.176441490650177
[2m[36m(func pid=182810)[0m mae:  0.12900951504707336
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.26, 0.093, 0.331, 0.089, 0.19, 0.284, 0.138, 0.145, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1703154444694519
[2m[36m(func pid=184036)[0m mae:  0.10539858043193817
[2m[36m(func pid=184036)[0m rmse_per_class: [0.138, 0.217, 0.053, 0.325, 0.076, 0.171, 0.231, 0.233, 0.165, 0.095]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3036 | Steps: 4 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3632 | Steps: 4 | Val loss: 0.2882 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3389 | Steps: 4 | Val loss: 0.3139 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4220 | Steps: 4 | Val loss: 0.3250 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 19:54:23 (running for 00:04:40.78)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.423 |  0.176 |                   42 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.384 |  0.146 |                   42 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.304 |  0.154 |                   43 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.343 |  0.17  |                   41 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15366923809051514
[2m[36m(func pid=183612)[0m mae:  0.08994344621896744
[2m[36m(func pid=183612)[0m rmse_per_class: [0.08, 0.213, 0.029, 0.3, 0.148, 0.171, 0.206, 0.148, 0.151, 0.09]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1451534926891327
[2m[36m(func pid=183191)[0m mae:  0.09906670451164246
[2m[36m(func pid=183191)[0m rmse_per_class: [0.091, 0.233, 0.032, 0.294, 0.055, 0.173, 0.227, 0.118, 0.137, 0.092]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16882506012916565
[2m[36m(func pid=184036)[0m mae:  0.10518528521060944
[2m[36m(func pid=184036)[0m rmse_per_class: [0.133, 0.226, 0.048, 0.325, 0.074, 0.171, 0.23, 0.226, 0.159, 0.097]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17610017955303192
[2m[36m(func pid=182810)[0m mae:  0.12864306569099426
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.094, 0.331, 0.087, 0.19, 0.284, 0.136, 0.147, 0.116]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3279 | Steps: 4 | Val loss: 0.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3629 | Steps: 4 | Val loss: 0.2820 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3552 | Steps: 4 | Val loss: 0.3062 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4187 | Steps: 4 | Val loss: 0.3226 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 19:54:28 (running for 00:04:46.02)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.422 |  0.176 |                   43 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.363 |  0.145 |                   43 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.328 |  0.156 |                   44 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.339 |  0.169 |                   42 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15571747720241547
[2m[36m(func pid=183612)[0m mae:  0.0903703048825264
[2m[36m(func pid=183612)[0m rmse_per_class: [0.087, 0.208, 0.035, 0.293, 0.155, 0.163, 0.204, 0.171, 0.149, 0.094]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14387837052345276
[2m[36m(func pid=183191)[0m mae:  0.0982138067483902
[2m[36m(func pid=183191)[0m rmse_per_class: [0.092, 0.231, 0.032, 0.293, 0.055, 0.171, 0.221, 0.116, 0.137, 0.09]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16546477377414703
[2m[36m(func pid=184036)[0m mae:  0.10350976884365082
[2m[36m(func pid=184036)[0m rmse_per_class: [0.127, 0.219, 0.047, 0.324, 0.076, 0.168, 0.231, 0.209, 0.154, 0.1]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17570172250270844
[2m[36m(func pid=182810)[0m mae:  0.12835121154785156
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.093, 0.33, 0.087, 0.19, 0.283, 0.136, 0.146, 0.116]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3030 | Steps: 4 | Val loss: 0.2916 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3702 | Steps: 4 | Val loss: 0.2785 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3466 | Steps: 4 | Val loss: 0.2978 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4123 | Steps: 4 | Val loss: 0.3200 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 19:54:33 (running for 00:04:51.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.419 |  0.176 |                   44 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.363 |  0.144 |                   44 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.303 |  0.163 |                   45 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.355 |  0.165 |                   43 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16316258907318115
[2m[36m(func pid=183612)[0m mae:  0.09459422528743744
[2m[36m(func pid=183612)[0m rmse_per_class: [0.093, 0.21, 0.045, 0.297, 0.152, 0.169, 0.22, 0.181, 0.162, 0.103]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14433534443378448
[2m[36m(func pid=183191)[0m mae:  0.09824040532112122
[2m[36m(func pid=183191)[0m rmse_per_class: [0.091, 0.232, 0.031, 0.295, 0.055, 0.171, 0.221, 0.119, 0.138, 0.092]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1624649316072464
[2m[36m(func pid=184036)[0m mae:  0.1025926023721695
[2m[36m(func pid=184036)[0m rmse_per_class: [0.127, 0.214, 0.05, 0.326, 0.075, 0.166, 0.231, 0.171, 0.159, 0.105]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17540915310382843
[2m[36m(func pid=182810)[0m mae:  0.12812677025794983
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.092, 0.331, 0.086, 0.191, 0.28, 0.135, 0.146, 0.116]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2983 | Steps: 4 | Val loss: 0.2910 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3646 | Steps: 4 | Val loss: 0.2758 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3339 | Steps: 4 | Val loss: 0.2889 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4135 | Steps: 4 | Val loss: 0.3186 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 19:54:38 (running for 00:04:56.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.412 |  0.175 |                   45 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.37  |  0.144 |                   45 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.298 |  0.165 |                   46 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.347 |  0.162 |                   44 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16523221135139465
[2m[36m(func pid=183612)[0m mae:  0.09694787859916687
[2m[36m(func pid=183612)[0m rmse_per_class: [0.089, 0.212, 0.054, 0.299, 0.158, 0.172, 0.237, 0.162, 0.162, 0.108]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14496400952339172
[2m[36m(func pid=183191)[0m mae:  0.09847461432218552
[2m[36m(func pid=183191)[0m rmse_per_class: [0.091, 0.233, 0.03, 0.298, 0.055, 0.169, 0.221, 0.122, 0.138, 0.093]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1585783064365387
[2m[36m(func pid=184036)[0m mae:  0.10243536531925201
[2m[36m(func pid=184036)[0m rmse_per_class: [0.112, 0.218, 0.049, 0.32, 0.07, 0.17, 0.23, 0.138, 0.164, 0.114]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1751331388950348
[2m[36m(func pid=182810)[0m mae:  0.12791089713573456
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.09, 0.33, 0.086, 0.191, 0.28, 0.135, 0.146, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3065 | Steps: 4 | Val loss: 0.2928 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3548 | Steps: 4 | Val loss: 0.2726 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3495 | Steps: 4 | Val loss: 0.2820 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4124 | Steps: 4 | Val loss: 0.3168 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 19:54:44 (running for 00:05:01.91)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.414 |  0.175 |                   46 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.365 |  0.145 |                   46 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.307 |  0.168 |                   47 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.334 |  0.159 |                   45 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16802456974983215
[2m[36m(func pid=183612)[0m mae:  0.09812070429325104
[2m[36m(func pid=183612)[0m rmse_per_class: [0.089, 0.212, 0.072, 0.303, 0.172, 0.174, 0.238, 0.142, 0.165, 0.112]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1443553864955902
[2m[36m(func pid=183191)[0m mae:  0.09871320426464081
[2m[36m(func pid=183191)[0m rmse_per_class: [0.096, 0.229, 0.031, 0.297, 0.055, 0.168, 0.22, 0.113, 0.14, 0.095]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1566585898399353
[2m[36m(func pid=184036)[0m mae:  0.10110682249069214
[2m[36m(func pid=184036)[0m rmse_per_class: [0.098, 0.217, 0.049, 0.311, 0.075, 0.173, 0.231, 0.14, 0.154, 0.118]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17480389773845673
[2m[36m(func pid=182810)[0m mae:  0.1275758296251297
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.09, 0.329, 0.085, 0.191, 0.278, 0.135, 0.147, 0.116]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3087 | Steps: 4 | Val loss: 0.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3583 | Steps: 4 | Val loss: 0.2701 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3460 | Steps: 4 | Val loss: 0.2823 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4091 | Steps: 4 | Val loss: 0.3162 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 19:54:49 (running for 00:05:07.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.412 |  0.175 |                   47 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.355 |  0.144 |                   47 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.309 |  0.164 |                   48 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.35  |  0.157 |                   46 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.163803368806839
[2m[36m(func pid=183612)[0m mae:  0.09636270254850388
[2m[36m(func pid=183612)[0m rmse_per_class: [0.081, 0.214, 0.069, 0.296, 0.134, 0.174, 0.243, 0.134, 0.164, 0.129]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1437353491783142
[2m[36m(func pid=183191)[0m mae:  0.09857811033725739
[2m[36m(func pid=183191)[0m rmse_per_class: [0.095, 0.229, 0.03, 0.295, 0.055, 0.168, 0.221, 0.111, 0.14, 0.094]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15590450167655945
[2m[36m(func pid=184036)[0m mae:  0.10113607347011566
[2m[36m(func pid=184036)[0m rmse_per_class: [0.102, 0.221, 0.054, 0.312, 0.079, 0.173, 0.243, 0.109, 0.149, 0.118]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17465321719646454
[2m[36m(func pid=182810)[0m mae:  0.12743189930915833
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.09, 0.329, 0.084, 0.191, 0.279, 0.135, 0.147, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2838 | Steps: 4 | Val loss: 0.2809 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3455 | Steps: 4 | Val loss: 0.2674 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3230 | Steps: 4 | Val loss: 0.2840 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4072 | Steps: 4 | Val loss: 0.3138 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 19:54:54 (running for 00:05:12.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.409 |  0.175 |                   48 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.358 |  0.144 |                   48 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.284 |  0.159 |                   49 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.346 |  0.156 |                   47 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15901610255241394
[2m[36m(func pid=183612)[0m mae:  0.09488797187805176
[2m[36m(func pid=183612)[0m rmse_per_class: [0.073, 0.222, 0.062, 0.293, 0.107, 0.172, 0.246, 0.115, 0.153, 0.148]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14320747554302216
[2m[36m(func pid=183191)[0m mae:  0.09802073985338211
[2m[36m(func pid=183191)[0m rmse_per_class: [0.094, 0.228, 0.03, 0.294, 0.055, 0.166, 0.221, 0.113, 0.139, 0.092]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15745951235294342
[2m[36m(func pid=184036)[0m mae:  0.10051894187927246
[2m[36m(func pid=184036)[0m rmse_per_class: [0.109, 0.222, 0.068, 0.313, 0.09, 0.174, 0.257, 0.105, 0.133, 0.103]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17407278716564178
[2m[36m(func pid=182810)[0m mae:  0.12692277133464813
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.089, 0.329, 0.084, 0.19, 0.277, 0.135, 0.146, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2892 | Steps: 4 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3409 | Steps: 4 | Val loss: 0.2648 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3094 | Steps: 4 | Val loss: 0.2892 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4041 | Steps: 4 | Val loss: 0.3135 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 19:55:00 (running for 00:05:17.76)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.407 |  0.174 |                   49 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.346 |  0.143 |                   49 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.289 |  0.16  |                   50 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.323 |  0.157 |                   48 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15972639620304108
[2m[36m(func pid=183612)[0m mae:  0.09658375382423401
[2m[36m(func pid=183612)[0m rmse_per_class: [0.072, 0.232, 0.069, 0.3, 0.094, 0.174, 0.243, 0.106, 0.155, 0.152]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1424543261528015
[2m[36m(func pid=183191)[0m mae:  0.09749729931354523
[2m[36m(func pid=183191)[0m rmse_per_class: [0.092, 0.227, 0.031, 0.292, 0.054, 0.163, 0.223, 0.114, 0.138, 0.092]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16075453162193298
[2m[36m(func pid=184036)[0m mae:  0.10125599056482315
[2m[36m(func pid=184036)[0m rmse_per_class: [0.113, 0.226, 0.069, 0.313, 0.111, 0.179, 0.256, 0.112, 0.138, 0.091]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1740453541278839
[2m[36m(func pid=182810)[0m mae:  0.12696000933647156
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.258, 0.088, 0.329, 0.084, 0.189, 0.277, 0.135, 0.146, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2990 | Steps: 4 | Val loss: 0.2876 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3729 | Steps: 4 | Val loss: 0.2633 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3025 | Steps: 4 | Val loss: 0.2915 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4033 | Steps: 4 | Val loss: 0.3134 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:55:05 (running for 00:05:23.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.404 |  0.174 |                   50 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.341 |  0.142 |                   50 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.299 |  0.161 |                   51 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.309 |  0.161 |                   49 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16091281175613403
[2m[36m(func pid=183612)[0m mae:  0.09737040102481842
[2m[36m(func pid=183612)[0m rmse_per_class: [0.073, 0.245, 0.074, 0.305, 0.082, 0.173, 0.237, 0.111, 0.17, 0.138]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14166896045207977
[2m[36m(func pid=183191)[0m mae:  0.09652118384838104
[2m[36m(func pid=183191)[0m rmse_per_class: [0.088, 0.225, 0.031, 0.29, 0.055, 0.162, 0.224, 0.116, 0.136, 0.091]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16360196471214294
[2m[36m(func pid=184036)[0m mae:  0.10109038650989532
[2m[36m(func pid=184036)[0m rmse_per_class: [0.12, 0.229, 0.072, 0.312, 0.118, 0.175, 0.248, 0.135, 0.141, 0.087]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17424556612968445
[2m[36m(func pid=182810)[0m mae:  0.12720605731010437
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.259, 0.089, 0.328, 0.082, 0.19, 0.278, 0.135, 0.146, 0.116]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2917 | Steps: 4 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3842 | Steps: 4 | Val loss: 0.2632 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3230 | Steps: 4 | Val loss: 0.2973 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4026 | Steps: 4 | Val loss: 0.3132 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 19:55:10 (running for 00:05:28.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.403 |  0.174 |                   51 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.373 |  0.142 |                   51 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.292 |  0.159 |                   52 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.302 |  0.164 |                   50 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15945729613304138
[2m[36m(func pid=183612)[0m mae:  0.09573202580213547
[2m[36m(func pid=183612)[0m rmse_per_class: [0.073, 0.26, 0.071, 0.307, 0.071, 0.164, 0.225, 0.111, 0.167, 0.144]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14176355302333832
[2m[36m(func pid=183191)[0m mae:  0.0962824746966362
[2m[36m(func pid=183191)[0m rmse_per_class: [0.088, 0.224, 0.029, 0.291, 0.055, 0.162, 0.223, 0.117, 0.136, 0.093]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16652365028858185
[2m[36m(func pid=184036)[0m mae:  0.1011117473244667
[2m[36m(func pid=184036)[0m rmse_per_class: [0.141, 0.237, 0.071, 0.316, 0.12, 0.176, 0.244, 0.128, 0.14, 0.092]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1742294728755951
[2m[36m(func pid=182810)[0m mae:  0.12718257308006287
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.259, 0.089, 0.328, 0.082, 0.19, 0.278, 0.136, 0.145, 0.115]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2935 | Steps: 4 | Val loss: 0.2846 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3482 | Steps: 4 | Val loss: 0.2633 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3065 | Steps: 4 | Val loss: 0.2951 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4012 | Steps: 4 | Val loss: 0.3124 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 19:55:15 (running for 00:05:33.49)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.403 |  0.174 |                   52 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.384 |  0.142 |                   52 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.293 |  0.156 |                   53 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.323 |  0.167 |                   51 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15630194544792175
[2m[36m(func pid=183612)[0m mae:  0.09399522840976715
[2m[36m(func pid=183612)[0m rmse_per_class: [0.072, 0.257, 0.075, 0.306, 0.064, 0.159, 0.219, 0.112, 0.157, 0.14]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14175771176815033
[2m[36m(func pid=183191)[0m mae:  0.09674854576587677
[2m[36m(func pid=183191)[0m rmse_per_class: [0.091, 0.225, 0.028, 0.29, 0.054, 0.162, 0.223, 0.113, 0.138, 0.094]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16525204479694366
[2m[36m(func pid=184036)[0m mae:  0.09990936517715454
[2m[36m(func pid=184036)[0m rmse_per_class: [0.14, 0.247, 0.072, 0.307, 0.11, 0.161, 0.245, 0.134, 0.139, 0.098]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17381054162979126
[2m[36m(func pid=182810)[0m mae:  0.12682044506072998
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.259, 0.089, 0.328, 0.082, 0.189, 0.277, 0.136, 0.144, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3027 | Steps: 4 | Val loss: 0.2904 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3334 | Steps: 4 | Val loss: 0.2636 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3072 | Steps: 4 | Val loss: 0.2956 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4001 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 19:55:20 (running for 00:05:38.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.401 |  0.174 |                   53 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.348 |  0.142 |                   53 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.303 |  0.16  |                   54 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.307 |  0.165 |                   52 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15953512489795685
[2m[36m(func pid=183612)[0m mae:  0.09586699306964874
[2m[36m(func pid=183612)[0m rmse_per_class: [0.072, 0.242, 0.089, 0.311, 0.063, 0.161, 0.219, 0.116, 0.172, 0.15]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14189782738685608
[2m[36m(func pid=183191)[0m mae:  0.09716843068599701
[2m[36m(func pid=183191)[0m rmse_per_class: [0.093, 0.225, 0.028, 0.29, 0.054, 0.162, 0.225, 0.11, 0.138, 0.094]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16617894172668457
[2m[36m(func pid=184036)[0m mae:  0.09954395890235901
[2m[36m(func pid=184036)[0m rmse_per_class: [0.14, 0.248, 0.068, 0.294, 0.102, 0.167, 0.245, 0.141, 0.134, 0.124]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1737578809261322
[2m[36m(func pid=182810)[0m mae:  0.12681791186332703
[2m[36m(func pid=182810)[0m rmse_per_class: [0.12, 0.258, 0.087, 0.328, 0.082, 0.19, 0.278, 0.136, 0.145, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2939 | Steps: 4 | Val loss: 0.2929 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3426 | Steps: 4 | Val loss: 0.2642 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3360 | Steps: 4 | Val loss: 0.2954 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4040 | Steps: 4 | Val loss: 0.3112 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 19:55:26 (running for 00:05:44.21)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.4   |  0.174 |                   54 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.333 |  0.142 |                   54 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.294 |  0.161 |                   55 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.307 |  0.166 |                   53 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1606186032295227
[2m[36m(func pid=183612)[0m mae:  0.09672597795724869
[2m[36m(func pid=183612)[0m rmse_per_class: [0.072, 0.227, 0.1, 0.312, 0.062, 0.162, 0.22, 0.118, 0.172, 0.162]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14192616939544678
[2m[36m(func pid=183191)[0m mae:  0.09754060208797455
[2m[36m(func pid=183191)[0m rmse_per_class: [0.093, 0.224, 0.027, 0.289, 0.054, 0.163, 0.227, 0.107, 0.139, 0.096]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16241559386253357
[2m[36m(func pid=184036)[0m mae:  0.09833953529596329
[2m[36m(func pid=184036)[0m rmse_per_class: [0.127, 0.259, 0.057, 0.3, 0.086, 0.164, 0.24, 0.137, 0.142, 0.111]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1729545295238495
[2m[36m(func pid=182810)[0m mae:  0.12607774138450623
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.258, 0.085, 0.328, 0.081, 0.19, 0.274, 0.136, 0.145, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2774 | Steps: 4 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3332 | Steps: 4 | Val loss: 0.3008 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3366 | Steps: 4 | Val loss: 0.2661 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3995 | Steps: 4 | Val loss: 0.3109 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 19:55:31 (running for 00:05:49.66)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.404 |  0.173 |                   55 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.343 |  0.142 |                   55 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.277 |  0.156 |                   56 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.336 |  0.162 |                   54 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15553860366344452
[2m[36m(func pid=183612)[0m mae:  0.09366998076438904
[2m[36m(func pid=183612)[0m rmse_per_class: [0.071, 0.224, 0.089, 0.304, 0.063, 0.16, 0.217, 0.117, 0.151, 0.159]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16538813710212708
[2m[36m(func pid=184036)[0m mae:  0.09962194412946701
[2m[36m(func pid=184036)[0m rmse_per_class: [0.13, 0.283, 0.046, 0.312, 0.081, 0.164, 0.235, 0.15, 0.148, 0.106]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1428987830877304
[2m[36m(func pid=183191)[0m mae:  0.09868801385164261
[2m[36m(func pid=183191)[0m rmse_per_class: [0.098, 0.225, 0.028, 0.29, 0.054, 0.163, 0.23, 0.104, 0.14, 0.097]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17263823747634888
[2m[36m(func pid=182810)[0m mae:  0.12583959102630615
[2m[36m(func pid=182810)[0m rmse_per_class: [0.12, 0.258, 0.084, 0.327, 0.081, 0.19, 0.273, 0.136, 0.145, 0.113]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2718 | Steps: 4 | Val loss: 0.2828 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3428 | Steps: 4 | Val loss: 0.3107 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3386 | Steps: 4 | Val loss: 0.2662 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3997 | Steps: 4 | Val loss: 0.3103 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 19:55:37 (running for 00:05:55.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.4   |  0.173 |                   56 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.337 |  0.143 |                   56 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.272 |  0.154 |                   57 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.333 |  0.165 |                   55 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.153649240732193
[2m[36m(func pid=183612)[0m mae:  0.09191052615642548
[2m[36m(func pid=183612)[0m rmse_per_class: [0.073, 0.225, 0.093, 0.3, 0.064, 0.16, 0.211, 0.123, 0.133, 0.155]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1657276600599289
[2m[36m(func pid=184036)[0m mae:  0.10018198192119598
[2m[36m(func pid=184036)[0m rmse_per_class: [0.132, 0.31, 0.043, 0.331, 0.08, 0.165, 0.229, 0.12, 0.157, 0.09]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1426243782043457
[2m[36m(func pid=183191)[0m mae:  0.09830552339553833
[2m[36m(func pid=183191)[0m rmse_per_class: [0.096, 0.224, 0.028, 0.29, 0.054, 0.162, 0.231, 0.105, 0.14, 0.097]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1723831743001938
[2m[36m(func pid=182810)[0m mae:  0.1255345344543457
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.258, 0.085, 0.326, 0.08, 0.189, 0.273, 0.135, 0.145, 0.113]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2806 | Steps: 4 | Val loss: 0.2803 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3056 | Steps: 4 | Val loss: 0.3265 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3420 | Steps: 4 | Val loss: 0.2669 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4031 | Steps: 4 | Val loss: 0.3100 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 19:55:42 (running for 00:06:00.48)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.4   |  0.172 |                   57 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.339 |  0.143 |                   57 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.281 |  0.152 |                   58 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.343 |  0.166 |                   56 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1520748883485794
[2m[36m(func pid=183612)[0m mae:  0.09061302244663239
[2m[36m(func pid=183612)[0m rmse_per_class: [0.073, 0.231, 0.092, 0.292, 0.067, 0.159, 0.211, 0.12, 0.13, 0.145]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1696380078792572
[2m[36m(func pid=184036)[0m mae:  0.1027878150343895
[2m[36m(func pid=184036)[0m rmse_per_class: [0.137, 0.327, 0.047, 0.338, 0.087, 0.181, 0.233, 0.1, 0.155, 0.091]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1429322063922882
[2m[36m(func pid=183191)[0m mae:  0.09772394597530365
[2m[36m(func pid=183191)[0m rmse_per_class: [0.094, 0.224, 0.028, 0.291, 0.054, 0.162, 0.23, 0.109, 0.14, 0.099]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1720130294561386
[2m[36m(func pid=182810)[0m mae:  0.12521429359912872
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.257, 0.084, 0.325, 0.079, 0.189, 0.272, 0.134, 0.145, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2837 | Steps: 4 | Val loss: 0.2816 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3077 | Steps: 4 | Val loss: 0.3307 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3385 | Steps: 4 | Val loss: 0.2685 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4015 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 19:55:48 (running for 00:06:05.94)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.403 |  0.172 |                   58 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.342 |  0.143 |                   58 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.284 |  0.153 |                   59 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.306 |  0.17  |                   57 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1532665193080902
[2m[36m(func pid=183612)[0m mae:  0.09112973511219025
[2m[36m(func pid=183612)[0m rmse_per_class: [0.076, 0.237, 0.095, 0.292, 0.063, 0.161, 0.214, 0.119, 0.139, 0.138]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17011424899101257
[2m[36m(func pid=184036)[0m mae:  0.10424064099788666
[2m[36m(func pid=184036)[0m rmse_per_class: [0.121, 0.317, 0.044, 0.344, 0.098, 0.191, 0.232, 0.101, 0.16, 0.091]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1434432417154312
[2m[36m(func pid=183191)[0m mae:  0.09781132638454437
[2m[36m(func pid=183191)[0m rmse_per_class: [0.094, 0.224, 0.028, 0.292, 0.054, 0.161, 0.229, 0.112, 0.14, 0.099]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17152222990989685
[2m[36m(func pid=182810)[0m mae:  0.12475768476724625
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.256, 0.083, 0.325, 0.078, 0.19, 0.271, 0.134, 0.145, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2720 | Steps: 4 | Val loss: 0.2766 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3008 | Steps: 4 | Val loss: 0.3269 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3465 | Steps: 4 | Val loss: 0.2685 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3983 | Steps: 4 | Val loss: 0.3093 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 19:55:53 (running for 00:06:11.25)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.402 |  0.172 |                   59 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.339 |  0.143 |                   59 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.272 |  0.149 |                   60 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.308 |  0.17  |                   58 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1486184448003769
[2m[36m(func pid=183612)[0m mae:  0.08802339434623718
[2m[36m(func pid=183612)[0m rmse_per_class: [0.077, 0.234, 0.071, 0.289, 0.064, 0.16, 0.21, 0.119, 0.142, 0.121]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17045773565769196
[2m[36m(func pid=184036)[0m mae:  0.1052633747458458
[2m[36m(func pid=184036)[0m rmse_per_class: [0.127, 0.3, 0.041, 0.349, 0.1, 0.186, 0.231, 0.104, 0.172, 0.096]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14337554574012756
[2m[36m(func pid=183191)[0m mae:  0.0985385850071907
[2m[36m(func pid=183191)[0m rmse_per_class: [0.096, 0.224, 0.029, 0.289, 0.054, 0.163, 0.232, 0.104, 0.142, 0.101]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17104926705360413
[2m[36m(func pid=182810)[0m mae:  0.12435587495565414
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.256, 0.083, 0.324, 0.078, 0.19, 0.27, 0.133, 0.145, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2643 | Steps: 4 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3143 | Steps: 4 | Val loss: 0.3236 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3359 | Steps: 4 | Val loss: 0.2691 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3996 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 19:55:58 (running for 00:06:16.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.398 |  0.171 |                   60 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.346 |  0.143 |                   60 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.264 |  0.147 |                   61 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.301 |  0.17  |                   59 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.14689673483371735
[2m[36m(func pid=183612)[0m mae:  0.0868220254778862
[2m[36m(func pid=183612)[0m rmse_per_class: [0.077, 0.23, 0.073, 0.29, 0.061, 0.159, 0.205, 0.119, 0.142, 0.113]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1728106141090393
[2m[36m(func pid=184036)[0m mae:  0.1068427786231041
[2m[36m(func pid=184036)[0m rmse_per_class: [0.127, 0.301, 0.037, 0.35, 0.11, 0.186, 0.233, 0.106, 0.172, 0.105]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1435401290655136
[2m[36m(func pid=183191)[0m mae:  0.09898324310779572
[2m[36m(func pid=183191)[0m rmse_per_class: [0.098, 0.225, 0.029, 0.29, 0.054, 0.163, 0.233, 0.102, 0.143, 0.1]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17089961469173431
[2m[36m(func pid=182810)[0m mae:  0.12426330894231796
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.255, 0.082, 0.325, 0.077, 0.189, 0.271, 0.132, 0.145, 0.114]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2801 | Steps: 4 | Val loss: 0.2716 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3088 | Steps: 4 | Val loss: 0.3201 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3390 | Steps: 4 | Val loss: 0.2698 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3916 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 19:56:04 (running for 00:06:21.76)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.4   |  0.171 |                   61 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.336 |  0.144 |                   61 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.28  |  0.145 |                   62 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.173 |                   60 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.14460252225399017
[2m[36m(func pid=183612)[0m mae:  0.08462763577699661
[2m[36m(func pid=183612)[0m rmse_per_class: [0.075, 0.223, 0.063, 0.286, 0.059, 0.156, 0.202, 0.135, 0.137, 0.109]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.17158322036266327
[2m[36m(func pid=184036)[0m mae:  0.10490886121988297
[2m[36m(func pid=184036)[0m rmse_per_class: [0.135, 0.306, 0.042, 0.348, 0.103, 0.188, 0.23, 0.106, 0.151, 0.106]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1438036412000656
[2m[36m(func pid=183191)[0m mae:  0.09882690012454987
[2m[36m(func pid=183191)[0m rmse_per_class: [0.095, 0.224, 0.03, 0.291, 0.054, 0.163, 0.233, 0.104, 0.143, 0.103]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17079603672027588
[2m[36m(func pid=182810)[0m mae:  0.12430319935083389
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.256, 0.082, 0.324, 0.078, 0.189, 0.272, 0.132, 0.144, 0.113]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2784 | Steps: 4 | Val loss: 0.2712 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2962 | Steps: 4 | Val loss: 0.2989 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3474 | Steps: 4 | Val loss: 0.2694 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3936 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:56:09 (running for 00:06:27.02)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.392 |  0.171 |                   62 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.339 |  0.144 |                   62 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.278 |  0.146 |                   63 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.309 |  0.172 |                   61 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1464027464389801
[2m[36m(func pid=183612)[0m mae:  0.08550991117954254
[2m[36m(func pid=183612)[0m rmse_per_class: [0.076, 0.218, 0.073, 0.29, 0.055, 0.158, 0.204, 0.137, 0.147, 0.106]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1641569435596466
[2m[36m(func pid=184036)[0m mae:  0.10121860355138779
[2m[36m(func pid=184036)[0m rmse_per_class: [0.138, 0.273, 0.042, 0.328, 0.106, 0.188, 0.229, 0.104, 0.135, 0.099]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17054490745067596
[2m[36m(func pid=182810)[0m mae:  0.12404803931713104
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.255, 0.082, 0.325, 0.077, 0.189, 0.27, 0.132, 0.144, 0.113]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14359456300735474
[2m[36m(func pid=183191)[0m mae:  0.09839819371700287
[2m[36m(func pid=183191)[0m rmse_per_class: [0.095, 0.223, 0.029, 0.289, 0.054, 0.162, 0.232, 0.106, 0.143, 0.104]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2845 | Steps: 4 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3045 | Steps: 4 | Val loss: 0.2850 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3414 | Steps: 4 | Val loss: 0.2701 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3969 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 19:56:14 (running for 00:06:32.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.394 |  0.171 |                   63 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.347 |  0.144 |                   63 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.285 |  0.151 |                   64 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.296 |  0.164 |                   62 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15054212510585785
[2m[36m(func pid=183612)[0m mae:  0.08856149017810822
[2m[36m(func pid=183612)[0m rmse_per_class: [0.077, 0.216, 0.075, 0.288, 0.057, 0.169, 0.211, 0.129, 0.18, 0.103]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15623590350151062
[2m[36m(func pid=184036)[0m mae:  0.09616178274154663
[2m[36m(func pid=184036)[0m rmse_per_class: [0.13, 0.235, 0.045, 0.297, 0.102, 0.185, 0.23, 0.106, 0.131, 0.102]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.144167959690094
[2m[36m(func pid=183191)[0m mae:  0.09894557297229767
[2m[36m(func pid=183191)[0m rmse_per_class: [0.097, 0.223, 0.03, 0.288, 0.053, 0.162, 0.233, 0.104, 0.145, 0.107]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17033866047859192
[2m[36m(func pid=182810)[0m mae:  0.1239146739244461
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.255, 0.081, 0.324, 0.077, 0.189, 0.27, 0.132, 0.144, 0.112]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2810 | Steps: 4 | Val loss: 0.2769 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2924 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3437 | Steps: 4 | Val loss: 0.2691 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3948 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 19:56:20 (running for 00:06:37.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.397 |  0.17  |                   64 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.341 |  0.144 |                   64 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.281 |  0.152 |                   65 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.305 |  0.156 |                   63 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15177300572395325
[2m[36m(func pid=183612)[0m mae:  0.08942829072475433
[2m[36m(func pid=183612)[0m rmse_per_class: [0.079, 0.213, 0.071, 0.288, 0.062, 0.173, 0.209, 0.139, 0.189, 0.095]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15223467350006104
[2m[36m(func pid=184036)[0m mae:  0.09422777593135834
[2m[36m(func pid=184036)[0m rmse_per_class: [0.113, 0.221, 0.042, 0.283, 0.102, 0.179, 0.235, 0.112, 0.13, 0.107]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.17004504799842834
[2m[36m(func pid=182810)[0m mae:  0.12367284297943115
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.255, 0.079, 0.324, 0.077, 0.189, 0.269, 0.133, 0.144, 0.112]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1435707062482834
[2m[36m(func pid=183191)[0m mae:  0.0983317643404007
[2m[36m(func pid=183191)[0m rmse_per_class: [0.098, 0.222, 0.03, 0.288, 0.053, 0.16, 0.233, 0.103, 0.144, 0.105]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2661 | Steps: 4 | Val loss: 0.2819 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3112 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3983 | Steps: 4 | Val loss: 0.3088 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3238 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 19:56:25 (running for 00:06:42.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.395 |  0.17  |                   65 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.344 |  0.144 |                   65 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.266 |  0.155 |                   66 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.292 |  0.152 |                   64 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.15524914860725403
[2m[36m(func pid=183612)[0m mae:  0.09102089703083038
[2m[36m(func pid=183612)[0m rmse_per_class: [0.081, 0.216, 0.071, 0.289, 0.066, 0.176, 0.21, 0.147, 0.197, 0.1]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1525772362947464
[2m[36m(func pid=184036)[0m mae:  0.09412898868322372
[2m[36m(func pid=184036)[0m rmse_per_class: [0.113, 0.222, 0.037, 0.28, 0.097, 0.171, 0.243, 0.126, 0.129, 0.108]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1692778468132019
[2m[36m(func pid=182810)[0m mae:  0.1229858547449112
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.253, 0.078, 0.324, 0.077, 0.188, 0.267, 0.133, 0.144, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14238964021205902
[2m[36m(func pid=183191)[0m mae:  0.09744523465633392
[2m[36m(func pid=183191)[0m rmse_per_class: [0.096, 0.221, 0.028, 0.285, 0.053, 0.159, 0.232, 0.104, 0.141, 0.105]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2934 | Steps: 4 | Val loss: 0.2765 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3124 | Steps: 4 | Val loss: 0.2795 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3952 | Steps: 4 | Val loss: 0.3087 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3416 | Steps: 4 | Val loss: 0.2682 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=183612)[0m rmse: 0.1537868231534958
[2m[36m(func pid=183612)[0m mae:  0.09075118601322174
[2m[36m(func pid=183612)[0m rmse_per_class: [0.089, 0.219, 0.077, 0.288, 0.08, 0.169, 0.215, 0.132, 0.17, 0.099]
[2m[36m(func pid=183612)[0m 
== Status ==
Current time: 2024-01-07 19:56:30 (running for 00:06:48.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.398 |  0.169 |                   66 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.324 |  0.142 |                   66 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.293 |  0.154 |                   67 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.311 |  0.153 |                   65 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184036)[0m rmse: 0.15466777980327606
[2m[36m(func pid=184036)[0m mae:  0.09470555186271667
[2m[36m(func pid=184036)[0m rmse_per_class: [0.109, 0.22, 0.034, 0.283, 0.088, 0.161, 0.246, 0.152, 0.131, 0.124]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.16938284039497375
[2m[36m(func pid=182810)[0m mae:  0.12313985824584961
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.254, 0.077, 0.323, 0.077, 0.188, 0.268, 0.133, 0.144, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1429208219051361
[2m[36m(func pid=183191)[0m mae:  0.09803162515163422
[2m[36m(func pid=183191)[0m rmse_per_class: [0.095, 0.222, 0.03, 0.285, 0.053, 0.16, 0.236, 0.101, 0.142, 0.104]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2815 | Steps: 4 | Val loss: 0.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3070 | Steps: 4 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3283 | Steps: 4 | Val loss: 0.2682 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4017 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 19:56:35 (running for 00:06:53.69)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.395 |  0.169 |                   67 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.342 |  0.143 |                   67 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.282 |  0.163 |                   68 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.312 |  0.155 |                   66 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16284571588039398
[2m[36m(func pid=183612)[0m mae:  0.0967775359749794
[2m[36m(func pid=183612)[0m rmse_per_class: [0.114, 0.221, 0.108, 0.304, 0.081, 0.169, 0.22, 0.133, 0.15, 0.127]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1546052098274231
[2m[36m(func pid=184036)[0m mae:  0.0955352708697319
[2m[36m(func pid=184036)[0m rmse_per_class: [0.1, 0.223, 0.031, 0.281, 0.09, 0.16, 0.254, 0.157, 0.131, 0.119]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1692376285791397
[2m[36m(func pid=182810)[0m mae:  0.12295578420162201
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.254, 0.078, 0.323, 0.076, 0.188, 0.267, 0.133, 0.144, 0.112]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14348778128623962
[2m[36m(func pid=183191)[0m mae:  0.09827365726232529
[2m[36m(func pid=183191)[0m rmse_per_class: [0.094, 0.223, 0.031, 0.283, 0.053, 0.162, 0.238, 0.101, 0.143, 0.106]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3185 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3215 | Steps: 4 | Val loss: 0.2871 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3332 | Steps: 4 | Val loss: 0.2682 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3963 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 19:56:41 (running for 00:06:59.11)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.402 |  0.169 |                   68 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.328 |  0.143 |                   68 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.319 |  0.17  |                   69 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.307 |  0.155 |                   67 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16973093152046204
[2m[36m(func pid=183612)[0m mae:  0.10059841722249985
[2m[36m(func pid=183612)[0m rmse_per_class: [0.127, 0.221, 0.145, 0.322, 0.084, 0.165, 0.223, 0.135, 0.149, 0.129]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.16122373938560486
[2m[36m(func pid=184036)[0m mae:  0.09939287602901459
[2m[36m(func pid=184036)[0m rmse_per_class: [0.094, 0.228, 0.03, 0.297, 0.086, 0.16, 0.255, 0.192, 0.136, 0.134]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14364808797836304
[2m[36m(func pid=183191)[0m mae:  0.09853099286556244
[2m[36m(func pid=183191)[0m rmse_per_class: [0.093, 0.223, 0.029, 0.279, 0.053, 0.163, 0.243, 0.101, 0.143, 0.11]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.16974905133247375
[2m[36m(func pid=182810)[0m mae:  0.1234450712800026
[2m[36m(func pid=182810)[0m rmse_per_class: [0.12, 0.255, 0.079, 0.323, 0.076, 0.187, 0.269, 0.133, 0.144, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2806 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3027 | Steps: 4 | Val loss: 0.2886 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3424 | Steps: 4 | Val loss: 0.2681 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3976 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 19:56:46 (running for 00:07:04.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.396 |  0.17  |                   69 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.333 |  0.144 |                   69 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.281 |  0.169 |                   70 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.321 |  0.161 |                   68 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16861949861049652
[2m[36m(func pid=183612)[0m mae:  0.09960561245679855
[2m[36m(func pid=183612)[0m rmse_per_class: [0.133, 0.219, 0.162, 0.323, 0.089, 0.164, 0.221, 0.12, 0.149, 0.106]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.1607341170310974
[2m[36m(func pid=184036)[0m mae:  0.09890450537204742
[2m[36m(func pid=184036)[0m rmse_per_class: [0.094, 0.23, 0.03, 0.299, 0.084, 0.16, 0.251, 0.184, 0.137, 0.138]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14412932097911835
[2m[36m(func pid=183191)[0m mae:  0.09791241586208344
[2m[36m(func pid=183191)[0m rmse_per_class: [0.089, 0.223, 0.03, 0.28, 0.053, 0.162, 0.241, 0.106, 0.142, 0.116]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.16911010444164276
[2m[36m(func pid=182810)[0m mae:  0.12287741899490356
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.254, 0.078, 0.322, 0.075, 0.188, 0.268, 0.133, 0.143, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3069 | Steps: 4 | Val loss: 0.3034 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2809 | Steps: 4 | Val loss: 0.2907 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3555 | Steps: 4 | Val loss: 0.2685 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3976 | Steps: 4 | Val loss: 0.3093 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=183612)[0m rmse: 0.171656996011734
[2m[36m(func pid=183612)[0m mae:  0.10072894394397736
[2m[36m(func pid=183612)[0m rmse_per_class: [0.124, 0.222, 0.179, 0.329, 0.089, 0.167, 0.216, 0.123, 0.162, 0.105]
== Status ==
Current time: 2024-01-07 19:56:51 (running for 00:07:09.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.398 |  0.169 |                   70 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.342 |  0.144 |                   70 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.307 |  0.172 |                   71 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.303 |  0.161 |                   69 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15891364216804504
[2m[36m(func pid=184036)[0m mae:  0.09869702905416489
[2m[36m(func pid=184036)[0m rmse_per_class: [0.097, 0.241, 0.031, 0.315, 0.09, 0.161, 0.241, 0.148, 0.141, 0.124]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14565211534500122
[2m[36m(func pid=183191)[0m mae:  0.09807130694389343
[2m[36m(func pid=183191)[0m rmse_per_class: [0.088, 0.223, 0.032, 0.283, 0.053, 0.162, 0.237, 0.113, 0.145, 0.12]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.16869086027145386
[2m[36m(func pid=182810)[0m mae:  0.1225043386220932
[2m[36m(func pid=182810)[0m rmse_per_class: [0.119, 0.254, 0.078, 0.322, 0.074, 0.188, 0.266, 0.132, 0.144, 0.11]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2679 | Steps: 4 | Val loss: 0.3040 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3031 | Steps: 4 | Val loss: 0.2987 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3259 | Steps: 4 | Val loss: 0.2650 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3977 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 19:56:57 (running for 00:07:14.99)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.398 |  0.169 |                   71 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.355 |  0.146 |                   71 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.268 |  0.173 |                   72 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.281 |  0.159 |                   70 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.17294837534427643
[2m[36m(func pid=183612)[0m mae:  0.10125185549259186
[2m[36m(func pid=183612)[0m rmse_per_class: [0.12, 0.222, 0.17, 0.325, 0.096, 0.169, 0.215, 0.12, 0.183, 0.109]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15916655957698822
[2m[36m(func pid=184036)[0m mae:  0.0993422120809555
[2m[36m(func pid=184036)[0m rmse_per_class: [0.097, 0.249, 0.035, 0.327, 0.091, 0.162, 0.236, 0.119, 0.152, 0.123]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1429785043001175
[2m[36m(func pid=183191)[0m mae:  0.09642846882343292
[2m[36m(func pid=183191)[0m rmse_per_class: [0.087, 0.222, 0.029, 0.279, 0.053, 0.16, 0.237, 0.109, 0.142, 0.112]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.16841235756874084
[2m[36m(func pid=182810)[0m mae:  0.12221882492303848
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.253, 0.078, 0.322, 0.074, 0.188, 0.265, 0.131, 0.144, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2588 | Steps: 4 | Val loss: 0.2969 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2874 | Steps: 4 | Val loss: 0.2976 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3209 | Steps: 4 | Val loss: 0.2677 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3928 | Steps: 4 | Val loss: 0.3090 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:57:02 (running for 00:07:20.20)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.398 |  0.168 |                   72 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.326 |  0.143 |                   72 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.259 |  0.17  |                   73 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.303 |  0.159 |                   71 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.17003867030143738
[2m[36m(func pid=183612)[0m mae:  0.09940364956855774
[2m[36m(func pid=183612)[0m rmse_per_class: [0.109, 0.22, 0.147, 0.313, 0.108, 0.169, 0.214, 0.121, 0.192, 0.107]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15677985548973083
[2m[36m(func pid=184036)[0m mae:  0.09760782867670059
[2m[36m(func pid=184036)[0m rmse_per_class: [0.092, 0.252, 0.035, 0.325, 0.092, 0.161, 0.228, 0.11, 0.157, 0.115]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14556869864463806
[2m[36m(func pid=183191)[0m mae:  0.09832923859357834
[2m[36m(func pid=183191)[0m rmse_per_class: [0.089, 0.222, 0.032, 0.282, 0.053, 0.162, 0.24, 0.108, 0.146, 0.121]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.168097585439682
[2m[36m(func pid=182810)[0m mae:  0.12204377353191376
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.253, 0.077, 0.321, 0.074, 0.187, 0.266, 0.131, 0.145, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3104 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2982 | Steps: 4 | Val loss: 0.3027 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3253 | Steps: 4 | Val loss: 0.2664 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4004 | Steps: 4 | Val loss: 0.3094 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 19:57:07 (running for 00:07:25.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.168 |                   73 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.321 |  0.146 |                   73 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.31  |  0.174 |                   74 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.287 |  0.157 |                   72 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.17388403415679932
[2m[36m(func pid=183612)[0m mae:  0.10176335275173187
[2m[36m(func pid=183612)[0m rmse_per_class: [0.104, 0.22, 0.102, 0.299, 0.105, 0.182, 0.221, 0.127, 0.247, 0.132]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15794672071933746
[2m[36m(func pid=184036)[0m mae:  0.09780296683311462
[2m[36m(func pid=184036)[0m rmse_per_class: [0.09, 0.266, 0.031, 0.332, 0.1, 0.164, 0.222, 0.104, 0.163, 0.108]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1449451893568039
[2m[36m(func pid=183191)[0m mae:  0.09813518822193146
[2m[36m(func pid=183191)[0m rmse_per_class: [0.088, 0.221, 0.034, 0.278, 0.053, 0.162, 0.242, 0.105, 0.146, 0.12]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.1680608093738556
[2m[36m(func pid=182810)[0m mae:  0.12201274931430817
[2m[36m(func pid=182810)[0m rmse_per_class: [0.117, 0.253, 0.077, 0.322, 0.073, 0.187, 0.266, 0.131, 0.144, 0.111]
[2m[36m(func pid=182810)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2723 | Steps: 4 | Val loss: 0.3027 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2980 | Steps: 4 | Val loss: 0.2929 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3268 | Steps: 4 | Val loss: 0.2666 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=182810)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3933 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 19:57:12 (running for 00:07:30.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: -0.1679999977350235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | RUNNING  | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.4   |  0.168 |                   74 |
| train_84a75_00001 | RUNNING  | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.325 |  0.145 |                   74 |
| train_84a75_00002 | RUNNING  | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.272 |  0.168 |                   75 |
| train_84a75_00003 | RUNNING  | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.298 |  0.158 |                   73 |
| train_84a75_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16816852986812592
[2m[36m(func pid=183612)[0m mae:  0.09834875166416168
[2m[36m(func pid=183612)[0m rmse_per_class: [0.095, 0.211, 0.074, 0.285, 0.115, 0.182, 0.221, 0.123, 0.252, 0.125]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15548530220985413
[2m[36m(func pid=184036)[0m mae:  0.09486863017082214
[2m[36m(func pid=184036)[0m rmse_per_class: [0.09, 0.264, 0.033, 0.316, 0.106, 0.166, 0.219, 0.101, 0.158, 0.101]
[2m[36m(func pid=184036)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14492133259773254
[2m[36m(func pid=183191)[0m mae:  0.09860270470380783
[2m[36m(func pid=183191)[0m rmse_per_class: [0.09, 0.222, 0.036, 0.278, 0.053, 0.162, 0.245, 0.103, 0.144, 0.116]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=182810)[0m rmse: 0.16710445284843445
[2m[36m(func pid=182810)[0m mae:  0.12122417986392975
[2m[36m(func pid=182810)[0m rmse_per_class: [0.118, 0.252, 0.074, 0.321, 0.073, 0.187, 0.263, 0.132, 0.143, 0.109]
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2808 | Steps: 4 | Val loss: 0.2813 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184036)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3138 | Steps: 4 | Val loss: 0.2970 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3104 | Steps: 4 | Val loss: 0.2649 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=183612)[0m rmse: 0.15469196438789368
[2m[36m(func pid=183612)[0m mae:  0.0896868109703064
[2m[36m(func pid=183612)[0m rmse_per_class: [0.092, 0.205, 0.054, 0.276, 0.125, 0.172, 0.202, 0.12, 0.201, 0.099]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=184036)[0m rmse: 0.15905345976352692
[2m[36m(func pid=184036)[0m mae:  0.09701085090637207
[2m[36m(func pid=184036)[0m rmse_per_class: [0.092, 0.279, 0.033, 0.308, 0.139, 0.171, 0.225, 0.103, 0.147, 0.092]
[2m[36m(func pid=183191)[0m rmse: 0.1440512239933014
[2m[36m(func pid=183191)[0m mae:  0.0974520668387413
[2m[36m(func pid=183191)[0m rmse_per_class: [0.086, 0.221, 0.036, 0.276, 0.053, 0.162, 0.244, 0.104, 0.141, 0.118]
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2579 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 19:57:17 (running for 00:07:35.55)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: -0.15599999576807022
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.327 |  0.145 |                   75 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.281 |  0.155 |                   76 |
| train_84a75_00003 | RUNNING    | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.298 |  0.155 |                   74 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14018)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=14018)[0m Configuration completed!
[2m[36m(func pid=14018)[0m New optimizer parameters:
[2m[36m(func pid=14018)[0m SGD (
[2m[36m(func pid=14018)[0m Parameter Group 0
[2m[36m(func pid=14018)[0m     dampening: 0
[2m[36m(func pid=14018)[0m     differentiable: False
[2m[36m(func pid=14018)[0m     foreach: None
[2m[36m(func pid=14018)[0m     lr: 0.0001
[2m[36m(func pid=14018)[0m     maximize: False
[2m[36m(func pid=14018)[0m     momentum: 0.9
[2m[36m(func pid=14018)[0m     nesterov: False
[2m[36m(func pid=14018)[0m     weight_decay: 0
[2m[36m(func pid=14018)[0m )
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m 
== Status ==
Current time: 2024-01-07 19:57:23 (running for 00:07:40.75)
Memory usage on this node: 21.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.31  |  0.144 |                   76 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.258 |  0.145 |                   77 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.14497046172618866
[2m[36m(func pid=183612)[0m mae:  0.08503635227680206
[2m[36m(func pid=183612)[0m rmse_per_class: [0.086, 0.203, 0.04, 0.272, 0.135, 0.159, 0.203, 0.127, 0.141, 0.084]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3185 | Steps: 4 | Val loss: 0.2630 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2703 | Steps: 4 | Val loss: 0.2750 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8974 | Steps: 4 | Val loss: 0.7037 | Batch size: 32 | lr: 0.0001 | Duration: 4.58s
[2m[36m(func pid=183191)[0m rmse: 0.14305201172828674
[2m[36m(func pid=183191)[0m mae:  0.09596964716911316
[2m[36m(func pid=183191)[0m rmse_per_class: [0.084, 0.22, 0.032, 0.276, 0.053, 0.16, 0.239, 0.109, 0.138, 0.12]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1468856781721115
[2m[36m(func pid=183612)[0m mae:  0.08703606575727463
[2m[36m(func pid=183612)[0m rmse_per_class: [0.081, 0.209, 0.033, 0.276, 0.137, 0.164, 0.211, 0.135, 0.14, 0.083]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.18259978294372559
[2m[36m(func pid=14018)[0m mae:  0.13439473509788513
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.267, 0.108, 0.339, 0.111, 0.191, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3263 | Steps: 4 | Val loss: 0.2629 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2664 | Steps: 4 | Val loss: 0.2731 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 19:57:28 (running for 00:07:45.89)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.319 |  0.143 |                   77 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.27  |  0.147 |                   78 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |        |        |                      |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14601)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=14601)[0m Configuration completed!
[2m[36m(func pid=14601)[0m New optimizer parameters:
[2m[36m(func pid=14601)[0m SGD (
[2m[36m(func pid=14601)[0m Parameter Group 0
[2m[36m(func pid=14601)[0m     dampening: 0
[2m[36m(func pid=14601)[0m     differentiable: False
[2m[36m(func pid=14601)[0m     foreach: None
[2m[36m(func pid=14601)[0m     lr: 0.001
[2m[36m(func pid=14601)[0m     maximize: False
[2m[36m(func pid=14601)[0m     momentum: 0.9
[2m[36m(func pid=14601)[0m     nesterov: False
[2m[36m(func pid=14601)[0m     weight_decay: 0
[2m[36m(func pid=14601)[0m )
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1434645652770996
[2m[36m(func pid=183191)[0m mae:  0.09567894041538239
[2m[36m(func pid=183191)[0m rmse_per_class: [0.084, 0.22, 0.032, 0.276, 0.053, 0.159, 0.234, 0.115, 0.138, 0.124]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.14494602382183075
[2m[36m(func pid=183612)[0m mae:  0.0863787829875946
[2m[36m(func pid=183612)[0m rmse_per_class: [0.077, 0.208, 0.028, 0.274, 0.133, 0.165, 0.212, 0.128, 0.141, 0.083]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8942 | Steps: 4 | Val loss: 0.6974 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3147 | Steps: 4 | Val loss: 0.2615 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2754 | Steps: 4 | Val loss: 0.2680 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8937 | Steps: 4 | Val loss: 0.6982 | Batch size: 32 | lr: 0.001 | Duration: 4.91s
== Status ==
Current time: 2024-01-07 19:57:37 (running for 00:07:54.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.326 |  0.143 |                   78 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.266 |  0.145 |                   79 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.894 |  0.182 |                    2 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |        |        |                      |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.18203221261501312
[2m[36m(func pid=14018)[0m mae:  0.13396331667900085
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14234140515327454
[2m[36m(func pid=183191)[0m mae:  0.09466099739074707
[2m[36m(func pid=183191)[0m rmse_per_class: [0.083, 0.22, 0.034, 0.274, 0.053, 0.159, 0.234, 0.112, 0.135, 0.12]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.14267870783805847
[2m[36m(func pid=183612)[0m mae:  0.08474677056074142
[2m[36m(func pid=183612)[0m rmse_per_class: [0.077, 0.206, 0.029, 0.275, 0.117, 0.165, 0.211, 0.115, 0.144, 0.09]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1825539767742157
[2m[36m(func pid=14601)[0m mae:  0.13436061143875122
[2m[36m(func pid=14601)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.113, 0.191, 0.294, 0.143, 0.144, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8942 | Steps: 4 | Val loss: 0.6901 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2947 | Steps: 4 | Val loss: 0.2633 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3024 | Steps: 4 | Val loss: 0.2595 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8778 | Steps: 4 | Val loss: 0.6720 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 19:57:42 (running for 00:08:00.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.315 |  0.142 |                   79 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.275 |  0.143 |                   80 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.894 |  0.181 |                    3 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.894 |  0.183 |                    1 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.18118171393871307
[2m[36m(func pid=14018)[0m mae:  0.1332826465368271
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1402330994606018
[2m[36m(func pid=183612)[0m mae:  0.08409736305475235
[2m[36m(func pid=183612)[0m rmse_per_class: [0.076, 0.2, 0.03, 0.271, 0.104, 0.17, 0.211, 0.101, 0.148, 0.091]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1401778906583786
[2m[36m(func pid=183191)[0m mae:  0.09357695281505585
[2m[36m(func pid=183191)[0m rmse_per_class: [0.082, 0.217, 0.031, 0.27, 0.053, 0.157, 0.234, 0.108, 0.132, 0.116]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.18168315291404724
[2m[36m(func pid=14601)[0m mae:  0.13370582461357117
[2m[36m(func pid=14601)[0m rmse_per_class: [0.117, 0.266, 0.104, 0.339, 0.113, 0.19, 0.294, 0.141, 0.143, 0.111]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8916 | Steps: 4 | Val loss: 0.6871 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2886 | Steps: 4 | Val loss: 0.2606 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3079 | Steps: 4 | Val loss: 0.2613 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8474 | Steps: 4 | Val loss: 0.6457 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 19:57:47 (running for 00:08:05.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                   80 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.295 |  0.14  |                   81 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.892 |  0.181 |                    4 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.878 |  0.182 |                    2 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.18070927262306213
[2m[36m(func pid=14018)[0m mae:  0.13286352157592773
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.112, 0.19, 0.295, 0.14, 0.142, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1375816911458969
[2m[36m(func pid=183612)[0m mae:  0.0819658562541008
[2m[36m(func pid=183612)[0m rmse_per_class: [0.074, 0.202, 0.029, 0.269, 0.085, 0.158, 0.205, 0.107, 0.146, 0.1]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14159470796585083
[2m[36m(func pid=183191)[0m mae:  0.09433376044034958
[2m[36m(func pid=183191)[0m rmse_per_class: [0.081, 0.217, 0.034, 0.275, 0.053, 0.159, 0.235, 0.107, 0.133, 0.123]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.18084564805030823
[2m[36m(func pid=14601)[0m mae:  0.13299360871315002
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.19, 0.293, 0.14, 0.143, 0.111]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8848 | Steps: 4 | Val loss: 0.6881 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2975 | Steps: 4 | Val loss: 0.2620 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3149 | Steps: 4 | Val loss: 0.2617 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8116 | Steps: 4 | Val loss: 0.6163 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:57:53 (running for 00:08:11.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                   81 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.289 |  0.138 |                   82 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.885 |  0.181 |                    5 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.847 |  0.181 |                    3 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.18075858056545258
[2m[36m(func pid=14018)[0m mae:  0.13291843235492706
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.338, 0.112, 0.19, 0.295, 0.14, 0.142, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1390269696712494
[2m[36m(func pid=183612)[0m mae:  0.08390991389751434
[2m[36m(func pid=183612)[0m rmse_per_class: [0.075, 0.208, 0.03, 0.276, 0.079, 0.163, 0.21, 0.098, 0.147, 0.104]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14148598909378052
[2m[36m(func pid=183191)[0m mae:  0.09426431357860565
[2m[36m(func pid=183191)[0m rmse_per_class: [0.08, 0.216, 0.035, 0.278, 0.053, 0.159, 0.234, 0.106, 0.134, 0.121]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.18040265142917633
[2m[36m(func pid=14601)[0m mae:  0.13257667422294617
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.264, 0.101, 0.337, 0.112, 0.19, 0.293, 0.139, 0.142, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8796 | Steps: 4 | Val loss: 0.6846 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2726 | Steps: 4 | Val loss: 0.2628 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2980 | Steps: 4 | Val loss: 0.2614 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7761 | Steps: 4 | Val loss: 0.5874 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 19:57:58 (running for 00:08:16.36)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.315 |  0.141 |                   82 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.298 |  0.139 |                   83 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.88  |  0.181 |                    6 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.812 |  0.18  |                    4 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.18051409721374512
[2m[36m(func pid=14018)[0m mae:  0.13267162442207336
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.338, 0.112, 0.19, 0.295, 0.14, 0.142, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.1402183473110199
[2m[36m(func pid=183612)[0m mae:  0.08494336158037186
[2m[36m(func pid=183612)[0m rmse_per_class: [0.073, 0.208, 0.032, 0.281, 0.076, 0.161, 0.207, 0.099, 0.154, 0.112]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1409319043159485
[2m[36m(func pid=183191)[0m mae:  0.09401609003543854
[2m[36m(func pid=183191)[0m rmse_per_class: [0.08, 0.217, 0.036, 0.275, 0.054, 0.158, 0.234, 0.104, 0.133, 0.118]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1798103153705597
[2m[36m(func pid=14601)[0m mae:  0.13203531503677368
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.263, 0.099, 0.336, 0.111, 0.19, 0.293, 0.139, 0.142, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8736 | Steps: 4 | Val loss: 0.6797 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2524 | Steps: 4 | Val loss: 0.2631 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3057 | Steps: 4 | Val loss: 0.2630 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7414 | Steps: 4 | Val loss: 0.5609 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 19:58:04 (running for 00:08:21.76)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.298 |  0.141 |                   83 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.273 |  0.14  |                   84 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.874 |  0.18  |                    7 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.776 |  0.18  |                    5 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.18031202256679535
[2m[36m(func pid=14018)[0m mae:  0.13249246776103973
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.112, 0.19, 0.295, 0.14, 0.142, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.14191067218780518
[2m[36m(func pid=183612)[0m mae:  0.0864577516913414
[2m[36m(func pid=183612)[0m rmse_per_class: [0.072, 0.205, 0.033, 0.282, 0.088, 0.161, 0.21, 0.102, 0.153, 0.113]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1423797905445099
[2m[36m(func pid=183191)[0m mae:  0.09500639140605927
[2m[36m(func pid=183191)[0m rmse_per_class: [0.079, 0.217, 0.039, 0.276, 0.055, 0.159, 0.234, 0.104, 0.137, 0.124]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17939522862434387
[2m[36m(func pid=14601)[0m mae:  0.13170024752616882
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.262, 0.098, 0.336, 0.109, 0.19, 0.292, 0.139, 0.142, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8721 | Steps: 4 | Val loss: 0.6769 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2739 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3040 | Steps: 4 | Val loss: 0.2612 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7102 | Steps: 4 | Val loss: 0.5366 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 19:58:09 (running for 00:08:27.07)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.306 |  0.142 |                   84 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.252 |  0.142 |                   85 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.872 |  0.18  |                    8 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.741 |  0.179 |                    6 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.18022653460502625
[2m[36m(func pid=14018)[0m mae:  0.13243314623832703
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.112, 0.19, 0.295, 0.14, 0.142, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183612)[0m rmse: 0.15094919502735138
[2m[36m(func pid=183612)[0m mae:  0.09187328070402145
[2m[36m(func pid=183612)[0m rmse_per_class: [0.074, 0.208, 0.039, 0.29, 0.092, 0.163, 0.224, 0.106, 0.166, 0.146]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1402696818113327
[2m[36m(func pid=183191)[0m mae:  0.09395910799503326
[2m[36m(func pid=183191)[0m rmse_per_class: [0.078, 0.216, 0.037, 0.273, 0.056, 0.158, 0.234, 0.102, 0.136, 0.113]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1792035549879074
[2m[36m(func pid=14601)[0m mae:  0.13155382871627808
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.108, 0.19, 0.292, 0.14, 0.142, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8653 | Steps: 4 | Val loss: 0.6726 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2632 | Steps: 4 | Val loss: 0.2820 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3023 | Steps: 4 | Val loss: 0.2602 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6768 | Steps: 4 | Val loss: 0.5167 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=14018)[0m rmse: 0.1799338310956955
[2m[36m(func pid=14018)[0m mae:  0.13218353688716888
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.112, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=14018)[0m 
== Status ==
Current time: 2024-01-07 19:58:14 (running for 00:08:32.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.304 |  0.14  |                   85 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.274 |  0.151 |                   86 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.865 |  0.18  |                    9 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.71  |  0.179 |                    7 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.1603652834892273
[2m[36m(func pid=183612)[0m mae:  0.09727306663990021
[2m[36m(func pid=183612)[0m rmse_per_class: [0.085, 0.21, 0.066, 0.312, 0.106, 0.164, 0.24, 0.11, 0.169, 0.143]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.13952234387397766
[2m[36m(func pid=183191)[0m mae:  0.09270688146352768
[2m[36m(func pid=183191)[0m rmse_per_class: [0.078, 0.217, 0.039, 0.27, 0.056, 0.158, 0.231, 0.102, 0.135, 0.111]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17940418422222137
[2m[36m(func pid=14601)[0m mae:  0.13173779845237732
[2m[36m(func pid=14601)[0m rmse_per_class: [0.117, 0.262, 0.098, 0.336, 0.106, 0.19, 0.291, 0.14, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2510 | Steps: 4 | Val loss: 0.2879 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8657 | Steps: 4 | Val loss: 0.6668 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3031 | Steps: 4 | Val loss: 0.2596 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6503 | Steps: 4 | Val loss: 0.4975 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:58:19 (running for 00:08:37.68)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                   86 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.251 |  0.164 |                   88 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.865 |  0.18  |                    9 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.677 |  0.179 |                    8 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m rmse: 0.16392405331134796
[2m[36m(func pid=183612)[0m mae:  0.09897228330373764
[2m[36m(func pid=183612)[0m rmse_per_class: [0.086, 0.214, 0.089, 0.322, 0.115, 0.166, 0.247, 0.113, 0.164, 0.123]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17990340292453766
[2m[36m(func pid=14018)[0m mae:  0.13219337165355682
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.262, 0.099, 0.337, 0.111, 0.189, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.13824281096458435
[2m[36m(func pid=183191)[0m mae:  0.0915587991476059
[2m[36m(func pid=183191)[0m rmse_per_class: [0.078, 0.217, 0.039, 0.27, 0.056, 0.156, 0.228, 0.1, 0.131, 0.107]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17948678135871887
[2m[36m(func pid=14601)[0m mae:  0.1317487210035324
[2m[36m(func pid=14601)[0m rmse_per_class: [0.117, 0.261, 0.099, 0.337, 0.105, 0.19, 0.29, 0.14, 0.145, 0.112]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2738 | Steps: 4 | Val loss: 0.2927 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8563 | Steps: 4 | Val loss: 0.6644 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3061 | Steps: 4 | Val loss: 0.2614 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6264 | Steps: 4 | Val loss: 0.4814 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=183612)[0m rmse: 0.1672864854335785
[2m[36m(func pid=183612)[0m mae:  0.10100679099559784
[2m[36m(func pid=183612)[0m rmse_per_class: [0.099, 0.219, 0.104, 0.329, 0.129, 0.172, 0.248, 0.113, 0.158, 0.102]
[2m[36m(func pid=183612)[0m 
== Status ==
Current time: 2024-01-07 19:58:25 (running for 00:08:42.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.303 |  0.138 |                   87 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.274 |  0.167 |                   89 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.866 |  0.18  |                   10 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.65  |  0.179 |                    9 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17995460331439972
[2m[36m(func pid=14018)[0m mae:  0.13222435116767883
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.189, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.13953959941864014
[2m[36m(func pid=183191)[0m mae:  0.09243718534708023
[2m[36m(func pid=183191)[0m rmse_per_class: [0.078, 0.217, 0.041, 0.272, 0.057, 0.159, 0.23, 0.1, 0.134, 0.108]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17940856516361237
[2m[36m(func pid=14601)[0m mae:  0.13164260983467102
[2m[36m(func pid=14601)[0m rmse_per_class: [0.118, 0.261, 0.097, 0.337, 0.103, 0.19, 0.289, 0.14, 0.145, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2685 | Steps: 4 | Val loss: 0.2860 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8486 | Steps: 4 | Val loss: 0.6606 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3139 | Steps: 4 | Val loss: 0.2630 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5974 | Steps: 4 | Val loss: 0.4617 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=183612)[0m rmse: 0.16313746571540833
[2m[36m(func pid=183612)[0m mae:  0.09833955764770508
[2m[36m(func pid=183612)[0m rmse_per_class: [0.101, 0.222, 0.089, 0.322, 0.137, 0.176, 0.237, 0.116, 0.137, 0.096]
== Status ==
Current time: 2024-01-07 19:58:30 (running for 00:08:47.93)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.306 |  0.14  |                   88 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.269 |  0.163 |                   90 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.856 |  0.18  |                   11 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.626 |  0.179 |                   10 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.1798781007528305
[2m[36m(func pid=14018)[0m mae:  0.13215520977973938
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.189, 0.295, 0.14, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.14095744490623474
[2m[36m(func pid=183191)[0m mae:  0.0930066704750061
[2m[36m(func pid=183191)[0m rmse_per_class: [0.079, 0.218, 0.043, 0.276, 0.058, 0.158, 0.227, 0.103, 0.136, 0.112]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17924864590168
[2m[36m(func pid=14601)[0m mae:  0.13143353164196014
[2m[36m(func pid=14601)[0m rmse_per_class: [0.118, 0.262, 0.097, 0.337, 0.103, 0.189, 0.288, 0.14, 0.145, 0.114]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2695 | Steps: 4 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8491 | Steps: 4 | Val loss: 0.6571 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3034 | Steps: 4 | Val loss: 0.2631 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=183612)[0m rmse: 0.16272279620170593
[2m[36m(func pid=183612)[0m mae:  0.09696178883314133
[2m[36m(func pid=183612)[0m rmse_per_class: [0.106, 0.228, 0.092, 0.323, 0.141, 0.17, 0.227, 0.117, 0.141, 0.082]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5781 | Steps: 4 | Val loss: 0.4438 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 19:58:36 (running for 00:08:54.04)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.314 |  0.141 |                   89 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.269 |  0.163 |                   91 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.849 |  0.18  |                   13 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.597 |  0.179 |                   11 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17993921041488647
[2m[36m(func pid=14018)[0m mae:  0.13220378756523132
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.295, 0.14, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1412109136581421
[2m[36m(func pid=183191)[0m mae:  0.09258049726486206
[2m[36m(func pid=183191)[0m rmse_per_class: [0.079, 0.219, 0.045, 0.277, 0.058, 0.157, 0.226, 0.103, 0.133, 0.114]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17872390151023865
[2m[36m(func pid=14601)[0m mae:  0.13089317083358765
[2m[36m(func pid=14601)[0m rmse_per_class: [0.118, 0.261, 0.096, 0.336, 0.102, 0.19, 0.288, 0.139, 0.144, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2986 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8371 | Steps: 4 | Val loss: 0.6518 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2982 | Steps: 4 | Val loss: 0.2607 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=183612)[0m rmse: 0.15815399587154388
[2m[36m(func pid=183612)[0m mae:  0.09414035081863403
[2m[36m(func pid=183612)[0m rmse_per_class: [0.092, 0.239, 0.058, 0.306, 0.139, 0.162, 0.228, 0.155, 0.125, 0.077]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5615 | Steps: 4 | Val loss: 0.4297 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 19:58:41 (running for 00:08:59.45)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.303 |  0.141 |                   90 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.299 |  0.158 |                   92 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.837 |  0.18  |                   14 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.578 |  0.179 |                   12 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17997950315475464
[2m[36m(func pid=14018)[0m mae:  0.13228440284729004
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.262, 0.098, 0.337, 0.111, 0.189, 0.295, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1393754631280899
[2m[36m(func pid=183191)[0m mae:  0.09172477573156357
[2m[36m(func pid=183191)[0m rmse_per_class: [0.078, 0.22, 0.038, 0.271, 0.061, 0.158, 0.228, 0.1, 0.132, 0.108]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1784300059080124
[2m[36m(func pid=14601)[0m mae:  0.13057257235050201
[2m[36m(func pid=14601)[0m rmse_per_class: [0.118, 0.261, 0.095, 0.335, 0.102, 0.19, 0.287, 0.139, 0.145, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2705 | Steps: 4 | Val loss: 0.2789 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2925 | Steps: 4 | Val loss: 0.2616 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8384 | Steps: 4 | Val loss: 0.6503 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=183612)[0m rmse: 0.15690158307552338
[2m[36m(func pid=183612)[0m mae:  0.09383242577314377
[2m[36m(func pid=183612)[0m rmse_per_class: [0.09, 0.247, 0.052, 0.303, 0.135, 0.156, 0.229, 0.16, 0.122, 0.074]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5438 | Steps: 4 | Val loss: 0.4166 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 19:58:47 (running for 00:09:04.90)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.298 |  0.139 |                   91 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.271 |  0.157 |                   93 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.838 |  0.18  |                   15 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.561 |  0.178 |                   13 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.14032085239887238
[2m[36m(func pid=183191)[0m mae:  0.09215501695871353
[2m[36m(func pid=183191)[0m rmse_per_class: [0.081, 0.221, 0.042, 0.273, 0.063, 0.156, 0.228, 0.101, 0.131, 0.107]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17990942299365997
[2m[36m(func pid=14018)[0m mae:  0.13219667971134186
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.295, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1783570498228073
[2m[36m(func pid=14601)[0m mae:  0.13051176071166992
[2m[36m(func pid=14601)[0m rmse_per_class: [0.119, 0.261, 0.096, 0.334, 0.102, 0.19, 0.286, 0.139, 0.144, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2781 | Steps: 4 | Val loss: 0.2741 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2992 | Steps: 4 | Val loss: 0.2614 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8318 | Steps: 4 | Val loss: 0.6443 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=183612)[0m rmse: 0.15091142058372498
[2m[36m(func pid=183612)[0m mae:  0.09138592332601547
[2m[36m(func pid=183612)[0m rmse_per_class: [0.087, 0.246, 0.046, 0.298, 0.125, 0.157, 0.231, 0.125, 0.122, 0.072]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5286 | Steps: 4 | Val loss: 0.4048 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 19:58:52 (running for 00:09:10.23)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.299 |  0.14  |                   93 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.278 |  0.151 |                   94 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.838 |  0.18  |                   15 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.544 |  0.178 |                   14 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.1403212696313858
[2m[36m(func pid=183191)[0m mae:  0.09228099882602692
[2m[36m(func pid=183191)[0m rmse_per_class: [0.079, 0.221, 0.041, 0.272, 0.067, 0.157, 0.228, 0.099, 0.132, 0.107]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.1797427088022232
[2m[36m(func pid=14018)[0m mae:  0.1320495307445526
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.111, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17810627818107605
[2m[36m(func pid=14601)[0m mae:  0.13027730584144592
[2m[36m(func pid=14601)[0m rmse_per_class: [0.118, 0.261, 0.096, 0.334, 0.101, 0.19, 0.285, 0.138, 0.144, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2895 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2980 | Steps: 4 | Val loss: 0.2606 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8250 | Steps: 4 | Val loss: 0.6405 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=183612)[0m rmse: 0.14898937940597534
[2m[36m(func pid=183612)[0m mae:  0.0903998538851738
[2m[36m(func pid=183612)[0m rmse_per_class: [0.081, 0.239, 0.037, 0.29, 0.127, 0.158, 0.231, 0.133, 0.122, 0.072]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5137 | Steps: 4 | Val loss: 0.3953 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:58:58 (running for 00:09:15.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.299 |  0.14  |                   93 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.289 |  0.149 |                   95 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.825 |  0.18  |                   17 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.529 |  0.178 |                   15 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17967328429222107
[2m[36m(func pid=14018)[0m mae:  0.13195772469043732
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.111, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=183191)[0m rmse: 0.1395808905363083
[2m[36m(func pid=183191)[0m mae:  0.09109218418598175
[2m[36m(func pid=183191)[0m rmse_per_class: [0.077, 0.221, 0.038, 0.272, 0.066, 0.156, 0.224, 0.103, 0.128, 0.111]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17785868048667908
[2m[36m(func pid=14601)[0m mae:  0.13006122410297394
[2m[36m(func pid=14601)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.333, 0.1, 0.19, 0.285, 0.138, 0.144, 0.114]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2616 | Steps: 4 | Val loss: 0.2663 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2930 | Steps: 4 | Val loss: 0.2604 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8204 | Steps: 4 | Val loss: 0.6376 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=183612)[0m rmse: 0.14447176456451416
[2m[36m(func pid=183612)[0m mae:  0.08820174634456635
[2m[36m(func pid=183612)[0m rmse_per_class: [0.083, 0.224, 0.036, 0.283, 0.126, 0.16, 0.229, 0.106, 0.124, 0.074]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5005 | Steps: 4 | Val loss: 0.3854 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:59:03 (running for 00:09:21.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.298 |  0.14  |                   94 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.262 |  0.144 |                   96 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.82  |  0.18  |                   18 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.514 |  0.178 |                   16 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.13950149714946747
[2m[36m(func pid=183191)[0m mae:  0.09064981341362
[2m[36m(func pid=183191)[0m rmse_per_class: [0.075, 0.221, 0.041, 0.274, 0.069, 0.156, 0.222, 0.102, 0.126, 0.109]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17972271144390106
[2m[36m(func pid=14018)[0m mae:  0.13195660710334778
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17721833288669586
[2m[36m(func pid=14601)[0m mae:  0.12946003675460815
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.261, 0.096, 0.332, 0.1, 0.19, 0.284, 0.137, 0.144, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2761 | Steps: 4 | Val loss: 0.2644 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2884 | Steps: 4 | Val loss: 0.2602 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8183 | Steps: 4 | Val loss: 0.6354 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=183612)[0m rmse: 0.1425202637910843
[2m[36m(func pid=183612)[0m mae:  0.08665404468774796
[2m[36m(func pid=183612)[0m rmse_per_class: [0.085, 0.221, 0.03, 0.281, 0.115, 0.164, 0.224, 0.1, 0.123, 0.082]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4918 | Steps: 4 | Val loss: 0.3761 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 19:59:09 (running for 00:09:26.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.288 |  0.139 |                   96 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.276 |  0.143 |                   97 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.82  |  0.18  |                   18 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.501 |  0.177 |                   17 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.13942956924438477
[2m[36m(func pid=183191)[0m mae:  0.09037455916404724
[2m[36m(func pid=183191)[0m rmse_per_class: [0.075, 0.221, 0.044, 0.272, 0.071, 0.156, 0.221, 0.102, 0.126, 0.108]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17971262335777283
[2m[36m(func pid=14018)[0m mae:  0.13198064267635345
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.111, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17690496146678925
[2m[36m(func pid=14601)[0m mae:  0.12924186885356903
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.331, 0.099, 0.19, 0.284, 0.136, 0.144, 0.113]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2751 | Steps: 4 | Val loss: 0.2626 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2928 | Steps: 4 | Val loss: 0.2623 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8114 | Steps: 4 | Val loss: 0.6309 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=183612)[0m rmse: 0.1424875110387802
[2m[36m(func pid=183612)[0m mae:  0.08721721917390823
[2m[36m(func pid=183612)[0m rmse_per_class: [0.083, 0.216, 0.033, 0.28, 0.105, 0.173, 0.223, 0.097, 0.128, 0.087]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4839 | Steps: 4 | Val loss: 0.3684 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 19:59:14 (running for 00:09:32.04)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.293 |  0.141 |                   97 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.275 |  0.142 |                   98 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.818 |  0.18  |                   19 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.492 |  0.177 |                   18 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.1413801610469818
[2m[36m(func pid=183191)[0m mae:  0.09115232527256012
[2m[36m(func pid=183191)[0m rmse_per_class: [0.073, 0.222, 0.044, 0.277, 0.074, 0.157, 0.221, 0.104, 0.127, 0.115]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17950841784477234
[2m[36m(func pid=14018)[0m mae:  0.13185159862041473
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.11, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17646482586860657
[2m[36m(func pid=14601)[0m mae:  0.12890034914016724
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.26, 0.095, 0.331, 0.098, 0.19, 0.283, 0.136, 0.145, 0.112]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2595 | Steps: 4 | Val loss: 0.2624 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3032 | Steps: 4 | Val loss: 0.2618 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8078 | Steps: 4 | Val loss: 0.6272 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=183612)[0m rmse: 0.1431387960910797
[2m[36m(func pid=183612)[0m mae:  0.08778403699398041
[2m[36m(func pid=183612)[0m rmse_per_class: [0.083, 0.214, 0.031, 0.275, 0.107, 0.175, 0.22, 0.1, 0.134, 0.091]
[2m[36m(func pid=183612)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4729 | Steps: 4 | Val loss: 0.3602 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 19:59:19 (running for 00:09:37.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.303 |  0.141 |                   98 |
| train_84a75_00002 | RUNNING    | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.259 |  0.143 |                   99 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.811 |  0.18  |                   20 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.484 |  0.176 |                   19 |
| train_84a75_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183191)[0m rmse: 0.14062854647636414
[2m[36m(func pid=183191)[0m mae:  0.09021921455860138
[2m[36m(func pid=183191)[0m rmse_per_class: [0.074, 0.22, 0.046, 0.275, 0.072, 0.158, 0.217, 0.102, 0.128, 0.114]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17962166666984558
[2m[36m(func pid=14018)[0m mae:  0.131956547498703
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.337, 0.11, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1756764054298401
[2m[36m(func pid=14601)[0m mae:  0.12823227047920227
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.259, 0.093, 0.331, 0.096, 0.189, 0.281, 0.136, 0.145, 0.112]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=183612)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2500 | Steps: 4 | Val loss: 0.2637 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2923 | Steps: 4 | Val loss: 0.2605 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8061 | Steps: 4 | Val loss: 0.6244 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=183612)[0m rmse: 0.14535632729530334
[2m[36m(func pid=183612)[0m mae:  0.08905129134654999
[2m[36m(func pid=183612)[0m rmse_per_class: [0.084, 0.214, 0.034, 0.274, 0.108, 0.183, 0.219, 0.103, 0.141, 0.094]
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4632 | Steps: 4 | Val loss: 0.3536 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=183191)[0m rmse: 0.13968519866466522
[2m[36m(func pid=183191)[0m mae:  0.09000200778245926
[2m[36m(func pid=183191)[0m rmse_per_class: [0.075, 0.219, 0.043, 0.272, 0.075, 0.158, 0.217, 0.101, 0.128, 0.109]
[2m[36m(func pid=183191)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.1796461045742035
[2m[36m(func pid=14018)[0m mae:  0.1319577544927597
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=14601)[0m rmse: 0.1753315031528473
[2m[36m(func pid=14601)[0m mae:  0.1279769390821457
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.258, 0.093, 0.33, 0.095, 0.189, 0.28, 0.136, 0.144, 0.112]
[2m[36m(func pid=183191)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3022 | Steps: 4 | Val loss: 0.2606 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=183191)[0m rmse: 0.14030446112155914
[2m[36m(func pid=183191)[0m mae:  0.08979727327823639
[2m[36m(func pid=183191)[0m rmse_per_class: [0.074, 0.221, 0.044, 0.272, 0.077, 0.159, 0.216, 0.102, 0.129, 0.111]
== Status ==
Current time: 2024-01-07 19:59:25 (running for 00:09:42.70)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00001 | RUNNING    | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.292 |  0.14  |                   99 |
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.808 |  0.18  |                   21 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.473 |  0.176 |                   20 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=19964)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=19964)[0m Configuration completed!
[2m[36m(func pid=19964)[0m New optimizer parameters:
[2m[36m(func pid=19964)[0m SGD (
[2m[36m(func pid=19964)[0m Parameter Group 0
[2m[36m(func pid=19964)[0m     dampening: 0
[2m[36m(func pid=19964)[0m     differentiable: False
[2m[36m(func pid=19964)[0m     foreach: None
[2m[36m(func pid=19964)[0m     lr: 0.01
[2m[36m(func pid=19964)[0m     maximize: False
[2m[36m(func pid=19964)[0m     momentum: 0.9
[2m[36m(func pid=19964)[0m     nesterov: False
[2m[36m(func pid=19964)[0m     weight_decay: 0
[2m[36m(func pid=19964)[0m )
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 19:59:31 (running for 00:09:49.49)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 3 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.808 |  0.18  |                   21 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.473 |  0.176 |                   20 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8005 | Steps: 4 | Val loss: 0.6204 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4572 | Steps: 4 | Val loss: 0.3485 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8651 | Steps: 4 | Val loss: 0.6313 | Batch size: 32 | lr: 0.01 | Duration: 5.15s
[2m[36m(func pid=14018)[0m rmse: 0.17961333692073822
[2m[36m(func pid=14018)[0m mae:  0.13196519017219543
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.109, 0.19, 0.294, 0.139, 0.143, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17511489987373352
[2m[36m(func pid=14601)[0m mae:  0.1278032809495926
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.329, 0.095, 0.189, 0.28, 0.136, 0.144, 0.111]
[2m[36m(func pid=19964)[0m rmse: 0.18265390396118164
[2m[36m(func pid=19964)[0m mae:  0.13426576554775238
[2m[36m(func pid=19964)[0m rmse_per_class: [0.117, 0.267, 0.109, 0.34, 0.112, 0.19, 0.295, 0.144, 0.141, 0.111]
== Status ==
Current time: 2024-01-07 19:59:37 (running for 00:09:55.09)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.8   |  0.18  |                   23 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.463 |  0.175 |                   21 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |        |        |                      |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7965 | Steps: 4 | Val loss: 0.6162 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=20486)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=20486)[0m Configuration completed!
[2m[36m(func pid=20486)[0m New optimizer parameters:
[2m[36m(func pid=20486)[0m SGD (
[2m[36m(func pid=20486)[0m Parameter Group 0
[2m[36m(func pid=20486)[0m     dampening: 0
[2m[36m(func pid=20486)[0m     differentiable: False
[2m[36m(func pid=20486)[0m     foreach: None
[2m[36m(func pid=20486)[0m     lr: 0.1
[2m[36m(func pid=20486)[0m     maximize: False
[2m[36m(func pid=20486)[0m     momentum: 0.9
[2m[36m(func pid=20486)[0m     nesterov: False
[2m[36m(func pid=20486)[0m     weight_decay: 0
[2m[36m(func pid=20486)[0m )
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 19:59:43 (running for 00:10:00.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.796 |  0.18  |                   24 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.457 |  0.175 |                   22 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.865 |  0.183 |                    1 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |        |        |                      |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17963388562202454
[2m[36m(func pid=14018)[0m mae:  0.13195063173770905
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.337, 0.109, 0.19, 0.294, 0.139, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4547 | Steps: 4 | Val loss: 0.3471 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7240 | Steps: 4 | Val loss: 0.5055 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6916 | Steps: 4 | Val loss: 0.3444 | Batch size: 32 | lr: 0.1 | Duration: 4.73s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7908 | Steps: 4 | Val loss: 0.6135 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=14601)[0m rmse: 0.17528054118156433
[2m[36m(func pid=14601)[0m mae:  0.12804247438907623
[2m[36m(func pid=14601)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.328, 0.094, 0.189, 0.281, 0.137, 0.144, 0.112]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.18111489713191986
[2m[36m(func pid=19964)[0m mae:  0.13315504789352417
[2m[36m(func pid=19964)[0m rmse_per_class: [0.114, 0.266, 0.109, 0.337, 0.107, 0.19, 0.294, 0.141, 0.143, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.17976336181163788
[2m[36m(func pid=20486)[0m mae:  0.1316106617450714
[2m[36m(func pid=20486)[0m rmse_per_class: [0.114, 0.264, 0.122, 0.335, 0.089, 0.19, 0.29, 0.144, 0.141, 0.11]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 19:59:48 (running for 00:10:05.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.791 |  0.18  |                   25 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.455 |  0.175 |                   23 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.724 |  0.181 |                    2 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.692 |  0.18  |                    1 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1796010434627533
[2m[36m(func pid=14018)[0m mae:  0.13190868496894836
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.109, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4466 | Steps: 4 | Val loss: 0.3417 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5616 | Steps: 4 | Val loss: 0.4060 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4219 | Steps: 4 | Val loss: 0.3323 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7882 | Steps: 4 | Val loss: 0.6101 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=14601)[0m rmse: 0.174752876162529
[2m[36m(func pid=14601)[0m mae:  0.12756332755088806
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.259, 0.092, 0.329, 0.093, 0.189, 0.279, 0.136, 0.145, 0.111]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.17895209789276123
[2m[36m(func pid=19964)[0m mae:  0.13124994933605194
[2m[36m(func pid=19964)[0m rmse_per_class: [0.115, 0.266, 0.104, 0.336, 0.1, 0.188, 0.286, 0.139, 0.145, 0.109]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.16618093848228455
[2m[36m(func pid=20486)[0m mae:  0.12024376541376114
[2m[36m(func pid=20486)[0m rmse_per_class: [0.114, 0.243, 0.097, 0.317, 0.066, 0.189, 0.271, 0.129, 0.137, 0.097]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 19:59:53 (running for 00:10:11.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.788 |  0.18  |                   26 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.447 |  0.175 |                   24 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.562 |  0.179 |                    3 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.422 |  0.166 |                    2 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17957881093025208
[2m[36m(func pid=14018)[0m mae:  0.1318804919719696
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.337, 0.11, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4406 | Steps: 4 | Val loss: 0.3381 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4588 | Steps: 4 | Val loss: 0.3434 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4793 | Steps: 4 | Val loss: 0.3375 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7802 | Steps: 4 | Val loss: 0.6063 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=14601)[0m rmse: 0.1745758354663849
[2m[36m(func pid=14601)[0m mae:  0.12740497291088104
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.258, 0.092, 0.329, 0.093, 0.189, 0.279, 0.136, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.17602495849132538
[2m[36m(func pid=19964)[0m mae:  0.12861454486846924
[2m[36m(func pid=19964)[0m rmse_per_class: [0.117, 0.265, 0.095, 0.334, 0.092, 0.188, 0.279, 0.135, 0.146, 0.108]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14949937164783478
[2m[36m(func pid=20486)[0m mae:  0.10308192670345306
[2m[36m(func pid=20486)[0m rmse_per_class: [0.11, 0.233, 0.054, 0.284, 0.054, 0.173, 0.24, 0.124, 0.132, 0.09]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 19:59:58 (running for 00:10:16.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.78  |  0.18  |                   27 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.441 |  0.175 |                   25 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.459 |  0.176 |                    4 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.479 |  0.149 |                    3 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17955282330513
[2m[36m(func pid=14018)[0m mae:  0.13189610838890076
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.337, 0.109, 0.19, 0.293, 0.14, 0.143, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4110 | Steps: 4 | Val loss: 0.3178 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4362 | Steps: 4 | Val loss: 0.3347 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4232 | Steps: 4 | Val loss: 0.2992 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7819 | Steps: 4 | Val loss: 0.6032 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=19964)[0m rmse: 0.17424127459526062
[2m[36m(func pid=19964)[0m mae:  0.12693384289741516
[2m[36m(func pid=19964)[0m rmse_per_class: [0.118, 0.266, 0.09, 0.33, 0.086, 0.189, 0.273, 0.134, 0.146, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17395612597465515
[2m[36m(func pid=14601)[0m mae:  0.12692642211914062
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.258, 0.089, 0.328, 0.093, 0.188, 0.278, 0.136, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1516290009021759
[2m[36m(func pid=20486)[0m mae:  0.10151945054531097
[2m[36m(func pid=20486)[0m rmse_per_class: [0.121, 0.237, 0.048, 0.308, 0.055, 0.168, 0.241, 0.118, 0.133, 0.087]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:00:04 (running for 00:10:21.82)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.782 |  0.179 |                   28 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.436 |  0.174 |                   26 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.411 |  0.174 |                    5 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.423 |  0.152 |                    4 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17943121492862701
[2m[36m(func pid=14018)[0m mae:  0.13178496062755585
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.337, 0.109, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4026 | Steps: 4 | Val loss: 0.3096 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4356 | Steps: 4 | Val loss: 0.3319 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.3590 | Steps: 4 | Val loss: 0.2906 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7776 | Steps: 4 | Val loss: 0.5980 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=19964)[0m rmse: 0.1716470569372177
[2m[36m(func pid=19964)[0m mae:  0.12480262666940689
[2m[36m(func pid=19964)[0m rmse_per_class: [0.114, 0.266, 0.086, 0.328, 0.08, 0.186, 0.267, 0.133, 0.146, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17381492257118225
[2m[36m(func pid=14601)[0m mae:  0.12676100432872772
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.257, 0.089, 0.328, 0.094, 0.188, 0.279, 0.135, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:09 (running for 00:10:26.99)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.782 |  0.179 |                   28 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.436 |  0.174 |                   27 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.403 |  0.172 |                    6 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.359 |  0.158 |                    5 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17953814566135406
[2m[36m(func pid=14018)[0m mae:  0.13191023468971252
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.109, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=20486)[0m rmse: 0.15837766230106354
[2m[36m(func pid=20486)[0m mae:  0.10360584408044815
[2m[36m(func pid=20486)[0m rmse_per_class: [0.141, 0.24, 0.042, 0.334, 0.055, 0.169, 0.267, 0.117, 0.133, 0.085]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3960 | Steps: 4 | Val loss: 0.3084 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4292 | Steps: 4 | Val loss: 0.3276 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.3507 | Steps: 4 | Val loss: 0.2948 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7669 | Steps: 4 | Val loss: 0.5961 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=19964)[0m rmse: 0.16801974177360535
[2m[36m(func pid=19964)[0m mae:  0.12192566692829132
[2m[36m(func pid=19964)[0m rmse_per_class: [0.11, 0.264, 0.079, 0.327, 0.076, 0.177, 0.265, 0.128, 0.144, 0.109]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.173233300447464
[2m[36m(func pid=14601)[0m mae:  0.12623557448387146
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.257, 0.087, 0.327, 0.094, 0.188, 0.277, 0.135, 0.144, 0.109]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:14 (running for 00:10:32.29)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.778 |  0.18  |                   29 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.429 |  0.173 |                   28 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.396 |  0.168 |                    7 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.351 |  0.157 |                    6 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=20486)[0m rmse: 0.1566949337720871
[2m[36m(func pid=20486)[0m mae:  0.10256298631429672
[2m[36m(func pid=20486)[0m rmse_per_class: [0.127, 0.231, 0.043, 0.336, 0.054, 0.163, 0.237, 0.151, 0.135, 0.089]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17926332354545593
[2m[36m(func pid=14018)[0m mae:  0.1316417157649994
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.109, 0.19, 0.293, 0.139, 0.144, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3987 | Steps: 4 | Val loss: 0.3043 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4316 | Steps: 4 | Val loss: 0.3258 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7673 | Steps: 4 | Val loss: 0.5938 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3474 | Steps: 4 | Val loss: 0.2815 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=19964)[0m rmse: 0.16432756185531616
[2m[36m(func pid=19964)[0m mae:  0.11871232092380524
[2m[36m(func pid=19964)[0m rmse_per_class: [0.108, 0.259, 0.069, 0.321, 0.072, 0.178, 0.258, 0.127, 0.144, 0.107]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17276479303836823
[2m[36m(func pid=14601)[0m mae:  0.12584517896175385
[2m[36m(func pid=14601)[0m rmse_per_class: [0.113, 0.256, 0.087, 0.327, 0.093, 0.187, 0.277, 0.135, 0.144, 0.108]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:19 (running for 00:10:37.57)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.767 |  0.179 |                   31 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.432 |  0.173 |                   29 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.399 |  0.164 |                    8 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.351 |  0.157 |                    6 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17922894656658173
[2m[36m(func pid=14018)[0m mae:  0.13162267208099365
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.109, 0.19, 0.293, 0.139, 0.144, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15546007454395294
[2m[36m(func pid=20486)[0m mae:  0.10367442667484283
[2m[36m(func pid=20486)[0m rmse_per_class: [0.097, 0.223, 0.037, 0.327, 0.051, 0.166, 0.227, 0.152, 0.167, 0.11]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4284 | Steps: 4 | Val loss: 0.3255 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3950 | Steps: 4 | Val loss: 0.3026 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7662 | Steps: 4 | Val loss: 0.5904 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3126 | Steps: 4 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=19964)[0m rmse: 0.16256758570671082
[2m[36m(func pid=19964)[0m mae:  0.11708197742700577
[2m[36m(func pid=19964)[0m rmse_per_class: [0.108, 0.257, 0.066, 0.318, 0.07, 0.179, 0.253, 0.127, 0.143, 0.106]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1732778549194336
[2m[36m(func pid=14601)[0m mae:  0.12629267573356628
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.257, 0.089, 0.326, 0.093, 0.187, 0.278, 0.135, 0.144, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.1793171614408493
[2m[36m(func pid=14018)[0m mae:  0.13171449303627014
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.108, 0.19, 0.294, 0.139, 0.144, 0.108]
[2m[36m(func pid=14018)[0m 
== Status ==
Current time: 2024-01-07 20:00:25 (running for 00:10:42.76)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.766 |  0.179 |                   32 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.428 |  0.173 |                   30 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.395 |  0.163 |                    9 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.347 |  0.155 |                    7 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=20486)[0m rmse: 0.1531480997800827
[2m[36m(func pid=20486)[0m mae:  0.1018839031457901
[2m[36m(func pid=20486)[0m rmse_per_class: [0.088, 0.209, 0.038, 0.305, 0.049, 0.167, 0.233, 0.138, 0.195, 0.11]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3877 | Steps: 4 | Val loss: 0.2997 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4229 | Steps: 4 | Val loss: 0.3224 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7604 | Steps: 4 | Val loss: 0.5877 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3049 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=19964)[0m rmse: 0.16131432354450226
[2m[36m(func pid=19964)[0m mae:  0.11597953736782074
[2m[36m(func pid=19964)[0m rmse_per_class: [0.106, 0.255, 0.064, 0.317, 0.068, 0.178, 0.25, 0.124, 0.141, 0.109]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17290422320365906
[2m[36m(func pid=14601)[0m mae:  0.12597587704658508
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.257, 0.089, 0.326, 0.092, 0.187, 0.277, 0.135, 0.144, 0.109]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:30 (running for 00:10:47.86)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.76  |  0.179 |                   33 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.423 |  0.173 |                   31 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.388 |  0.161 |                   10 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.313 |  0.153 |                    8 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1793624609708786
[2m[36m(func pid=14018)[0m mae:  0.13175256550312042
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.337, 0.108, 0.189, 0.294, 0.139, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15280118584632874
[2m[36m(func pid=20486)[0m mae:  0.09810043126344681
[2m[36m(func pid=20486)[0m rmse_per_class: [0.08, 0.208, 0.042, 0.305, 0.051, 0.174, 0.201, 0.143, 0.188, 0.137]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3844 | Steps: 4 | Val loss: 0.2937 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4177 | Steps: 4 | Val loss: 0.3212 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7585 | Steps: 4 | Val loss: 0.5877 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.2979 | Steps: 4 | Val loss: 0.2660 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=19964)[0m rmse: 0.15884767472743988
[2m[36m(func pid=19964)[0m mae:  0.11364389955997467
[2m[36m(func pid=19964)[0m rmse_per_class: [0.103, 0.253, 0.061, 0.314, 0.066, 0.176, 0.244, 0.122, 0.14, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17297697067260742
[2m[36m(func pid=14601)[0m mae:  0.1260739266872406
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.257, 0.09, 0.325, 0.09, 0.187, 0.277, 0.135, 0.144, 0.108]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:35 (running for 00:10:53.14)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.759 |  0.179 |                   34 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.418 |  0.173 |                   32 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.384 |  0.159 |                   11 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.305 |  0.153 |                    9 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17943806946277618
[2m[36m(func pid=14018)[0m mae:  0.13178859651088715
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.109, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14265525341033936
[2m[36m(func pid=20486)[0m mae:  0.09131098538637161
[2m[36m(func pid=20486)[0m rmse_per_class: [0.077, 0.204, 0.037, 0.287, 0.068, 0.157, 0.207, 0.102, 0.157, 0.13]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3738 | Steps: 4 | Val loss: 0.2889 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4212 | Steps: 4 | Val loss: 0.3202 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7541 | Steps: 4 | Val loss: 0.5827 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2851 | Steps: 4 | Val loss: 0.2690 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=19964)[0m rmse: 0.15696483850479126
[2m[36m(func pid=19964)[0m mae:  0.11207835376262665
[2m[36m(func pid=19964)[0m rmse_per_class: [0.101, 0.251, 0.06, 0.308, 0.064, 0.175, 0.242, 0.119, 0.139, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1727214902639389
[2m[36m(func pid=14601)[0m mae:  0.1258222758769989
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.257, 0.09, 0.325, 0.09, 0.187, 0.275, 0.135, 0.144, 0.109]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:40 (running for 00:10:58.60)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.754 |  0.179 |                   35 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.421 |  0.173 |                   33 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.374 |  0.157 |                   12 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.298 |  0.143 |                   10 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17930088937282562
[2m[36m(func pid=14018)[0m mae:  0.13166597485542297
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.109, 0.189, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14892283082008362
[2m[36m(func pid=20486)[0m mae:  0.09241835027933121
[2m[36m(func pid=20486)[0m rmse_per_class: [0.076, 0.211, 0.077, 0.29, 0.058, 0.167, 0.196, 0.115, 0.148, 0.151]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3603 | Steps: 4 | Val loss: 0.2843 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4131 | Steps: 4 | Val loss: 0.3182 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7492 | Steps: 4 | Val loss: 0.5790 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2946 | Steps: 4 | Val loss: 0.2620 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=19964)[0m rmse: 0.1553943157196045
[2m[36m(func pid=19964)[0m mae:  0.11084184795618057
[2m[36m(func pid=19964)[0m rmse_per_class: [0.1, 0.25, 0.059, 0.303, 0.064, 0.174, 0.24, 0.117, 0.139, 0.108]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17248129844665527
[2m[36m(func pid=14601)[0m mae:  0.12558618187904358
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.257, 0.089, 0.324, 0.089, 0.187, 0.275, 0.135, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:46 (running for 00:11:03.88)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.749 |  0.179 |                   36 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.413 |  0.172 |                   34 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.36  |  0.155 |                   13 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.285 |  0.149 |                   11 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17911198735237122
[2m[36m(func pid=14018)[0m mae:  0.13152748346328735
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14512261748313904
[2m[36m(func pid=20486)[0m mae:  0.08910419046878815
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.197, 0.079, 0.289, 0.072, 0.156, 0.204, 0.105, 0.142, 0.137]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4118 | Steps: 4 | Val loss: 0.3154 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3602 | Steps: 4 | Val loss: 0.2809 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7451 | Steps: 4 | Val loss: 0.5772 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2893 | Steps: 4 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=14601)[0m rmse: 0.17168353497982025
[2m[36m(func pid=14601)[0m mae:  0.12491993606090546
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.257, 0.087, 0.323, 0.089, 0.187, 0.272, 0.135, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.15367810428142548
[2m[36m(func pid=19964)[0m mae:  0.10947851091623306
[2m[36m(func pid=19964)[0m rmse_per_class: [0.1, 0.247, 0.058, 0.3, 0.063, 0.172, 0.239, 0.114, 0.138, 0.106]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:00:51 (running for 00:11:09.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.745 |  0.179 |                   37 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.412 |  0.172 |                   35 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.36  |  0.154 |                   14 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.295 |  0.145 |                   12 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1791445016860962
[2m[36m(func pid=14018)[0m mae:  0.13152238726615906
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.108, 0.19, 0.293, 0.14, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14775848388671875
[2m[36m(func pid=20486)[0m mae:  0.0900745764374733
[2m[36m(func pid=20486)[0m rmse_per_class: [0.084, 0.203, 0.087, 0.296, 0.061, 0.148, 0.208, 0.123, 0.13, 0.137]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4134 | Steps: 4 | Val loss: 0.3129 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3611 | Steps: 4 | Val loss: 0.2783 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7415 | Steps: 4 | Val loss: 0.5744 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2806 | Steps: 4 | Val loss: 0.2886 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=19964)[0m rmse: 0.1521795690059662
[2m[36m(func pid=19964)[0m mae:  0.10830837488174438
[2m[36m(func pid=19964)[0m rmse_per_class: [0.098, 0.245, 0.057, 0.297, 0.061, 0.17, 0.24, 0.112, 0.139, 0.103]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.17092517018318176
[2m[36m(func pid=14601)[0m mae:  0.1242516040802002
[2m[36m(func pid=14601)[0m rmse_per_class: [0.113, 0.256, 0.086, 0.322, 0.089, 0.186, 0.27, 0.134, 0.144, 0.109]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:00:56 (running for 00:11:14.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.741 |  0.179 |                   38 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.413 |  0.171 |                   36 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.361 |  0.152 |                   15 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.289 |  0.148 |                   13 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17934611439704895
[2m[36m(func pid=14018)[0m mae:  0.13166050612926483
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.16365906596183777
[2m[36m(func pid=20486)[0m mae:  0.10569919645786285
[2m[36m(func pid=20486)[0m rmse_per_class: [0.073, 0.224, 0.055, 0.311, 0.075, 0.165, 0.246, 0.12, 0.149, 0.219]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3523 | Steps: 4 | Val loss: 0.2758 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4084 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7394 | Steps: 4 | Val loss: 0.5729 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2783 | Steps: 4 | Val loss: 0.2650 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=14601)[0m rmse: 0.1708732694387436
[2m[36m(func pid=14601)[0m mae:  0.12423703819513321
[2m[36m(func pid=14601)[0m rmse_per_class: [0.113, 0.256, 0.086, 0.322, 0.088, 0.186, 0.271, 0.134, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.1503150910139084
[2m[36m(func pid=19964)[0m mae:  0.10635785013437271
[2m[36m(func pid=19964)[0m rmse_per_class: [0.096, 0.245, 0.054, 0.298, 0.061, 0.165, 0.236, 0.11, 0.139, 0.099]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:01:01 (running for 00:11:19.66)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.739 |  0.179 |                   39 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.408 |  0.171 |                   37 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.352 |  0.15  |                   16 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.281 |  0.164 |                   14 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17933985590934753
[2m[36m(func pid=14018)[0m mae:  0.13168378174304962
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.11, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14689430594444275
[2m[36m(func pid=20486)[0m mae:  0.09194744378328323
[2m[36m(func pid=20486)[0m rmse_per_class: [0.078, 0.22, 0.041, 0.296, 0.083, 0.167, 0.201, 0.104, 0.123, 0.157]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4077 | Steps: 4 | Val loss: 0.3119 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3499 | Steps: 4 | Val loss: 0.2754 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7354 | Steps: 4 | Val loss: 0.5708 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2775 | Steps: 4 | Val loss: 0.2599 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=14601)[0m rmse: 0.17080776393413544
[2m[36m(func pid=14601)[0m mae:  0.12430691719055176
[2m[36m(func pid=14601)[0m rmse_per_class: [0.113, 0.255, 0.085, 0.321, 0.088, 0.186, 0.273, 0.133, 0.144, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.1502726972103119
[2m[36m(func pid=19964)[0m mae:  0.10669545829296112
[2m[36m(func pid=19964)[0m rmse_per_class: [0.096, 0.245, 0.053, 0.295, 0.062, 0.165, 0.241, 0.107, 0.139, 0.101]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:01:07 (running for 00:11:24.82)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.735 |  0.179 |                   40 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.408 |  0.171 |                   38 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.35  |  0.15  |                   17 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.278 |  0.147 |                   15 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1793157160282135
[2m[36m(func pid=14018)[0m mae:  0.13163058459758759
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.109, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14142243564128876
[2m[36m(func pid=20486)[0m mae:  0.08661595731973648
[2m[36m(func pid=20486)[0m rmse_per_class: [0.074, 0.219, 0.046, 0.261, 0.075, 0.162, 0.199, 0.117, 0.123, 0.138]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3521 | Steps: 4 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4045 | Steps: 4 | Val loss: 0.3110 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7337 | Steps: 4 | Val loss: 0.5663 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2579 | Steps: 4 | Val loss: 0.2750 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 20:01:12 (running for 00:11:29.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.735 |  0.179 |                   40 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.408 |  0.171 |                   38 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.352 |  0.149 |                   18 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.278 |  0.141 |                   16 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14601)[0m rmse: 0.17061397433280945
[2m[36m(func pid=14601)[0m mae:  0.12412188947200775
[2m[36m(func pid=14601)[0m rmse_per_class: [0.113, 0.255, 0.084, 0.321, 0.088, 0.186, 0.273, 0.133, 0.144, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14910176396369934
[2m[36m(func pid=19964)[0m mae:  0.10534000396728516
[2m[36m(func pid=19964)[0m rmse_per_class: [0.094, 0.243, 0.052, 0.295, 0.062, 0.162, 0.239, 0.104, 0.137, 0.103]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17898502945899963
[2m[36m(func pid=14018)[0m mae:  0.1313403695821762
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.11, 0.19, 0.294, 0.14, 0.143, 0.107]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15729887783527374
[2m[36m(func pid=20486)[0m mae:  0.09366398304700851
[2m[36m(func pid=20486)[0m rmse_per_class: [0.105, 0.228, 0.105, 0.274, 0.071, 0.16, 0.228, 0.15, 0.133, 0.119]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4032 | Steps: 4 | Val loss: 0.3101 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3521 | Steps: 4 | Val loss: 0.2748 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7309 | Steps: 4 | Val loss: 0.5642 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 20:01:17 (running for 00:11:35.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.734 |  0.179 |                   41 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.404 |  0.171 |                   39 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.352 |  0.15  |                   19 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.258 |  0.157 |                   17 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.14990511536598206
[2m[36m(func pid=19964)[0m mae:  0.10623574256896973
[2m[36m(func pid=19964)[0m rmse_per_class: [0.095, 0.244, 0.053, 0.295, 0.062, 0.163, 0.243, 0.101, 0.138, 0.104]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2860 | Steps: 4 | Val loss: 0.3135 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=14018)[0m rmse: 0.17909923195838928
[2m[36m(func pid=14018)[0m mae:  0.13140447437763214
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.11, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=14601)[0m rmse: 0.1701248586177826
[2m[36m(func pid=14601)[0m mae:  0.12377288192510605
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.255, 0.082, 0.32, 0.088, 0.185, 0.271, 0.134, 0.143, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.17980070412158966
[2m[36m(func pid=20486)[0m mae:  0.10616572201251984
[2m[36m(func pid=20486)[0m rmse_per_class: [0.124, 0.239, 0.125, 0.324, 0.063, 0.171, 0.25, 0.134, 0.239, 0.128]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3502 | Steps: 4 | Val loss: 0.2730 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3988 | Steps: 4 | Val loss: 0.3088 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7244 | Steps: 4 | Val loss: 0.5629 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2792 | Steps: 4 | Val loss: 0.3766 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:01:23 (running for 00:11:40.74)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.731 |  0.179 |                   42 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.403 |  0.17  |                   40 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.35  |  0.149 |                   20 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.286 |  0.18  |                   18 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1487058848142624
[2m[36m(func pid=19964)[0m mae:  0.10486570745706558
[2m[36m(func pid=19964)[0m rmse_per_class: [0.094, 0.242, 0.05, 0.293, 0.062, 0.162, 0.239, 0.103, 0.139, 0.104]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1698489934206009
[2m[36m(func pid=14601)[0m mae:  0.12354743480682373
[2m[36m(func pid=14601)[0m rmse_per_class: [0.115, 0.255, 0.082, 0.32, 0.087, 0.185, 0.271, 0.133, 0.143, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17913572490215302
[2m[36m(func pid=14018)[0m mae:  0.1314287930727005
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.335, 0.109, 0.19, 0.294, 0.14, 0.143, 0.107]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.18366964161396027
[2m[36m(func pid=20486)[0m mae:  0.11078138649463654
[2m[36m(func pid=20486)[0m rmse_per_class: [0.103, 0.233, 0.057, 0.343, 0.061, 0.171, 0.228, 0.113, 0.404, 0.125]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3514 | Steps: 4 | Val loss: 0.2719 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4003 | Steps: 4 | Val loss: 0.3083 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7196 | Steps: 4 | Val loss: 0.5602 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2780 | Steps: 4 | Val loss: 0.3327 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:01:28 (running for 00:11:46.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.724 |  0.179 |                   43 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.399 |  0.17  |                   41 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.351 |  0.148 |                   21 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.279 |  0.184 |                   19 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1479288935661316
[2m[36m(func pid=19964)[0m mae:  0.10413189232349396
[2m[36m(func pid=19964)[0m rmse_per_class: [0.093, 0.241, 0.047, 0.291, 0.061, 0.16, 0.24, 0.105, 0.14, 0.102]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.1792113333940506
[2m[36m(func pid=14018)[0m mae:  0.1315542310476303
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.336, 0.109, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16972030699253082
[2m[36m(func pid=14601)[0m mae:  0.12345445156097412
[2m[36m(func pid=14601)[0m rmse_per_class: [0.114, 0.255, 0.082, 0.319, 0.086, 0.184, 0.271, 0.133, 0.142, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.18115629255771637
[2m[36m(func pid=20486)[0m mae:  0.1123947948217392
[2m[36m(func pid=20486)[0m rmse_per_class: [0.107, 0.208, 0.066, 0.324, 0.073, 0.182, 0.249, 0.129, 0.366, 0.107]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7184 | Steps: 4 | Val loss: 0.5582 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3347 | Steps: 4 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4030 | Steps: 4 | Val loss: 0.3066 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:01:34 (running for 00:11:51.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.718 |  0.179 |                   45 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.4   |  0.17  |                   42 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.351 |  0.148 |                   21 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.278 |  0.181 |                   20 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.179216668009758
[2m[36m(func pid=14018)[0m mae:  0.13147367537021637
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.259, 0.098, 0.335, 0.108, 0.19, 0.294, 0.139, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16897538304328918
[2m[36m(func pid=14601)[0m mae:  0.12271122634410858
[2m[36m(func pid=14601)[0m rmse_per_class: [0.113, 0.255, 0.081, 0.319, 0.085, 0.184, 0.268, 0.132, 0.143, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14841711521148682
[2m[36m(func pid=19964)[0m mae:  0.10458860546350479
[2m[36m(func pid=19964)[0m rmse_per_class: [0.093, 0.24, 0.048, 0.291, 0.062, 0.159, 0.246, 0.104, 0.141, 0.101]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2714 | Steps: 4 | Val loss: 0.2719 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=20486)[0m rmse: 0.15302903950214386
[2m[36m(func pid=20486)[0m mae:  0.0905497595667839
[2m[36m(func pid=20486)[0m rmse_per_class: [0.095, 0.207, 0.068, 0.29, 0.079, 0.164, 0.217, 0.124, 0.174, 0.113]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3997 | Steps: 4 | Val loss: 0.3060 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3347 | Steps: 4 | Val loss: 0.2741 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7183 | Steps: 4 | Val loss: 0.5560 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=14018)[0m rmse: 0.17927099764347076
[2m[36m(func pid=14018)[0m mae:  0.13150960206985474
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.259, 0.098, 0.335, 0.109, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14601)[0m rmse: 0.16880127787590027
[2m[36m(func pid=14601)[0m mae:  0.12249957025051117
[2m[36m(func pid=14601)[0m rmse_per_class: [0.112, 0.255, 0.081, 0.319, 0.085, 0.184, 0.267, 0.132, 0.142, 0.11]
[2m[36m(func pid=14601)[0m 
== Status ==
Current time: 2024-01-07 20:01:39 (running for 00:11:57.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.718 |  0.179 |                   45 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.4   |  0.169 |                   44 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.335 |  0.148 |                   22 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.271 |  0.153 |                   21 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14911869168281555
[2m[36m(func pid=19964)[0m mae:  0.1048288494348526
[2m[36m(func pid=19964)[0m rmse_per_class: [0.094, 0.24, 0.052, 0.295, 0.061, 0.158, 0.246, 0.103, 0.141, 0.101]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2791 | Steps: 4 | Val loss: 0.3109 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=20486)[0m rmse: 0.18179084360599518
[2m[36m(func pid=20486)[0m mae:  0.10834981501102448
[2m[36m(func pid=20486)[0m rmse_per_class: [0.103, 0.218, 0.12, 0.336, 0.081, 0.17, 0.251, 0.267, 0.13, 0.141]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3261 | Steps: 4 | Val loss: 0.2745 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3975 | Steps: 4 | Val loss: 0.3050 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7102 | Steps: 4 | Val loss: 0.5522 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 20:01:45 (running for 00:12:02.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.718 |  0.179 |                   46 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.4   |  0.169 |                   44 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.326 |  0.149 |                   24 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.279 |  0.182 |                   22 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.14915184676647186
[2m[36m(func pid=19964)[0m mae:  0.10502956807613373
[2m[36m(func pid=19964)[0m rmse_per_class: [0.096, 0.239, 0.053, 0.296, 0.061, 0.159, 0.246, 0.099, 0.142, 0.101]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16861125826835632
[2m[36m(func pid=14601)[0m mae:  0.12241475284099579
[2m[36m(func pid=14601)[0m rmse_per_class: [0.112, 0.254, 0.081, 0.32, 0.085, 0.184, 0.268, 0.132, 0.142, 0.11]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17915862798690796
[2m[36m(func pid=14018)[0m mae:  0.13144102692604065
[2m[36m(func pid=14018)[0m rmse_per_class: [0.115, 0.259, 0.098, 0.336, 0.11, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2860 | Steps: 4 | Val loss: 0.3264 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=20486)[0m rmse: 0.19129690527915955
[2m[36m(func pid=20486)[0m mae:  0.11550439894199371
[2m[36m(func pid=20486)[0m rmse_per_class: [0.081, 0.222, 0.143, 0.341, 0.103, 0.182, 0.261, 0.316, 0.125, 0.139]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3245 | Steps: 4 | Val loss: 0.2740 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3929 | Steps: 4 | Val loss: 0.3047 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7004 | Steps: 4 | Val loss: 0.5502 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=19964)[0m rmse: 0.14899536967277527
[2m[36m(func pid=19964)[0m mae:  0.10502171516418457
[2m[36m(func pid=19964)[0m rmse_per_class: [0.099, 0.238, 0.055, 0.296, 0.061, 0.16, 0.245, 0.097, 0.14, 0.099]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:01:50 (running for 00:12:08.18)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.71  |  0.179 |                   47 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.398 |  0.169 |                   45 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.325 |  0.149 |                   25 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.286 |  0.191 |                   23 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17929372191429138
[2m[36m(func pid=14018)[0m mae:  0.1315687596797943
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.098, 0.336, 0.109, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=14601)[0m rmse: 0.1684356927871704
[2m[36m(func pid=14601)[0m mae:  0.12232720851898193
[2m[36m(func pid=14601)[0m rmse_per_class: [0.112, 0.253, 0.081, 0.319, 0.084, 0.184, 0.268, 0.132, 0.143, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2855 | Steps: 4 | Val loss: 0.2818 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3327 | Steps: 4 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=20486)[0m rmse: 0.15511073172092438
[2m[36m(func pid=20486)[0m mae:  0.09530757367610931
[2m[36m(func pid=20486)[0m rmse_per_class: [0.092, 0.205, 0.054, 0.33, 0.102, 0.163, 0.19, 0.129, 0.156, 0.13]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7062 | Steps: 4 | Val loss: 0.5468 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3949 | Steps: 4 | Val loss: 0.3040 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:01:55 (running for 00:12:13.62)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.7   |  0.179 |                   48 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.393 |  0.168 |                   46 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.333 |  0.149 |                   26 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.285 |  0.155 |                   24 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.14890794456005096
[2m[36m(func pid=19964)[0m mae:  0.10473580658435822
[2m[36m(func pid=19964)[0m rmse_per_class: [0.103, 0.234, 0.056, 0.3, 0.06, 0.162, 0.237, 0.097, 0.14, 0.1]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1682080775499344
[2m[36m(func pid=14601)[0m mae:  0.1221504956483841
[2m[36m(func pid=14601)[0m rmse_per_class: [0.112, 0.254, 0.081, 0.318, 0.084, 0.183, 0.267, 0.132, 0.142, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17921166121959686
[2m[36m(func pid=14018)[0m mae:  0.13154354691505432
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.098, 0.336, 0.108, 0.19, 0.294, 0.139, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2779 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=20486)[0m rmse: 0.15519757568836212
[2m[36m(func pid=20486)[0m mae:  0.0943245217204094
[2m[36m(func pid=20486)[0m rmse_per_class: [0.091, 0.203, 0.053, 0.323, 0.109, 0.164, 0.186, 0.114, 0.164, 0.145]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3958 | Steps: 4 | Val loss: 0.3025 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3226 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7047 | Steps: 4 | Val loss: 0.5438 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:02:01 (running for 00:12:19.12)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.706 |  0.179 |                   49 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.396 |  0.168 |                   48 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.333 |  0.149 |                   26 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.278 |  0.155 |                   25 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14601)[0m rmse: 0.16763296723365784
[2m[36m(func pid=14601)[0m mae:  0.12171635776758194
[2m[36m(func pid=14601)[0m rmse_per_class: [0.112, 0.253, 0.08, 0.317, 0.083, 0.183, 0.267, 0.131, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.1480504721403122
[2m[36m(func pid=19964)[0m mae:  0.1035623773932457
[2m[36m(func pid=19964)[0m rmse_per_class: [0.099, 0.233, 0.053, 0.298, 0.061, 0.162, 0.235, 0.098, 0.139, 0.103]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17909109592437744
[2m[36m(func pid=14018)[0m mae:  0.13145312666893005
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.098, 0.336, 0.108, 0.19, 0.293, 0.138, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2717 | Steps: 4 | Val loss: 0.2958 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3952 | Steps: 4 | Val loss: 0.3019 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3323 | Steps: 4 | Val loss: 0.2668 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6980 | Steps: 4 | Val loss: 0.5423 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=20486)[0m rmse: 0.16298413276672363
[2m[36m(func pid=20486)[0m mae:  0.10164555162191391
[2m[36m(func pid=20486)[0m rmse_per_class: [0.073, 0.212, 0.036, 0.329, 0.089, 0.182, 0.197, 0.13, 0.138, 0.245]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:02:06 (running for 00:12:24.50)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.705 |  0.179 |                   50 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.396 |  0.168 |                   48 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.332 |  0.145 |                   28 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.272 |  0.163 |                   26 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.14453792572021484
[2m[36m(func pid=19964)[0m mae:  0.09992183744907379
[2m[36m(func pid=19964)[0m rmse_per_class: [0.093, 0.232, 0.048, 0.29, 0.061, 0.156, 0.233, 0.103, 0.133, 0.098]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17932479083538055
[2m[36m(func pid=14018)[0m mae:  0.13169078528881073
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.337, 0.107, 0.19, 0.293, 0.139, 0.144, 0.109]
[2m[36m(func pid=14601)[0m rmse: 0.1674707531929016
[2m[36m(func pid=14601)[0m mae:  0.1216530054807663
[2m[36m(func pid=14601)[0m rmse_per_class: [0.111, 0.253, 0.08, 0.316, 0.083, 0.183, 0.268, 0.132, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2509 | Steps: 4 | Val loss: 0.3119 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3287 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6944 | Steps: 4 | Val loss: 0.5370 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3927 | Steps: 4 | Val loss: 0.3015 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=20486)[0m rmse: 0.16920766234397888
[2m[36m(func pid=20486)[0m mae:  0.10488488525152206
[2m[36m(func pid=20486)[0m rmse_per_class: [0.074, 0.226, 0.038, 0.336, 0.086, 0.167, 0.202, 0.138, 0.143, 0.283]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:02:12 (running for 00:12:29.97)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.698 |  0.179 |                   51 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.393 |  0.167 |                   50 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.332 |  0.145 |                   28 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.251 |  0.169 |                   27 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14601)[0m rmse: 0.16733121871948242
[2m[36m(func pid=14601)[0m mae:  0.12153021991252899
[2m[36m(func pid=14601)[0m rmse_per_class: [0.111, 0.252, 0.079, 0.316, 0.083, 0.183, 0.268, 0.131, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14521929621696472
[2m[36m(func pid=19964)[0m mae:  0.10031865537166595
[2m[36m(func pid=19964)[0m rmse_per_class: [0.094, 0.232, 0.054, 0.289, 0.062, 0.156, 0.236, 0.097, 0.132, 0.101]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17907680571079254
[2m[36m(func pid=14018)[0m mae:  0.13151809573173523
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.106, 0.19, 0.293, 0.139, 0.144, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2883 | Steps: 4 | Val loss: 0.2967 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6927 | Steps: 4 | Val loss: 0.5359 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3960 | Steps: 4 | Val loss: 0.3001 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3143 | Steps: 4 | Val loss: 0.2666 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=20486)[0m rmse: 0.1626971960067749
[2m[36m(func pid=20486)[0m mae:  0.09846083074808121
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.225, 0.042, 0.318, 0.086, 0.164, 0.194, 0.154, 0.147, 0.227]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14397333562374115
[2m[36m(func pid=19964)[0m mae:  0.09916529804468155
[2m[36m(func pid=19964)[0m rmse_per_class: [0.091, 0.231, 0.054, 0.286, 0.063, 0.155, 0.237, 0.096, 0.13, 0.098]
== Status ==
Current time: 2024-01-07 20:02:17 (running for 00:12:35.40)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.694 |  0.179 |                   52 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.393 |  0.167 |                   50 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.314 |  0.144 |                   30 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.288 |  0.163 |                   28 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14601)[0m rmse: 0.16663797199726105
[2m[36m(func pid=14601)[0m mae:  0.12088391929864883
[2m[36m(func pid=14601)[0m rmse_per_class: [0.111, 0.252, 0.077, 0.316, 0.082, 0.182, 0.265, 0.131, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17896007001399994
[2m[36m(func pid=14018)[0m mae:  0.1313936561346054
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.106, 0.19, 0.292, 0.139, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2750 | Steps: 4 | Val loss: 0.2718 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3265 | Steps: 4 | Val loss: 0.2656 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6857 | Steps: 4 | Val loss: 0.5340 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3934 | Steps: 4 | Val loss: 0.2998 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=20486)[0m rmse: 0.14959785342216492
[2m[36m(func pid=20486)[0m mae:  0.09057660400867462
[2m[36m(func pid=20486)[0m rmse_per_class: [0.069, 0.227, 0.043, 0.296, 0.129, 0.16, 0.189, 0.105, 0.145, 0.132]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.1789242923259735
[2m[36m(func pid=14018)[0m mae:  0.13136225938796997
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.336, 0.106, 0.19, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=14018)[0m 
== Status ==
Current time: 2024-01-07 20:02:23 (running for 00:12:40.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.686 |  0.179 |                   54 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.396 |  0.167 |                   51 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.314 |  0.144 |                   30 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.275 |  0.15  |                   29 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.14342796802520752
[2m[36m(func pid=19964)[0m mae:  0.09829086065292358
[2m[36m(func pid=19964)[0m rmse_per_class: [0.089, 0.231, 0.049, 0.285, 0.063, 0.157, 0.234, 0.096, 0.129, 0.102]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16662946343421936
[2m[36m(func pid=14601)[0m mae:  0.12092296034097672
[2m[36m(func pid=14601)[0m rmse_per_class: [0.112, 0.252, 0.076, 0.315, 0.082, 0.182, 0.266, 0.131, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2644 | Steps: 4 | Val loss: 0.2861 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6836 | Steps: 4 | Val loss: 0.5281 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3901 | Steps: 4 | Val loss: 0.3003 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3098 | Steps: 4 | Val loss: 0.2643 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=20486)[0m rmse: 0.1618579775094986
[2m[36m(func pid=20486)[0m mae:  0.09793543815612793
[2m[36m(func pid=20486)[0m rmse_per_class: [0.087, 0.218, 0.044, 0.302, 0.112, 0.171, 0.222, 0.133, 0.159, 0.171]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:02:28 (running for 00:12:46.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.684 |  0.179 |                   55 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.393 |  0.167 |                   52 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.326 |  0.143 |                   31 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.264 |  0.162 |                   30 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17880432307720184
[2m[36m(func pid=14018)[0m mae:  0.13124242424964905
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.106, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14309504628181458
[2m[36m(func pid=19964)[0m mae:  0.09765340387821198
[2m[36m(func pid=19964)[0m rmse_per_class: [0.088, 0.232, 0.052, 0.28, 0.063, 0.156, 0.232, 0.096, 0.128, 0.105]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16689661145210266
[2m[36m(func pid=14601)[0m mae:  0.12121232599020004
[2m[36m(func pid=14601)[0m rmse_per_class: [0.112, 0.253, 0.077, 0.315, 0.082, 0.182, 0.267, 0.131, 0.142, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2714 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6874 | Steps: 4 | Val loss: 0.5248 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3214 | Steps: 4 | Val loss: 0.2655 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3889 | Steps: 4 | Val loss: 0.2992 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=20486)[0m rmse: 0.1408367156982422
[2m[36m(func pid=20486)[0m mae:  0.08421645313501358
[2m[36m(func pid=20486)[0m rmse_per_class: [0.079, 0.219, 0.034, 0.306, 0.061, 0.153, 0.193, 0.107, 0.15, 0.106]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:02:34 (running for 00:12:51.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.687 |  0.179 |                   56 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.39  |  0.167 |                   53 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.31  |  0.143 |                   32 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.271 |  0.141 |                   31 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1786959171295166
[2m[36m(func pid=14018)[0m mae:  0.1311282515525818
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.106, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.1446884721517563
[2m[36m(func pid=19964)[0m mae:  0.09855421632528305
[2m[36m(func pid=19964)[0m rmse_per_class: [0.09, 0.232, 0.058, 0.281, 0.062, 0.156, 0.23, 0.097, 0.129, 0.113]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1663215458393097
[2m[36m(func pid=14601)[0m mae:  0.1206938624382019
[2m[36m(func pid=14601)[0m rmse_per_class: [0.111, 0.252, 0.076, 0.316, 0.082, 0.181, 0.265, 0.131, 0.141, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2668 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6831 | Steps: 4 | Val loss: 0.5228 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3891 | Steps: 4 | Val loss: 0.2989 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3100 | Steps: 4 | Val loss: 0.2635 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=20486)[0m rmse: 0.14419648051261902
[2m[36m(func pid=20486)[0m mae:  0.08649523556232452
[2m[36m(func pid=20486)[0m rmse_per_class: [0.082, 0.209, 0.035, 0.309, 0.051, 0.154, 0.202, 0.126, 0.158, 0.115]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:02:39 (running for 00:12:57.37)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.683 |  0.179 |                   57 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.389 |  0.166 |                   54 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.321 |  0.145 |                   33 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.267 |  0.144 |                   32 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17880412936210632
[2m[36m(func pid=14018)[0m mae:  0.1312219500541687
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.106, 0.189, 0.292, 0.139, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16618409752845764
[2m[36m(func pid=14601)[0m mae:  0.12065517902374268
[2m[36m(func pid=14601)[0m rmse_per_class: [0.111, 0.252, 0.076, 0.315, 0.082, 0.181, 0.266, 0.131, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14301516115665436
[2m[36m(func pid=19964)[0m mae:  0.09746347367763519
[2m[36m(func pid=19964)[0m rmse_per_class: [0.088, 0.229, 0.054, 0.28, 0.062, 0.154, 0.227, 0.095, 0.128, 0.112]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2794 | Steps: 4 | Val loss: 0.2652 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6737 | Steps: 4 | Val loss: 0.5212 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3927 | Steps: 4 | Val loss: 0.2973 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3206 | Steps: 4 | Val loss: 0.2603 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=20486)[0m rmse: 0.14339198172092438
[2m[36m(func pid=20486)[0m mae:  0.08523780107498169
[2m[36m(func pid=20486)[0m rmse_per_class: [0.082, 0.209, 0.057, 0.302, 0.057, 0.15, 0.193, 0.137, 0.137, 0.11]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:02:45 (running for 00:13:02.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.674 |  0.179 |                   58 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.389 |  0.166 |                   55 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.31  |  0.143 |                   34 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.279 |  0.143 |                   33 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17887373268604279
[2m[36m(func pid=14018)[0m mae:  0.13129156827926636
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.105, 0.19, 0.291, 0.139, 0.144, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1652454435825348
[2m[36m(func pid=14601)[0m mae:  0.11977200210094452
[2m[36m(func pid=14601)[0m rmse_per_class: [0.109, 0.251, 0.074, 0.315, 0.083, 0.181, 0.264, 0.13, 0.141, 0.106]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14064577221870422
[2m[36m(func pid=19964)[0m mae:  0.09540331363677979
[2m[36m(func pid=19964)[0m rmse_per_class: [0.083, 0.227, 0.045, 0.275, 0.063, 0.154, 0.225, 0.097, 0.126, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2987 | Steps: 4 | Val loss: 0.2617 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6728 | Steps: 4 | Val loss: 0.5192 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3906 | Steps: 4 | Val loss: 0.2974 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=20486)[0m rmse: 0.1419055312871933
[2m[36m(func pid=20486)[0m mae:  0.08587508648633957
[2m[36m(func pid=20486)[0m rmse_per_class: [0.064, 0.22, 0.061, 0.28, 0.082, 0.153, 0.193, 0.126, 0.133, 0.107]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3098 | Steps: 4 | Val loss: 0.2593 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:02:50 (running for 00:13:08.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.673 |  0.179 |                   59 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.393 |  0.165 |                   56 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.321 |  0.141 |                   35 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.299 |  0.142 |                   34 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17876726388931274
[2m[36m(func pid=14018)[0m mae:  0.1312045305967331
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.106, 0.19, 0.292, 0.139, 0.144, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1652635633945465
[2m[36m(func pid=14601)[0m mae:  0.11978413164615631
[2m[36m(func pid=14601)[0m rmse_per_class: [0.109, 0.25, 0.074, 0.315, 0.083, 0.181, 0.264, 0.129, 0.141, 0.107]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.13974064588546753
[2m[36m(func pid=19964)[0m mae:  0.0952599048614502
[2m[36m(func pid=19964)[0m rmse_per_class: [0.081, 0.224, 0.045, 0.27, 0.065, 0.154, 0.229, 0.095, 0.126, 0.109]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2663 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6707 | Steps: 4 | Val loss: 0.5151 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3867 | Steps: 4 | Val loss: 0.2967 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3224 | Steps: 4 | Val loss: 0.2597 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=20486)[0m rmse: 0.15946578979492188
[2m[36m(func pid=20486)[0m mae:  0.09864245355129242
[2m[36m(func pid=20486)[0m rmse_per_class: [0.074, 0.232, 0.121, 0.296, 0.096, 0.175, 0.231, 0.121, 0.139, 0.11]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:02:55 (running for 00:13:13.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.671 |  0.179 |                   60 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.391 |  0.165 |                   57 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.31  |  0.14  |                   36 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.266 |  0.159 |                   35 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17872647941112518
[2m[36m(func pid=14018)[0m mae:  0.13114595413208008
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.107, 0.189, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16498084366321564
[2m[36m(func pid=14601)[0m mae:  0.11955302953720093
[2m[36m(func pid=14601)[0m rmse_per_class: [0.109, 0.25, 0.074, 0.315, 0.081, 0.18, 0.263, 0.129, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14017850160598755
[2m[36m(func pid=19964)[0m mae:  0.09459462016820908
[2m[36m(func pid=19964)[0m rmse_per_class: [0.079, 0.224, 0.05, 0.272, 0.062, 0.154, 0.227, 0.096, 0.126, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2770 | Steps: 4 | Val loss: 0.3141 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6683 | Steps: 4 | Val loss: 0.5149 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3883 | Steps: 4 | Val loss: 0.2962 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3103 | Steps: 4 | Val loss: 0.2587 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=20486)[0m rmse: 0.1777210235595703
[2m[36m(func pid=20486)[0m mae:  0.10883045196533203
[2m[36m(func pid=20486)[0m rmse_per_class: [0.084, 0.247, 0.12, 0.33, 0.1, 0.173, 0.249, 0.135, 0.213, 0.126]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:03:01 (running for 00:13:18.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.668 |  0.179 |                   61 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.387 |  0.165 |                   58 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.322 |  0.14  |                   37 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.277 |  0.178 |                   36 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17878010869026184
[2m[36m(func pid=14018)[0m mae:  0.1311769038438797
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.107, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16465139389038086
[2m[36m(func pid=14601)[0m mae:  0.11919422447681427
[2m[36m(func pid=14601)[0m rmse_per_class: [0.108, 0.25, 0.073, 0.315, 0.081, 0.18, 0.261, 0.129, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.13888739049434662
[2m[36m(func pid=19964)[0m mae:  0.0938471183180809
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.223, 0.049, 0.273, 0.065, 0.153, 0.231, 0.097, 0.124, 0.099]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2712 | Steps: 4 | Val loss: 0.3044 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6660 | Steps: 4 | Val loss: 0.5129 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3868 | Steps: 4 | Val loss: 0.2965 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3077 | Steps: 4 | Val loss: 0.2617 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=20486)[0m rmse: 0.17180413007736206
[2m[36m(func pid=20486)[0m mae:  0.10419298708438873
[2m[36m(func pid=20486)[0m rmse_per_class: [0.083, 0.237, 0.096, 0.332, 0.107, 0.158, 0.216, 0.106, 0.238, 0.145]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.1788126528263092
[2m[36m(func pid=14018)[0m mae:  0.1311918944120407
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.107, 0.19, 0.292, 0.139, 0.143, 0.11]
== Status ==
Current time: 2024-01-07 20:03:06 (running for 00:13:24.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.666 |  0.179 |                   62 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.388 |  0.165 |                   59 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.31  |  0.139 |                   38 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.271 |  0.172 |                   37 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16486042737960815
[2m[36m(func pid=14601)[0m mae:  0.11940719932317734
[2m[36m(func pid=14601)[0m rmse_per_class: [0.108, 0.25, 0.075, 0.315, 0.08, 0.18, 0.262, 0.129, 0.141, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14159056544303894
[2m[36m(func pid=19964)[0m mae:  0.09506022930145264
[2m[36m(func pid=19964)[0m rmse_per_class: [0.077, 0.225, 0.056, 0.278, 0.063, 0.156, 0.232, 0.096, 0.127, 0.107]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2816 | Steps: 4 | Val loss: 0.3006 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6631 | Steps: 4 | Val loss: 0.5107 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3855 | Steps: 4 | Val loss: 0.2962 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=20486)[0m rmse: 0.16789504885673523
[2m[36m(func pid=20486)[0m mae:  0.10089647769927979
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.212, 0.051, 0.323, 0.089, 0.181, 0.213, 0.115, 0.291, 0.134]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3153 | Steps: 4 | Val loss: 0.2618 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:03:11 (running for 00:13:29.55)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.663 |  0.179 |                   63 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.387 |  0.165 |                   60 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.308 |  0.142 |                   39 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.282 |  0.168 |                   38 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1788206398487091
[2m[36m(func pid=14018)[0m mae:  0.13114804029464722
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.259, 0.098, 0.335, 0.107, 0.19, 0.292, 0.139, 0.144, 0.109]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16476240754127502
[2m[36m(func pid=14601)[0m mae:  0.11931955814361572
[2m[36m(func pid=14601)[0m rmse_per_class: [0.108, 0.25, 0.075, 0.314, 0.08, 0.18, 0.262, 0.128, 0.142, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14213056862354279
[2m[36m(func pid=19964)[0m mae:  0.09463384002447128
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.226, 0.059, 0.278, 0.062, 0.154, 0.229, 0.098, 0.128, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2667 | Steps: 4 | Val loss: 0.3186 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6644 | Steps: 4 | Val loss: 0.5096 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3814 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3054 | Steps: 4 | Val loss: 0.2616 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=20486)[0m rmse: 0.1782095581293106
[2m[36m(func pid=20486)[0m mae:  0.10780253261327744
[2m[36m(func pid=20486)[0m rmse_per_class: [0.089, 0.211, 0.048, 0.325, 0.075, 0.173, 0.238, 0.127, 0.334, 0.162]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14018)[0m rmse: 0.17906329035758972
[2m[36m(func pid=14018)[0m mae:  0.13134337961673737
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.335, 0.107, 0.19, 0.292, 0.139, 0.143, 0.11]
== Status ==
Current time: 2024-01-07 20:03:17 (running for 00:13:35.02)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.664 |  0.179 |                   64 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.386 |  0.165 |                   61 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.315 |  0.142 |                   40 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.267 |  0.178 |                   39 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16467571258544922
[2m[36m(func pid=14601)[0m mae:  0.1192786917090416
[2m[36m(func pid=14601)[0m rmse_per_class: [0.108, 0.25, 0.075, 0.314, 0.08, 0.18, 0.262, 0.128, 0.142, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14175423979759216
[2m[36m(func pid=19964)[0m mae:  0.09500650316476822
[2m[36m(func pid=19964)[0m rmse_per_class: [0.077, 0.224, 0.061, 0.276, 0.063, 0.155, 0.231, 0.093, 0.128, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2549 | Steps: 4 | Val loss: 0.2794 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6575 | Steps: 4 | Val loss: 0.5086 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=20486)[0m rmse: 0.15422280132770538
[2m[36m(func pid=20486)[0m mae:  0.09129367768764496
[2m[36m(func pid=20486)[0m rmse_per_class: [0.084, 0.23, 0.045, 0.318, 0.068, 0.16, 0.205, 0.113, 0.198, 0.12]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3852 | Steps: 4 | Val loss: 0.2951 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3181 | Steps: 4 | Val loss: 0.2608 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=14018)[0m rmse: 0.17922759056091309
[2m[36m(func pid=14018)[0m mae:  0.13152527809143066
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.26, 0.098, 0.336, 0.107, 0.189, 0.291, 0.139, 0.144, 0.11]
== Status ==
Current time: 2024-01-07 20:03:22 (running for 00:13:40.47)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.657 |  0.179 |                   65 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.381 |  0.165 |                   62 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.305 |  0.142 |                   41 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.255 |  0.154 |                   40 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=14018)[0m 

[2m[36m(func pid=14601)[0m rmse: 0.16413842141628265
[2m[36m(func pid=14601)[0m mae:  0.11877541244029999
[2m[36m(func pid=14601)[0m rmse_per_class: [0.108, 0.25, 0.073, 0.314, 0.08, 0.18, 0.26, 0.127, 0.142, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.1410524547100067
[2m[36m(func pid=19964)[0m mae:  0.09457496553659439
[2m[36m(func pid=19964)[0m rmse_per_class: [0.078, 0.223, 0.057, 0.274, 0.062, 0.153, 0.226, 0.094, 0.13, 0.114]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2623 | Steps: 4 | Val loss: 0.2665 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6588 | Steps: 4 | Val loss: 0.5071 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3808 | Steps: 4 | Val loss: 0.2947 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=20486)[0m rmse: 0.14710357785224915
[2m[36m(func pid=20486)[0m mae:  0.08668959885835648
[2m[36m(func pid=20486)[0m rmse_per_class: [0.078, 0.213, 0.069, 0.29, 0.059, 0.153, 0.207, 0.158, 0.142, 0.102]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2892 | Steps: 4 | Val loss: 0.2594 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:03:28 (running for 00:13:45.81)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.659 |  0.179 |                   66 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.385 |  0.164 |                   63 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.318 |  0.141 |                   42 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.262 |  0.147 |                   41 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17938408255577087
[2m[36m(func pid=14018)[0m mae:  0.13160255551338196
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.261, 0.099, 0.335, 0.107, 0.189, 0.292, 0.139, 0.144, 0.111]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1638614386320114
[2m[36m(func pid=14601)[0m mae:  0.11856050789356232
[2m[36m(func pid=14601)[0m rmse_per_class: [0.107, 0.249, 0.074, 0.314, 0.078, 0.18, 0.26, 0.128, 0.142, 0.108]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.13940629363059998
[2m[36m(func pid=19964)[0m mae:  0.09399019181728363
[2m[36m(func pid=19964)[0m rmse_per_class: [0.078, 0.222, 0.051, 0.273, 0.065, 0.153, 0.22, 0.093, 0.131, 0.108]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2836 | Steps: 4 | Val loss: 0.2718 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6536 | Steps: 4 | Val loss: 0.5077 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3811 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=20486)[0m rmse: 0.15262380242347717
[2m[36m(func pid=20486)[0m mae:  0.08946869522333145
[2m[36m(func pid=20486)[0m rmse_per_class: [0.109, 0.205, 0.1, 0.298, 0.06, 0.149, 0.207, 0.159, 0.125, 0.115]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2979 | Steps: 4 | Val loss: 0.2568 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 20:03:33 (running for 00:13:51.05)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.654 |  0.179 |                   67 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.381 |  0.164 |                   64 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.289 |  0.139 |                   43 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.284 |  0.153 |                   42 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17921921610832214
[2m[36m(func pid=14018)[0m mae:  0.13142715394496918
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.335, 0.107, 0.19, 0.292, 0.139, 0.144, 0.111]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16449818015098572
[2m[36m(func pid=14601)[0m mae:  0.11906218528747559
[2m[36m(func pid=14601)[0m rmse_per_class: [0.107, 0.249, 0.075, 0.316, 0.079, 0.179, 0.262, 0.127, 0.142, 0.109]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.13719744980335236
[2m[36m(func pid=19964)[0m mae:  0.09218654036521912
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.221, 0.047, 0.267, 0.064, 0.153, 0.214, 0.094, 0.132, 0.105]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2635 | Steps: 4 | Val loss: 0.2632 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6483 | Steps: 4 | Val loss: 0.5070 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3876 | Steps: 4 | Val loss: 0.2946 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=20486)[0m rmse: 0.14474961161613464
[2m[36m(func pid=20486)[0m mae:  0.08359067142009735
[2m[36m(func pid=20486)[0m rmse_per_class: [0.091, 0.202, 0.058, 0.28, 0.092, 0.155, 0.199, 0.133, 0.117, 0.121]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3094 | Steps: 4 | Val loss: 0.2574 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=14018)[0m rmse: 0.17934267222881317
[2m[36m(func pid=14018)[0m mae:  0.13155730068683624
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.107, 0.19, 0.291, 0.139, 0.144, 0.111]
== Status ==
Current time: 2024-01-07 20:03:38 (running for 00:13:56.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.648 |  0.179 |                   68 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.381 |  0.164 |                   65 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.298 |  0.137 |                   44 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.264 |  0.145 |                   43 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1635493040084839
[2m[36m(func pid=14601)[0m mae:  0.11833363771438599
[2m[36m(func pid=14601)[0m rmse_per_class: [0.107, 0.249, 0.071, 0.315, 0.08, 0.178, 0.261, 0.127, 0.142, 0.106]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.1380506455898285
[2m[36m(func pid=19964)[0m mae:  0.09307941794395447
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.222, 0.047, 0.262, 0.066, 0.154, 0.22, 0.093, 0.133, 0.107]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2588 | Steps: 4 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6486 | Steps: 4 | Val loss: 0.5031 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3790 | Steps: 4 | Val loss: 0.2945 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=20486)[0m rmse: 0.1479252427816391
[2m[36m(func pid=20486)[0m mae:  0.08593691885471344
[2m[36m(func pid=20486)[0m rmse_per_class: [0.085, 0.207, 0.043, 0.287, 0.105, 0.164, 0.213, 0.127, 0.123, 0.125]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3191 | Steps: 4 | Val loss: 0.2569 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 20:03:44 (running for 00:14:01.69)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.649 |  0.179 |                   69 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.388 |  0.164 |                   66 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.309 |  0.138 |                   45 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.259 |  0.148 |                   44 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1791374534368515
[2m[36m(func pid=14018)[0m mae:  0.13139183819293976
[2m[36m(func pid=14018)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.106, 0.19, 0.291, 0.139, 0.144, 0.111]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1635596603155136
[2m[36m(func pid=14601)[0m mae:  0.11833220720291138
[2m[36m(func pid=14601)[0m rmse_per_class: [0.106, 0.249, 0.072, 0.315, 0.08, 0.178, 0.26, 0.127, 0.142, 0.107]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2831 | Steps: 4 | Val loss: 0.2747 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=19964)[0m rmse: 0.13827645778656006
[2m[36m(func pid=19964)[0m mae:  0.09161651879549026
[2m[36m(func pid=19964)[0m rmse_per_class: [0.075, 0.223, 0.047, 0.268, 0.068, 0.15, 0.218, 0.1, 0.126, 0.107]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6471 | Steps: 4 | Val loss: 0.5024 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3813 | Steps: 4 | Val loss: 0.2936 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=20486)[0m rmse: 0.15716633200645447
[2m[36m(func pid=20486)[0m mae:  0.09244906902313232
[2m[36m(func pid=20486)[0m rmse_per_class: [0.081, 0.21, 0.069, 0.29, 0.135, 0.176, 0.213, 0.123, 0.147, 0.128]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3143 | Steps: 4 | Val loss: 0.2559 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 20:03:49 (running for 00:14:07.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.647 |  0.179 |                   70 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.379 |  0.164 |                   67 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.319 |  0.138 |                   46 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.283 |  0.157 |                   45 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17901511490345
[2m[36m(func pid=14018)[0m mae:  0.13131269812583923
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.335, 0.106, 0.19, 0.291, 0.139, 0.144, 0.111]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16308195888996124
[2m[36m(func pid=14601)[0m mae:  0.11785349994897842
[2m[36m(func pid=14601)[0m rmse_per_class: [0.107, 0.249, 0.071, 0.315, 0.079, 0.178, 0.258, 0.125, 0.142, 0.107]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2538 | Steps: 4 | Val loss: 0.2759 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=19964)[0m rmse: 0.13784675300121307
[2m[36m(func pid=19964)[0m mae:  0.09071718156337738
[2m[36m(func pid=19964)[0m rmse_per_class: [0.072, 0.221, 0.04, 0.266, 0.068, 0.15, 0.215, 0.108, 0.127, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6438 | Steps: 4 | Val loss: 0.4997 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3772 | Steps: 4 | Val loss: 0.2931 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=20486)[0m rmse: 0.15833353996276855
[2m[36m(func pid=20486)[0m mae:  0.09342209994792938
[2m[36m(func pid=20486)[0m rmse_per_class: [0.074, 0.204, 0.085, 0.294, 0.121, 0.168, 0.222, 0.128, 0.165, 0.123]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3118 | Steps: 4 | Val loss: 0.2592 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:03:54 (running for 00:14:12.30)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.644 |  0.179 |                   71 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.381 |  0.163 |                   68 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.314 |  0.138 |                   47 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.254 |  0.158 |                   46 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17915154993534088
[2m[36m(func pid=14018)[0m mae:  0.13144811987876892
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.26, 0.098, 0.335, 0.105, 0.19, 0.291, 0.139, 0.144, 0.111]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16288818418979645
[2m[36m(func pid=14601)[0m mae:  0.11779674142599106
[2m[36m(func pid=14601)[0m rmse_per_class: [0.107, 0.249, 0.072, 0.313, 0.078, 0.178, 0.259, 0.126, 0.142, 0.106]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2591 | Steps: 4 | Val loss: 0.2666 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=19964)[0m rmse: 0.14102701842784882
[2m[36m(func pid=19964)[0m mae:  0.09247605502605438
[2m[36m(func pid=19964)[0m rmse_per_class: [0.075, 0.222, 0.043, 0.274, 0.068, 0.152, 0.216, 0.113, 0.129, 0.118]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6375 | Steps: 4 | Val loss: 0.4931 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3745 | Steps: 4 | Val loss: 0.2926 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=20486)[0m rmse: 0.14918792247772217
[2m[36m(func pid=20486)[0m mae:  0.08848528563976288
[2m[36m(func pid=20486)[0m rmse_per_class: [0.064, 0.229, 0.037, 0.284, 0.077, 0.152, 0.239, 0.19, 0.122, 0.098]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2993 | Steps: 4 | Val loss: 0.2587 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:04:00 (running for 00:14:17.72)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.637 |  0.179 |                   72 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.377 |  0.163 |                   69 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.312 |  0.141 |                   48 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.259 |  0.149 |                   47 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17879892885684967
[2m[36m(func pid=14018)[0m mae:  0.13114497065544128
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.335, 0.106, 0.189, 0.29, 0.139, 0.144, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.1626977026462555
[2m[36m(func pid=14601)[0m mae:  0.11766616255044937
[2m[36m(func pid=14601)[0m rmse_per_class: [0.106, 0.249, 0.072, 0.311, 0.078, 0.179, 0.259, 0.125, 0.142, 0.106]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2653 | Steps: 4 | Val loss: 0.2701 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=19964)[0m rmse: 0.14000287652015686
[2m[36m(func pid=19964)[0m mae:  0.09264110773801804
[2m[36m(func pid=19964)[0m rmse_per_class: [0.081, 0.219, 0.045, 0.275, 0.071, 0.153, 0.221, 0.101, 0.125, 0.108]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6399 | Steps: 4 | Val loss: 0.4905 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3792 | Steps: 4 | Val loss: 0.2922 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=20486)[0m rmse: 0.15009528398513794
[2m[36m(func pid=20486)[0m mae:  0.0887894332408905
[2m[36m(func pid=20486)[0m rmse_per_class: [0.075, 0.256, 0.045, 0.29, 0.077, 0.17, 0.227, 0.12, 0.125, 0.116]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2926 | Steps: 4 | Val loss: 0.2642 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 20:04:05 (running for 00:14:23.04)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.64  |  0.179 |                   73 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.375 |  0.163 |                   70 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.299 |  0.14  |                   49 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.265 |  0.15  |                   48 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.1787624955177307
[2m[36m(func pid=14018)[0m mae:  0.13104988634586334
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.106, 0.189, 0.29, 0.139, 0.144, 0.11]
[2m[36m(func pid=14018)[0m 
[2m[36m(func pid=14601)[0m rmse: 0.16246722638607025
[2m[36m(func pid=14601)[0m mae:  0.11745629459619522
[2m[36m(func pid=14601)[0m rmse_per_class: [0.106, 0.249, 0.072, 0.311, 0.077, 0.179, 0.258, 0.126, 0.142, 0.106]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2759 | Steps: 4 | Val loss: 0.3080 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=19964)[0m rmse: 0.14388255774974823
[2m[36m(func pid=19964)[0m mae:  0.09576568007469177
[2m[36m(func pid=19964)[0m rmse_per_class: [0.098, 0.221, 0.051, 0.289, 0.069, 0.154, 0.224, 0.096, 0.132, 0.105]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6321 | Steps: 4 | Val loss: 0.4875 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3732 | Steps: 4 | Val loss: 0.2919 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=20486)[0m rmse: 0.1742643266916275
[2m[36m(func pid=20486)[0m mae:  0.10457144677639008
[2m[36m(func pid=20486)[0m rmse_per_class: [0.092, 0.259, 0.042, 0.304, 0.085, 0.178, 0.263, 0.135, 0.232, 0.152]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2939 | Steps: 4 | Val loss: 0.2639 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=14018)[0m rmse: 0.17867429554462433
[2m[36m(func pid=14018)[0m mae:  0.13102521002292633
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.105, 0.189, 0.289, 0.14, 0.144, 0.11]
[2m[36m(func pid=14018)[0m 
== Status ==
Current time: 2024-01-07 20:04:10 (running for 00:14:28.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.15549999475479126
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00004 | RUNNING    | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.632 |  0.179 |                   74 |
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.379 |  0.162 |                   71 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.293 |  0.144 |                   50 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.276 |  0.174 |                   49 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14601)[0m rmse: 0.16231882572174072
[2m[36m(func pid=14601)[0m mae:  0.11733213812112808
[2m[36m(func pid=14601)[0m rmse_per_class: [0.106, 0.249, 0.071, 0.31, 0.077, 0.178, 0.258, 0.125, 0.142, 0.107]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2481 | Steps: 4 | Val loss: 0.2845 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=19964)[0m rmse: 0.14379790425300598
[2m[36m(func pid=19964)[0m mae:  0.09572742134332657
[2m[36m(func pid=19964)[0m rmse_per_class: [0.096, 0.22, 0.048, 0.284, 0.068, 0.154, 0.225, 0.098, 0.137, 0.108]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14018)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6335 | Steps: 4 | Val loss: 0.4874 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3862 | Steps: 4 | Val loss: 0.2908 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=20486)[0m rmse: 0.1568155288696289
[2m[36m(func pid=20486)[0m mae:  0.09523431956768036
[2m[36m(func pid=20486)[0m rmse_per_class: [0.073, 0.302, 0.04, 0.313, 0.087, 0.156, 0.241, 0.12, 0.136, 0.099]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:04:15 (running for 00:14:33.36)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.373 |  0.162 |                   72 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.294 |  0.144 |                   51 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.248 |  0.157 |                   50 |
| train_84a75_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14018)[0m rmse: 0.17871077358722687
[2m[36m(func pid=14018)[0m mae:  0.13106487691402435
[2m[36m(func pid=14018)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.106, 0.19, 0.291, 0.139, 0.144, 0.109]
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3023 | Steps: 4 | Val loss: 0.2558 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=14601)[0m rmse: 0.16160798072814941
[2m[36m(func pid=14601)[0m mae:  0.11668707430362701
[2m[36m(func pid=14601)[0m rmse_per_class: [0.105, 0.248, 0.069, 0.31, 0.077, 0.177, 0.258, 0.124, 0.141, 0.106]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2704 | Steps: 4 | Val loss: 0.2798 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=19964)[0m rmse: 0.13719859719276428
[2m[36m(func pid=19964)[0m mae:  0.09142430871725082
[2m[36m(func pid=19964)[0m rmse_per_class: [0.08, 0.217, 0.037, 0.268, 0.075, 0.15, 0.219, 0.099, 0.127, 0.101]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3788 | Steps: 4 | Val loss: 0.2905 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=20486)[0m rmse: 0.15329191088676453
[2m[36m(func pid=20486)[0m mae:  0.0923738181591034
[2m[36m(func pid=20486)[0m rmse_per_class: [0.077, 0.306, 0.037, 0.31, 0.079, 0.151, 0.229, 0.129, 0.126, 0.088]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2971 | Steps: 4 | Val loss: 0.2554 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=14601)[0m rmse: 0.16144366562366486
[2m[36m(func pid=14601)[0m mae:  0.11658128350973129
[2m[36m(func pid=14601)[0m rmse_per_class: [0.106, 0.247, 0.068, 0.309, 0.078, 0.178, 0.258, 0.124, 0.141, 0.106]
[2m[36m(func pid=14601)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2610 | Steps: 4 | Val loss: 0.2637 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=19964)[0m rmse: 0.13742110133171082
[2m[36m(func pid=19964)[0m mae:  0.09123821556568146
[2m[36m(func pid=19964)[0m rmse_per_class: [0.08, 0.217, 0.04, 0.261, 0.073, 0.151, 0.218, 0.099, 0.126, 0.109]
== Status ==
Current time: 2024-01-07 20:04:21 (running for 00:14:39.39)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00005 | RUNNING    | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.379 |  0.161 |                   74 |
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.302 |  0.137 |                   52 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.27  |  0.153 |                   51 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=32639)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=32639)[0m Configuration completed!
[2m[36m(func pid=32639)[0m New optimizer parameters:
[2m[36m(func pid=32639)[0m SGD (
[2m[36m(func pid=32639)[0m Parameter Group 0
[2m[36m(func pid=32639)[0m     dampening: 0
[2m[36m(func pid=32639)[0m     differentiable: False
[2m[36m(func pid=32639)[0m     foreach: None
[2m[36m(func pid=32639)[0m     lr: 0.0001
[2m[36m(func pid=32639)[0m     maximize: False
[2m[36m(func pid=32639)[0m     momentum: 0.99
[2m[36m(func pid=32639)[0m     nesterov: False
[2m[36m(func pid=32639)[0m     weight_decay: 0.0001
[2m[36m(func pid=32639)[0m )
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=14601)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3744 | Steps: 4 | Val loss: 0.2905 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=20486)[0m rmse: 0.14277085661888123
[2m[36m(func pid=20486)[0m mae:  0.08596711605787277
[2m[36m(func pid=20486)[0m rmse_per_class: [0.075, 0.253, 0.035, 0.288, 0.07, 0.161, 0.21, 0.11, 0.142, 0.084]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:04:27 (running for 00:14:44.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.297 |  0.137 |                   53 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.261 |  0.143 |                   52 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14601)[0m rmse: 0.1615046262741089
[2m[36m(func pid=14601)[0m mae:  0.11665709316730499
[2m[36m(func pid=14601)[0m rmse_per_class: [0.106, 0.247, 0.07, 0.308, 0.078, 0.178, 0.258, 0.124, 0.141, 0.106]
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2897 | Steps: 4 | Val loss: 0.2558 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2556 | Steps: 4 | Val loss: 0.2598 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8933 | Steps: 4 | Val loss: 0.7025 | Batch size: 32 | lr: 0.0001 | Duration: 4.88s
[2m[36m(func pid=19964)[0m rmse: 0.1373443603515625
[2m[36m(func pid=19964)[0m mae:  0.0915025994181633
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.215, 0.041, 0.259, 0.075, 0.153, 0.225, 0.095, 0.128, 0.106]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1406015157699585
[2m[36m(func pid=20486)[0m mae:  0.08428739756345749
[2m[36m(func pid=20486)[0m rmse_per_class: [0.072, 0.216, 0.034, 0.271, 0.065, 0.177, 0.21, 0.116, 0.158, 0.086]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.18252749741077423
[2m[36m(func pid=32639)[0m mae:  0.13437876105308533
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.266, 0.106, 0.339, 0.113, 0.19, 0.294, 0.143, 0.144, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2898 | Steps: 4 | Val loss: 0.2564 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2414 | Steps: 4 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8951 | Steps: 4 | Val loss: 0.6946 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=19964)[0m rmse: 0.1376589834690094
[2m[36m(func pid=19964)[0m mae:  0.09107164293527603
[2m[36m(func pid=19964)[0m rmse_per_class: [0.072, 0.216, 0.042, 0.261, 0.077, 0.162, 0.228, 0.097, 0.122, 0.098]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:04:35 (running for 00:14:53.01)
Memory usage on this node: 24.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.29  |  0.138 |                   55 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.256 |  0.141 |                   53 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.893 |  0.183 |                    1 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=33359)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=33359)[0m Configuration completed!
[2m[36m(func pid=33359)[0m New optimizer parameters:
[2m[36m(func pid=33359)[0m SGD (
[2m[36m(func pid=33359)[0m Parameter Group 0
[2m[36m(func pid=33359)[0m     dampening: 0
[2m[36m(func pid=33359)[0m     differentiable: False
[2m[36m(func pid=33359)[0m     foreach: None
[2m[36m(func pid=33359)[0m     lr: 0.001
[2m[36m(func pid=33359)[0m     maximize: False
[2m[36m(func pid=33359)[0m     momentum: 0.99
[2m[36m(func pid=33359)[0m     nesterov: False
[2m[36m(func pid=33359)[0m     weight_decay: 0.0001
[2m[36m(func pid=33359)[0m )
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1440478414297104
[2m[36m(func pid=20486)[0m mae:  0.08760721236467361
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.215, 0.034, 0.302, 0.061, 0.185, 0.212, 0.11, 0.156, 0.095]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.18201042711734772
[2m[36m(func pid=32639)[0m mae:  0.13398045301437378
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2852 | Steps: 4 | Val loss: 0.2569 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2530 | Steps: 4 | Val loss: 0.2791 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8895 | Steps: 4 | Val loss: 0.6879 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8942 | Steps: 4 | Val loss: 0.6974 | Batch size: 32 | lr: 0.001 | Duration: 4.66s
== Status ==
Current time: 2024-01-07 20:04:40 (running for 00:14:58.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.285 |  0.138 |                   56 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.241 |  0.144 |                   54 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.895 |  0.182 |                    2 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13804996013641357
[2m[36m(func pid=19964)[0m mae:  0.09147044271230698
[2m[36m(func pid=19964)[0m rmse_per_class: [0.073, 0.218, 0.043, 0.258, 0.076, 0.162, 0.232, 0.096, 0.124, 0.1]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15017937123775482
[2m[36m(func pid=20486)[0m mae:  0.0907650962471962
[2m[36m(func pid=20486)[0m rmse_per_class: [0.087, 0.209, 0.042, 0.321, 0.063, 0.153, 0.2, 0.11, 0.196, 0.121]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.18130873143672943
[2m[36m(func pid=32639)[0m mae:  0.1334151178598404
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.265, 0.102, 0.338, 0.113, 0.189, 0.295, 0.141, 0.142, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.18281295895576477
[2m[36m(func pid=33359)[0m mae:  0.13458652794361115
[2m[36m(func pid=33359)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.34, 0.112, 0.19, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2911 | Steps: 4 | Val loss: 0.2591 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2537 | Steps: 4 | Val loss: 0.2874 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8851 | Steps: 4 | Val loss: 0.6844 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8713 | Steps: 4 | Val loss: 0.6655 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 20:04:46 (running for 00:15:03.99)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.291 |  0.14  |                   57 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.253 |  0.15  |                   55 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.889 |  0.181 |                    3 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.894 |  0.183 |                    1 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1396111398935318
[2m[36m(func pid=19964)[0m mae:  0.092629075050354
[2m[36m(func pid=19964)[0m rmse_per_class: [0.078, 0.217, 0.04, 0.261, 0.076, 0.162, 0.232, 0.096, 0.125, 0.108]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.16011564433574677
[2m[36m(func pid=20486)[0m mae:  0.09664653241634369
[2m[36m(func pid=20486)[0m rmse_per_class: [0.092, 0.211, 0.092, 0.315, 0.077, 0.164, 0.21, 0.158, 0.172, 0.111]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.18081142008304596
[2m[36m(func pid=32639)[0m mae:  0.1329609602689743
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.338, 0.112, 0.189, 0.295, 0.141, 0.142, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.18173201382160187
[2m[36m(func pid=33359)[0m mae:  0.13373330235481262
[2m[36m(func pid=33359)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2931 | Steps: 4 | Val loss: 0.2569 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2551 | Steps: 4 | Val loss: 0.2768 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8759 | Steps: 4 | Val loss: 0.6768 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8350 | Steps: 4 | Val loss: 0.6263 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 20:04:51 (running for 00:15:09.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.293 |  0.138 |                   58 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.254 |  0.16  |                   56 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.885 |  0.181 |                    4 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.871 |  0.182 |                    2 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13783946633338928
[2m[36m(func pid=19964)[0m mae:  0.09069620072841644
[2m[36m(func pid=19964)[0m rmse_per_class: [0.075, 0.217, 0.043, 0.258, 0.071, 0.159, 0.226, 0.096, 0.122, 0.112]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1530436873435974
[2m[36m(func pid=20486)[0m mae:  0.09426800161600113
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.232, 0.054, 0.26, 0.102, 0.199, 0.198, 0.133, 0.136, 0.146]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.18042702972888947
[2m[36m(func pid=32639)[0m mae:  0.13262006640434265
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.112, 0.189, 0.295, 0.141, 0.142, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.18094190955162048
[2m[36m(func pid=33359)[0m mae:  0.13293008506298065
[2m[36m(func pid=33359)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.113, 0.19, 0.294, 0.141, 0.141, 0.109]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2930 | Steps: 4 | Val loss: 0.2560 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2481 | Steps: 4 | Val loss: 0.2615 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8655 | Steps: 4 | Val loss: 0.6686 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7826 | Steps: 4 | Val loss: 0.5805 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=19964)[0m rmse: 0.1377333253622055
[2m[36m(func pid=19964)[0m mae:  0.08991243690252304
[2m[36m(func pid=19964)[0m rmse_per_class: [0.075, 0.218, 0.055, 0.254, 0.067, 0.155, 0.22, 0.094, 0.122, 0.118]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:04:56 (running for 00:15:14.31)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.293 |  0.138 |                   59 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.255 |  0.153 |                   57 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.876 |  0.18  |                    5 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.835 |  0.181 |                    3 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=20486)[0m rmse: 0.1433173418045044
[2m[36m(func pid=20486)[0m mae:  0.08798839896917343
[2m[36m(func pid=20486)[0m rmse_per_class: [0.083, 0.247, 0.03, 0.279, 0.081, 0.168, 0.197, 0.116, 0.127, 0.106]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1801551878452301
[2m[36m(func pid=32639)[0m mae:  0.13236330449581146
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.262, 0.1, 0.337, 0.112, 0.19, 0.295, 0.14, 0.142, 0.108]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17990262806415558
[2m[36m(func pid=33359)[0m mae:  0.13201546669006348
[2m[36m(func pid=33359)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.336, 0.111, 0.19, 0.293, 0.14, 0.141, 0.109]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3061 | Steps: 4 | Val loss: 0.2534 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2635 | Steps: 4 | Val loss: 0.2744 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8588 | Steps: 4 | Val loss: 0.6602 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7221 | Steps: 4 | Val loss: 0.5296 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 20:05:02 (running for 00:15:19.74)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.306 |  0.136 |                   60 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.248 |  0.143 |                   58 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.865 |  0.18  |                    6 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.783 |  0.18  |                    4 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1364196240901947
[2m[36m(func pid=19964)[0m mae:  0.08818415552377701
[2m[36m(func pid=19964)[0m rmse_per_class: [0.067, 0.215, 0.05, 0.255, 0.068, 0.148, 0.214, 0.107, 0.12, 0.12]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15396732091903687
[2m[36m(func pid=20486)[0m mae:  0.0937163382768631
[2m[36m(func pid=20486)[0m rmse_per_class: [0.069, 0.232, 0.037, 0.287, 0.086, 0.228, 0.217, 0.13, 0.154, 0.099]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1799650937318802
[2m[36m(func pid=32639)[0m mae:  0.1321995109319687
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.112, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17925772070884705
[2m[36m(func pid=33359)[0m mae:  0.13152219355106354
[2m[36m(func pid=33359)[0m rmse_per_class: [0.115, 0.262, 0.099, 0.335, 0.11, 0.19, 0.293, 0.139, 0.141, 0.11]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2827 | Steps: 4 | Val loss: 0.2554 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8467 | Steps: 4 | Val loss: 0.6498 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2443 | Steps: 4 | Val loss: 0.2841 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6606 | Steps: 4 | Val loss: 0.4819 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=19964)[0m rmse: 0.13874469697475433
[2m[36m(func pid=19964)[0m mae:  0.08977623283863068
[2m[36m(func pid=19964)[0m rmse_per_class: [0.069, 0.215, 0.059, 0.257, 0.065, 0.153, 0.211, 0.107, 0.125, 0.126]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:05:07 (running for 00:15:24.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.283 |  0.139 |                   61 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.263 |  0.154 |                   59 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.859 |  0.18  |                    7 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.722 |  0.179 |                    5 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=20486)[0m rmse: 0.16038891673088074
[2m[36m(func pid=20486)[0m mae:  0.09627635776996613
[2m[36m(func pid=20486)[0m rmse_per_class: [0.069, 0.225, 0.038, 0.3, 0.088, 0.202, 0.22, 0.115, 0.223, 0.125]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17995470762252808
[2m[36m(func pid=32639)[0m mae:  0.13215987384319305
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.112, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1785547435283661
[2m[36m(func pid=33359)[0m mae:  0.13095979392528534
[2m[36m(func pid=33359)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.334, 0.106, 0.19, 0.292, 0.139, 0.142, 0.109]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2894 | Steps: 4 | Val loss: 0.2576 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8312 | Steps: 4 | Val loss: 0.6391 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5985 | Steps: 4 | Val loss: 0.4361 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2650 | Steps: 4 | Val loss: 0.2828 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:05:12 (running for 00:15:30.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.289 |  0.14  |                   62 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.244 |  0.16  |                   60 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.847 |  0.18  |                    8 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.661 |  0.179 |                    6 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.14037851989269257
[2m[36m(func pid=19964)[0m mae:  0.09116841852664948
[2m[36m(func pid=19964)[0m rmse_per_class: [0.07, 0.211, 0.059, 0.26, 0.065, 0.159, 0.214, 0.101, 0.138, 0.125]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15890556573867798
[2m[36m(func pid=20486)[0m mae:  0.0931696966290474
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.217, 0.084, 0.306, 0.086, 0.164, 0.206, 0.13, 0.209, 0.117]
[2m[36m(func pid=33359)[0m rmse: 0.1780889481306076
[2m[36m(func pid=33359)[0m mae:  0.1306043565273285
[2m[36m(func pid=33359)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.333, 0.102, 0.19, 0.291, 0.138, 0.143, 0.11]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1798357218503952
[2m[36m(func pid=32639)[0m mae:  0.13203300535678864
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.337, 0.112, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2905 | Steps: 4 | Val loss: 0.2582 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8153 | Steps: 4 | Val loss: 0.6274 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5428 | Steps: 4 | Val loss: 0.3980 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2532 | Steps: 4 | Val loss: 0.3128 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=19964)[0m rmse: 0.14073364436626434
[2m[36m(func pid=19964)[0m mae:  0.09102703630924225
[2m[36m(func pid=19964)[0m rmse_per_class: [0.073, 0.216, 0.053, 0.266, 0.068, 0.155, 0.214, 0.102, 0.136, 0.125]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:05:18 (running for 00:15:35.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.291 |  0.141 |                   63 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.265 |  0.159 |                   61 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.831 |  0.18  |                    9 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.599 |  0.178 |                    7 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17975954711437225
[2m[36m(func pid=32639)[0m mae:  0.13196727633476257
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.112, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17770667374134064
[2m[36m(func pid=33359)[0m mae:  0.13031382858753204
[2m[36m(func pid=33359)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.333, 0.097, 0.19, 0.289, 0.137, 0.144, 0.112]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.16974543035030365
[2m[36m(func pid=20486)[0m mae:  0.09950364381074905
[2m[36m(func pid=20486)[0m rmse_per_class: [0.069, 0.226, 0.107, 0.33, 0.069, 0.16, 0.205, 0.164, 0.262, 0.107]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2968 | Steps: 4 | Val loss: 0.2598 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4934 | Steps: 4 | Val loss: 0.3697 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8013 | Steps: 4 | Val loss: 0.6147 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2502 | Steps: 4 | Val loss: 0.3000 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:05:23 (running for 00:15:41.19)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.297 |  0.142 |                   64 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.253 |  0.17  |                   62 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.815 |  0.18  |                   10 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.543 |  0.178 |                    8 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1420317143201828
[2m[36m(func pid=19964)[0m mae:  0.091248519718647
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.22, 0.057, 0.271, 0.064, 0.154, 0.215, 0.1, 0.138, 0.127]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17794464528560638
[2m[36m(func pid=33359)[0m mae:  0.13059459626674652
[2m[36m(func pid=33359)[0m rmse_per_class: [0.118, 0.261, 0.097, 0.333, 0.093, 0.191, 0.288, 0.137, 0.146, 0.115]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1796794831752777
[2m[36m(func pid=32639)[0m mae:  0.1318998634815216
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15990427136421204
[2m[36m(func pid=20486)[0m mae:  0.09361708909273148
[2m[36m(func pid=20486)[0m rmse_per_class: [0.067, 0.226, 0.096, 0.318, 0.066, 0.159, 0.195, 0.124, 0.239, 0.109]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2960 | Steps: 4 | Val loss: 0.2582 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4547 | Steps: 4 | Val loss: 0.3463 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7871 | Steps: 4 | Val loss: 0.6010 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2508 | Steps: 4 | Val loss: 0.2717 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 20:05:28 (running for 00:15:46.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.296 |  0.14  |                   65 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.25  |  0.16  |                   63 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.801 |  0.18  |                   11 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.493 |  0.178 |                    9 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1400505006313324
[2m[36m(func pid=19964)[0m mae:  0.09023965895175934
[2m[36m(func pid=19964)[0m rmse_per_class: [0.077, 0.217, 0.06, 0.272, 0.068, 0.15, 0.219, 0.094, 0.131, 0.113]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17798760533332825
[2m[36m(func pid=33359)[0m mae:  0.1305537223815918
[2m[36m(func pid=33359)[0m rmse_per_class: [0.121, 0.261, 0.096, 0.332, 0.09, 0.191, 0.288, 0.138, 0.147, 0.117]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17951181530952454
[2m[36m(func pid=32639)[0m mae:  0.13178320229053497
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.11, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1454862505197525
[2m[36m(func pid=20486)[0m mae:  0.08593250066041946
[2m[36m(func pid=20486)[0m rmse_per_class: [0.065, 0.204, 0.053, 0.291, 0.069, 0.158, 0.192, 0.1, 0.218, 0.105]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2947 | Steps: 4 | Val loss: 0.2581 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4283 | Steps: 4 | Val loss: 0.3284 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7700 | Steps: 4 | Val loss: 0.5874 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2521 | Steps: 4 | Val loss: 0.2618 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:05:34 (running for 00:15:51.72)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.295 |  0.141 |                   66 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.251 |  0.145 |                   64 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.787 |  0.18  |                   12 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.455 |  0.178 |                   10 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.14055418968200684
[2m[36m(func pid=19964)[0m mae:  0.0899590402841568
[2m[36m(func pid=19964)[0m rmse_per_class: [0.081, 0.22, 0.066, 0.272, 0.067, 0.149, 0.214, 0.096, 0.127, 0.114]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17710736393928528
[2m[36m(func pid=33359)[0m mae:  0.12974241375923157
[2m[36m(func pid=33359)[0m rmse_per_class: [0.122, 0.261, 0.093, 0.331, 0.086, 0.191, 0.285, 0.138, 0.146, 0.118]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17936700582504272
[2m[36m(func pid=32639)[0m mae:  0.131678506731987
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.11, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1414935290813446
[2m[36m(func pid=20486)[0m mae:  0.08401598036289215
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.19, 0.026, 0.266, 0.077, 0.177, 0.224, 0.114, 0.13, 0.142]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2836 | Steps: 4 | Val loss: 0.2552 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4120 | Steps: 4 | Val loss: 0.3183 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7511 | Steps: 4 | Val loss: 0.5734 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2463 | Steps: 4 | Val loss: 0.2628 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 20:05:39 (running for 00:15:57.03)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.284 |  0.138 |                   67 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.252 |  0.141 |                   65 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.77  |  0.179 |                   13 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.428 |  0.177 |                   11 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13758942484855652
[2m[36m(func pid=19964)[0m mae:  0.08827535808086395
[2m[36m(func pid=19964)[0m rmse_per_class: [0.08, 0.215, 0.062, 0.269, 0.065, 0.147, 0.21, 0.094, 0.122, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17616012692451477
[2m[36m(func pid=33359)[0m mae:  0.12887637317180634
[2m[36m(func pid=33359)[0m rmse_per_class: [0.123, 0.26, 0.092, 0.329, 0.082, 0.191, 0.282, 0.138, 0.146, 0.118]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17920508980751038
[2m[36m(func pid=32639)[0m mae:  0.13150654733181
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14501240849494934
[2m[36m(func pid=20486)[0m mae:  0.08507632464170456
[2m[36m(func pid=20486)[0m rmse_per_class: [0.083, 0.195, 0.027, 0.266, 0.084, 0.184, 0.23, 0.115, 0.131, 0.136]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2821 | Steps: 4 | Val loss: 0.2513 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4050 | Steps: 4 | Val loss: 0.3144 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7360 | Steps: 4 | Val loss: 0.5595 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2572 | Steps: 4 | Val loss: 0.2683 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 20:05:44 (running for 00:16:02.28)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.282 |  0.134 |                   68 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.246 |  0.145 |                   66 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.751 |  0.179 |                   14 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.412 |  0.176 |                   12 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13393230736255646
[2m[36m(func pid=19964)[0m mae:  0.08620458841323853
[2m[36m(func pid=19964)[0m rmse_per_class: [0.074, 0.211, 0.052, 0.262, 0.068, 0.144, 0.21, 0.097, 0.116, 0.104]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1750471442937851
[2m[36m(func pid=33359)[0m mae:  0.12782971560955048
[2m[36m(func pid=33359)[0m rmse_per_class: [0.122, 0.259, 0.091, 0.328, 0.079, 0.192, 0.279, 0.136, 0.146, 0.117]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1791335642337799
[2m[36m(func pid=32639)[0m mae:  0.13143855333328247
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.108]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15109871327877045
[2m[36m(func pid=20486)[0m mae:  0.08785171806812286
[2m[36m(func pid=20486)[0m rmse_per_class: [0.128, 0.198, 0.069, 0.294, 0.095, 0.172, 0.223, 0.111, 0.128, 0.092]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2796 | Steps: 4 | Val loss: 0.2528 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4069 | Steps: 4 | Val loss: 0.3142 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2585 | Steps: 4 | Val loss: 0.3368 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7139 | Steps: 4 | Val loss: 0.5455 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=19964)[0m rmse: 0.13459542393684387
[2m[36m(func pid=19964)[0m mae:  0.08713831007480621
[2m[36m(func pid=19964)[0m rmse_per_class: [0.073, 0.209, 0.055, 0.264, 0.07, 0.148, 0.216, 0.095, 0.117, 0.1]
[2m[36m(func pid=19964)[0m 
== Status ==
Current time: 2024-01-07 20:05:50 (running for 00:16:07.77)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.28  |  0.135 |                   69 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.257 |  0.151 |                   67 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.736 |  0.179 |                   15 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.405 |  0.175 |                   13 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.17304562032222748
[2m[36m(func pid=33359)[0m mae:  0.1261201798915863
[2m[36m(func pid=33359)[0m rmse_per_class: [0.12, 0.257, 0.088, 0.325, 0.077, 0.192, 0.274, 0.135, 0.145, 0.117]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1790042221546173
[2m[36m(func pid=32639)[0m mae:  0.13135883212089539
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.335, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.18492016196250916
[2m[36m(func pid=20486)[0m mae:  0.10803274810314178
[2m[36m(func pid=20486)[0m rmse_per_class: [0.183, 0.242, 0.175, 0.359, 0.073, 0.164, 0.232, 0.153, 0.131, 0.137]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3092 | Steps: 4 | Val loss: 0.2527 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4071 | Steps: 4 | Val loss: 0.3191 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2503 | Steps: 4 | Val loss: 0.2943 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7003 | Steps: 4 | Val loss: 0.5331 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:05:55 (running for 00:16:12.84)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.309 |  0.134 |                   70 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.259 |  0.185 |                   68 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.714 |  0.179 |                   16 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.407 |  0.173 |                   14 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1342620700597763
[2m[36m(func pid=19964)[0m mae:  0.08699383586645126
[2m[36m(func pid=19964)[0m rmse_per_class: [0.067, 0.209, 0.045, 0.265, 0.077, 0.152, 0.218, 0.097, 0.117, 0.096]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17258325219154358
[2m[36m(func pid=33359)[0m mae:  0.12568239867687225
[2m[36m(func pid=33359)[0m rmse_per_class: [0.121, 0.258, 0.088, 0.324, 0.075, 0.192, 0.272, 0.134, 0.145, 0.117]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.16375502943992615
[2m[36m(func pid=20486)[0m mae:  0.09834407269954681
[2m[36m(func pid=20486)[0m rmse_per_class: [0.095, 0.259, 0.057, 0.34, 0.07, 0.154, 0.214, 0.159, 0.148, 0.141]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17893819510936737
[2m[36m(func pid=32639)[0m mae:  0.13129767775535583
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.335, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2737 | Steps: 4 | Val loss: 0.2529 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4155 | Steps: 4 | Val loss: 0.3253 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2793 | Steps: 4 | Val loss: 0.2985 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6836 | Steps: 4 | Val loss: 0.5184 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 20:06:00 (running for 00:16:18.31)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.274 |  0.134 |                   71 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.25  |  0.164 |                   69 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.7   |  0.179 |                   17 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.407 |  0.173 |                   15 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1343996822834015
[2m[36m(func pid=19964)[0m mae:  0.08718647807836533
[2m[36m(func pid=19964)[0m rmse_per_class: [0.068, 0.209, 0.047, 0.262, 0.076, 0.153, 0.22, 0.094, 0.121, 0.094]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.17105558514595032
[2m[36m(func pid=33359)[0m mae:  0.12436357885599136
[2m[36m(func pid=33359)[0m rmse_per_class: [0.119, 0.257, 0.086, 0.322, 0.073, 0.192, 0.269, 0.133, 0.144, 0.117]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.16562964022159576
[2m[36m(func pid=20486)[0m mae:  0.09832592308521271
[2m[36m(func pid=20486)[0m rmse_per_class: [0.078, 0.243, 0.057, 0.343, 0.066, 0.154, 0.207, 0.146, 0.222, 0.138]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17889812588691711
[2m[36m(func pid=32639)[0m mae:  0.13123048841953278
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.259, 0.098, 0.335, 0.107, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2791 | Steps: 4 | Val loss: 0.2531 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4218 | Steps: 4 | Val loss: 0.3329 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:06:05 (running for 00:16:23.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.279 |  0.135 |                   72 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.279 |  0.166 |                   70 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.684 |  0.179 |                   18 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.415 |  0.171 |                   16 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13481207191944122
[2m[36m(func pid=19964)[0m mae:  0.08743298053741455
[2m[36m(func pid=19964)[0m rmse_per_class: [0.071, 0.21, 0.049, 0.264, 0.074, 0.149, 0.213, 0.095, 0.125, 0.1]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2947 | Steps: 4 | Val loss: 0.3178 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6648 | Steps: 4 | Val loss: 0.5061 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=33359)[0m rmse: 0.16914118826389313
[2m[36m(func pid=33359)[0m mae:  0.12278862297534943
[2m[36m(func pid=33359)[0m rmse_per_class: [0.118, 0.256, 0.081, 0.319, 0.07, 0.192, 0.264, 0.131, 0.144, 0.116]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.17785654962062836
[2m[36m(func pid=20486)[0m mae:  0.10453259944915771
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.221, 0.086, 0.354, 0.066, 0.155, 0.231, 0.182, 0.255, 0.159]
[2m[36m(func pid=32639)[0m rmse: 0.17869886755943298
[2m[36m(func pid=32639)[0m mae:  0.13111959397792816
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.106, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2711 | Steps: 4 | Val loss: 0.2552 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4321 | Steps: 4 | Val loss: 0.3415 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6454 | Steps: 4 | Val loss: 0.4911 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 20:06:11 (running for 00:16:29.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.271 |  0.137 |                   73 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.295 |  0.178 |                   71 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.665 |  0.179 |                   19 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.422 |  0.169 |                   17 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13664963841438293
[2m[36m(func pid=19964)[0m mae:  0.08908265829086304
[2m[36m(func pid=19964)[0m rmse_per_class: [0.077, 0.208, 0.047, 0.269, 0.073, 0.149, 0.208, 0.099, 0.137, 0.1]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2599 | Steps: 4 | Val loss: 0.3025 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=33359)[0m rmse: 0.16712674498558044
[2m[36m(func pid=33359)[0m mae:  0.12118735164403915
[2m[36m(func pid=33359)[0m rmse_per_class: [0.117, 0.253, 0.078, 0.318, 0.068, 0.19, 0.261, 0.131, 0.142, 0.114]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.173894003033638
[2m[36m(func pid=20486)[0m mae:  0.10313954204320908
[2m[36m(func pid=20486)[0m rmse_per_class: [0.07, 0.209, 0.136, 0.324, 0.072, 0.172, 0.257, 0.148, 0.198, 0.152]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17829321324825287
[2m[36m(func pid=32639)[0m mae:  0.13076424598693848
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.105, 0.19, 0.292, 0.139, 0.143, 0.108]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2870 | Steps: 4 | Val loss: 0.2561 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4457 | Steps: 4 | Val loss: 0.3475 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 20:06:16 (running for 00:16:34.35)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15974999591708183
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.287 |  0.138 |                   74 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.26  |  0.174 |                   72 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.645 |  0.178 |                   20 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.432 |  0.167 |                   18 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13794420659542084
[2m[36m(func pid=19964)[0m mae:  0.08903631567955017
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.21, 0.051, 0.27, 0.071, 0.15, 0.203, 0.097, 0.138, 0.114]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2937 | Steps: 4 | Val loss: 0.2791 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6339 | Steps: 4 | Val loss: 0.4787 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=33359)[0m rmse: 0.16527500748634338
[2m[36m(func pid=33359)[0m mae:  0.11951225996017456
[2m[36m(func pid=33359)[0m rmse_per_class: [0.115, 0.251, 0.074, 0.316, 0.066, 0.189, 0.258, 0.129, 0.142, 0.114]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17829233407974243
[2m[36m(func pid=32639)[0m mae:  0.13078300654888153
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.104, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15550516545772552
[2m[36m(func pid=20486)[0m mae:  0.09464412182569504
[2m[36m(func pid=20486)[0m rmse_per_class: [0.068, 0.207, 0.064, 0.311, 0.066, 0.18, 0.254, 0.126, 0.163, 0.116]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2742 | Steps: 4 | Val loss: 0.2542 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4460 | Steps: 4 | Val loss: 0.3545 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6149 | Steps: 4 | Val loss: 0.4678 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2750 | Steps: 4 | Val loss: 0.2600 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:06:22 (running for 00:16:40.01)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.274 |  0.137 |                   75 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.294 |  0.156 |                   73 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.634 |  0.178 |                   21 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.446 |  0.165 |                   19 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13686829805374146
[2m[36m(func pid=19964)[0m mae:  0.08784257620573044
[2m[36m(func pid=19964)[0m rmse_per_class: [0.071, 0.211, 0.052, 0.265, 0.071, 0.149, 0.206, 0.097, 0.132, 0.115]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.16432563960552216
[2m[36m(func pid=33359)[0m mae:  0.11873175948858261
[2m[36m(func pid=33359)[0m rmse_per_class: [0.115, 0.251, 0.073, 0.313, 0.064, 0.188, 0.258, 0.127, 0.141, 0.114]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17812040448188782
[2m[36m(func pid=32639)[0m mae:  0.13056187331676483
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.258, 0.096, 0.335, 0.104, 0.19, 0.292, 0.138, 0.143, 0.108]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.13779672980308533
[2m[36m(func pid=20486)[0m mae:  0.08450815081596375
[2m[36m(func pid=20486)[0m rmse_per_class: [0.074, 0.205, 0.029, 0.288, 0.062, 0.153, 0.218, 0.114, 0.142, 0.091]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2758 | Steps: 4 | Val loss: 0.2557 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4684 | Steps: 4 | Val loss: 0.3620 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5990 | Steps: 4 | Val loss: 0.4555 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:06:27 (running for 00:16:45.51)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.276 |  0.139 |                   76 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.275 |  0.138 |                   74 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.615 |  0.178 |                   22 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.446 |  0.164 |                   20 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13863693177700043
[2m[36m(func pid=19964)[0m mae:  0.08862977474927902
[2m[36m(func pid=19964)[0m rmse_per_class: [0.07, 0.216, 0.057, 0.267, 0.07, 0.148, 0.207, 0.106, 0.136, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2493 | Steps: 4 | Val loss: 0.2597 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=33359)[0m rmse: 0.16208428144454956
[2m[36m(func pid=33359)[0m mae:  0.1166975274682045
[2m[36m(func pid=33359)[0m rmse_per_class: [0.113, 0.246, 0.069, 0.313, 0.063, 0.186, 0.254, 0.124, 0.14, 0.112]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17829658091068268
[2m[36m(func pid=32639)[0m mae:  0.13073739409446716
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.259, 0.095, 0.335, 0.103, 0.19, 0.292, 0.139, 0.144, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.13825790584087372
[2m[36m(func pid=20486)[0m mae:  0.0852469950914383
[2m[36m(func pid=20486)[0m rmse_per_class: [0.074, 0.195, 0.049, 0.283, 0.075, 0.162, 0.203, 0.117, 0.142, 0.082]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2821 | Steps: 4 | Val loss: 0.2568 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4707 | Steps: 4 | Val loss: 0.3693 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5872 | Steps: 4 | Val loss: 0.4457 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:06:33 (running for 00:16:50.81)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.282 |  0.14  |                   77 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.249 |  0.138 |                   75 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.599 |  0.178 |                   23 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.468 |  0.162 |                   21 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13982753455638885
[2m[36m(func pid=19964)[0m mae:  0.08911935240030289
[2m[36m(func pid=19964)[0m rmse_per_class: [0.07, 0.216, 0.056, 0.265, 0.07, 0.15, 0.21, 0.105, 0.142, 0.116]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2896 | Steps: 4 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=33359)[0m rmse: 0.1605861783027649
[2m[36m(func pid=33359)[0m mae:  0.11531031131744385
[2m[36m(func pid=33359)[0m rmse_per_class: [0.113, 0.245, 0.066, 0.31, 0.061, 0.186, 0.253, 0.124, 0.139, 0.11]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17840197682380676
[2m[36m(func pid=32639)[0m mae:  0.1308225393295288
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.335, 0.102, 0.19, 0.292, 0.139, 0.144, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1485716849565506
[2m[36m(func pid=20486)[0m mae:  0.09018369019031525
[2m[36m(func pid=20486)[0m rmse_per_class: [0.063, 0.197, 0.08, 0.302, 0.099, 0.17, 0.22, 0.139, 0.125, 0.092]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2778 | Steps: 4 | Val loss: 0.2557 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4701 | Steps: 4 | Val loss: 0.3747 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5714 | Steps: 4 | Val loss: 0.4358 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:06:38 (running for 00:16:56.06)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.278 |  0.139 |                   78 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.29  |  0.149 |                   76 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.587 |  0.178 |                   24 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.471 |  0.161 |                   22 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13885360956192017
[2m[36m(func pid=19964)[0m mae:  0.0883704125881195
[2m[36m(func pid=19964)[0m rmse_per_class: [0.069, 0.216, 0.05, 0.259, 0.069, 0.151, 0.212, 0.102, 0.142, 0.117]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2624 | Steps: 4 | Val loss: 0.3128 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=33359)[0m rmse: 0.15949776768684387
[2m[36m(func pid=33359)[0m mae:  0.11431393772363663
[2m[36m(func pid=33359)[0m rmse_per_class: [0.112, 0.244, 0.064, 0.308, 0.059, 0.186, 0.251, 0.124, 0.139, 0.109]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17824484407901764
[2m[36m(func pid=32639)[0m mae:  0.13068518042564392
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.259, 0.095, 0.334, 0.103, 0.19, 0.291, 0.138, 0.144, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.16944918036460876
[2m[36m(func pid=20486)[0m mae:  0.1012154370546341
[2m[36m(func pid=20486)[0m rmse_per_class: [0.125, 0.223, 0.092, 0.359, 0.112, 0.176, 0.22, 0.163, 0.119, 0.106]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2880 | Steps: 4 | Val loss: 0.2541 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4753 | Steps: 4 | Val loss: 0.3775 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5558 | Steps: 4 | Val loss: 0.4255 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:06:44 (running for 00:17:01.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.288 |  0.137 |                   79 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.262 |  0.169 |                   77 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.571 |  0.178 |                   25 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.47  |  0.159 |                   23 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13745799660682678
[2m[36m(func pid=19964)[0m mae:  0.0869496613740921
[2m[36m(func pid=19964)[0m rmse_per_class: [0.07, 0.212, 0.05, 0.258, 0.066, 0.151, 0.211, 0.102, 0.137, 0.119]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2970 | Steps: 4 | Val loss: 0.3142 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=33359)[0m rmse: 0.15737053751945496
[2m[36m(func pid=33359)[0m mae:  0.11238707602024078
[2m[36m(func pid=33359)[0m rmse_per_class: [0.111, 0.241, 0.06, 0.304, 0.058, 0.184, 0.248, 0.124, 0.138, 0.107]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17816922068595886
[2m[36m(func pid=32639)[0m mae:  0.1306496113538742
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.259, 0.095, 0.334, 0.101, 0.19, 0.291, 0.138, 0.144, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.17058365046977997
[2m[36m(func pid=20486)[0m mae:  0.10377280414104462
[2m[36m(func pid=20486)[0m rmse_per_class: [0.109, 0.213, 0.06, 0.345, 0.121, 0.185, 0.213, 0.169, 0.119, 0.171]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2819 | Steps: 4 | Val loss: 0.2506 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4772 | Steps: 4 | Val loss: 0.3782 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5419 | Steps: 4 | Val loss: 0.4176 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:06:49 (running for 00:17:07.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.282 |  0.133 |                   80 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.297 |  0.171 |                   78 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.556 |  0.178 |                   26 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.475 |  0.157 |                   24 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.1334591805934906
[2m[36m(func pid=19964)[0m mae:  0.08501185476779938
[2m[36m(func pid=19964)[0m rmse_per_class: [0.07, 0.21, 0.044, 0.256, 0.068, 0.149, 0.207, 0.098, 0.124, 0.109]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2641 | Steps: 4 | Val loss: 0.2790 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=33359)[0m rmse: 0.1559474766254425
[2m[36m(func pid=33359)[0m mae:  0.1109112873673439
[2m[36m(func pid=33359)[0m rmse_per_class: [0.109, 0.239, 0.057, 0.302, 0.057, 0.183, 0.246, 0.123, 0.138, 0.106]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17804841697216034
[2m[36m(func pid=32639)[0m mae:  0.13054253160953522
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.334, 0.1, 0.191, 0.29, 0.137, 0.145, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14972740411758423
[2m[36m(func pid=20486)[0m mae:  0.08675841242074966
[2m[36m(func pid=20486)[0m rmse_per_class: [0.063, 0.218, 0.024, 0.283, 0.062, 0.163, 0.187, 0.114, 0.139, 0.245]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4905 | Steps: 4 | Val loss: 0.3785 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2767 | Steps: 4 | Val loss: 0.2512 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5340 | Steps: 4 | Val loss: 0.4092 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2465 | Steps: 4 | Val loss: 0.2591 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 20:06:54 (running for 00:17:12.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.282 |  0.133 |                   80 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.264 |  0.15  |                   79 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.542 |  0.178 |                   27 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.49  |  0.155 |                   26 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.15495191514492035
[2m[36m(func pid=33359)[0m mae:  0.1103706806898117
[2m[36m(func pid=33359)[0m rmse_per_class: [0.106, 0.235, 0.054, 0.303, 0.057, 0.181, 0.246, 0.125, 0.138, 0.106]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.13299605250358582
[2m[36m(func pid=19964)[0m mae:  0.08547260612249374
[2m[36m(func pid=19964)[0m rmse_per_class: [0.072, 0.204, 0.04, 0.263, 0.072, 0.149, 0.207, 0.099, 0.121, 0.103]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17808231711387634
[2m[36m(func pid=32639)[0m mae:  0.13059750199317932
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.259, 0.095, 0.334, 0.1, 0.19, 0.29, 0.138, 0.145, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.13585208356380463
[2m[36m(func pid=20486)[0m mae:  0.07933105528354645
[2m[36m(func pid=20486)[0m rmse_per_class: [0.069, 0.212, 0.023, 0.273, 0.059, 0.154, 0.186, 0.104, 0.156, 0.123]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4789 | Steps: 4 | Val loss: 0.3795 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2911 | Steps: 4 | Val loss: 0.2515 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5210 | Steps: 4 | Val loss: 0.3984 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2506 | Steps: 4 | Val loss: 0.2573 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:07:00 (running for 00:17:17.84)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.277 |  0.133 |                   81 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.136 |                   80 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.534 |  0.178 |                   28 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.479 |  0.154 |                   27 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.15389986336231232
[2m[36m(func pid=33359)[0m mae:  0.10919853299856186
[2m[36m(func pid=33359)[0m rmse_per_class: [0.107, 0.233, 0.052, 0.302, 0.056, 0.18, 0.244, 0.124, 0.137, 0.104]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.13343936204910278
[2m[36m(func pid=19964)[0m mae:  0.08442933857440948
[2m[36m(func pid=19964)[0m rmse_per_class: [0.075, 0.205, 0.046, 0.264, 0.063, 0.148, 0.203, 0.099, 0.12, 0.113]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1777261197566986
[2m[36m(func pid=32639)[0m mae:  0.13031281530857086
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.259, 0.094, 0.334, 0.099, 0.19, 0.289, 0.138, 0.145, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1391260176897049
[2m[36m(func pid=20486)[0m mae:  0.08319979161024094
[2m[36m(func pid=20486)[0m rmse_per_class: [0.072, 0.228, 0.037, 0.27, 0.09, 0.147, 0.205, 0.121, 0.148, 0.075]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4744 | Steps: 4 | Val loss: 0.3801 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2688 | Steps: 4 | Val loss: 0.2518 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5081 | Steps: 4 | Val loss: 0.3899 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2528 | Steps: 4 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=33359)[0m rmse: 0.15316464006900787
[2m[36m(func pid=33359)[0m mae:  0.10820938646793365
[2m[36m(func pid=33359)[0m rmse_per_class: [0.106, 0.233, 0.05, 0.3, 0.055, 0.18, 0.243, 0.124, 0.136, 0.103]
[2m[36m(func pid=33359)[0m 
== Status ==
Current time: 2024-01-07 20:07:05 (running for 00:17:23.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.291 |  0.133 |                   82 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.251 |  0.139 |                   81 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.521 |  0.178 |                   29 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.474 |  0.153 |                   28 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19964)[0m rmse: 0.13458721339702606
[2m[36m(func pid=19964)[0m mae:  0.08466805517673492
[2m[36m(func pid=19964)[0m rmse_per_class: [0.072, 0.206, 0.053, 0.26, 0.062, 0.149, 0.207, 0.1, 0.123, 0.113]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17808383703231812
[2m[36m(func pid=32639)[0m mae:  0.13059809803962708
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.095, 0.334, 0.098, 0.19, 0.29, 0.138, 0.145, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.1500004380941391
[2m[36m(func pid=20486)[0m mae:  0.09152935445308685
[2m[36m(func pid=20486)[0m rmse_per_class: [0.073, 0.246, 0.048, 0.294, 0.129, 0.166, 0.224, 0.109, 0.132, 0.079]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4716 | Steps: 4 | Val loss: 0.3761 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2711 | Steps: 4 | Val loss: 0.2549 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4981 | Steps: 4 | Val loss: 0.3830 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2337 | Steps: 4 | Val loss: 0.2756 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:07:10 (running for 00:17:28.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.269 |  0.135 |                   83 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.253 |  0.15  |                   82 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.508 |  0.178 |                   30 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.472 |  0.152 |                   29 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.1523965299129486
[2m[36m(func pid=33359)[0m mae:  0.10740828514099121
[2m[36m(func pid=33359)[0m rmse_per_class: [0.107, 0.233, 0.049, 0.298, 0.055, 0.179, 0.242, 0.123, 0.136, 0.102]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.1381581574678421
[2m[36m(func pid=19964)[0m mae:  0.0870087593793869
[2m[36m(func pid=19964)[0m rmse_per_class: [0.074, 0.207, 0.059, 0.258, 0.062, 0.154, 0.213, 0.104, 0.13, 0.121]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1779049038887024
[2m[36m(func pid=32639)[0m mae:  0.13041798770427704
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.095, 0.334, 0.097, 0.19, 0.289, 0.138, 0.145, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15298734605312347
[2m[36m(func pid=20486)[0m mae:  0.09349895268678665
[2m[36m(func pid=20486)[0m rmse_per_class: [0.08, 0.257, 0.05, 0.317, 0.121, 0.165, 0.221, 0.103, 0.131, 0.086]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4704 | Steps: 4 | Val loss: 0.3727 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2862 | Steps: 4 | Val loss: 0.2578 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4906 | Steps: 4 | Val loss: 0.3752 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=33359)[0m rmse: 0.15157850086688995
[2m[36m(func pid=33359)[0m mae:  0.10629266500473022
[2m[36m(func pid=33359)[0m rmse_per_class: [0.105, 0.232, 0.047, 0.299, 0.055, 0.179, 0.24, 0.122, 0.135, 0.102]
[2m[36m(func pid=33359)[0m 
== Status ==
Current time: 2024-01-07 20:07:16 (running for 00:17:33.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.271 |  0.138 |                   84 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.234 |  0.153 |                   83 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.498 |  0.178 |                   31 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.47  |  0.152 |                   30 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2556 | Steps: 4 | Val loss: 0.2708 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=19964)[0m rmse: 0.14167791604995728
[2m[36m(func pid=19964)[0m mae:  0.08836929500102997
[2m[36m(func pid=19964)[0m rmse_per_class: [0.075, 0.209, 0.06, 0.262, 0.062, 0.155, 0.212, 0.123, 0.131, 0.128]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1778147667646408
[2m[36m(func pid=32639)[0m mae:  0.13035787642002106
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.259, 0.094, 0.334, 0.096, 0.19, 0.289, 0.138, 0.146, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14978590607643127
[2m[36m(func pid=20486)[0m mae:  0.09129517525434494
[2m[36m(func pid=20486)[0m rmse_per_class: [0.089, 0.228, 0.067, 0.312, 0.11, 0.159, 0.209, 0.106, 0.13, 0.087]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4660 | Steps: 4 | Val loss: 0.3656 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3037 | Steps: 4 | Val loss: 0.2602 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4800 | Steps: 4 | Val loss: 0.3693 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:07:21 (running for 00:17:39.27)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.286 |  0.142 |                   85 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.256 |  0.15  |                   84 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.491 |  0.178 |                   32 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.466 |  0.151 |                   31 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2543 | Steps: 4 | Val loss: 0.2695 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=33359)[0m rmse: 0.1508122980594635
[2m[36m(func pid=33359)[0m mae:  0.10546007007360458
[2m[36m(func pid=33359)[0m rmse_per_class: [0.104, 0.231, 0.045, 0.299, 0.055, 0.178, 0.239, 0.121, 0.135, 0.102]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m rmse: 0.14428603649139404
[2m[36m(func pid=19964)[0m mae:  0.08981523662805557
[2m[36m(func pid=19964)[0m rmse_per_class: [0.072, 0.215, 0.05, 0.269, 0.067, 0.154, 0.215, 0.139, 0.135, 0.127]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17796209454536438
[2m[36m(func pid=32639)[0m mae:  0.13042601943016052
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.26, 0.095, 0.334, 0.095, 0.19, 0.289, 0.138, 0.147, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.14936256408691406
[2m[36m(func pid=20486)[0m mae:  0.09057046473026276
[2m[36m(func pid=20486)[0m rmse_per_class: [0.082, 0.208, 0.08, 0.301, 0.109, 0.157, 0.206, 0.118, 0.133, 0.099]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4645 | Steps: 4 | Val loss: 0.3607 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2902 | Steps: 4 | Val loss: 0.2590 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4686 | Steps: 4 | Val loss: 0.3617 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:07:26 (running for 00:17:44.65)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.304 |  0.144 |                   86 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.254 |  0.149 |                   85 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.48  |  0.178 |                   33 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.464 |  0.15  |                   32 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.1498866230249405
[2m[36m(func pid=33359)[0m mae:  0.10426213592290878
[2m[36m(func pid=33359)[0m rmse_per_class: [0.102, 0.23, 0.044, 0.298, 0.055, 0.177, 0.237, 0.12, 0.134, 0.102]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2631 | Steps: 4 | Val loss: 0.2830 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=19964)[0m rmse: 0.14217233657836914
[2m[36m(func pid=19964)[0m mae:  0.08955726027488708
[2m[36m(func pid=19964)[0m rmse_per_class: [0.078, 0.213, 0.044, 0.271, 0.072, 0.151, 0.219, 0.121, 0.132, 0.12]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17764613032341003
[2m[36m(func pid=32639)[0m mae:  0.13017091155052185
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.333, 0.094, 0.19, 0.288, 0.138, 0.147, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=20486)[0m rmse: 0.15614789724349976
[2m[36m(func pid=20486)[0m mae:  0.09417229145765305
[2m[36m(func pid=20486)[0m rmse_per_class: [0.093, 0.199, 0.046, 0.301, 0.123, 0.173, 0.208, 0.123, 0.187, 0.11]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4576 | Steps: 4 | Val loss: 0.3547 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2744 | Steps: 4 | Val loss: 0.2574 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4618 | Steps: 4 | Val loss: 0.3547 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:07:32 (running for 00:17:49.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.29  |  0.142 |                   87 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.263 |  0.156 |                   86 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.469 |  0.178 |                   34 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.458 |  0.149 |                   33 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.1491645723581314
[2m[36m(func pid=33359)[0m mae:  0.10343865305185318
[2m[36m(func pid=33359)[0m rmse_per_class: [0.101, 0.227, 0.043, 0.298, 0.055, 0.176, 0.238, 0.12, 0.133, 0.1]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2515 | Steps: 4 | Val loss: 0.2848 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=19964)[0m rmse: 0.13985499739646912
[2m[36m(func pid=19964)[0m mae:  0.08872554451227188
[2m[36m(func pid=19964)[0m rmse_per_class: [0.08, 0.212, 0.047, 0.267, 0.071, 0.15, 0.22, 0.104, 0.136, 0.111]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17759735882282257
[2m[36m(func pid=32639)[0m mae:  0.13011401891708374
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.094, 0.332, 0.093, 0.19, 0.287, 0.138, 0.146, 0.115]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4434 | Steps: 4 | Val loss: 0.3494 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=20486)[0m rmse: 0.15810361504554749
[2m[36m(func pid=20486)[0m mae:  0.09621226787567139
[2m[36m(func pid=20486)[0m rmse_per_class: [0.11, 0.206, 0.043, 0.295, 0.137, 0.16, 0.253, 0.115, 0.184, 0.079]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2764 | Steps: 4 | Val loss: 0.2564 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4575 | Steps: 4 | Val loss: 0.3504 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=33359)[0m rmse: 0.14872977137565613
[2m[36m(func pid=33359)[0m mae:  0.10292734205722809
[2m[36m(func pid=33359)[0m rmse_per_class: [0.101, 0.228, 0.042, 0.299, 0.054, 0.176, 0.237, 0.12, 0.133, 0.098]
[2m[36m(func pid=33359)[0m 
== Status ==
Current time: 2024-01-07 20:07:37 (running for 00:17:54.99)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.274 |  0.14  |                   88 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.251 |  0.158 |                   87 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.462 |  0.178 |                   35 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.443 |  0.149 |                   34 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2556 | Steps: 4 | Val loss: 0.2624 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=19964)[0m rmse: 0.13867482542991638
[2m[36m(func pid=19964)[0m mae:  0.08764176070690155
[2m[36m(func pid=19964)[0m rmse_per_class: [0.08, 0.21, 0.052, 0.266, 0.072, 0.149, 0.214, 0.102, 0.131, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17719285190105438
[2m[36m(func pid=32639)[0m mae:  0.129714697599411
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.093, 0.332, 0.093, 0.19, 0.287, 0.138, 0.146, 0.115]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4407 | Steps: 4 | Val loss: 0.3426 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=20486)[0m rmse: 0.14616969227790833
[2m[36m(func pid=20486)[0m mae:  0.08691103011369705
[2m[36m(func pid=20486)[0m rmse_per_class: [0.085, 0.2, 0.048, 0.261, 0.118, 0.157, 0.222, 0.149, 0.136, 0.085]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2632 | Steps: 4 | Val loss: 0.2550 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4477 | Steps: 4 | Val loss: 0.3449 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:07:42 (running for 00:18:00.32)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.276 |  0.139 |                   89 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.256 |  0.146 |                   88 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.457 |  0.177 |                   36 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.441 |  0.148 |                   35 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.1481260359287262
[2m[36m(func pid=33359)[0m mae:  0.10226686298847198
[2m[36m(func pid=33359)[0m rmse_per_class: [0.1, 0.227, 0.04, 0.3, 0.054, 0.175, 0.234, 0.119, 0.133, 0.1]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2480 | Steps: 4 | Val loss: 0.2615 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=19964)[0m rmse: 0.1368924081325531
[2m[36m(func pid=19964)[0m mae:  0.08690683543682098
[2m[36m(func pid=19964)[0m rmse_per_class: [0.077, 0.207, 0.045, 0.265, 0.072, 0.15, 0.211, 0.098, 0.137, 0.108]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1769334226846695
[2m[36m(func pid=32639)[0m mae:  0.12945516407489777
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.094, 0.331, 0.091, 0.19, 0.285, 0.138, 0.146, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4364 | Steps: 4 | Val loss: 0.3346 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=20486)[0m rmse: 0.14017906785011292
[2m[36m(func pid=20486)[0m mae:  0.08538690209388733
[2m[36m(func pid=20486)[0m rmse_per_class: [0.082, 0.193, 0.039, 0.282, 0.078, 0.154, 0.2, 0.11, 0.153, 0.11]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2721 | Steps: 4 | Val loss: 0.2548 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4462 | Steps: 4 | Val loss: 0.3403 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:07:47 (running for 00:18:05.66)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.263 |  0.137 |                   90 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.248 |  0.14  |                   89 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.448 |  0.177 |                   37 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.436 |  0.148 |                   36 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.1475951373577118
[2m[36m(func pid=33359)[0m mae:  0.10181702673435211
[2m[36m(func pid=33359)[0m rmse_per_class: [0.099, 0.227, 0.039, 0.301, 0.054, 0.174, 0.232, 0.117, 0.132, 0.099]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2405 | Steps: 4 | Val loss: 0.2824 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=19964)[0m rmse: 0.13654406368732452
[2m[36m(func pid=19964)[0m mae:  0.08674786984920502
[2m[36m(func pid=19964)[0m rmse_per_class: [0.079, 0.204, 0.042, 0.262, 0.07, 0.152, 0.206, 0.098, 0.141, 0.112]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17698490619659424
[2m[36m(func pid=32639)[0m mae:  0.129511296749115
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.26, 0.094, 0.331, 0.091, 0.19, 0.285, 0.138, 0.146, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4216 | Steps: 4 | Val loss: 0.3278 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=20486)[0m rmse: 0.15313200652599335
[2m[36m(func pid=20486)[0m mae:  0.09311229735612869
[2m[36m(func pid=20486)[0m rmse_per_class: [0.074, 0.213, 0.045, 0.319, 0.077, 0.157, 0.214, 0.113, 0.186, 0.133]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2698 | Steps: 4 | Val loss: 0.2535 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4388 | Steps: 4 | Val loss: 0.3350 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=33359)[0m rmse: 0.1472279578447342
[2m[36m(func pid=33359)[0m mae:  0.101304791867733
[2m[36m(func pid=33359)[0m rmse_per_class: [0.097, 0.226, 0.038, 0.302, 0.054, 0.175, 0.231, 0.117, 0.132, 0.1]
== Status ==
Current time: 2024-01-07 20:07:53 (running for 00:18:11.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.272 |  0.137 |                   91 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.24  |  0.153 |                   90 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.446 |  0.177 |                   38 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.422 |  0.147 |                   37 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2659 | Steps: 4 | Val loss: 0.2723 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=19964)[0m rmse: 0.13560751080513
[2m[36m(func pid=19964)[0m mae:  0.08580613881349564
[2m[36m(func pid=19964)[0m rmse_per_class: [0.074, 0.205, 0.042, 0.257, 0.071, 0.15, 0.206, 0.099, 0.142, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1764538437128067
[2m[36m(func pid=32639)[0m mae:  0.12905283272266388
[2m[36m(func pid=32639)[0m rmse_per_class: [0.121, 0.26, 0.092, 0.331, 0.09, 0.19, 0.283, 0.137, 0.146, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4250 | Steps: 4 | Val loss: 0.3199 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=20486)[0m rmse: 0.1500129997730255
[2m[36m(func pid=20486)[0m mae:  0.09145281463861465
[2m[36m(func pid=20486)[0m rmse_per_class: [0.073, 0.205, 0.056, 0.295, 0.082, 0.173, 0.228, 0.11, 0.191, 0.087]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2812 | Steps: 4 | Val loss: 0.2526 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4359 | Steps: 4 | Val loss: 0.3333 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 20:07:58 (running for 00:18:16.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.27  |  0.136 |                   92 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.266 |  0.15  |                   91 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.439 |  0.176 |                   39 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.425 |  0.147 |                   38 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.146712064743042
[2m[36m(func pid=33359)[0m mae:  0.10060825198888779
[2m[36m(func pid=33359)[0m rmse_per_class: [0.095, 0.226, 0.036, 0.304, 0.054, 0.174, 0.23, 0.115, 0.132, 0.1]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2326 | Steps: 4 | Val loss: 0.2782 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=19964)[0m rmse: 0.13521888852119446
[2m[36m(func pid=19964)[0m mae:  0.08486180007457733
[2m[36m(func pid=19964)[0m rmse_per_class: [0.068, 0.208, 0.044, 0.256, 0.069, 0.157, 0.208, 0.103, 0.129, 0.11]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17665119469165802
[2m[36m(func pid=32639)[0m mae:  0.129213348031044
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.261, 0.094, 0.331, 0.09, 0.19, 0.284, 0.137, 0.146, 0.115]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4052 | Steps: 4 | Val loss: 0.3129 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=20486)[0m rmse: 0.15625369548797607
[2m[36m(func pid=20486)[0m mae:  0.09617279469966888
[2m[36m(func pid=20486)[0m rmse_per_class: [0.075, 0.206, 0.104, 0.296, 0.073, 0.174, 0.233, 0.113, 0.205, 0.083]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2835 | Steps: 4 | Val loss: 0.2544 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4257 | Steps: 4 | Val loss: 0.3303 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:08:03 (running for 00:18:21.48)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.281 |  0.135 |                   93 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.233 |  0.156 |                   92 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.436 |  0.177 |                   40 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.405 |  0.147 |                   39 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.14651350677013397
[2m[36m(func pid=33359)[0m mae:  0.100608691573143
[2m[36m(func pid=33359)[0m rmse_per_class: [0.096, 0.225, 0.036, 0.305, 0.054, 0.173, 0.231, 0.115, 0.132, 0.098]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2600 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=19964)[0m rmse: 0.13726259768009186
[2m[36m(func pid=19964)[0m mae:  0.0856776088476181
[2m[36m(func pid=19964)[0m rmse_per_class: [0.068, 0.214, 0.043, 0.257, 0.067, 0.162, 0.211, 0.105, 0.129, 0.118]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1764870285987854
[2m[36m(func pid=32639)[0m mae:  0.1290481984615326
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.26, 0.093, 0.331, 0.089, 0.19, 0.283, 0.137, 0.146, 0.115]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3986 | Steps: 4 | Val loss: 0.3044 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=20486)[0m rmse: 0.15219752490520477
[2m[36m(func pid=20486)[0m mae:  0.09068888425827026
[2m[36m(func pid=20486)[0m rmse_per_class: [0.073, 0.211, 0.109, 0.291, 0.079, 0.16, 0.22, 0.112, 0.173, 0.095]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2686 | Steps: 4 | Val loss: 0.2544 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4252 | Steps: 4 | Val loss: 0.3267 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:08:09 (running for 00:18:26.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.284 |  0.137 |                   94 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.26  |  0.152 |                   93 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.426 |  0.176 |                   41 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.399 |  0.146 |                   40 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.1456381380558014
[2m[36m(func pid=33359)[0m mae:  0.09973368793725967
[2m[36m(func pid=33359)[0m rmse_per_class: [0.092, 0.226, 0.034, 0.304, 0.054, 0.172, 0.228, 0.114, 0.132, 0.1]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2546 | Steps: 4 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=19964)[0m rmse: 0.13751797378063202
[2m[36m(func pid=19964)[0m mae:  0.0854586511850357
[2m[36m(func pid=19964)[0m rmse_per_class: [0.07, 0.215, 0.047, 0.256, 0.071, 0.153, 0.212, 0.104, 0.126, 0.121]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17617519199848175
[2m[36m(func pid=32639)[0m mae:  0.12875553965568542
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.26, 0.093, 0.33, 0.089, 0.191, 0.282, 0.137, 0.145, 0.115]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3823 | Steps: 4 | Val loss: 0.2991 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=20486)[0m rmse: 0.1547556221485138
[2m[36m(func pid=20486)[0m mae:  0.090613953769207
[2m[36m(func pid=20486)[0m rmse_per_class: [0.073, 0.215, 0.043, 0.29, 0.096, 0.154, 0.218, 0.161, 0.155, 0.144]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2643 | Steps: 4 | Val loss: 0.2521 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 20:08:14 (running for 00:18:32.10)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.269 |  0.138 |                   95 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.255 |  0.155 |                   94 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.425 |  0.176 |                   42 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.382 |  0.145 |                   41 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4227 | Steps: 4 | Val loss: 0.3237 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=33359)[0m rmse: 0.14537067711353302
[2m[36m(func pid=33359)[0m mae:  0.09946688264608383
[2m[36m(func pid=33359)[0m rmse_per_class: [0.094, 0.225, 0.033, 0.305, 0.054, 0.173, 0.228, 0.112, 0.132, 0.098]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2488 | Steps: 4 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=19964)[0m rmse: 0.1353154480457306
[2m[36m(func pid=19964)[0m mae:  0.08421352505683899
[2m[36m(func pid=19964)[0m rmse_per_class: [0.069, 0.212, 0.054, 0.254, 0.066, 0.151, 0.209, 0.104, 0.12, 0.116]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17552992701530457
[2m[36m(func pid=32639)[0m mae:  0.12814965844154358
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.33, 0.087, 0.191, 0.281, 0.135, 0.146, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3871 | Steps: 4 | Val loss: 0.2928 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=20486)[0m rmse: 0.15532977879047394
[2m[36m(func pid=20486)[0m mae:  0.09256932139396667
[2m[36m(func pid=20486)[0m rmse_per_class: [0.077, 0.227, 0.025, 0.29, 0.101, 0.16, 0.241, 0.154, 0.144, 0.134]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2676 | Steps: 4 | Val loss: 0.2529 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=33359)[0m rmse: 0.14506861567497253
[2m[36m(func pid=33359)[0m mae:  0.09905127435922623
[2m[36m(func pid=33359)[0m rmse_per_class: [0.095, 0.225, 0.033, 0.305, 0.054, 0.172, 0.226, 0.112, 0.132, 0.097]
[2m[36m(func pid=33359)[0m 
== Status ==
Current time: 2024-01-07 20:08:19 (running for 00:18:37.21)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.264 |  0.135 |                   96 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.249 |  0.155 |                   95 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.423 |  0.176 |                   43 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.387 |  0.145 |                   42 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4189 | Steps: 4 | Val loss: 0.3217 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2508 | Steps: 4 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=19964)[0m rmse: 0.13477686047554016
[2m[36m(func pid=19964)[0m mae:  0.08447735011577606
[2m[36m(func pid=19964)[0m rmse_per_class: [0.07, 0.209, 0.053, 0.26, 0.063, 0.15, 0.207, 0.099, 0.122, 0.115]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.175606831908226
[2m[36m(func pid=32639)[0m mae:  0.1282309889793396
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.093, 0.33, 0.086, 0.191, 0.281, 0.136, 0.146, 0.115]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3679 | Steps: 4 | Val loss: 0.2880 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=20486)[0m rmse: 0.15603289008140564
[2m[36m(func pid=20486)[0m mae:  0.09379910677671432
[2m[36m(func pid=20486)[0m rmse_per_class: [0.081, 0.222, 0.025, 0.28, 0.103, 0.18, 0.253, 0.141, 0.149, 0.127]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2596 | Steps: 4 | Val loss: 0.2535 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 20:08:24 (running for 00:18:42.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.268 |  0.135 |                   97 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.251 |  0.156 |                   96 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.419 |  0.176 |                   44 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.368 |  0.145 |                   43 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.14481604099273682
[2m[36m(func pid=33359)[0m mae:  0.09895319491624832
[2m[36m(func pid=33359)[0m rmse_per_class: [0.093, 0.225, 0.032, 0.305, 0.054, 0.172, 0.226, 0.11, 0.132, 0.098]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4151 | Steps: 4 | Val loss: 0.3194 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2654 | Steps: 4 | Val loss: 0.2685 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=19964)[0m rmse: 0.13503465056419373
[2m[36m(func pid=19964)[0m mae:  0.08519941568374634
[2m[36m(func pid=19964)[0m rmse_per_class: [0.074, 0.205, 0.048, 0.26, 0.065, 0.151, 0.206, 0.099, 0.127, 0.115]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17529019713401794
[2m[36m(func pid=32639)[0m mae:  0.12796515226364136
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.259, 0.093, 0.33, 0.085, 0.191, 0.282, 0.135, 0.147, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3706 | Steps: 4 | Val loss: 0.2834 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=20486)[0m rmse: 0.1511099636554718
[2m[36m(func pid=20486)[0m mae:  0.08885832130908966
[2m[36m(func pid=20486)[0m rmse_per_class: [0.075, 0.21, 0.029, 0.267, 0.103, 0.175, 0.236, 0.128, 0.16, 0.127]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:08:30 (running for 00:18:47.75)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.26  |  0.135 |                   98 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.265 |  0.151 |                   97 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.415 |  0.175 |                   45 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.371 |  0.144 |                   44 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.14439116418361664
[2m[36m(func pid=33359)[0m mae:  0.09851840883493423
[2m[36m(func pid=33359)[0m rmse_per_class: [0.093, 0.225, 0.031, 0.303, 0.054, 0.172, 0.225, 0.109, 0.132, 0.099]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2731 | Steps: 4 | Val loss: 0.2560 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4120 | Steps: 4 | Val loss: 0.3179 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2609 | Steps: 4 | Val loss: 0.2583 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=19964)[0m rmse: 0.13706880807876587
[2m[36m(func pid=19964)[0m mae:  0.0864597037434578
[2m[36m(func pid=19964)[0m rmse_per_class: [0.076, 0.207, 0.047, 0.267, 0.068, 0.15, 0.207, 0.1, 0.133, 0.115]
[2m[36m(func pid=19964)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.17493335902690887
[2m[36m(func pid=32639)[0m mae:  0.12772038578987122
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.258, 0.091, 0.33, 0.085, 0.191, 0.281, 0.135, 0.146, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3621 | Steps: 4 | Val loss: 0.2798 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=20486)[0m rmse: 0.14158819615840912
[2m[36m(func pid=20486)[0m mae:  0.08415721356868744
[2m[36m(func pid=20486)[0m rmse_per_class: [0.071, 0.21, 0.031, 0.256, 0.093, 0.157, 0.204, 0.117, 0.167, 0.112]
[2m[36m(func pid=20486)[0m 
== Status ==
Current time: 2024-01-07 20:08:35 (running for 00:18:53.05)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00006 | RUNNING    | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.273 |  0.137 |                   99 |
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.261 |  0.142 |                   98 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.412 |  0.175 |                   46 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.362 |  0.145 |                   45 |
| train_84a75_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=33359)[0m rmse: 0.14451086521148682
[2m[36m(func pid=33359)[0m mae:  0.0985393226146698
[2m[36m(func pid=33359)[0m rmse_per_class: [0.093, 0.226, 0.03, 0.305, 0.054, 0.172, 0.225, 0.109, 0.132, 0.098]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=19964)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2585 | Steps: 4 | Val loss: 0.2489 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4086 | Steps: 4 | Val loss: 0.3167 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2559 | Steps: 4 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=19964)[0m rmse: 0.1311393529176712
[2m[36m(func pid=19964)[0m mae:  0.08261603116989136
[2m[36m(func pid=19964)[0m rmse_per_class: [0.068, 0.204, 0.043, 0.255, 0.07, 0.145, 0.202, 0.096, 0.12, 0.107]
[2m[36m(func pid=32639)[0m rmse: 0.1746504008769989
[2m[36m(func pid=32639)[0m mae:  0.1276075690984726
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.259, 0.088, 0.33, 0.084, 0.19, 0.28, 0.136, 0.146, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3639 | Steps: 4 | Val loss: 0.2759 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=20486)[0m rmse: 0.14831623435020447
[2m[36m(func pid=20486)[0m mae:  0.090910404920578
[2m[36m(func pid=20486)[0m rmse_per_class: [0.068, 0.222, 0.057, 0.302, 0.105, 0.163, 0.201, 0.109, 0.154, 0.103]
[2m[36m(func pid=20486)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14432094991207123
[2m[36m(func pid=33359)[0m mae:  0.09836667776107788
[2m[36m(func pid=33359)[0m rmse_per_class: [0.094, 0.227, 0.03, 0.304, 0.054, 0.172, 0.222, 0.109, 0.132, 0.1]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4070 | Steps: 4 | Val loss: 0.3152 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=20486)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2468 | Steps: 4 | Val loss: 0.2901 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=32639)[0m rmse: 0.17426931858062744
[2m[36m(func pid=32639)[0m mae:  0.12722985446453094
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.259, 0.088, 0.33, 0.084, 0.189, 0.279, 0.136, 0.145, 0.113]
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3578 | Steps: 4 | Val loss: 0.2727 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=20486)[0m rmse: 0.16032035648822784
[2m[36m(func pid=20486)[0m mae:  0.09850824624300003
[2m[36m(func pid=20486)[0m rmse_per_class: [0.08, 0.227, 0.095, 0.345, 0.107, 0.161, 0.205, 0.107, 0.165, 0.113]
[2m[36m(func pid=33359)[0m rmse: 0.14399853348731995
[2m[36m(func pid=33359)[0m mae:  0.09847699105739594
[2m[36m(func pid=33359)[0m rmse_per_class: [0.095, 0.227, 0.03, 0.305, 0.054, 0.17, 0.221, 0.107, 0.132, 0.1]
== Status ==
Current time: 2024-01-07 20:08:40 (running for 00:18:58.37)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.256 |  0.148 |                   99 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.409 |  0.175 |                   47 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.364 |  0.144 |                   46 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 20:08:46 (running for 00:19:04.60)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00007 | RUNNING    | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.256 |  0.148 |                   99 |
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.409 |  0.175 |                   47 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.358 |  0.144 |                   47 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=44519)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=44519)[0m Configuration completed!
[2m[36m(func pid=44519)[0m New optimizer parameters:
[2m[36m(func pid=44519)[0m SGD (
[2m[36m(func pid=44519)[0m Parameter Group 0
[2m[36m(func pid=44519)[0m     dampening: 0
[2m[36m(func pid=44519)[0m     differentiable: False
[2m[36m(func pid=44519)[0m     foreach: None
[2m[36m(func pid=44519)[0m     lr: 0.01
[2m[36m(func pid=44519)[0m     maximize: False
[2m[36m(func pid=44519)[0m     momentum: 0.99
[2m[36m(func pid=44519)[0m     nesterov: False
[2m[36m(func pid=44519)[0m     weight_decay: 0.0001
[2m[36m(func pid=44519)[0m )
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4070 | Steps: 4 | Val loss: 0.3143 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3559 | Steps: 4 | Val loss: 0.2706 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8674 | Steps: 4 | Val loss: 0.6253 | Batch size: 32 | lr: 0.01 | Duration: 4.79s
[2m[36m(func pid=32639)[0m rmse: 0.1742963194847107
[2m[36m(func pid=32639)[0m mae:  0.12727823853492737
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.26, 0.088, 0.328, 0.084, 0.189, 0.279, 0.136, 0.145, 0.114]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14383560419082642
[2m[36m(func pid=33359)[0m mae:  0.09853678941726685
[2m[36m(func pid=33359)[0m rmse_per_class: [0.096, 0.227, 0.029, 0.304, 0.054, 0.168, 0.221, 0.107, 0.132, 0.1]
[2m[36m(func pid=44519)[0m rmse: 0.18272565305233002
[2m[36m(func pid=44519)[0m mae:  0.13434162735939026
[2m[36m(func pid=44519)[0m rmse_per_class: [0.117, 0.268, 0.109, 0.34, 0.112, 0.19, 0.295, 0.144, 0.141, 0.111]
== Status ==
Current time: 2024-01-07 20:08:52 (running for 00:19:10.17)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.407 |  0.174 |                   49 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.358 |  0.144 |                   47 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4063 | Steps: 4 | Val loss: 0.3133 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=45122)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=45122)[0m Configuration completed!
[2m[36m(func pid=45122)[0m New optimizer parameters:
[2m[36m(func pid=45122)[0m SGD (
[2m[36m(func pid=45122)[0m Parameter Group 0
[2m[36m(func pid=45122)[0m     dampening: 0
[2m[36m(func pid=45122)[0m     differentiable: False
[2m[36m(func pid=45122)[0m     foreach: None
[2m[36m(func pid=45122)[0m     lr: 0.1
[2m[36m(func pid=45122)[0m     maximize: False
[2m[36m(func pid=45122)[0m     momentum: 0.99
[2m[36m(func pid=45122)[0m     nesterov: False
[2m[36m(func pid=45122)[0m     weight_decay: 0.0001
[2m[36m(func pid=45122)[0m )
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:08:57 (running for 00:19:15.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.406 |  0.174 |                   50 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.356 |  0.144 |                   48 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.867 |  0.183 |                    1 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17400655150413513
[2m[36m(func pid=32639)[0m mae:  0.1270475536584854
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.088, 0.328, 0.083, 0.189, 0.279, 0.137, 0.145, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3648 | Steps: 4 | Val loss: 0.2694 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7028 | Steps: 4 | Val loss: 0.4651 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6880 | Steps: 4 | Val loss: 0.3434 | Batch size: 32 | lr: 0.1 | Duration: 4.84s
[2m[36m(func pid=33359)[0m rmse: 0.14381740987300873
[2m[36m(func pid=33359)[0m mae:  0.09854970872402191
[2m[36m(func pid=33359)[0m rmse_per_class: [0.095, 0.227, 0.029, 0.305, 0.054, 0.169, 0.22, 0.107, 0.132, 0.101]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.18054500222206116
[2m[36m(func pid=44519)[0m mae:  0.13246609270572662
[2m[36m(func pid=44519)[0m rmse_per_class: [0.115, 0.266, 0.107, 0.336, 0.107, 0.19, 0.295, 0.141, 0.139, 0.11]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4076 | Steps: 4 | Val loss: 0.3120 | Batch size: 32 | lr: 0.0001 | Duration: 3.40s
[2m[36m(func pid=45122)[0m rmse: 0.18106034398078918
[2m[36m(func pid=45122)[0m mae:  0.13291016221046448
[2m[36m(func pid=45122)[0m rmse_per_class: [0.111, 0.268, 0.128, 0.338, 0.079, 0.191, 0.292, 0.15, 0.143, 0.112]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:09:03 (running for 00:19:21.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.408 |  0.174 |                   51 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.365 |  0.144 |                   49 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.703 |  0.181 |                    2 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.688 |  0.181 |                    1 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.1735561192035675
[2m[36m(func pid=32639)[0m mae:  0.12659116089344025
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.259, 0.088, 0.327, 0.083, 0.189, 0.278, 0.135, 0.145, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3519 | Steps: 4 | Val loss: 0.2683 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5122 | Steps: 4 | Val loss: 0.3506 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4436 | Steps: 4 | Val loss: 0.3880 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=33359)[0m rmse: 0.1444433331489563
[2m[36m(func pid=33359)[0m mae:  0.09936020523309708
[2m[36m(func pid=33359)[0m rmse_per_class: [0.1, 0.227, 0.029, 0.303, 0.054, 0.17, 0.219, 0.106, 0.132, 0.104]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1778344213962555
[2m[36m(func pid=44519)[0m mae:  0.13047002255916595
[2m[36m(func pid=44519)[0m rmse_per_class: [0.114, 0.263, 0.103, 0.331, 0.093, 0.189, 0.293, 0.139, 0.14, 0.113]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4032 | Steps: 4 | Val loss: 0.3127 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=45122)[0m rmse: 0.16359281539916992
[2m[36m(func pid=45122)[0m mae:  0.11696143448352814
[2m[36m(func pid=45122)[0m rmse_per_class: [0.109, 0.26, 0.075, 0.306, 0.059, 0.173, 0.273, 0.148, 0.136, 0.096]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:09:09 (running for 00:19:27.00)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.403 |  0.174 |                   52 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.352 |  0.144 |                   50 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.512 |  0.178 |                    3 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.444 |  0.164 |                    2 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17388686537742615
[2m[36m(func pid=32639)[0m mae:  0.12683698534965515
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.26, 0.088, 0.328, 0.082, 0.189, 0.278, 0.136, 0.145, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3554 | Steps: 4 | Val loss: 0.2679 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4160 | Steps: 4 | Val loss: 0.3167 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5994 | Steps: 4 | Val loss: 0.4659 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=33359)[0m rmse: 0.14484803378582
[2m[36m(func pid=33359)[0m mae:  0.10022672265768051
[2m[36m(func pid=33359)[0m rmse_per_class: [0.101, 0.228, 0.03, 0.301, 0.054, 0.169, 0.222, 0.105, 0.133, 0.104]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.17536437511444092
[2m[36m(func pid=44519)[0m mae:  0.12866899371147156
[2m[36m(func pid=44519)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.078, 0.187, 0.288, 0.134, 0.142, 0.121]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3981 | Steps: 4 | Val loss: 0.3114 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=45122)[0m rmse: 0.15736354887485504
[2m[36m(func pid=45122)[0m mae:  0.10503166913986206
[2m[36m(func pid=45122)[0m rmse_per_class: [0.099, 0.257, 0.053, 0.311, 0.055, 0.162, 0.255, 0.158, 0.136, 0.089]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:09:14 (running for 00:19:32.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.398 |  0.173 |                   53 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.355 |  0.145 |                   51 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.416 |  0.175 |                    4 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.599 |  0.157 |                    3 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17317995429039001
[2m[36m(func pid=32639)[0m mae:  0.12625855207443237
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.259, 0.086, 0.328, 0.082, 0.189, 0.275, 0.137, 0.144, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3498 | Steps: 4 | Val loss: 0.2663 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4166 | Steps: 4 | Val loss: 0.3345 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5863 | Steps: 4 | Val loss: 1.0517 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=33359)[0m rmse: 0.14392854273319244
[2m[36m(func pid=33359)[0m mae:  0.09936931729316711
[2m[36m(func pid=33359)[0m rmse_per_class: [0.1, 0.228, 0.029, 0.299, 0.054, 0.169, 0.222, 0.105, 0.132, 0.101]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.17084059119224548
[2m[36m(func pid=44519)[0m mae:  0.12487409263849258
[2m[36m(func pid=44519)[0m rmse_per_class: [0.116, 0.257, 0.085, 0.321, 0.068, 0.187, 0.281, 0.13, 0.14, 0.123]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3972 | Steps: 4 | Val loss: 0.3107 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=45122)[0m rmse: 0.19554424285888672
[2m[36m(func pid=45122)[0m mae:  0.12201578915119171
[2m[36m(func pid=45122)[0m rmse_per_class: [0.106, 0.284, 0.049, 0.375, 0.056, 0.171, 0.552, 0.133, 0.136, 0.093]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3493 | Steps: 4 | Val loss: 0.2666 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 20:09:20 (running for 00:19:37.91)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.397 |  0.173 |                   54 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.35  |  0.144 |                   52 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.417 |  0.171 |                    5 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.586 |  0.196 |                    4 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17281250655651093
[2m[36m(func pid=32639)[0m mae:  0.12592844665050507
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.258, 0.085, 0.327, 0.082, 0.189, 0.276, 0.135, 0.144, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4601 | Steps: 4 | Val loss: 0.3750 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4432 | Steps: 4 | Val loss: 2.2322 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=33359)[0m rmse: 0.14399316906929016
[2m[36m(func pid=33359)[0m mae:  0.09970857203006744
[2m[36m(func pid=33359)[0m rmse_per_class: [0.097, 0.228, 0.029, 0.3, 0.054, 0.168, 0.224, 0.105, 0.133, 0.103]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.16373886168003082
[2m[36m(func pid=44519)[0m mae:  0.11874179542064667
[2m[36m(func pid=44519)[0m rmse_per_class: [0.12, 0.243, 0.066, 0.305, 0.061, 0.182, 0.277, 0.127, 0.137, 0.121]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4002 | Steps: 4 | Val loss: 0.3106 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=45122)[0m rmse: 0.21243827044963837
[2m[36m(func pid=45122)[0m mae:  0.13270506262779236
[2m[36m(func pid=45122)[0m rmse_per_class: [0.11, 0.297, 0.049, 0.383, 0.056, 0.193, 0.677, 0.129, 0.136, 0.095]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3589 | Steps: 4 | Val loss: 0.2672 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:09:25 (running for 00:19:43.43)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.4   |  0.173 |                   55 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.349 |  0.144 |                   53 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.46  |  0.164 |                    6 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.443 |  0.212 |                    5 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17277097702026367
[2m[36m(func pid=32639)[0m mae:  0.12583832442760468
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.258, 0.086, 0.327, 0.081, 0.189, 0.275, 0.135, 0.144, 0.113]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5057 | Steps: 4 | Val loss: 0.4203 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4064 | Steps: 4 | Val loss: 1.4142 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=33359)[0m rmse: 0.1444222778081894
[2m[36m(func pid=33359)[0m mae:  0.09964381158351898
[2m[36m(func pid=33359)[0m rmse_per_class: [0.097, 0.228, 0.028, 0.301, 0.054, 0.169, 0.222, 0.106, 0.134, 0.105]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15851566195487976
[2m[36m(func pid=44519)[0m mae:  0.11365801095962524
[2m[36m(func pid=44519)[0m rmse_per_class: [0.12, 0.234, 0.056, 0.295, 0.056, 0.177, 0.268, 0.125, 0.136, 0.118]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.2078550159931183
[2m[36m(func pid=45122)[0m mae:  0.12924739718437195
[2m[36m(func pid=45122)[0m rmse_per_class: [0.111, 0.294, 0.049, 0.373, 0.056, 0.197, 0.634, 0.134, 0.136, 0.094]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4003 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=32639)[0m rmse: 0.17192016541957855
[2m[36m(func pid=32639)[0m mae:  0.12516765296459198
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.256, 0.083, 0.327, 0.081, 0.189, 0.274, 0.134, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 20:09:31 (running for 00:19:48.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.4   |  0.172 |                   56 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.359 |  0.144 |                   54 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.506 |  0.159 |                    7 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.406 |  0.208 |                    6 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3581 | Steps: 4 | Val loss: 0.2670 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5363 | Steps: 4 | Val loss: 0.4466 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3894 | Steps: 4 | Val loss: 0.6007 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=33359)[0m rmse: 0.14393433928489685
[2m[36m(func pid=33359)[0m mae:  0.09900978207588196
[2m[36m(func pid=33359)[0m rmse_per_class: [0.093, 0.228, 0.028, 0.3, 0.054, 0.168, 0.222, 0.108, 0.134, 0.104]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1539607048034668
[2m[36m(func pid=44519)[0m mae:  0.10854656994342804
[2m[36m(func pid=44519)[0m rmse_per_class: [0.116, 0.228, 0.05, 0.286, 0.054, 0.175, 0.256, 0.124, 0.134, 0.114]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.21173372864723206
[2m[36m(func pid=45122)[0m mae:  0.1276383101940155
[2m[36m(func pid=45122)[0m rmse_per_class: [0.516, 0.283, 0.05, 0.364, 0.056, 0.196, 0.266, 0.157, 0.138, 0.09]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4001 | Steps: 4 | Val loss: 0.3097 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 20:09:36 (running for 00:19:54.20)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.4   |  0.172 |                   57 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.358 |  0.144 |                   55 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.536 |  0.154 |                    8 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.389 |  0.212 |                    7 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17191079258918762
[2m[36m(func pid=32639)[0m mae:  0.1252497434616089
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.257, 0.084, 0.326, 0.08, 0.189, 0.274, 0.135, 0.144, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3420 | Steps: 4 | Val loss: 0.2680 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5574 | Steps: 4 | Val loss: 0.4532 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3929 | Steps: 4 | Val loss: 0.8315 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=33359)[0m rmse: 0.1443750411272049
[2m[36m(func pid=33359)[0m mae:  0.09955291450023651
[2m[36m(func pid=33359)[0m rmse_per_class: [0.094, 0.228, 0.028, 0.3, 0.054, 0.166, 0.224, 0.107, 0.135, 0.107]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14990819990634918
[2m[36m(func pid=44519)[0m mae:  0.10340164601802826
[2m[36m(func pid=44519)[0m rmse_per_class: [0.107, 0.226, 0.047, 0.28, 0.054, 0.173, 0.244, 0.124, 0.132, 0.11]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.22940143942832947
[2m[36m(func pid=45122)[0m mae:  0.13703610002994537
[2m[36m(func pid=45122)[0m rmse_per_class: [0.137, 0.285, 0.578, 0.371, 0.056, 0.201, 0.262, 0.176, 0.137, 0.092]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3976 | Steps: 4 | Val loss: 0.3096 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3550 | Steps: 4 | Val loss: 0.2689 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 20:09:41 (running for 00:19:59.65)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.398 |  0.172 |                   58 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.342 |  0.144 |                   56 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.557 |  0.15  |                    9 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.393 |  0.229 |                    8 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.1717713326215744
[2m[36m(func pid=32639)[0m mae:  0.12512251734733582
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.257, 0.084, 0.326, 0.079, 0.189, 0.274, 0.134, 0.145, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3775 | Steps: 4 | Val loss: 0.8675 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5456 | Steps: 4 | Val loss: 0.4344 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=33359)[0m rmse: 0.14478300511837006
[2m[36m(func pid=33359)[0m mae:  0.0991801768541336
[2m[36m(func pid=33359)[0m rmse_per_class: [0.094, 0.228, 0.028, 0.301, 0.054, 0.167, 0.222, 0.111, 0.135, 0.108]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.2299535572528839
[2m[36m(func pid=45122)[0m mae:  0.1374138593673706
[2m[36m(func pid=45122)[0m rmse_per_class: [0.116, 0.284, 0.411, 0.367, 0.056, 0.208, 0.244, 0.268, 0.137, 0.207]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14677557349205017
[2m[36m(func pid=44519)[0m mae:  0.09875664860010147
[2m[36m(func pid=44519)[0m rmse_per_class: [0.099, 0.226, 0.046, 0.283, 0.055, 0.169, 0.233, 0.124, 0.131, 0.101]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4000 | Steps: 4 | Val loss: 0.3103 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3495 | Steps: 4 | Val loss: 0.2691 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4387 | Steps: 4 | Val loss: 0.7962 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 20:09:47 (running for 00:20:05.00)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.4   |  0.172 |                   59 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.355 |  0.145 |                   57 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.546 |  0.147 |                   10 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.378 |  0.23  |                    9 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17188309133052826
[2m[36m(func pid=32639)[0m mae:  0.12526966631412506
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.257, 0.084, 0.326, 0.079, 0.188, 0.274, 0.135, 0.144, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5284 | Steps: 4 | Val loss: 0.4056 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=33359)[0m rmse: 0.14484362304210663
[2m[36m(func pid=33359)[0m mae:  0.09951183944940567
[2m[36m(func pid=33359)[0m rmse_per_class: [0.097, 0.228, 0.029, 0.297, 0.054, 0.166, 0.225, 0.109, 0.136, 0.107]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.23905201256275177
[2m[36m(func pid=45122)[0m mae:  0.14321444928646088
[2m[36m(func pid=45122)[0m rmse_per_class: [0.115, 0.282, 0.085, 0.365, 0.056, 0.212, 0.251, 0.375, 0.141, 0.508]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14874425530433655
[2m[36m(func pid=44519)[0m mae:  0.09801112115383148
[2m[36m(func pid=44519)[0m rmse_per_class: [0.099, 0.229, 0.048, 0.295, 0.055, 0.173, 0.243, 0.122, 0.131, 0.094]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3994 | Steps: 4 | Val loss: 0.3103 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4072 | Steps: 4 | Val loss: 0.7575 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3604 | Steps: 4 | Val loss: 0.2688 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:09:52 (running for 00:20:10.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.399 |  0.172 |                   60 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.35  |  0.145 |                   58 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.528 |  0.149 |                   11 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.439 |  0.239 |                   10 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17164698243141174
[2m[36m(func pid=32639)[0m mae:  0.12503007054328918
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.257, 0.084, 0.327, 0.079, 0.189, 0.273, 0.134, 0.144, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4952 | Steps: 4 | Val loss: 0.3860 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=45122)[0m rmse: 0.2416437566280365
[2m[36m(func pid=45122)[0m mae:  0.14576034247875214
[2m[36m(func pid=45122)[0m rmse_per_class: [0.115, 0.274, 0.093, 0.367, 0.056, 0.205, 0.262, 0.39, 0.153, 0.501]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1446855366230011
[2m[36m(func pid=33359)[0m mae:  0.0994013324379921
[2m[36m(func pid=33359)[0m rmse_per_class: [0.098, 0.228, 0.029, 0.294, 0.053, 0.165, 0.227, 0.108, 0.136, 0.108]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15407946705818176
[2m[36m(func pid=44519)[0m mae:  0.09960241615772247
[2m[36m(func pid=44519)[0m rmse_per_class: [0.099, 0.234, 0.049, 0.311, 0.055, 0.176, 0.278, 0.118, 0.132, 0.09]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3972 | Steps: 4 | Val loss: 0.3103 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4030 | Steps: 4 | Val loss: 0.7374 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3436 | Steps: 4 | Val loss: 0.2696 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 20:09:57 (running for 00:20:15.67)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.397 |  0.171 |                   61 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.36  |  0.145 |                   59 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.495 |  0.154 |                   12 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.407 |  0.242 |                   11 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17130081355571747
[2m[36m(func pid=32639)[0m mae:  0.12469419091939926
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.257, 0.083, 0.326, 0.078, 0.188, 0.271, 0.133, 0.144, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4606 | Steps: 4 | Val loss: 0.3959 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=45122)[0m rmse: 0.2375773936510086
[2m[36m(func pid=45122)[0m mae:  0.1402777135372162
[2m[36m(func pid=45122)[0m rmse_per_class: [0.122, 0.27, 0.355, 0.37, 0.055, 0.201, 0.242, 0.338, 0.242, 0.18]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14488454163074493
[2m[36m(func pid=33359)[0m mae:  0.10008081048727036
[2m[36m(func pid=33359)[0m rmse_per_class: [0.099, 0.229, 0.028, 0.293, 0.053, 0.165, 0.232, 0.106, 0.136, 0.107]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3956 | Steps: 4 | Val loss: 0.3096 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=44519)[0m rmse: 0.1597703993320465
[2m[36m(func pid=44519)[0m mae:  0.10096345096826553
[2m[36m(func pid=44519)[0m rmse_per_class: [0.096, 0.241, 0.049, 0.316, 0.056, 0.176, 0.327, 0.116, 0.133, 0.089]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3589 | Steps: 4 | Val loss: 0.5966 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:10:03 (running for 00:20:21.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.396 |  0.171 |                   62 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.344 |  0.145 |                   60 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.461 |  0.16  |                   13 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.403 |  0.238 |                   12 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.1705317199230194
[2m[36m(func pid=32639)[0m mae:  0.12405169010162354
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.256, 0.081, 0.326, 0.078, 0.188, 0.269, 0.134, 0.144, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3417 | Steps: 4 | Val loss: 0.2710 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4191 | Steps: 4 | Val loss: 0.3973 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=45122)[0m rmse: 0.22458355128765106
[2m[36m(func pid=45122)[0m mae:  0.13280555605888367
[2m[36m(func pid=45122)[0m rmse_per_class: [0.141, 0.265, 0.334, 0.37, 0.054, 0.212, 0.222, 0.321, 0.207, 0.119]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14590734243392944
[2m[36m(func pid=33359)[0m mae:  0.10052845627069473
[2m[36m(func pid=33359)[0m rmse_per_class: [0.098, 0.23, 0.029, 0.295, 0.053, 0.165, 0.232, 0.107, 0.139, 0.111]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3968 | Steps: 4 | Val loss: 0.3102 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=44519)[0m rmse: 0.16228099167346954
[2m[36m(func pid=44519)[0m mae:  0.10139892250299454
[2m[36m(func pid=44519)[0m rmse_per_class: [0.094, 0.245, 0.049, 0.314, 0.056, 0.175, 0.352, 0.116, 0.135, 0.087]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3318 | Steps: 4 | Val loss: 0.5234 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 20:10:08 (running for 00:20:26.17)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.397 |  0.171 |                   63 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.342 |  0.146 |                   61 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.419 |  0.162 |                   14 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.359 |  0.225 |                   13 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17056763172149658
[2m[36m(func pid=32639)[0m mae:  0.12416347116231918
[2m[36m(func pid=32639)[0m rmse_per_class: [0.12, 0.256, 0.08, 0.325, 0.078, 0.188, 0.27, 0.134, 0.144, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3302 | Steps: 4 | Val loss: 0.2717 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4002 | Steps: 4 | Val loss: 0.4294 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=45122)[0m rmse: 0.20175015926361084
[2m[36m(func pid=45122)[0m mae:  0.12641115486621857
[2m[36m(func pid=45122)[0m rmse_per_class: [0.16, 0.253, 0.067, 0.371, 0.056, 0.214, 0.233, 0.326, 0.191, 0.146]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1462078094482422
[2m[36m(func pid=33359)[0m mae:  0.10084991157054901
[2m[36m(func pid=33359)[0m rmse_per_class: [0.099, 0.23, 0.03, 0.295, 0.053, 0.164, 0.235, 0.107, 0.138, 0.111]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3966 | Steps: 4 | Val loss: 0.3105 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=44519)[0m rmse: 0.1646748185157776
[2m[36m(func pid=44519)[0m mae:  0.1019909530878067
[2m[36m(func pid=44519)[0m rmse_per_class: [0.097, 0.244, 0.049, 0.312, 0.056, 0.174, 0.356, 0.118, 0.154, 0.087]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3432 | Steps: 4 | Val loss: 0.4235 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3489 | Steps: 4 | Val loss: 0.2735 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 20:10:13 (running for 00:20:31.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.397 |  0.171 |                   64 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.33  |  0.146 |                   62 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.4   |  0.165 |                   15 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.332 |  0.202 |                   14 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.17060545086860657
[2m[36m(func pid=32639)[0m mae:  0.12414532899856567
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.257, 0.081, 0.325, 0.078, 0.187, 0.27, 0.133, 0.144, 0.112]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4083 | Steps: 4 | Val loss: 0.3960 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=45122)[0m rmse: 0.2054271697998047
[2m[36m(func pid=45122)[0m mae:  0.1246650367975235
[2m[36m(func pid=45122)[0m rmse_per_class: [0.249, 0.226, 0.076, 0.373, 0.065, 0.195, 0.231, 0.331, 0.158, 0.151]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1476861536502838
[2m[36m(func pid=33359)[0m mae:  0.10086991637945175
[2m[36m(func pid=33359)[0m rmse_per_class: [0.097, 0.23, 0.031, 0.296, 0.053, 0.165, 0.234, 0.116, 0.139, 0.115]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3957 | Steps: 4 | Val loss: 0.3101 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=44519)[0m rmse: 0.16372491419315338
[2m[36m(func pid=44519)[0m mae:  0.10121722519397736
[2m[36m(func pid=44519)[0m rmse_per_class: [0.092, 0.243, 0.049, 0.308, 0.056, 0.17, 0.335, 0.118, 0.18, 0.086]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4090 | Steps: 4 | Val loss: 0.3718 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3448 | Steps: 4 | Val loss: 0.2743 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 20:10:19 (running for 00:20:37.04)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.396 |  0.17  |                   65 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.349 |  0.148 |                   63 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.408 |  0.164 |                   16 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.343 |  0.205 |                   15 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.1698468029499054
[2m[36m(func pid=32639)[0m mae:  0.12346981465816498
[2m[36m(func pid=32639)[0m rmse_per_class: [0.119, 0.256, 0.079, 0.325, 0.077, 0.187, 0.269, 0.134, 0.144, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4087 | Steps: 4 | Val loss: 0.3345 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=45122)[0m rmse: 0.19258931279182434
[2m[36m(func pid=45122)[0m mae:  0.11868000030517578
[2m[36m(func pid=45122)[0m rmse_per_class: [0.178, 0.25, 0.039, 0.356, 0.082, 0.199, 0.233, 0.326, 0.155, 0.108]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1484462171792984
[2m[36m(func pid=33359)[0m mae:  0.10152485221624374
[2m[36m(func pid=33359)[0m rmse_per_class: [0.098, 0.23, 0.031, 0.297, 0.053, 0.165, 0.237, 0.113, 0.142, 0.119]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4002 | Steps: 4 | Val loss: 0.3106 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=44519)[0m rmse: 0.1588897705078125
[2m[36m(func pid=44519)[0m mae:  0.09961836040019989
[2m[36m(func pid=44519)[0m rmse_per_class: [0.086, 0.24, 0.049, 0.304, 0.055, 0.167, 0.332, 0.118, 0.152, 0.085]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3667 | Steps: 4 | Val loss: 0.3733 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3313 | Steps: 4 | Val loss: 0.2747 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=32639)[0m rmse: 0.16978153586387634
[2m[36m(func pid=32639)[0m mae:  0.12340054661035538
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.256, 0.08, 0.325, 0.076, 0.187, 0.267, 0.133, 0.144, 0.111]
== Status ==
Current time: 2024-01-07 20:10:24 (running for 00:20:42.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.4   |  0.17  |                   66 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.345 |  0.148 |                   64 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.409 |  0.159 |                   17 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.409 |  0.193 |                   16 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3926 | Steps: 4 | Val loss: 0.2985 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=45122)[0m rmse: 0.20018234848976135
[2m[36m(func pid=45122)[0m mae:  0.12687468528747559
[2m[36m(func pid=45122)[0m rmse_per_class: [0.151, 0.245, 0.054, 0.346, 0.077, 0.19, 0.236, 0.129, 0.307, 0.267]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14848078787326813
[2m[36m(func pid=33359)[0m mae:  0.10195893049240112
[2m[36m(func pid=33359)[0m rmse_per_class: [0.099, 0.23, 0.033, 0.295, 0.053, 0.164, 0.241, 0.11, 0.142, 0.118]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3986 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=44519)[0m rmse: 0.1554425060749054
[2m[36m(func pid=44519)[0m mae:  0.09876518696546555
[2m[36m(func pid=44519)[0m rmse_per_class: [0.083, 0.239, 0.049, 0.302, 0.055, 0.168, 0.319, 0.118, 0.134, 0.086]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3595 | Steps: 4 | Val loss: 0.4335 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3550 | Steps: 4 | Val loss: 0.2740 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:10:30 (running for 00:20:47.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.399 |  0.169 |                   67 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.331 |  0.148 |                   65 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.393 |  0.155 |                   18 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.367 |  0.2   |                   17 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.16902370750904083
[2m[36m(func pid=32639)[0m mae:  0.1228138655424118
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.255, 0.078, 0.325, 0.076, 0.187, 0.266, 0.132, 0.143, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3984 | Steps: 4 | Val loss: 0.2811 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=45122)[0m rmse: 0.22047798335552216
[2m[36m(func pid=45122)[0m mae:  0.14108866453170776
[2m[36m(func pid=45122)[0m rmse_per_class: [0.214, 0.266, 0.071, 0.366, 0.059, 0.215, 0.238, 0.136, 0.352, 0.288]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14797070622444153
[2m[36m(func pid=33359)[0m mae:  0.1010085716843605
[2m[36m(func pid=33359)[0m rmse_per_class: [0.098, 0.229, 0.03, 0.293, 0.053, 0.164, 0.238, 0.117, 0.141, 0.116]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3963 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=44519)[0m rmse: 0.1525936722755432
[2m[36m(func pid=44519)[0m mae:  0.09834418445825577
[2m[36m(func pid=44519)[0m rmse_per_class: [0.096, 0.237, 0.048, 0.304, 0.055, 0.171, 0.275, 0.117, 0.132, 0.09]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3554 | Steps: 4 | Val loss: 0.5889 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3261 | Steps: 4 | Val loss: 0.2711 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 20:10:35 (running for 00:20:53.37)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.396 |  0.169 |                   68 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.355 |  0.148 |                   66 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.398 |  0.153 |                   19 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.359 |  0.22  |                   18 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.1689114272594452
[2m[36m(func pid=32639)[0m mae:  0.12280645221471786
[2m[36m(func pid=32639)[0m rmse_per_class: [0.118, 0.254, 0.078, 0.326, 0.076, 0.187, 0.266, 0.133, 0.143, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3733 | Steps: 4 | Val loss: 0.2876 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=45122)[0m rmse: 0.2335188388824463
[2m[36m(func pid=45122)[0m mae:  0.15308299660682678
[2m[36m(func pid=45122)[0m rmse_per_class: [0.216, 0.263, 0.057, 0.364, 0.064, 0.228, 0.29, 0.148, 0.368, 0.337]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1464143693447113
[2m[36m(func pid=33359)[0m mae:  0.09984266757965088
[2m[36m(func pid=33359)[0m rmse_per_class: [0.097, 0.228, 0.031, 0.289, 0.053, 0.162, 0.239, 0.115, 0.14, 0.11]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4008 | Steps: 4 | Val loss: 0.3079 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=44519)[0m rmse: 0.15517304837703705
[2m[36m(func pid=44519)[0m mae:  0.1001797690987587
[2m[36m(func pid=44519)[0m rmse_per_class: [0.098, 0.236, 0.05, 0.316, 0.055, 0.172, 0.275, 0.118, 0.137, 0.093]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3483 | Steps: 4 | Val loss: 0.4928 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3255 | Steps: 4 | Val loss: 0.2704 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 20:10:41 (running for 00:20:58.79)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.168 |                   69 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.326 |  0.146 |                   67 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.373 |  0.155 |                   20 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.355 |  0.234 |                   19 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.1682119220495224
[2m[36m(func pid=32639)[0m mae:  0.12217892706394196
[2m[36m(func pid=32639)[0m rmse_per_class: [0.117, 0.253, 0.077, 0.325, 0.075, 0.187, 0.264, 0.131, 0.142, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.21916577219963074
[2m[36m(func pid=45122)[0m mae:  0.1425524652004242
[2m[36m(func pid=45122)[0m rmse_per_class: [0.238, 0.265, 0.049, 0.361, 0.082, 0.225, 0.266, 0.129, 0.285, 0.291]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3697 | Steps: 4 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=33359)[0m rmse: 0.14576902985572815
[2m[36m(func pid=33359)[0m mae:  0.09992887079715729
[2m[36m(func pid=33359)[0m rmse_per_class: [0.096, 0.228, 0.03, 0.284, 0.052, 0.162, 0.245, 0.109, 0.14, 0.111]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4013 | Steps: 4 | Val loss: 0.3076 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3434 | Steps: 4 | Val loss: 0.4115 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=44519)[0m rmse: 0.15356890857219696
[2m[36m(func pid=44519)[0m mae:  0.0994458794593811
[2m[36m(func pid=44519)[0m rmse_per_class: [0.097, 0.231, 0.047, 0.317, 0.055, 0.173, 0.261, 0.12, 0.137, 0.099]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3367 | Steps: 4 | Val loss: 0.2711 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 20:10:46 (running for 00:21:04.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.168 |                   70 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.326 |  0.146 |                   68 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.37  |  0.154 |                   21 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.348 |  0.219 |                   20 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.16764573752880096
[2m[36m(func pid=32639)[0m mae:  0.12155391275882721
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.253, 0.075, 0.325, 0.074, 0.187, 0.262, 0.131, 0.143, 0.111]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.20334994792938232
[2m[36m(func pid=45122)[0m mae:  0.1290455162525177
[2m[36m(func pid=45122)[0m rmse_per_class: [0.213, 0.256, 0.05, 0.344, 0.099, 0.219, 0.265, 0.121, 0.179, 0.287]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14659109711647034
[2m[36m(func pid=33359)[0m mae:  0.10012701898813248
[2m[36m(func pid=33359)[0m rmse_per_class: [0.098, 0.228, 0.033, 0.288, 0.052, 0.162, 0.247, 0.111, 0.137, 0.109]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3826 | Steps: 4 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3943 | Steps: 4 | Val loss: 0.3082 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3633 | Steps: 4 | Val loss: 0.3753 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=44519)[0m rmse: 0.15306687355041504
[2m[36m(func pid=44519)[0m mae:  0.09900947660207748
[2m[36m(func pid=44519)[0m rmse_per_class: [0.097, 0.227, 0.05, 0.319, 0.055, 0.173, 0.247, 0.12, 0.139, 0.104]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3203 | Steps: 4 | Val loss: 0.2710 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 20:10:52 (running for 00:21:09.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.168 |                   70 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.337 |  0.147 |                   69 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.383 |  0.153 |                   22 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.363 |  0.193 |                   22 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.16752083599567413
[2m[36m(func pid=32639)[0m mae:  0.12144924700260162
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.253, 0.076, 0.325, 0.074, 0.187, 0.263, 0.13, 0.143, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.19324633479118347
[2m[36m(func pid=45122)[0m mae:  0.12282488495111465
[2m[36m(func pid=45122)[0m rmse_per_class: [0.166, 0.266, 0.052, 0.345, 0.081, 0.2, 0.279, 0.183, 0.155, 0.204]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.147093266248703
[2m[36m(func pid=33359)[0m mae:  0.10078270733356476
[2m[36m(func pid=33359)[0m rmse_per_class: [0.101, 0.228, 0.035, 0.284, 0.052, 0.162, 0.25, 0.108, 0.141, 0.111]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3518 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3960 | Steps: 4 | Val loss: 0.3080 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3791 | Steps: 4 | Val loss: 0.3591 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3272 | Steps: 4 | Val loss: 0.2691 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=44519)[0m rmse: 0.15292046964168549
[2m[36m(func pid=44519)[0m mae:  0.09864631295204163
[2m[36m(func pid=44519)[0m rmse_per_class: [0.092, 0.227, 0.051, 0.319, 0.055, 0.17, 0.234, 0.123, 0.14, 0.118]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.18577034771442413
[2m[36m(func pid=45122)[0m mae:  0.11985117197036743
[2m[36m(func pid=45122)[0m rmse_per_class: [0.139, 0.267, 0.053, 0.343, 0.073, 0.171, 0.295, 0.131, 0.149, 0.236]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:10:57 (running for 00:21:15.11)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.394 |  0.168 |                   71 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.32  |  0.147 |                   70 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.352 |  0.153 |                   23 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.379 |  0.186 |                   23 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32639)[0m rmse: 0.1671774983406067
[2m[36m(func pid=32639)[0m mae:  0.1211869940161705
[2m[36m(func pid=32639)[0m rmse_per_class: [0.116, 0.253, 0.074, 0.324, 0.074, 0.187, 0.262, 0.131, 0.142, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14563091099262238
[2m[36m(func pid=33359)[0m mae:  0.09932069480419159
[2m[36m(func pid=33359)[0m rmse_per_class: [0.093, 0.227, 0.033, 0.28, 0.053, 0.161, 0.249, 0.11, 0.14, 0.11]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3690 | Steps: 4 | Val loss: 0.2831 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3563 | Steps: 4 | Val loss: 0.3777 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4022 | Steps: 4 | Val loss: 0.3077 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=44519)[0m rmse: 0.15578985214233398
[2m[36m(func pid=44519)[0m mae:  0.09963244199752808
[2m[36m(func pid=44519)[0m rmse_per_class: [0.087, 0.227, 0.047, 0.322, 0.055, 0.174, 0.228, 0.136, 0.142, 0.14]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3338 | Steps: 4 | Val loss: 0.2677 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 20:11:02 (running for 00:21:20.25)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.396 |  0.167 |                   72 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.327 |  0.146 |                   71 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.369 |  0.156 |                   24 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.356 |  0.198 |                   24 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45122)[0m rmse: 0.1983964443206787
[2m[36m(func pid=45122)[0m mae:  0.12930727005004883
[2m[36m(func pid=45122)[0m rmse_per_class: [0.151, 0.254, 0.106, 0.361, 0.066, 0.175, 0.273, 0.101, 0.184, 0.315]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1667715609073639
[2m[36m(func pid=32639)[0m mae:  0.12087490409612656
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.252, 0.073, 0.325, 0.074, 0.186, 0.26, 0.131, 0.142, 0.109]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14487135410308838
[2m[36m(func pid=33359)[0m mae:  0.09787684679031372
[2m[36m(func pid=33359)[0m rmse_per_class: [0.088, 0.227, 0.03, 0.276, 0.053, 0.161, 0.249, 0.118, 0.137, 0.109]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3456 | Steps: 4 | Val loss: 0.2890 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3397 | Steps: 4 | Val loss: 0.3082 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3972 | Steps: 4 | Val loss: 0.3078 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3242 | Steps: 4 | Val loss: 0.2675 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=44519)[0m rmse: 0.15847544372081757
[2m[36m(func pid=44519)[0m mae:  0.1006576418876648
[2m[36m(func pid=44519)[0m rmse_per_class: [0.084, 0.227, 0.045, 0.322, 0.055, 0.176, 0.23, 0.152, 0.144, 0.15]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:11:07 (running for 00:21:25.50)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.402 |  0.167 |                   73 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.334 |  0.145 |                   72 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.346 |  0.158 |                   25 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.34  |  0.167 |                   25 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45122)[0m rmse: 0.16653388738632202
[2m[36m(func pid=45122)[0m mae:  0.1067819818854332
[2m[36m(func pid=45122)[0m rmse_per_class: [0.115, 0.241, 0.077, 0.295, 0.068, 0.168, 0.256, 0.096, 0.185, 0.164]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.1667385995388031
[2m[36m(func pid=32639)[0m mae:  0.12081489711999893
[2m[36m(func pid=32639)[0m rmse_per_class: [0.115, 0.252, 0.073, 0.324, 0.074, 0.186, 0.26, 0.13, 0.142, 0.11]
[2m[36m(func pid=32639)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.14504654705524445
[2m[36m(func pid=33359)[0m mae:  0.09853534400463104
[2m[36m(func pid=33359)[0m rmse_per_class: [0.09, 0.227, 0.033, 0.274, 0.053, 0.16, 0.251, 0.113, 0.14, 0.109]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3371 | Steps: 4 | Val loss: 0.2938 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3310 | Steps: 4 | Val loss: 0.2814 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=32639)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4009 | Steps: 4 | Val loss: 0.3065 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3076 | Steps: 4 | Val loss: 0.2677 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=44519)[0m rmse: 0.16020941734313965
[2m[36m(func pid=44519)[0m mae:  0.10049284994602203
[2m[36m(func pid=44519)[0m rmse_per_class: [0.082, 0.226, 0.041, 0.32, 0.055, 0.173, 0.227, 0.174, 0.145, 0.159]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:11:13 (running for 00:21:30.77)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00008 | RUNNING    | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.397 |  0.167 |                   74 |
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.145 |                   73 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.337 |  0.16  |                   26 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.331 |  0.156 |                   26 |
| train_84a75_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45122)[0m rmse: 0.1564970314502716
[2m[36m(func pid=45122)[0m mae:  0.0980624109506607
[2m[36m(func pid=45122)[0m rmse_per_class: [0.099, 0.227, 0.05, 0.286, 0.072, 0.16, 0.242, 0.16, 0.157, 0.112]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=32639)[0m rmse: 0.16619914770126343
[2m[36m(func pid=32639)[0m mae:  0.1204468160867691
[2m[36m(func pid=32639)[0m rmse_per_class: [0.114, 0.251, 0.072, 0.325, 0.074, 0.185, 0.26, 0.13, 0.142, 0.109]
[2m[36m(func pid=33359)[0m rmse: 0.14556944370269775
[2m[36m(func pid=33359)[0m mae:  0.09917303919792175
[2m[36m(func pid=33359)[0m rmse_per_class: [0.093, 0.227, 0.035, 0.276, 0.053, 0.16, 0.251, 0.112, 0.142, 0.107]
[2m[36m(func pid=33359)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3354 | Steps: 4 | Val loss: 0.3026 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3710 | Steps: 4 | Val loss: 0.3042 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=33359)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3240 | Steps: 4 | Val loss: 0.2673 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=44519)[0m rmse: 0.16569654643535614
[2m[36m(func pid=44519)[0m mae:  0.10320383310317993
[2m[36m(func pid=44519)[0m rmse_per_class: [0.089, 0.228, 0.042, 0.324, 0.055, 0.17, 0.228, 0.184, 0.164, 0.174]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.1649516522884369
[2m[36m(func pid=45122)[0m mae:  0.09953786432743073
[2m[36m(func pid=45122)[0m rmse_per_class: [0.09, 0.24, 0.051, 0.294, 0.078, 0.179, 0.239, 0.222, 0.143, 0.114]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=33359)[0m rmse: 0.1460239440202713
[2m[36m(func pid=33359)[0m mae:  0.09886522591114044
[2m[36m(func pid=33359)[0m rmse_per_class: [0.091, 0.226, 0.035, 0.274, 0.053, 0.16, 0.252, 0.112, 0.145, 0.112]
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3326 | Steps: 4 | Val loss: 0.3101 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3373 | Steps: 4 | Val loss: 0.3006 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:11:18 (running for 00:21:36.02)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.14499999582767487
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00009 | RUNNING    | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.308 |  0.146 |                   74 |
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.335 |  0.166 |                   27 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.371 |  0.165 |                   27 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=51559)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=51559)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=51559)[0m Configuration completed!
[2m[36m(func pid=51559)[0m New optimizer parameters:
[2m[36m(func pid=51559)[0m SGD (
[2m[36m(func pid=51559)[0m Parameter Group 0
[2m[36m(func pid=51559)[0m     dampening: 0
[2m[36m(func pid=51559)[0m     differentiable: False
[2m[36m(func pid=51559)[0m     foreach: None
[2m[36m(func pid=51559)[0m     lr: 0.0001
[2m[36m(func pid=51559)[0m     maximize: False
[2m[36m(func pid=51559)[0m     momentum: 0.9
[2m[36m(func pid=51559)[0m     nesterov: False
[2m[36m(func pid=51559)[0m     weight_decay: 0.0001
[2m[36m(func pid=51559)[0m )
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.17111313343048096
[2m[36m(func pid=44519)[0m mae:  0.10538343340158463
[2m[36m(func pid=44519)[0m rmse_per_class: [0.086, 0.228, 0.04, 0.325, 0.054, 0.167, 0.234, 0.191, 0.18, 0.206]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.16195084154605865
[2m[36m(func pid=45122)[0m mae:  0.0968485027551651
[2m[36m(func pid=45122)[0m rmse_per_class: [0.1, 0.216, 0.049, 0.297, 0.103, 0.196, 0.233, 0.146, 0.151, 0.128]
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3291 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8983 | Steps: 4 | Val loss: 0.7065 | Batch size: 32 | lr: 0.0001 | Duration: 4.73s
[2m[36m(func pid=44519)[0m rmse: 0.17097759246826172
[2m[36m(func pid=44519)[0m mae:  0.10558537393808365
[2m[36m(func pid=44519)[0m rmse_per_class: [0.087, 0.231, 0.043, 0.323, 0.054, 0.162, 0.246, 0.175, 0.186, 0.203]
[2m[36m(func pid=51559)[0m rmse: 0.18264956772327423
[2m[36m(func pid=51559)[0m mae:  0.13446000218391418
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.267, 0.106, 0.339, 0.113, 0.191, 0.294, 0.144, 0.143, 0.113]
== Status ==
Current time: 2024-01-07 20:11:24 (running for 00:21:41.81)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.333 |  0.171 |                   28 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.371 |  0.165 |                   27 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


== Status ==
Current time: 2024-01-07 20:11:32 (running for 00:21:49.81)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.329 |  0.171 |                   29 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.371 |  0.165 |                   27 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=52139)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=52139)[0m Configuration completed!
[2m[36m(func pid=52139)[0m New optimizer parameters:
[2m[36m(func pid=52139)[0m SGD (
[2m[36m(func pid=52139)[0m Parameter Group 0
[2m[36m(func pid=52139)[0m     dampening: 0
[2m[36m(func pid=52139)[0m     differentiable: False
[2m[36m(func pid=52139)[0m     foreach: None
[2m[36m(func pid=52139)[0m     lr: 0.001
[2m[36m(func pid=52139)[0m     maximize: False
[2m[36m(func pid=52139)[0m     momentum: 0.9
[2m[36m(func pid=52139)[0m     nesterov: False
[2m[36m(func pid=52139)[0m     weight_decay: 0.0001
[2m[36m(func pid=52139)[0m )
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8946 | Steps: 4 | Val loss: 0.6973 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3286 | Steps: 4 | Val loss: 0.3108 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3304 | Steps: 4 | Val loss: 0.3290 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 20:11:37 (running for 00:21:54.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.329 |  0.171 |                   29 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.337 |  0.162 |                   28 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.898 |  0.183 |                    1 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8920 | Steps: 4 | Val loss: 0.6961 | Batch size: 32 | lr: 0.001 | Duration: 4.99s
[2m[36m(func pid=45122)[0m rmse: 0.17533034086227417
[2m[36m(func pid=45122)[0m mae:  0.10866068303585052
[2m[36m(func pid=45122)[0m rmse_per_class: [0.101, 0.215, 0.05, 0.366, 0.129, 0.207, 0.242, 0.101, 0.135, 0.206]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1817696988582611
[2m[36m(func pid=51559)[0m mae:  0.13378870487213135
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.266, 0.104, 0.339, 0.113, 0.19, 0.294, 0.141, 0.143, 0.112]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.17287807166576385
[2m[36m(func pid=44519)[0m mae:  0.10549648851156235
[2m[36m(func pid=44519)[0m rmse_per_class: [0.084, 0.236, 0.05, 0.321, 0.053, 0.162, 0.246, 0.191, 0.196, 0.19]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.18269412219524384
[2m[36m(func pid=52139)[0m mae:  0.13446612656116486
[2m[36m(func pid=52139)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3484 | Steps: 4 | Val loss: 0.3758 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8916 | Steps: 4 | Val loss: 0.6903 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3186 | Steps: 4 | Val loss: 0.2981 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8776 | Steps: 4 | Val loss: 0.6699 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=45122)[0m rmse: 0.1907627135515213
[2m[36m(func pid=45122)[0m mae:  0.11853261291980743
[2m[36m(func pid=45122)[0m rmse_per_class: [0.099, 0.238, 0.051, 0.388, 0.126, 0.209, 0.236, 0.102, 0.145, 0.314]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:11:42 (running for 00:22:00.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.329 |  0.173 |                   30 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.348 |  0.191 |                   30 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.895 |  0.182 |                    2 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.892 |  0.183 |                    1 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=51559)[0m rmse: 0.18108363449573517
[2m[36m(func pid=51559)[0m mae:  0.13322608172893524
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.189, 0.294, 0.141, 0.142, 0.111]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.16535918414592743
[2m[36m(func pid=44519)[0m mae:  0.10179825872182846
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.236, 0.05, 0.315, 0.053, 0.159, 0.243, 0.166, 0.188, 0.166]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.18165364861488342
[2m[36m(func pid=52139)[0m mae:  0.13363678753376007
[2m[36m(func pid=52139)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.338, 0.113, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3418 | Steps: 4 | Val loss: 0.3802 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8894 | Steps: 4 | Val loss: 0.6867 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3515 | Steps: 4 | Val loss: 0.2896 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8475 | Steps: 4 | Val loss: 0.6438 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:11:47 (running for 00:22:05.65)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.319 |  0.165 |                   31 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.342 |  0.19  |                   31 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.892 |  0.181 |                    3 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.878 |  0.182 |                    2 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.18999871611595154
[2m[36m(func pid=45122)[0m mae:  0.11359751224517822
[2m[36m(func pid=45122)[0m rmse_per_class: [0.098, 0.23, 0.05, 0.354, 0.122, 0.193, 0.222, 0.179, 0.175, 0.278]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.18054120242595673
[2m[36m(func pid=51559)[0m mae:  0.13278207182884216
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.264, 0.101, 0.337, 0.111, 0.189, 0.294, 0.14, 0.142, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15903259813785553
[2m[36m(func pid=44519)[0m mae:  0.0990910679101944
[2m[36m(func pid=44519)[0m rmse_per_class: [0.076, 0.231, 0.047, 0.312, 0.052, 0.16, 0.238, 0.153, 0.178, 0.144]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.18066829442977905
[2m[36m(func pid=52139)[0m mae:  0.1327441930770874
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.112, 0.19, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3598 | Steps: 4 | Val loss: 0.3633 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8847 | Steps: 4 | Val loss: 0.6847 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3524 | Steps: 4 | Val loss: 0.2794 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=45122)[0m rmse: 0.1886058896780014
[2m[36m(func pid=45122)[0m mae:  0.11415872722864151
[2m[36m(func pid=45122)[0m rmse_per_class: [0.108, 0.217, 0.055, 0.348, 0.087, 0.191, 0.226, 0.215, 0.229, 0.209]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:11:53 (running for 00:22:11.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.352 |  0.159 |                   32 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.36  |  0.189 |                   32 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.889 |  0.181 |                    4 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.848 |  0.181 |                    3 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8151 | Steps: 4 | Val loss: 0.6128 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=51559)[0m rmse: 0.18054042756557465
[2m[36m(func pid=51559)[0m mae:  0.13270671665668488
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.264, 0.101, 0.338, 0.112, 0.189, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15181612968444824
[2m[36m(func pid=44519)[0m mae:  0.09472836554050446
[2m[36m(func pid=44519)[0m rmse_per_class: [0.07, 0.233, 0.046, 0.297, 0.053, 0.162, 0.233, 0.142, 0.158, 0.124]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.1799010932445526
[2m[36m(func pid=52139)[0m mae:  0.1320870816707611
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.336, 0.111, 0.19, 0.293, 0.14, 0.141, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3259 | Steps: 4 | Val loss: 0.3520 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8773 | Steps: 4 | Val loss: 0.6814 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3031 | Steps: 4 | Val loss: 0.2771 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:11:58 (running for 00:22:16.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.352 |  0.152 |                   33 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.326 |  0.188 |                   33 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.885 |  0.181 |                    5 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.815 |  0.18  |                    4 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.18795904517173767
[2m[36m(func pid=45122)[0m mae:  0.11557209491729736
[2m[36m(func pid=45122)[0m rmse_per_class: [0.101, 0.213, 0.057, 0.308, 0.08, 0.201, 0.274, 0.147, 0.219, 0.28]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7799 | Steps: 4 | Val loss: 0.5848 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=51559)[0m rmse: 0.18023738265037537
[2m[36m(func pid=51559)[0m mae:  0.13245722651481628
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.263, 0.1, 0.337, 0.112, 0.189, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14970549941062927
[2m[36m(func pid=44519)[0m mae:  0.09412267804145813
[2m[36m(func pid=44519)[0m rmse_per_class: [0.068, 0.228, 0.043, 0.3, 0.059, 0.167, 0.236, 0.119, 0.156, 0.121]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.17945560812950134
[2m[36m(func pid=52139)[0m mae:  0.13170713186264038
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.262, 0.1, 0.335, 0.11, 0.19, 0.293, 0.139, 0.142, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3310 | Steps: 4 | Val loss: 0.3026 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8766 | Steps: 4 | Val loss: 0.6797 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3253 | Steps: 4 | Val loss: 0.2800 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 20:12:03 (running for 00:22:21.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.303 |  0.15  |                   34 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.331 |  0.169 |                   34 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.877 |  0.18  |                    6 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.78  |  0.179 |                    5 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.16920778155326843
[2m[36m(func pid=45122)[0m mae:  0.10180213302373886
[2m[36m(func pid=45122)[0m rmse_per_class: [0.102, 0.228, 0.04, 0.297, 0.072, 0.182, 0.251, 0.195, 0.166, 0.161]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7452 | Steps: 4 | Val loss: 0.5586 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=51559)[0m rmse: 0.18037793040275574
[2m[36m(func pid=51559)[0m mae:  0.1325526386499405
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.263, 0.1, 0.337, 0.112, 0.189, 0.295, 0.14, 0.142, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15171505510807037
[2m[36m(func pid=44519)[0m mae:  0.09541834890842438
[2m[36m(func pid=44519)[0m rmse_per_class: [0.072, 0.223, 0.05, 0.31, 0.064, 0.175, 0.227, 0.116, 0.164, 0.116]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.17913946509361267
[2m[36m(func pid=52139)[0m mae:  0.1314423531293869
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.335, 0.109, 0.19, 0.293, 0.139, 0.142, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3477 | Steps: 4 | Val loss: 0.3042 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8699 | Steps: 4 | Val loss: 0.6760 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2893 | Steps: 4 | Val loss: 0.2773 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:12:09 (running for 00:22:26.93)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.325 |  0.152 |                   35 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.348 |  0.165 |                   35 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.877 |  0.18  |                    7 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.745 |  0.179 |                    6 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.16533826291561127
[2m[36m(func pid=45122)[0m mae:  0.09781453013420105
[2m[36m(func pid=45122)[0m rmse_per_class: [0.107, 0.255, 0.045, 0.291, 0.074, 0.169, 0.248, 0.204, 0.161, 0.099]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7107 | Steps: 4 | Val loss: 0.5340 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=44519)[0m rmse: 0.1503497064113617
[2m[36m(func pid=44519)[0m mae:  0.09424683451652527
[2m[36m(func pid=44519)[0m rmse_per_class: [0.076, 0.214, 0.047, 0.31, 0.076, 0.173, 0.221, 0.113, 0.159, 0.114]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1801588088274002
[2m[36m(func pid=51559)[0m mae:  0.13238278031349182
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.262, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3478 | Steps: 4 | Val loss: 0.3263 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=52139)[0m rmse: 0.17886202037334442
[2m[36m(func pid=52139)[0m mae:  0.1311759054660797
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.259, 0.099, 0.335, 0.107, 0.191, 0.292, 0.139, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8661 | Steps: 4 | Val loss: 0.6720 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3003 | Steps: 4 | Val loss: 0.2740 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=45122)[0m rmse: 0.16481547057628632
[2m[36m(func pid=45122)[0m mae:  0.09642132371664047
[2m[36m(func pid=45122)[0m rmse_per_class: [0.109, 0.297, 0.051, 0.284, 0.065, 0.182, 0.243, 0.147, 0.149, 0.121]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:12:14 (running for 00:22:32.13)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.289 |  0.15  |                   36 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.348 |  0.165 |                   36 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.87  |  0.18  |                    8 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.711 |  0.179 |                    7 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6758 | Steps: 4 | Val loss: 0.5112 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=44519)[0m rmse: 0.14953747391700745
[2m[36m(func pid=44519)[0m mae:  0.09222906082868576
[2m[36m(func pid=44519)[0m rmse_per_class: [0.083, 0.209, 0.049, 0.305, 0.092, 0.171, 0.218, 0.116, 0.146, 0.107]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.18012194335460663
[2m[36m(func pid=51559)[0m mae:  0.13233357667922974
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.112, 0.19, 0.295, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3437 | Steps: 4 | Val loss: 0.3691 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=52139)[0m rmse: 0.17867781221866608
[2m[36m(func pid=52139)[0m mae:  0.13098359107971191
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.259, 0.098, 0.334, 0.107, 0.191, 0.292, 0.139, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3044 | Steps: 4 | Val loss: 0.2705 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8594 | Steps: 4 | Val loss: 0.6686 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:12:19 (running for 00:22:37.46)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.3   |  0.15  |                   37 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.344 |  0.182 |                   37 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.866 |  0.18  |                    9 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.676 |  0.179 |                    8 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.1818290501832962
[2m[36m(func pid=45122)[0m mae:  0.10807349532842636
[2m[36m(func pid=45122)[0m rmse_per_class: [0.107, 0.267, 0.053, 0.278, 0.059, 0.201, 0.268, 0.103, 0.251, 0.231]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6496 | Steps: 4 | Val loss: 0.4901 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=44519)[0m rmse: 0.14823266863822937
[2m[36m(func pid=44519)[0m mae:  0.08968620002269745
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.205, 0.053, 0.29, 0.113, 0.172, 0.214, 0.119, 0.137, 0.102]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.18003354966640472
[2m[36m(func pid=51559)[0m mae:  0.13224999606609344
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.295, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3474 | Steps: 4 | Val loss: 0.3430 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=52139)[0m rmse: 0.1785072386264801
[2m[36m(func pid=52139)[0m mae:  0.13080742955207825
[2m[36m(func pid=52139)[0m rmse_per_class: [0.117, 0.258, 0.098, 0.334, 0.106, 0.191, 0.291, 0.139, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3103 | Steps: 4 | Val loss: 0.2716 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8600 | Steps: 4 | Val loss: 0.6624 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:12:25 (running for 00:22:42.88)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.304 |  0.148 |                   38 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.347 |  0.181 |                   38 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.859 |  0.18  |                   10 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.65  |  0.179 |                    9 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.1813931167125702
[2m[36m(func pid=45122)[0m mae:  0.10918154567480087
[2m[36m(func pid=45122)[0m rmse_per_class: [0.092, 0.236, 0.053, 0.282, 0.074, 0.204, 0.269, 0.109, 0.252, 0.242]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1498195081949234
[2m[36m(func pid=44519)[0m mae:  0.089399553835392
[2m[36m(func pid=44519)[0m rmse_per_class: [0.079, 0.202, 0.055, 0.287, 0.136, 0.169, 0.216, 0.108, 0.132, 0.114]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6254 | Steps: 4 | Val loss: 0.4730 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=51559)[0m rmse: 0.17995166778564453
[2m[36m(func pid=51559)[0m mae:  0.13222193717956543
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.295, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3475 | Steps: 4 | Val loss: 0.2961 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=52139)[0m rmse: 0.17866596579551697
[2m[36m(func pid=52139)[0m mae:  0.13098649680614471
[2m[36m(func pid=52139)[0m rmse_per_class: [0.117, 0.259, 0.098, 0.335, 0.104, 0.19, 0.291, 0.139, 0.144, 0.11]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3062 | Steps: 4 | Val loss: 0.2726 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8460 | Steps: 4 | Val loss: 0.6605 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=45122)[0m rmse: 0.15433908998966217
[2m[36m(func pid=45122)[0m mae:  0.09240452200174332
[2m[36m(func pid=45122)[0m rmse_per_class: [0.089, 0.209, 0.055, 0.29, 0.119, 0.186, 0.223, 0.127, 0.163, 0.084]
== Status ==
Current time: 2024-01-07 20:12:30 (running for 00:22:48.34)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.31  |  0.15  |                   39 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.348 |  0.154 |                   39 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.86  |  0.18  |                   11 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.625 |  0.179 |                   10 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14965729415416718
[2m[36m(func pid=44519)[0m mae:  0.08719900250434875
[2m[36m(func pid=44519)[0m rmse_per_class: [0.084, 0.207, 0.051, 0.283, 0.132, 0.162, 0.211, 0.115, 0.131, 0.121]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17985376715660095
[2m[36m(func pid=51559)[0m mae:  0.1320895105600357
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6035 | Steps: 4 | Val loss: 0.4583 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3492 | Steps: 4 | Val loss: 0.2982 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=52139)[0m rmse: 0.17871615290641785
[2m[36m(func pid=52139)[0m mae:  0.13106851279735565
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.098, 0.336, 0.101, 0.19, 0.29, 0.139, 0.145, 0.111]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3126 | Steps: 4 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8459 | Steps: 4 | Val loss: 0.6563 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=45122)[0m rmse: 0.15692320466041565
[2m[36m(func pid=45122)[0m mae:  0.09396754205226898
[2m[36m(func pid=45122)[0m rmse_per_class: [0.092, 0.203, 0.069, 0.291, 0.135, 0.174, 0.227, 0.148, 0.141, 0.088]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:12:35 (running for 00:22:53.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.306 |  0.15  |                   40 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.349 |  0.157 |                   40 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.846 |  0.18  |                   12 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.604 |  0.179 |                   11 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=44519)[0m rmse: 0.15234874188899994
[2m[36m(func pid=44519)[0m mae:  0.08823418617248535
[2m[36m(func pid=44519)[0m rmse_per_class: [0.086, 0.209, 0.05, 0.275, 0.129, 0.172, 0.215, 0.125, 0.132, 0.131]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17966891825199127
[2m[36m(func pid=51559)[0m mae:  0.13193561136722565
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5816 | Steps: 4 | Val loss: 0.4437 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3401 | Steps: 4 | Val loss: 0.3138 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=52139)[0m rmse: 0.1787422001361847
[2m[36m(func pid=52139)[0m mae:  0.13108469545841217
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.098, 0.336, 0.1, 0.19, 0.289, 0.139, 0.145, 0.113]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3323 | Steps: 4 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8411 | Steps: 4 | Val loss: 0.6529 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:12:41 (running for 00:22:58.75)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.313 |  0.152 |                   41 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.34  |  0.163 |                   41 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.846 |  0.18  |                   13 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.582 |  0.179 |                   12 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.16347433626651764
[2m[36m(func pid=45122)[0m mae:  0.09920988976955414
[2m[36m(func pid=45122)[0m rmse_per_class: [0.095, 0.215, 0.063, 0.295, 0.131, 0.187, 0.237, 0.177, 0.144, 0.091]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15937143564224243
[2m[36m(func pid=44519)[0m mae:  0.09240002185106277
[2m[36m(func pid=44519)[0m rmse_per_class: [0.083, 0.212, 0.046, 0.275, 0.118, 0.172, 0.226, 0.112, 0.136, 0.214]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5634 | Steps: 4 | Val loss: 0.4316 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=51559)[0m rmse: 0.17971794307231903
[2m[36m(func pid=51559)[0m mae:  0.13199208676815033
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.111, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3384 | Steps: 4 | Val loss: 0.2955 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=52139)[0m rmse: 0.17846161127090454
[2m[36m(func pid=52139)[0m mae:  0.13082824647426605
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.096, 0.335, 0.099, 0.189, 0.289, 0.139, 0.145, 0.114]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2939 | Steps: 4 | Val loss: 0.2731 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8357 | Steps: 4 | Val loss: 0.6488 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 20:12:46 (running for 00:23:04.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.332 |  0.159 |                   42 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.338 |  0.163 |                   42 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.841 |  0.18  |                   14 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.563 |  0.178 |                   13 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.1627628058195114
[2m[36m(func pid=45122)[0m mae:  0.09981916844844818
[2m[36m(func pid=45122)[0m rmse_per_class: [0.096, 0.224, 0.06, 0.298, 0.117, 0.181, 0.234, 0.16, 0.148, 0.111]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1797601580619812
[2m[36m(func pid=51559)[0m mae:  0.132014662027359
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1512802541255951
[2m[36m(func pid=44519)[0m mae:  0.08848531544208527
[2m[36m(func pid=44519)[0m rmse_per_class: [0.082, 0.216, 0.04, 0.273, 0.13, 0.166, 0.217, 0.103, 0.132, 0.154]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5427 | Steps: 4 | Val loss: 0.4198 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3345 | Steps: 4 | Val loss: 0.2919 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=52139)[0m rmse: 0.17842277884483337
[2m[36m(func pid=52139)[0m mae:  0.13079147040843964
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.095, 0.334, 0.099, 0.189, 0.289, 0.139, 0.145, 0.114]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8331 | Steps: 4 | Val loss: 0.6453 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2893 | Steps: 4 | Val loss: 0.2718 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:12:51 (running for 00:23:09.38)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.294 |  0.151 |                   43 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.334 |  0.166 |                   43 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.836 |  0.18  |                   15 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.543 |  0.178 |                   14 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.1661394089460373
[2m[36m(func pid=45122)[0m mae:  0.10251963138580322
[2m[36m(func pid=45122)[0m rmse_per_class: [0.114, 0.231, 0.056, 0.306, 0.103, 0.179, 0.229, 0.145, 0.161, 0.137]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17976561188697815
[2m[36m(func pid=51559)[0m mae:  0.13201387226581573
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1501329243183136
[2m[36m(func pid=44519)[0m mae:  0.08928342908620834
[2m[36m(func pid=44519)[0m rmse_per_class: [0.092, 0.209, 0.043, 0.28, 0.153, 0.172, 0.214, 0.098, 0.127, 0.114]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5288 | Steps: 4 | Val loss: 0.4086 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3419 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8245 | Steps: 4 | Val loss: 0.6404 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=52139)[0m rmse: 0.17806383967399597
[2m[36m(func pid=52139)[0m mae:  0.13051876425743103
[2m[36m(func pid=52139)[0m rmse_per_class: [0.119, 0.26, 0.094, 0.334, 0.098, 0.189, 0.288, 0.139, 0.145, 0.114]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2937 | Steps: 4 | Val loss: 0.2699 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=45122)[0m rmse: 0.17488276958465576
[2m[36m(func pid=45122)[0m mae:  0.10921841859817505
[2m[36m(func pid=45122)[0m rmse_per_class: [0.139, 0.235, 0.053, 0.29, 0.089, 0.197, 0.249, 0.113, 0.228, 0.156]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:12:56 (running for 00:23:14.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.289 |  0.15  |                   44 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.342 |  0.175 |                   44 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.833 |  0.18  |                   16 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.529 |  0.178 |                   15 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=51559)[0m rmse: 0.17973412573337555
[2m[36m(func pid=51559)[0m mae:  0.13198153674602509
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.111, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14789317548274994
[2m[36m(func pid=44519)[0m mae:  0.08763287216424942
[2m[36m(func pid=44519)[0m rmse_per_class: [0.085, 0.209, 0.039, 0.283, 0.15, 0.169, 0.207, 0.099, 0.131, 0.108]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5169 | Steps: 4 | Val loss: 0.3981 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3320 | Steps: 4 | Val loss: 0.3155 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8193 | Steps: 4 | Val loss: 0.6383 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=52139)[0m rmse: 0.17759454250335693
[2m[36m(func pid=52139)[0m mae:  0.13010810315608978
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.334, 0.098, 0.189, 0.287, 0.138, 0.145, 0.114]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2860 | Steps: 4 | Val loss: 0.2738 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 20:13:02 (running for 00:23:19.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.294 |  0.148 |                   45 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.332 |  0.177 |                   45 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.824 |  0.18  |                   17 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.517 |  0.178 |                   16 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.1769305020570755
[2m[36m(func pid=45122)[0m mae:  0.11189015209674835
[2m[36m(func pid=45122)[0m rmse_per_class: [0.138, 0.234, 0.047, 0.28, 0.079, 0.203, 0.267, 0.13, 0.213, 0.178]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1796911656856537
[2m[36m(func pid=51559)[0m mae:  0.13200566172599792
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1498837172985077
[2m[36m(func pid=44519)[0m mae:  0.08889791369438171
[2m[36m(func pid=44519)[0m rmse_per_class: [0.085, 0.214, 0.036, 0.285, 0.131, 0.174, 0.203, 0.105, 0.148, 0.118]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5072 | Steps: 4 | Val loss: 0.3888 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3343 | Steps: 4 | Val loss: 0.2894 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8206 | Steps: 4 | Val loss: 0.6359 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=52139)[0m rmse: 0.1775285303592682
[2m[36m(func pid=52139)[0m mae:  0.12997886538505554
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.333, 0.098, 0.19, 0.286, 0.137, 0.145, 0.113]
[2m[36m(func pid=52139)[0m 
== Status ==
Current time: 2024-01-07 20:13:07 (running for 00:23:25.04)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.286 |  0.15  |                   46 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.334 |  0.167 |                   46 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.819 |  0.18  |                   18 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.507 |  0.178 |                   17 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.167120099067688
[2m[36m(func pid=45122)[0m mae:  0.10536782443523407
[2m[36m(func pid=45122)[0m rmse_per_class: [0.116, 0.231, 0.046, 0.276, 0.074, 0.18, 0.26, 0.171, 0.154, 0.163]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3097 | Steps: 4 | Val loss: 0.2783 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=51559)[0m rmse: 0.1798948496580124
[2m[36m(func pid=51559)[0m mae:  0.13222536444664001
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.337, 0.11, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15343759953975677
[2m[36m(func pid=44519)[0m mae:  0.08988702297210693
[2m[36m(func pid=44519)[0m rmse_per_class: [0.085, 0.219, 0.047, 0.294, 0.107, 0.157, 0.212, 0.131, 0.15, 0.133]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4950 | Steps: 4 | Val loss: 0.3817 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3365 | Steps: 4 | Val loss: 0.2878 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 20:13:12 (running for 00:23:30.21)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.31  |  0.153 |                   47 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.337 |  0.166 |                   47 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.821 |  0.18  |                   19 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.507 |  0.178 |                   17 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.16645947098731995
[2m[36m(func pid=45122)[0m mae:  0.10602234303951263
[2m[36m(func pid=45122)[0m rmse_per_class: [0.119, 0.228, 0.048, 0.277, 0.066, 0.173, 0.269, 0.174, 0.151, 0.161]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8113 | Steps: 4 | Val loss: 0.6305 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=52139)[0m rmse: 0.17754846811294556
[2m[36m(func pid=52139)[0m mae:  0.13004617393016815
[2m[36m(func pid=52139)[0m rmse_per_class: [0.119, 0.26, 0.093, 0.334, 0.098, 0.189, 0.286, 0.138, 0.145, 0.114]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3380 | Steps: 4 | Val loss: 0.2958 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=51559)[0m rmse: 0.1797204315662384
[2m[36m(func pid=51559)[0m mae:  0.132040336728096
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3217 | Steps: 4 | Val loss: 0.2948 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=44519)[0m rmse: 0.16422368586063385
[2m[36m(func pid=44519)[0m mae:  0.09786435216665268
[2m[36m(func pid=44519)[0m rmse_per_class: [0.084, 0.221, 0.056, 0.292, 0.077, 0.177, 0.224, 0.15, 0.178, 0.182]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4832 | Steps: 4 | Val loss: 0.3741 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=45122)[0m rmse: 0.17153935134410858
[2m[36m(func pid=45122)[0m mae:  0.10842828452587128
[2m[36m(func pid=45122)[0m rmse_per_class: [0.136, 0.227, 0.049, 0.29, 0.061, 0.169, 0.272, 0.198, 0.154, 0.16]
== Status ==
Current time: 2024-01-07 20:13:17 (running for 00:23:35.44)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.338 |  0.164 |                   48 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.322 |  0.172 |                   48 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.811 |  0.18  |                   20 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.495 |  0.178 |                   18 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8095 | Steps: 4 | Val loss: 0.6256 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=52139)[0m rmse: 0.1773187220096588
[2m[36m(func pid=52139)[0m mae:  0.12981989979743958
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.259, 0.094, 0.334, 0.097, 0.19, 0.285, 0.138, 0.145, 0.113]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2998 | Steps: 4 | Val loss: 0.2974 | Batch size: 32 | lr: 0.01 | Duration: 3.35s
[2m[36m(func pid=51559)[0m rmse: 0.17957404255867004
[2m[36m(func pid=51559)[0m mae:  0.13188548386096954
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.337, 0.111, 0.19, 0.294, 0.139, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3127 | Steps: 4 | Val loss: 0.2934 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4747 | Steps: 4 | Val loss: 0.3677 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=44519)[0m rmse: 0.1646234542131424
[2m[36m(func pid=44519)[0m mae:  0.09847728908061981
[2m[36m(func pid=44519)[0m rmse_per_class: [0.076, 0.213, 0.054, 0.29, 0.08, 0.185, 0.226, 0.134, 0.213, 0.175]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:13:23 (running for 00:23:40.92)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.3   |  0.165 |                   49 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.313 |  0.17  |                   49 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.809 |  0.18  |                   21 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.483 |  0.177 |                   19 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.16974259912967682
[2m[36m(func pid=45122)[0m mae:  0.10681374371051788
[2m[36m(func pid=45122)[0m rmse_per_class: [0.141, 0.219, 0.045, 0.297, 0.065, 0.161, 0.267, 0.229, 0.148, 0.124]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8017 | Steps: 4 | Val loss: 0.6231 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=52139)[0m rmse: 0.17689523100852966
[2m[36m(func pid=52139)[0m mae:  0.12947048246860504
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.259, 0.093, 0.333, 0.096, 0.19, 0.285, 0.138, 0.144, 0.113]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2957 | Steps: 4 | Val loss: 0.2943 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=51559)[0m rmse: 0.17939382791519165
[2m[36m(func pid=51559)[0m mae:  0.13173161447048187
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.111, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3022 | Steps: 4 | Val loss: 0.2993 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4709 | Steps: 4 | Val loss: 0.3634 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=44519)[0m rmse: 0.16348575055599213
[2m[36m(func pid=44519)[0m mae:  0.09757605940103531
[2m[36m(func pid=44519)[0m rmse_per_class: [0.074, 0.211, 0.054, 0.286, 0.079, 0.185, 0.226, 0.14, 0.207, 0.172]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:13:28 (running for 00:23:46.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.296 |  0.163 |                   50 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.302 |  0.171 |                   50 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.802 |  0.179 |                   22 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.475 |  0.177 |                   20 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.17095138132572174
[2m[36m(func pid=45122)[0m mae:  0.10592396557331085
[2m[36m(func pid=45122)[0m rmse_per_class: [0.165, 0.207, 0.04, 0.31, 0.065, 0.164, 0.259, 0.235, 0.149, 0.116]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7996 | Steps: 4 | Val loss: 0.6196 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=52139)[0m rmse: 0.1770249307155609
[2m[36m(func pid=52139)[0m mae:  0.1295860856771469
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.332, 0.095, 0.189, 0.286, 0.138, 0.144, 0.113]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2950 | Steps: 4 | Val loss: 0.3081 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3030 | Steps: 4 | Val loss: 0.2965 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=51559)[0m rmse: 0.17935147881507874
[2m[36m(func pid=51559)[0m mae:  0.13167500495910645
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.111, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4600 | Steps: 4 | Val loss: 0.3570 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=44519)[0m rmse: 0.17117765545845032
[2m[36m(func pid=44519)[0m mae:  0.10332547128200531
[2m[36m(func pid=44519)[0m rmse_per_class: [0.073, 0.209, 0.054, 0.285, 0.078, 0.195, 0.237, 0.126, 0.207, 0.249]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:13:33 (running for 00:23:51.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.295 |  0.171 |                   51 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.303 |  0.168 |                   51 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.8   |  0.179 |                   23 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.471 |  0.177 |                   21 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.16767553985118866
[2m[36m(func pid=45122)[0m mae:  0.10257647931575775
[2m[36m(func pid=45122)[0m rmse_per_class: [0.173, 0.202, 0.039, 0.317, 0.067, 0.163, 0.248, 0.205, 0.156, 0.107]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7980 | Steps: 4 | Val loss: 0.6178 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=52139)[0m rmse: 0.17673741281032562
[2m[36m(func pid=52139)[0m mae:  0.12932103872299194
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.331, 0.094, 0.189, 0.285, 0.138, 0.144, 0.112]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2743 | Steps: 4 | Val loss: 0.3040 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=51559)[0m rmse: 0.17948377132415771
[2m[36m(func pid=51559)[0m mae:  0.13178212940692902
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.337, 0.111, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3260 | Steps: 4 | Val loss: 0.2957 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4546 | Steps: 4 | Val loss: 0.3507 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=44519)[0m rmse: 0.1700889766216278
[2m[36m(func pid=44519)[0m mae:  0.102828249335289
[2m[36m(func pid=44519)[0m rmse_per_class: [0.074, 0.203, 0.059, 0.287, 0.077, 0.188, 0.24, 0.117, 0.184, 0.272]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:13:39 (running for 00:23:56.86)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.274 |  0.17  |                   52 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.326 |  0.166 |                   52 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.798 |  0.179 |                   24 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.46  |  0.177 |                   22 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.1663900464773178
[2m[36m(func pid=45122)[0m mae:  0.09917213022708893
[2m[36m(func pid=45122)[0m rmse_per_class: [0.153, 0.205, 0.04, 0.317, 0.069, 0.166, 0.244, 0.167, 0.173, 0.13]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7943 | Steps: 4 | Val loss: 0.6141 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=52139)[0m rmse: 0.1762567013502121
[2m[36m(func pid=52139)[0m mae:  0.12894070148468018
[2m[36m(func pid=52139)[0m rmse_per_class: [0.119, 0.26, 0.092, 0.332, 0.095, 0.188, 0.283, 0.139, 0.144, 0.112]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2716 | Steps: 4 | Val loss: 0.2854 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3083 | Steps: 4 | Val loss: 0.2946 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=51559)[0m rmse: 0.17944924533367157
[2m[36m(func pid=51559)[0m mae:  0.1317564994096756
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4496 | Steps: 4 | Val loss: 0.3444 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=44519)[0m rmse: 0.1588219702243805
[2m[36m(func pid=44519)[0m mae:  0.09536334872245789
[2m[36m(func pid=44519)[0m rmse_per_class: [0.073, 0.202, 0.061, 0.287, 0.078, 0.17, 0.231, 0.113, 0.162, 0.212]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:13:44 (running for 00:24:02.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.272 |  0.159 |                   53 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.308 |  0.166 |                   53 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.794 |  0.179 |                   25 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.455 |  0.176 |                   23 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.16606487333774567
[2m[36m(func pid=45122)[0m mae:  0.0980525016784668
[2m[36m(func pid=45122)[0m rmse_per_class: [0.126, 0.208, 0.044, 0.307, 0.08, 0.176, 0.249, 0.115, 0.192, 0.164]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7899 | Steps: 4 | Val loss: 0.6105 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=52139)[0m rmse: 0.17552872002124786
[2m[36m(func pid=52139)[0m mae:  0.12835478782653809
[2m[36m(func pid=52139)[0m rmse_per_class: [0.119, 0.259, 0.091, 0.33, 0.094, 0.188, 0.282, 0.138, 0.144, 0.111]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2937 | Steps: 4 | Val loss: 0.2777 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2992 | Steps: 4 | Val loss: 0.2973 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=51559)[0m rmse: 0.1794862449169159
[2m[36m(func pid=51559)[0m mae:  0.1317664086818695
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4476 | Steps: 4 | Val loss: 0.3419 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=44519)[0m rmse: 0.15305450558662415
[2m[36m(func pid=44519)[0m mae:  0.09154529869556427
[2m[36m(func pid=44519)[0m rmse_per_class: [0.073, 0.202, 0.067, 0.294, 0.086, 0.157, 0.225, 0.106, 0.156, 0.164]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:13:49 (running for 00:24:07.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.294 |  0.153 |                   54 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.299 |  0.168 |                   54 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.79  |  0.179 |                   26 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.45  |  0.176 |                   24 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.167982280254364
[2m[36m(func pid=45122)[0m mae:  0.10034266859292984
[2m[36m(func pid=45122)[0m rmse_per_class: [0.127, 0.206, 0.045, 0.312, 0.088, 0.184, 0.254, 0.108, 0.195, 0.16]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7885 | Steps: 4 | Val loss: 0.6079 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=52139)[0m rmse: 0.17530757188796997
[2m[36m(func pid=52139)[0m mae:  0.12819981575012207
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.258, 0.091, 0.332, 0.094, 0.189, 0.282, 0.136, 0.145, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2766 | Steps: 4 | Val loss: 0.2665 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3124 | Steps: 4 | Val loss: 0.2909 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=51559)[0m rmse: 0.1795472651720047
[2m[36m(func pid=51559)[0m mae:  0.1318291574716568
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.11, 0.189, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4413 | Steps: 4 | Val loss: 0.3380 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=44519)[0m rmse: 0.14663608372211456
[2m[36m(func pid=44519)[0m mae:  0.0878906175494194
[2m[36m(func pid=44519)[0m rmse_per_class: [0.073, 0.199, 0.076, 0.289, 0.096, 0.153, 0.222, 0.102, 0.153, 0.104]
[2m[36m(func pid=44519)[0m 
== Status ==
Current time: 2024-01-07 20:13:55 (running for 00:24:12.90)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.277 |  0.147 |                   55 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.312 |  0.164 |                   55 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.789 |  0.18  |                   27 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.448 |  0.175 |                   25 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.1641349494457245
[2m[36m(func pid=45122)[0m mae:  0.09727879613637924
[2m[36m(func pid=45122)[0m rmse_per_class: [0.13, 0.208, 0.05, 0.31, 0.093, 0.179, 0.241, 0.107, 0.179, 0.145]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7817 | Steps: 4 | Val loss: 0.6040 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=52139)[0m rmse: 0.17495547235012054
[2m[36m(func pid=52139)[0m mae:  0.12788930535316467
[2m[36m(func pid=52139)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.331, 0.093, 0.189, 0.281, 0.136, 0.145, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2773 | Steps: 4 | Val loss: 0.2648 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3058 | Steps: 4 | Val loss: 0.2925 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=51559)[0m rmse: 0.1795075386762619
[2m[36m(func pid=51559)[0m mae:  0.13180339336395264
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.189, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4362 | Steps: 4 | Val loss: 0.3339 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=44519)[0m rmse: 0.1441243290901184
[2m[36m(func pid=44519)[0m mae:  0.08588548749685287
[2m[36m(func pid=44519)[0m rmse_per_class: [0.072, 0.2, 0.064, 0.285, 0.097, 0.153, 0.222, 0.103, 0.16, 0.085]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.16567064821720123
[2m[36m(func pid=45122)[0m mae:  0.09668849408626556
[2m[36m(func pid=45122)[0m rmse_per_class: [0.147, 0.215, 0.07, 0.309, 0.077, 0.173, 0.225, 0.103, 0.182, 0.157]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:14:01 (running for 00:24:19.11)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.277 |  0.144 |                   56 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.166 |                   56 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.782 |  0.18  |                   28 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.436 |  0.175 |                   27 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.17452767491340637
[2m[36m(func pid=52139)[0m mae:  0.12754881381988525
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.258, 0.09, 0.33, 0.093, 0.188, 0.28, 0.136, 0.144, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7741 | Steps: 4 | Val loss: 0.6010 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2681 | Steps: 4 | Val loss: 0.2655 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3325 | Steps: 4 | Val loss: 0.2871 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=51559)[0m rmse: 0.179566890001297
[2m[36m(func pid=51559)[0m mae:  0.13188479840755463
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.337, 0.11, 0.189, 0.293, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4294 | Steps: 4 | Val loss: 0.3303 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=44519)[0m rmse: 0.14569637179374695
[2m[36m(func pid=44519)[0m mae:  0.08681590110063553
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.196, 0.083, 0.284, 0.105, 0.164, 0.226, 0.101, 0.144, 0.076]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.16181737184524536
[2m[36m(func pid=45122)[0m mae:  0.09279696643352509
[2m[36m(func pid=45122)[0m rmse_per_class: [0.114, 0.22, 0.067, 0.288, 0.075, 0.167, 0.212, 0.146, 0.175, 0.155]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7702 | Steps: 4 | Val loss: 0.5985 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 20:14:06 (running for 00:24:24.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.268 |  0.146 |                   57 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.332 |  0.162 |                   57 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.774 |  0.18  |                   29 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.429 |  0.174 |                   28 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.17383316159248352
[2m[36m(func pid=52139)[0m mae:  0.12697212398052216
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.257, 0.088, 0.33, 0.092, 0.188, 0.278, 0.136, 0.144, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2799 | Steps: 4 | Val loss: 0.2639 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3147 | Steps: 4 | Val loss: 0.3112 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=51559)[0m rmse: 0.17953117191791534
[2m[36m(func pid=51559)[0m mae:  0.1318531483411789
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.337, 0.11, 0.189, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4304 | Steps: 4 | Val loss: 0.3279 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=44519)[0m rmse: 0.14374582469463348
[2m[36m(func pid=44519)[0m mae:  0.08575496822595596
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.193, 0.07, 0.274, 0.096, 0.171, 0.228, 0.102, 0.148, 0.079]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.17658916115760803
[2m[36m(func pid=45122)[0m mae:  0.10153567790985107
[2m[36m(func pid=45122)[0m rmse_per_class: [0.116, 0.23, 0.092, 0.292, 0.062, 0.177, 0.226, 0.165, 0.198, 0.207]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:14:12 (running for 00:24:29.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.28  |  0.144 |                   58 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.315 |  0.177 |                   58 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.77  |  0.18  |                   30 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.43  |  0.174 |                   29 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7673 | Steps: 4 | Val loss: 0.5949 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=52139)[0m rmse: 0.17360319197177887
[2m[36m(func pid=52139)[0m mae:  0.12674817442893982
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.257, 0.087, 0.329, 0.091, 0.188, 0.278, 0.136, 0.144, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2895 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3552 | Steps: 4 | Val loss: 0.3127 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=51559)[0m rmse: 0.17954422533512115
[2m[36m(func pid=51559)[0m mae:  0.1318722516298294
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.189, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4239 | Steps: 4 | Val loss: 0.3257 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=44519)[0m rmse: 0.14570656418800354
[2m[36m(func pid=44519)[0m mae:  0.08726756274700165
[2m[36m(func pid=44519)[0m rmse_per_class: [0.075, 0.192, 0.059, 0.27, 0.096, 0.2, 0.235, 0.098, 0.142, 0.089]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.17765890061855316
[2m[36m(func pid=45122)[0m mae:  0.10468486696481705
[2m[36m(func pid=45122)[0m rmse_per_class: [0.143, 0.225, 0.087, 0.301, 0.064, 0.173, 0.242, 0.149, 0.203, 0.19]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:14:17 (running for 00:24:35.43)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.289 |  0.146 |                   59 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.355 |  0.178 |                   59 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.767 |  0.18  |                   31 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.424 |  0.174 |                   30 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7627 | Steps: 4 | Val loss: 0.5883 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=52139)[0m rmse: 0.17356452345848083
[2m[36m(func pid=52139)[0m mae:  0.1266675740480423
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.257, 0.088, 0.328, 0.09, 0.188, 0.278, 0.136, 0.144, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3163 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3234 | Steps: 4 | Val loss: 0.3127 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=51559)[0m rmse: 0.17937913537025452
[2m[36m(func pid=51559)[0m mae:  0.1317117065191269
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.336, 0.11, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4208 | Steps: 4 | Val loss: 0.3239 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=44519)[0m rmse: 0.14741000533103943
[2m[36m(func pid=44519)[0m mae:  0.08795876055955887
[2m[36m(func pid=44519)[0m rmse_per_class: [0.074, 0.193, 0.05, 0.268, 0.093, 0.216, 0.238, 0.102, 0.14, 0.1]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.17637348175048828
[2m[36m(func pid=45122)[0m mae:  0.10636645555496216
[2m[36m(func pid=45122)[0m rmse_per_class: [0.196, 0.233, 0.071, 0.315, 0.068, 0.161, 0.236, 0.17, 0.178, 0.137]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:14:23 (running for 00:24:41.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.316 |  0.147 |                   60 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.323 |  0.176 |                   60 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.763 |  0.179 |                   32 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.421 |  0.174 |                   31 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7618 | Steps: 4 | Val loss: 0.5876 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=52139)[0m rmse: 0.17363429069519043
[2m[36m(func pid=52139)[0m mae:  0.12669163942337036
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.258, 0.089, 0.329, 0.089, 0.188, 0.277, 0.136, 0.145, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2851 | Steps: 4 | Val loss: 0.2738 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3223 | Steps: 4 | Val loss: 0.3188 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=51559)[0m rmse: 0.17951911687850952
[2m[36m(func pid=51559)[0m mae:  0.13180553913116455
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.295, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15153510868549347
[2m[36m(func pid=44519)[0m mae:  0.0900779590010643
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.194, 0.056, 0.27, 0.096, 0.224, 0.238, 0.105, 0.147, 0.108]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4144 | Steps: 4 | Val loss: 0.3229 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=45122)[0m rmse: 0.17832326889038086
[2m[36m(func pid=45122)[0m mae:  0.10903731733560562
[2m[36m(func pid=45122)[0m rmse_per_class: [0.206, 0.233, 0.06, 0.325, 0.069, 0.159, 0.241, 0.172, 0.181, 0.137]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:14:28 (running for 00:24:46.45)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.285 |  0.152 |                   61 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.322 |  0.178 |                   61 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.762 |  0.18  |                   33 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.414 |  0.173 |                   32 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.17346760630607605
[2m[36m(func pid=52139)[0m mae:  0.126569464802742
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.258, 0.089, 0.328, 0.088, 0.188, 0.277, 0.136, 0.144, 0.11]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7604 | Steps: 4 | Val loss: 0.5850 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3006 | Steps: 4 | Val loss: 0.2761 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3423 | Steps: 4 | Val loss: 0.3179 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=51559)[0m rmse: 0.17958858609199524
[2m[36m(func pid=51559)[0m mae:  0.13185688853263855
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.189, 0.295, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1533186137676239
[2m[36m(func pid=44519)[0m mae:  0.09064237773418427
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.197, 0.051, 0.272, 0.094, 0.22, 0.23, 0.109, 0.157, 0.126]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4173 | Steps: 4 | Val loss: 0.3213 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=45122)[0m rmse: 0.17658153176307678
[2m[36m(func pid=45122)[0m mae:  0.10908953845500946
[2m[36m(func pid=45122)[0m rmse_per_class: [0.217, 0.235, 0.056, 0.327, 0.069, 0.156, 0.243, 0.153, 0.175, 0.135]
[2m[36m(func pid=45122)[0m 
== Status ==
Current time: 2024-01-07 20:14:34 (running for 00:24:52.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.301 |  0.153 |                   62 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.342 |  0.177 |                   62 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.76  |  0.18  |                   34 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.417 |  0.173 |                   33 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7506 | Steps: 4 | Val loss: 0.5832 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=52139)[0m rmse: 0.17317898571491241
[2m[36m(func pid=52139)[0m mae:  0.12633219361305237
[2m[36m(func pid=52139)[0m rmse_per_class: [0.119, 0.258, 0.088, 0.328, 0.088, 0.187, 0.276, 0.135, 0.144, 0.11]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2736 | Steps: 4 | Val loss: 0.2716 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3367 | Steps: 4 | Val loss: 0.3085 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=51559)[0m rmse: 0.1794298142194748
[2m[36m(func pid=51559)[0m mae:  0.131676584482193
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1489173024892807
[2m[36m(func pid=44519)[0m mae:  0.0871056541800499
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.2, 0.054, 0.271, 0.086, 0.198, 0.219, 0.103, 0.153, 0.128]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.17112568020820618
[2m[36m(func pid=45122)[0m mae:  0.10644595324993134
[2m[36m(func pid=45122)[0m rmse_per_class: [0.211, 0.237, 0.051, 0.321, 0.066, 0.155, 0.238, 0.126, 0.171, 0.135]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4187 | Steps: 4 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7475 | Steps: 4 | Val loss: 0.5793 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:14:39 (running for 00:24:57.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.274 |  0.149 |                   63 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.337 |  0.171 |                   63 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.751 |  0.179 |                   35 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.419 |  0.173 |                   34 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.17300531268119812
[2m[36m(func pid=52139)[0m mae:  0.12623076140880585
[2m[36m(func pid=52139)[0m rmse_per_class: [0.118, 0.258, 0.087, 0.327, 0.087, 0.186, 0.275, 0.135, 0.144, 0.111]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3019 | Steps: 4 | Val loss: 0.2903 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2811 | Steps: 4 | Val loss: 0.2698 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=51559)[0m rmse: 0.17927362024784088
[2m[36m(func pid=51559)[0m mae:  0.13155175745487213
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1461162269115448
[2m[36m(func pid=44519)[0m mae:  0.08449526131153107
[2m[36m(func pid=44519)[0m rmse_per_class: [0.078, 0.205, 0.053, 0.27, 0.082, 0.176, 0.209, 0.113, 0.153, 0.122]
[2m[36m(func pid=45122)[0m rmse: 0.16267743706703186
[2m[36m(func pid=45122)[0m mae:  0.1006707176566124
[2m[36m(func pid=45122)[0m rmse_per_class: [0.163, 0.231, 0.061, 0.307, 0.066, 0.157, 0.227, 0.117, 0.172, 0.125]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4145 | Steps: 4 | Val loss: 0.3186 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7469 | Steps: 4 | Val loss: 0.5759 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 20:14:45 (running for 00:25:03.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.281 |  0.146 |                   64 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.302 |  0.163 |                   64 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.748 |  0.179 |                   36 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.415 |  0.172 |                   35 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1722448766231537
[2m[36m(func pid=52139)[0m mae:  0.12553741037845612
[2m[36m(func pid=52139)[0m rmse_per_class: [0.117, 0.258, 0.086, 0.328, 0.086, 0.187, 0.273, 0.135, 0.143, 0.111]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2982 | Steps: 4 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2700 | Steps: 4 | Val loss: 0.2698 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=51559)[0m rmse: 0.17923550307750702
[2m[36m(func pid=51559)[0m mae:  0.13156694173812866
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.15209373831748962
[2m[36m(func pid=45122)[0m mae:  0.09291823208332062
[2m[36m(func pid=45122)[0m rmse_per_class: [0.127, 0.226, 0.062, 0.29, 0.068, 0.157, 0.212, 0.121, 0.144, 0.114]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14527346193790436
[2m[36m(func pid=44519)[0m mae:  0.0839037224650383
[2m[36m(func pid=44519)[0m rmse_per_class: [0.071, 0.203, 0.051, 0.273, 0.084, 0.173, 0.205, 0.11, 0.157, 0.125]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4093 | Steps: 4 | Val loss: 0.3159 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7424 | Steps: 4 | Val loss: 0.5741 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3357 | Steps: 4 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:14:51 (running for 00:25:08.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.27  |  0.145 |                   65 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.298 |  0.152 |                   65 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   37 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.409 |  0.172 |                   36 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1716705709695816
[2m[36m(func pid=52139)[0m mae:  0.12500914931297302
[2m[36m(func pid=52139)[0m rmse_per_class: [0.117, 0.257, 0.085, 0.327, 0.088, 0.186, 0.271, 0.134, 0.143, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2755 | Steps: 4 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=45122)[0m rmse: 0.15127681195735931
[2m[36m(func pid=45122)[0m mae:  0.09068430960178375
[2m[36m(func pid=45122)[0m rmse_per_class: [0.107, 0.223, 0.079, 0.285, 0.071, 0.163, 0.215, 0.125, 0.133, 0.113]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17903555929660797
[2m[36m(func pid=51559)[0m mae:  0.13135847449302673
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.335, 0.109, 0.19, 0.293, 0.139, 0.142, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14615431427955627
[2m[36m(func pid=44519)[0m mae:  0.08398916572332382
[2m[36m(func pid=44519)[0m rmse_per_class: [0.074, 0.208, 0.052, 0.271, 0.079, 0.166, 0.201, 0.137, 0.156, 0.118]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4103 | Steps: 4 | Val loss: 0.3137 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3066 | Steps: 4 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7395 | Steps: 4 | Val loss: 0.5677 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 20:14:56 (running for 00:25:14.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.146 |                   66 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.336 |  0.151 |                   66 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.742 |  0.179 |                   38 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.41  |  0.171 |                   37 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1711128056049347
[2m[36m(func pid=52139)[0m mae:  0.12453894317150116
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.256, 0.084, 0.327, 0.087, 0.186, 0.27, 0.134, 0.143, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2962 | Steps: 4 | Val loss: 0.2713 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=45122)[0m rmse: 0.1540394276380539
[2m[36m(func pid=45122)[0m mae:  0.09117820858955383
[2m[36m(func pid=45122)[0m rmse_per_class: [0.129, 0.226, 0.082, 0.291, 0.078, 0.156, 0.212, 0.106, 0.137, 0.124]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17890889942646027
[2m[36m(func pid=51559)[0m mae:  0.13128378987312317
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.109, 0.19, 0.293, 0.139, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1459781527519226
[2m[36m(func pid=44519)[0m mae:  0.08419140428304672
[2m[36m(func pid=44519)[0m rmse_per_class: [0.077, 0.209, 0.051, 0.275, 0.079, 0.165, 0.208, 0.116, 0.164, 0.116]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4096 | Steps: 4 | Val loss: 0.3137 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3078 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7359 | Steps: 4 | Val loss: 0.5652 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2687 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 20:15:02 (running for 00:25:20.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.296 |  0.146 |                   67 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.307 |  0.154 |                   67 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.739 |  0.179 |                   39 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.41  |  0.171 |                   38 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.17127694189548492
[2m[36m(func pid=52139)[0m mae:  0.12470518052577972
[2m[36m(func pid=52139)[0m rmse_per_class: [0.116, 0.256, 0.085, 0.327, 0.086, 0.186, 0.271, 0.134, 0.143, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.1633637696504593
[2m[36m(func pid=45122)[0m mae:  0.09567662328481674
[2m[36m(func pid=45122)[0m rmse_per_class: [0.14, 0.228, 0.077, 0.29, 0.084, 0.174, 0.216, 0.099, 0.158, 0.167]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17901559174060822
[2m[36m(func pid=51559)[0m mae:  0.1314178705215454
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.139, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1455724537372589
[2m[36m(func pid=44519)[0m mae:  0.08402180671691895
[2m[36m(func pid=44519)[0m rmse_per_class: [0.072, 0.202, 0.041, 0.278, 0.082, 0.168, 0.208, 0.116, 0.171, 0.117]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4135 | Steps: 4 | Val loss: 0.3128 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3068 | Steps: 4 | Val loss: 0.2890 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7332 | Steps: 4 | Val loss: 0.5630 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2849 | Steps: 4 | Val loss: 0.2705 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:15:07 (running for 00:25:25.62)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.269 |  0.146 |                   68 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.308 |  0.163 |                   68 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.736 |  0.179 |                   40 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.171 |                   39 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.17102019488811493
[2m[36m(func pid=52139)[0m mae:  0.124481201171875
[2m[36m(func pid=52139)[0m rmse_per_class: [0.115, 0.255, 0.084, 0.326, 0.086, 0.187, 0.271, 0.133, 0.144, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.16618295013904572
[2m[36m(func pid=45122)[0m mae:  0.0975654274225235
[2m[36m(func pid=45122)[0m rmse_per_class: [0.135, 0.226, 0.064, 0.287, 0.091, 0.182, 0.224, 0.102, 0.152, 0.196]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.14481660723686218
[2m[36m(func pid=44519)[0m mae:  0.08394055813550949
[2m[36m(func pid=44519)[0m rmse_per_class: [0.074, 0.204, 0.042, 0.285, 0.085, 0.166, 0.204, 0.114, 0.164, 0.11]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17905014753341675
[2m[36m(func pid=51559)[0m mae:  0.13145653903484344
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4045 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3275 | Steps: 4 | Val loss: 0.2873 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2919 | Steps: 4 | Val loss: 0.2752 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7291 | Steps: 4 | Val loss: 0.5638 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:15:13 (running for 00:25:31.23)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.285 |  0.145 |                   69 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.307 |  0.166 |                   69 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.733 |  0.179 |                   41 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.404 |  0.171 |                   40 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1712440550327301
[2m[36m(func pid=52139)[0m mae:  0.12475593388080597
[2m[36m(func pid=52139)[0m rmse_per_class: [0.115, 0.255, 0.085, 0.325, 0.086, 0.186, 0.273, 0.133, 0.144, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.16533441841602325
[2m[36m(func pid=45122)[0m mae:  0.09763924777507782
[2m[36m(func pid=45122)[0m rmse_per_class: [0.138, 0.222, 0.054, 0.291, 0.101, 0.182, 0.226, 0.11, 0.136, 0.193]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15104204416275024
[2m[36m(func pid=44519)[0m mae:  0.08809712529182434
[2m[36m(func pid=44519)[0m rmse_per_class: [0.096, 0.215, 0.048, 0.302, 0.089, 0.165, 0.21, 0.099, 0.169, 0.118]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.179475799202919
[2m[36m(func pid=51559)[0m mae:  0.13187184929847717
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.337, 0.108, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4016 | Steps: 4 | Val loss: 0.3110 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3302 | Steps: 4 | Val loss: 0.2774 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2911 | Steps: 4 | Val loss: 0.2781 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7263 | Steps: 4 | Val loss: 0.5619 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=52139)[0m rmse: 0.17062592506408691
[2m[36m(func pid=52139)[0m mae:  0.1242711991071701
[2m[36m(func pid=52139)[0m rmse_per_class: [0.115, 0.254, 0.083, 0.324, 0.087, 0.186, 0.273, 0.133, 0.144, 0.108]
== Status ==
Current time: 2024-01-07 20:15:19 (running for 00:25:36.89)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.292 |  0.151 |                   70 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.327 |  0.165 |                   70 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.729 |  0.179 |                   42 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.402 |  0.171 |                   41 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.15967150032520294
[2m[36m(func pid=45122)[0m mae:  0.09646791219711304
[2m[36m(func pid=45122)[0m rmse_per_class: [0.119, 0.215, 0.041, 0.287, 0.138, 0.174, 0.232, 0.117, 0.132, 0.142]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15309910476207733
[2m[36m(func pid=44519)[0m mae:  0.08970323204994202
[2m[36m(func pid=44519)[0m rmse_per_class: [0.101, 0.218, 0.041, 0.311, 0.087, 0.163, 0.211, 0.099, 0.174, 0.125]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17945395410060883
[2m[36m(func pid=51559)[0m mae:  0.13182313740253448
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.108, 0.19, 0.294, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3996 | Steps: 4 | Val loss: 0.3102 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3161 | Steps: 4 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2719 | Steps: 4 | Val loss: 0.2819 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7242 | Steps: 4 | Val loss: 0.5588 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 20:15:24 (running for 00:25:42.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.291 |  0.153 |                   71 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.33  |  0.16  |                   71 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.726 |  0.179 |                   43 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.4   |  0.17  |                   42 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.17045600712299347
[2m[36m(func pid=52139)[0m mae:  0.12421776354312897
[2m[36m(func pid=52139)[0m rmse_per_class: [0.114, 0.254, 0.082, 0.324, 0.086, 0.186, 0.273, 0.133, 0.144, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.1592930108308792
[2m[36m(func pid=45122)[0m mae:  0.09894942492246628
[2m[36m(func pid=45122)[0m rmse_per_class: [0.115, 0.213, 0.042, 0.293, 0.156, 0.178, 0.244, 0.106, 0.133, 0.113]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.15579913556575775
[2m[36m(func pid=44519)[0m mae:  0.0915728509426117
[2m[36m(func pid=44519)[0m rmse_per_class: [0.105, 0.226, 0.041, 0.321, 0.088, 0.161, 0.216, 0.098, 0.173, 0.13]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17918288707733154
[2m[36m(func pid=51559)[0m mae:  0.1316031962633133
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3952 | Steps: 4 | Val loss: 0.3101 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3145 | Steps: 4 | Val loss: 0.2831 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2838 | Steps: 4 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7201 | Steps: 4 | Val loss: 0.5566 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:15:29 (running for 00:25:47.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.272 |  0.156 |                   72 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.316 |  0.159 |                   72 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.724 |  0.179 |                   44 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   43 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1704838126897812
[2m[36m(func pid=52139)[0m mae:  0.12429437786340714
[2m[36m(func pid=52139)[0m rmse_per_class: [0.115, 0.254, 0.083, 0.325, 0.085, 0.186, 0.273, 0.133, 0.144, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=45122)[0m rmse: 0.15934713184833527
[2m[36m(func pid=45122)[0m mae:  0.10206671059131622
[2m[36m(func pid=45122)[0m rmse_per_class: [0.107, 0.217, 0.039, 0.299, 0.17, 0.18, 0.255, 0.103, 0.14, 0.083]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1601259708404541
[2m[36m(func pid=44519)[0m mae:  0.09493812173604965
[2m[36m(func pid=44519)[0m rmse_per_class: [0.108, 0.229, 0.047, 0.326, 0.096, 0.167, 0.224, 0.096, 0.177, 0.132]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17920370399951935
[2m[36m(func pid=51559)[0m mae:  0.13158002495765686
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4022 | Steps: 4 | Val loss: 0.3078 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3125 | Steps: 4 | Val loss: 0.2788 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7138 | Steps: 4 | Val loss: 0.5529 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2785 | Steps: 4 | Val loss: 0.2871 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 20:15:35 (running for 00:25:53.10)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.284 |  0.16  |                   73 |
| train_84a75_00011 | RUNNING    | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.313 |  0.156 |                   74 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.72  |  0.179 |                   45 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   43 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.15573182702064514
[2m[36m(func pid=45122)[0m mae:  0.0990367978811264
[2m[36m(func pid=45122)[0m rmse_per_class: [0.093, 0.217, 0.035, 0.294, 0.158, 0.179, 0.25, 0.106, 0.14, 0.084]
[2m[36m(func pid=45122)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.1697009950876236
[2m[36m(func pid=52139)[0m mae:  0.12360572814941406
[2m[36m(func pid=52139)[0m rmse_per_class: [0.114, 0.254, 0.081, 0.324, 0.085, 0.185, 0.27, 0.133, 0.143, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1791052520275116
[2m[36m(func pid=51559)[0m mae:  0.13149210810661316
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.336, 0.109, 0.19, 0.294, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.1593373864889145
[2m[36m(func pid=44519)[0m mae:  0.09550027549266815
[2m[36m(func pid=44519)[0m rmse_per_class: [0.088, 0.221, 0.044, 0.327, 0.103, 0.175, 0.227, 0.098, 0.19, 0.122]
[2m[36m(func pid=44519)[0m 
[2m[36m(func pid=45122)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3057 | Steps: 4 | Val loss: 0.2686 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3971 | Steps: 4 | Val loss: 0.3075 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7147 | Steps: 4 | Val loss: 0.5511 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=44519)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2749 | Steps: 4 | Val loss: 0.2873 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:15:40 (running for 00:25:58.34)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.14549999684095383
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 3 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00010 | RUNNING    | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.279 |  0.159 |                   74 |
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.714 |  0.179 |                   46 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.402 |  0.17  |                   44 |
| train_84a75_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 2 TERMINATED)


[2m[36m(func pid=45122)[0m rmse: 0.14828890562057495
[2m[36m(func pid=45122)[0m mae:  0.0914243757724762
[2m[36m(func pid=45122)[0m rmse_per_class: [0.087, 0.209, 0.032, 0.28, 0.122, 0.167, 0.237, 0.125, 0.138, 0.086]
[2m[36m(func pid=52139)[0m rmse: 0.16971784830093384
[2m[36m(func pid=52139)[0m mae:  0.1236475333571434
[2m[36m(func pid=52139)[0m rmse_per_class: [0.115, 0.255, 0.081, 0.323, 0.085, 0.185, 0.27, 0.133, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17917391657829285
[2m[36m(func pid=51559)[0m mae:  0.131596639752388
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.109, 0.189, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=44519)[0m rmse: 0.16002240777015686
[2m[36m(func pid=44519)[0m mae:  0.09633670747280121
[2m[36m(func pid=44519)[0m rmse_per_class: [0.083, 0.215, 0.052, 0.325, 0.103, 0.181, 0.232, 0.099, 0.202, 0.109]
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4034 | Steps: 4 | Val loss: 0.3067 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7082 | Steps: 4 | Val loss: 0.5485 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=52139)[0m rmse: 0.1693076193332672
[2m[36m(func pid=52139)[0m mae:  0.12316270917654037
[2m[36m(func pid=52139)[0m rmse_per_class: [0.113, 0.255, 0.081, 0.323, 0.084, 0.185, 0.268, 0.131, 0.143, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17898359894752502
[2m[36m(func pid=51559)[0m mae:  0.1314132809638977
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.109, 0.189, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=63038)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63038)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=63038)[0m Configuration completed!
[2m[36m(func pid=63038)[0m New optimizer parameters:
[2m[36m(func pid=63038)[0m SGD (
[2m[36m(func pid=63038)[0m Parameter Group 0
[2m[36m(func pid=63038)[0m     dampening: 0
[2m[36m(func pid=63038)[0m     differentiable: False
[2m[36m(func pid=63038)[0m     foreach: None
[2m[36m(func pid=63038)[0m     lr: 0.01
[2m[36m(func pid=63038)[0m     maximize: False
[2m[36m(func pid=63038)[0m     momentum: 0.9
[2m[36m(func pid=63038)[0m     nesterov: False
[2m[36m(func pid=63038)[0m     weight_decay: 0.0001
[2m[36m(func pid=63038)[0m )
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3934 | Steps: 4 | Val loss: 0.3061 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 20:15:46 (running for 00:26:04.40)
Memory usage on this node: 20.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.715 |  0.179 |                   47 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.403 |  0.169 |                   46 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63128)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63128)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=63128)[0m Configuration completed!
[2m[36m(func pid=63128)[0m New optimizer parameters:
[2m[36m(func pid=63128)[0m SGD (
[2m[36m(func pid=63128)[0m Parameter Group 0
[2m[36m(func pid=63128)[0m     dampening: 0
[2m[36m(func pid=63128)[0m     differentiable: False
[2m[36m(func pid=63128)[0m     foreach: None
[2m[36m(func pid=63128)[0m     lr: 0.1
[2m[36m(func pid=63128)[0m     maximize: False
[2m[36m(func pid=63128)[0m     momentum: 0.9
[2m[36m(func pid=63128)[0m     nesterov: False
[2m[36m(func pid=63128)[0m     weight_decay: 0.0001
[2m[36m(func pid=63128)[0m )
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:15:52 (running for 00:26:09.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.708 |  0.179 |                   48 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.393 |  0.169 |                   47 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16902820765972137
[2m[36m(func pid=52139)[0m mae:  0.12308599799871445
[2m[36m(func pid=52139)[0m rmse_per_class: [0.114, 0.254, 0.079, 0.323, 0.085, 0.184, 0.269, 0.132, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7061 | Steps: 4 | Val loss: 0.5463 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8687 | Steps: 4 | Val loss: 0.6298 | Batch size: 32 | lr: 0.01 | Duration: 4.76s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6905 | Steps: 4 | Val loss: 0.3464 | Batch size: 32 | lr: 0.1 | Duration: 4.59s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3955 | Steps: 4 | Val loss: 0.3052 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=51559)[0m rmse: 0.1789657473564148
[2m[36m(func pid=51559)[0m mae:  0.1314212679862976
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.108, 0.189, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.18282721936702728
[2m[36m(func pid=63038)[0m mae:  0.13425888121128082
[2m[36m(func pid=63038)[0m rmse_per_class: [0.117, 0.268, 0.11, 0.34, 0.114, 0.191, 0.295, 0.144, 0.14, 0.11]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1795453280210495
[2m[36m(func pid=63128)[0m mae:  0.13137347996234894
[2m[36m(func pid=63128)[0m rmse_per_class: [0.11, 0.265, 0.118, 0.327, 0.089, 0.195, 0.295, 0.146, 0.144, 0.107]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:15:57 (running for 00:26:15.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.706 |  0.179 |                   49 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.169 |                   48 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.869 |  0.183 |                    1 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.69  |  0.18  |                    1 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1686965525150299
[2m[36m(func pid=52139)[0m mae:  0.12278282642364502
[2m[36m(func pid=52139)[0m rmse_per_class: [0.113, 0.253, 0.079, 0.323, 0.084, 0.184, 0.268, 0.132, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7025 | Steps: 4 | Val loss: 0.5447 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7199 | Steps: 4 | Val loss: 0.4960 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4120 | Steps: 4 | Val loss: 0.3324 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3937 | Steps: 4 | Val loss: 0.3044 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=51559)[0m rmse: 0.17890501022338867
[2m[36m(func pid=51559)[0m mae:  0.13132771849632263
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.18048372864723206
[2m[36m(func pid=63038)[0m mae:  0.13225404918193817
[2m[36m(func pid=63038)[0m rmse_per_class: [0.115, 0.266, 0.106, 0.333, 0.112, 0.191, 0.295, 0.141, 0.139, 0.108]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.16523686051368713
[2m[36m(func pid=63128)[0m mae:  0.11927515268325806
[2m[36m(func pid=63128)[0m rmse_per_class: [0.112, 0.248, 0.072, 0.308, 0.06, 0.196, 0.276, 0.144, 0.136, 0.099]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:16:03 (running for 00:26:21.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.702 |  0.179 |                   50 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.394 |  0.168 |                   49 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.72  |  0.18  |                    2 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.412 |  0.165 |                    2 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16842570900917053
[2m[36m(func pid=52139)[0m mae:  0.12253085523843765
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.253, 0.08, 0.322, 0.083, 0.184, 0.267, 0.132, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6949 | Steps: 4 | Val loss: 0.5393 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5596 | Steps: 4 | Val loss: 0.3947 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4621 | Steps: 4 | Val loss: 0.3463 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3934 | Steps: 4 | Val loss: 0.3039 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=51559)[0m rmse: 0.17870721220970154
[2m[36m(func pid=51559)[0m mae:  0.13113555312156677
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.336, 0.108, 0.19, 0.293, 0.139, 0.142, 0.108]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.17789480090141296
[2m[36m(func pid=63038)[0m mae:  0.1302107274532318
[2m[36m(func pid=63038)[0m rmse_per_class: [0.112, 0.265, 0.107, 0.33, 0.099, 0.189, 0.291, 0.138, 0.141, 0.108]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15586403012275696
[2m[36m(func pid=63128)[0m mae:  0.10729241371154785
[2m[36m(func pid=63128)[0m rmse_per_class: [0.115, 0.231, 0.05, 0.313, 0.053, 0.182, 0.246, 0.145, 0.133, 0.09]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:16:08 (running for 00:26:26.51)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.695 |  0.179 |                   51 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.393 |  0.168 |                   50 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.56  |  0.178 |                    3 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.462 |  0.156 |                    3 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16830042004585266
[2m[36m(func pid=52139)[0m mae:  0.12243638187646866
[2m[36m(func pid=52139)[0m rmse_per_class: [0.113, 0.254, 0.08, 0.322, 0.082, 0.183, 0.266, 0.132, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6947 | Steps: 4 | Val loss: 0.5363 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4634 | Steps: 4 | Val loss: 0.3427 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4518 | Steps: 4 | Val loss: 0.3197 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=51559)[0m rmse: 0.17900849878787994
[2m[36m(func pid=51559)[0m mae:  0.13136649131774902
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.108, 0.189, 0.292, 0.14, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3903 | Steps: 4 | Val loss: 0.3037 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=63038)[0m rmse: 0.17710967361927032
[2m[36m(func pid=63038)[0m mae:  0.12961384654045105
[2m[36m(func pid=63038)[0m rmse_per_class: [0.11, 0.265, 0.111, 0.334, 0.087, 0.186, 0.286, 0.135, 0.145, 0.111]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15784072875976562
[2m[36m(func pid=63128)[0m mae:  0.10231611877679825
[2m[36m(func pid=63128)[0m rmse_per_class: [0.103, 0.228, 0.046, 0.32, 0.055, 0.173, 0.238, 0.192, 0.134, 0.089]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=52139)[0m rmse: 0.1683221161365509
[2m[36m(func pid=52139)[0m mae:  0.12241773307323456
[2m[36m(func pid=52139)[0m rmse_per_class: [0.113, 0.254, 0.08, 0.321, 0.082, 0.184, 0.266, 0.132, 0.143, 0.109]
== Status ==
Current time: 2024-01-07 20:16:14 (running for 00:26:32.09)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.695 |  0.179 |                   52 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.39  |  0.168 |                   51 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.463 |  0.177 |                    4 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.452 |  0.158 |                    4 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6955 | Steps: 4 | Val loss: 0.5356 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4131 | Steps: 4 | Val loss: 0.3231 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.3656 | Steps: 4 | Val loss: 0.3214 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=51559)[0m rmse: 0.17897877097129822
[2m[36m(func pid=51559)[0m mae:  0.13136062026023865
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.108, 0.189, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3959 | Steps: 4 | Val loss: 0.3027 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=63038)[0m rmse: 0.17648740112781525
[2m[36m(func pid=63038)[0m mae:  0.12913700938224792
[2m[36m(func pid=63038)[0m rmse_per_class: [0.114, 0.266, 0.108, 0.336, 0.081, 0.186, 0.283, 0.134, 0.146, 0.112]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1682301163673401
[2m[36m(func pid=63128)[0m mae:  0.10528822988271713
[2m[36m(func pid=63128)[0m rmse_per_class: [0.109, 0.232, 0.049, 0.333, 0.056, 0.18, 0.311, 0.195, 0.134, 0.086]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:16:19 (running for 00:26:37.43)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.695 |  0.179 |                   53 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.396 |  0.168 |                   52 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.413 |  0.176 |                    5 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.366 |  0.168 |                    5 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16792121529579163
[2m[36m(func pid=52139)[0m mae:  0.1220024824142456
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.253, 0.08, 0.321, 0.08, 0.184, 0.264, 0.131, 0.144, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6923 | Steps: 4 | Val loss: 0.5341 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.3966 | Steps: 4 | Val loss: 0.3159 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.3388 | Steps: 4 | Val loss: 0.2961 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=51559)[0m rmse: 0.17909863591194153
[2m[36m(func pid=51559)[0m mae:  0.13146530091762543
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.108, 0.189, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3864 | Steps: 4 | Val loss: 0.3018 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=63038)[0m rmse: 0.17462864518165588
[2m[36m(func pid=63038)[0m mae:  0.1274982988834381
[2m[36m(func pid=63038)[0m rmse_per_class: [0.114, 0.265, 0.101, 0.335, 0.078, 0.185, 0.276, 0.133, 0.145, 0.113]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15658149123191833
[2m[36m(func pid=63128)[0m mae:  0.09857627749443054
[2m[36m(func pid=63128)[0m rmse_per_class: [0.098, 0.227, 0.044, 0.315, 0.055, 0.163, 0.272, 0.175, 0.132, 0.085]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:16:25 (running for 00:26:42.96)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.692 |  0.179 |                   54 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.386 |  0.167 |                   53 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.397 |  0.175 |                    6 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.339 |  0.157 |                    6 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16744139790534973
[2m[36m(func pid=52139)[0m mae:  0.12172573804855347
[2m[36m(func pid=52139)[0m rmse_per_class: [0.113, 0.253, 0.078, 0.32, 0.081, 0.184, 0.264, 0.132, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6857 | Steps: 4 | Val loss: 0.5328 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3975 | Steps: 4 | Val loss: 0.3076 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3698 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=51559)[0m rmse: 0.17896035313606262
[2m[36m(func pid=51559)[0m mae:  0.1313450038433075
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.108, 0.19, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3932 | Steps: 4 | Val loss: 0.3006 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=63038)[0m rmse: 0.16940823197364807
[2m[36m(func pid=63038)[0m mae:  0.12311021983623505
[2m[36m(func pid=63038)[0m rmse_per_class: [0.113, 0.258, 0.088, 0.33, 0.074, 0.184, 0.266, 0.128, 0.143, 0.109]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15366479754447937
[2m[36m(func pid=63128)[0m mae:  0.09986621141433716
[2m[36m(func pid=63128)[0m rmse_per_class: [0.09, 0.219, 0.047, 0.315, 0.054, 0.165, 0.222, 0.179, 0.147, 0.098]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:16:30 (running for 00:26:48.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.686 |  0.179 |                   55 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.393 |  0.167 |                   54 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.397 |  0.169 |                    7 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.37  |  0.154 |                    7 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16660206019878387
[2m[36m(func pid=52139)[0m mae:  0.12107095867395401
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.253, 0.075, 0.319, 0.082, 0.183, 0.263, 0.13, 0.143, 0.106]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6871 | Steps: 4 | Val loss: 0.5262 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3991 | Steps: 4 | Val loss: 0.3019 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3228 | Steps: 4 | Val loss: 0.2919 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=51559)[0m rmse: 0.17902016639709473
[2m[36m(func pid=51559)[0m mae:  0.1314302235841751
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.107, 0.189, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3936 | Steps: 4 | Val loss: 0.2996 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=63128)[0m rmse: 0.15886232256889343
[2m[36m(func pid=63128)[0m mae:  0.10239432007074356
[2m[36m(func pid=63128)[0m rmse_per_class: [0.089, 0.226, 0.047, 0.321, 0.051, 0.16, 0.222, 0.187, 0.167, 0.118]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1644657552242279
[2m[36m(func pid=63038)[0m mae:  0.1189754381775856
[2m[36m(func pid=63038)[0m rmse_per_class: [0.112, 0.25, 0.074, 0.324, 0.072, 0.181, 0.261, 0.125, 0.14, 0.107]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:16:36 (running for 00:26:53.83)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.687 |  0.179 |                   56 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.394 |  0.166 |                   55 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.399 |  0.164 |                    8 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.323 |  0.159 |                    8 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16616258025169373
[2m[36m(func pid=52139)[0m mae:  0.12062612920999527
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.252, 0.074, 0.319, 0.081, 0.184, 0.261, 0.129, 0.143, 0.106]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6852 | Steps: 4 | Val loss: 0.5253 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3139 | Steps: 4 | Val loss: 0.2797 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3896 | Steps: 4 | Val loss: 0.2990 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=51559)[0m rmse: 0.17920060455799103
[2m[36m(func pid=51559)[0m mae:  0.13153469562530518
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.108, 0.189, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3886 | Steps: 4 | Val loss: 0.2983 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=63038)[0m rmse: 0.16176271438598633
[2m[36m(func pid=63038)[0m mae:  0.11677052825689316
[2m[36m(func pid=63038)[0m rmse_per_class: [0.113, 0.244, 0.067, 0.323, 0.069, 0.178, 0.258, 0.124, 0.139, 0.103]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15353935956954956
[2m[36m(func pid=63128)[0m mae:  0.10260530561208725
[2m[36m(func pid=63128)[0m rmse_per_class: [0.075, 0.222, 0.05, 0.296, 0.061, 0.167, 0.246, 0.13, 0.177, 0.11]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:16:41 (running for 00:26:59.39)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.685 |  0.179 |                   57 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.389 |  0.166 |                   56 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.39  |  0.162 |                    9 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.314 |  0.154 |                    9 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16562911868095398
[2m[36m(func pid=52139)[0m mae:  0.12019016593694687
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.251, 0.073, 0.318, 0.081, 0.182, 0.261, 0.129, 0.143, 0.106]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6779 | Steps: 4 | Val loss: 0.5239 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.2926 | Steps: 4 | Val loss: 0.2671 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3830 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=51559)[0m rmse: 0.17903681099414825
[2m[36m(func pid=51559)[0m mae:  0.13141575455665588
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.107, 0.189, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3920 | Steps: 4 | Val loss: 0.2981 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=63128)[0m rmse: 0.14644448459148407
[2m[36m(func pid=63128)[0m mae:  0.09735798835754395
[2m[36m(func pid=63128)[0m rmse_per_class: [0.078, 0.212, 0.055, 0.285, 0.075, 0.16, 0.234, 0.113, 0.158, 0.095]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15987703204154968
[2m[36m(func pid=63038)[0m mae:  0.1152462512254715
[2m[36m(func pid=63038)[0m rmse_per_class: [0.115, 0.242, 0.06, 0.322, 0.066, 0.176, 0.255, 0.122, 0.138, 0.102]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:16:47 (running for 00:27:05.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.678 |  0.179 |                   58 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.392 |  0.166 |                   57 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.383 |  0.16  |                   10 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.293 |  0.146 |                   10 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1655482053756714
[2m[36m(func pid=52139)[0m mae:  0.12009383738040924
[2m[36m(func pid=52139)[0m rmse_per_class: [0.111, 0.252, 0.074, 0.318, 0.08, 0.182, 0.26, 0.129, 0.143, 0.106]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6767 | Steps: 4 | Val loss: 0.5224 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2858 | Steps: 4 | Val loss: 0.2682 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3873 | Steps: 4 | Val loss: 0.2920 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=51559)[0m rmse: 0.17899230122566223
[2m[36m(func pid=51559)[0m mae:  0.1313619315624237
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.107, 0.189, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1494983285665512
[2m[36m(func pid=63128)[0m mae:  0.09617890417575836
[2m[36m(func pid=63128)[0m rmse_per_class: [0.078, 0.209, 0.082, 0.302, 0.09, 0.154, 0.215, 0.109, 0.144, 0.111]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3836 | Steps: 4 | Val loss: 0.2988 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=63038)[0m rmse: 0.15778183937072754
[2m[36m(func pid=63038)[0m mae:  0.1133643165230751
[2m[36m(func pid=63038)[0m rmse_per_class: [0.114, 0.238, 0.056, 0.322, 0.064, 0.177, 0.249, 0.12, 0.137, 0.101]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:16:53 (running for 00:27:10.75)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.677 |  0.179 |                   59 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.384 |  0.166 |                   58 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.387 |  0.158 |                   11 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.286 |  0.149 |                   11 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16604824364185333
[2m[36m(func pid=52139)[0m mae:  0.12049634754657745
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.252, 0.076, 0.317, 0.081, 0.182, 0.262, 0.128, 0.143, 0.106]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6692 | Steps: 4 | Val loss: 0.5199 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2974 | Steps: 4 | Val loss: 0.2652 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3705 | Steps: 4 | Val loss: 0.2896 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=51559)[0m rmse: 0.17875266075134277
[2m[36m(func pid=51559)[0m mae:  0.13115593791007996
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.107, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14789219200611115
[2m[36m(func pid=63128)[0m mae:  0.09224622696638107
[2m[36m(func pid=63128)[0m rmse_per_class: [0.073, 0.212, 0.049, 0.293, 0.081, 0.153, 0.209, 0.131, 0.155, 0.122]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3816 | Steps: 4 | Val loss: 0.2979 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=63038)[0m rmse: 0.15718883275985718
[2m[36m(func pid=63038)[0m mae:  0.11298944801092148
[2m[36m(func pid=63038)[0m rmse_per_class: [0.112, 0.237, 0.055, 0.32, 0.062, 0.178, 0.247, 0.119, 0.138, 0.103]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6694 | Steps: 4 | Val loss: 0.5187 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2867 | Steps: 4 | Val loss: 0.2968 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:16:58 (running for 00:27:16.45)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.669 |  0.179 |                   60 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.382 |  0.166 |                   59 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.371 |  0.157 |                   12 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.297 |  0.148 |                   12 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16566723585128784
[2m[36m(func pid=52139)[0m mae:  0.12022644281387329
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.252, 0.075, 0.317, 0.08, 0.182, 0.261, 0.128, 0.142, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3711 | Steps: 4 | Val loss: 0.2841 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=51559)[0m rmse: 0.17857253551483154
[2m[36m(func pid=51559)[0m mae:  0.13100531697273254
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.108, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.16545093059539795
[2m[36m(func pid=63128)[0m mae:  0.10296165943145752
[2m[36m(func pid=63128)[0m rmse_per_class: [0.069, 0.211, 0.063, 0.336, 0.071, 0.161, 0.207, 0.152, 0.186, 0.197]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15408946573734283
[2m[36m(func pid=63038)[0m mae:  0.1102830022573471
[2m[36m(func pid=63038)[0m rmse_per_class: [0.107, 0.235, 0.052, 0.317, 0.062, 0.176, 0.241, 0.115, 0.136, 0.1]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3901 | Steps: 4 | Val loss: 0.2967 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6671 | Steps: 4 | Val loss: 0.5171 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2821 | Steps: 4 | Val loss: 0.3278 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 20:17:04 (running for 00:27:22.02)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.669 |  0.179 |                   61 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.39  |  0.165 |                   60 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.371 |  0.154 |                   13 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.287 |  0.165 |                   13 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16499164700508118
[2m[36m(func pid=52139)[0m mae:  0.11960642039775848
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.251, 0.074, 0.317, 0.079, 0.181, 0.26, 0.128, 0.142, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3740 | Steps: 4 | Val loss: 0.2811 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=63128)[0m rmse: 0.18313166499137878
[2m[36m(func pid=63128)[0m mae:  0.1118335947394371
[2m[36m(func pid=63128)[0m rmse_per_class: [0.073, 0.241, 0.153, 0.353, 0.079, 0.166, 0.218, 0.19, 0.194, 0.164]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17858906090259552
[2m[36m(func pid=51559)[0m mae:  0.1310041844844818
[2m[36m(func pid=51559)[0m rmse_per_class: [0.115, 0.26, 0.096, 0.335, 0.107, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1522500365972519
[2m[36m(func pid=63038)[0m mae:  0.10865114629268646
[2m[36m(func pid=63038)[0m rmse_per_class: [0.101, 0.236, 0.05, 0.317, 0.062, 0.171, 0.238, 0.113, 0.137, 0.097]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3877 | Steps: 4 | Val loss: 0.2982 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2815 | Steps: 4 | Val loss: 0.3087 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6637 | Steps: 4 | Val loss: 0.5139 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3569 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 20:17:10 (running for 00:27:27.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.667 |  0.179 |                   62 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.388 |  0.166 |                   61 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.374 |  0.152 |                   14 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.282 |  0.183 |                   14 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16571453213691711
[2m[36m(func pid=52139)[0m mae:  0.12024383246898651
[2m[36m(func pid=52139)[0m rmse_per_class: [0.111, 0.25, 0.076, 0.32, 0.078, 0.182, 0.262, 0.128, 0.142, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.17319755256175995
[2m[36m(func pid=63128)[0m mae:  0.1067771464586258
[2m[36m(func pid=63128)[0m rmse_per_class: [0.076, 0.234, 0.15, 0.336, 0.099, 0.165, 0.231, 0.125, 0.174, 0.141]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.151911199092865
[2m[36m(func pid=63038)[0m mae:  0.10832402855157852
[2m[36m(func pid=63038)[0m rmse_per_class: [0.103, 0.235, 0.053, 0.315, 0.062, 0.168, 0.24, 0.11, 0.138, 0.096]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17868101596832275
[2m[36m(func pid=51559)[0m mae:  0.13109833002090454
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.107, 0.189, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3817 | Steps: 4 | Val loss: 0.2973 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2806 | Steps: 4 | Val loss: 0.2855 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3610 | Steps: 4 | Val loss: 0.2769 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6619 | Steps: 4 | Val loss: 0.5116 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 20:17:15 (running for 00:27:33.37)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.664 |  0.179 |                   63 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.382 |  0.165 |                   62 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.357 |  0.152 |                   15 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.281 |  0.173 |                   15 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16520079970359802
[2m[36m(func pid=52139)[0m mae:  0.11983253806829453
[2m[36m(func pid=52139)[0m rmse_per_class: [0.111, 0.25, 0.075, 0.318, 0.078, 0.182, 0.262, 0.128, 0.142, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15849599242210388
[2m[36m(func pid=63128)[0m mae:  0.09839852154254913
[2m[36m(func pid=63128)[0m rmse_per_class: [0.073, 0.22, 0.07, 0.318, 0.081, 0.162, 0.207, 0.106, 0.163, 0.184]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15043418109416962
[2m[36m(func pid=63038)[0m mae:  0.10634024441242218
[2m[36m(func pid=63038)[0m rmse_per_class: [0.102, 0.232, 0.054, 0.314, 0.061, 0.164, 0.237, 0.11, 0.138, 0.094]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17865446209907532
[2m[36m(func pid=51559)[0m mae:  0.13109734654426575
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.335, 0.107, 0.189, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3826 | Steps: 4 | Val loss: 0.2968 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2649 | Steps: 4 | Val loss: 0.2782 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3495 | Steps: 4 | Val loss: 0.2768 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6593 | Steps: 4 | Val loss: 0.5077 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 20:17:21 (running for 00:27:38.88)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.662 |  0.179 |                   64 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.383 |  0.165 |                   63 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.361 |  0.15  |                   16 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.281 |  0.158 |                   16 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1649404764175415
[2m[36m(func pid=52139)[0m mae:  0.11960933357477188
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.25, 0.074, 0.318, 0.078, 0.181, 0.261, 0.127, 0.142, 0.106]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15587076544761658
[2m[36m(func pid=63128)[0m mae:  0.09693827480077744
[2m[36m(func pid=63128)[0m rmse_per_class: [0.068, 0.224, 0.087, 0.307, 0.085, 0.169, 0.207, 0.116, 0.171, 0.126]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15059685707092285
[2m[36m(func pid=63038)[0m mae:  0.10641268640756607
[2m[36m(func pid=63038)[0m rmse_per_class: [0.102, 0.231, 0.056, 0.313, 0.06, 0.162, 0.239, 0.108, 0.139, 0.096]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17876151204109192
[2m[36m(func pid=51559)[0m mae:  0.13116993010044098
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.106, 0.19, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3808 | Steps: 4 | Val loss: 0.2975 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2819 | Steps: 4 | Val loss: 0.2855 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3443 | Steps: 4 | Val loss: 0.2764 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6535 | Steps: 4 | Val loss: 0.5049 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:17:26 (running for 00:27:44.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.659 |  0.179 |                   65 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.381 |  0.165 |                   64 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.35  |  0.151 |                   17 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.265 |  0.156 |                   17 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16542544960975647
[2m[36m(func pid=52139)[0m mae:  0.12006314098834991
[2m[36m(func pid=52139)[0m rmse_per_class: [0.112, 0.25, 0.075, 0.318, 0.078, 0.182, 0.262, 0.128, 0.143, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.16099652647972107
[2m[36m(func pid=63128)[0m mae:  0.09585842490196228
[2m[36m(func pid=63128)[0m rmse_per_class: [0.113, 0.231, 0.142, 0.32, 0.074, 0.16, 0.203, 0.106, 0.152, 0.11]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15048207342624664
[2m[36m(func pid=63038)[0m mae:  0.10692298412322998
[2m[36m(func pid=63038)[0m rmse_per_class: [0.101, 0.231, 0.056, 0.309, 0.059, 0.165, 0.239, 0.107, 0.14, 0.098]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1785697489976883
[2m[36m(func pid=51559)[0m mae:  0.13097289204597473
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.107, 0.19, 0.291, 0.139, 0.143, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3878 | Steps: 4 | Val loss: 0.2962 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3067 | Steps: 4 | Val loss: 0.3350 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3415 | Steps: 4 | Val loss: 0.2766 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6533 | Steps: 4 | Val loss: 0.5017 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:17:32 (running for 00:27:49.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.653 |  0.179 |                   66 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.388 |  0.165 |                   65 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.344 |  0.15  |                   18 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.282 |  0.161 |                   18 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16463831067085266
[2m[36m(func pid=52139)[0m mae:  0.11933405697345734
[2m[36m(func pid=52139)[0m rmse_per_class: [0.11, 0.25, 0.073, 0.317, 0.077, 0.182, 0.259, 0.127, 0.143, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.18529415130615234
[2m[36m(func pid=63128)[0m mae:  0.10773841291666031
[2m[36m(func pid=63128)[0m rmse_per_class: [0.115, 0.236, 0.23, 0.352, 0.113, 0.162, 0.21, 0.184, 0.144, 0.106]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1506248414516449
[2m[36m(func pid=63038)[0m mae:  0.10733572393655777
[2m[36m(func pid=63038)[0m rmse_per_class: [0.1, 0.229, 0.055, 0.308, 0.059, 0.166, 0.242, 0.106, 0.142, 0.098]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17873987555503845
[2m[36m(func pid=51559)[0m mae:  0.1311146467924118
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.105, 0.19, 0.292, 0.138, 0.144, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3784 | Steps: 4 | Val loss: 0.2947 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2735 | Steps: 4 | Val loss: 0.3465 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3468 | Steps: 4 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6519 | Steps: 4 | Val loss: 0.5020 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:17:37 (running for 00:27:55.44)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.653 |  0.179 |                   67 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.378 |  0.164 |                   66 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.342 |  0.151 |                   19 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.307 |  0.185 |                   19 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1637926548719406
[2m[36m(func pid=52139)[0m mae:  0.11863156408071518
[2m[36m(func pid=52139)[0m rmse_per_class: [0.11, 0.249, 0.071, 0.315, 0.078, 0.181, 0.258, 0.127, 0.142, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1905207484960556
[2m[36m(func pid=63128)[0m mae:  0.11611853539943695
[2m[36m(func pid=63128)[0m rmse_per_class: [0.123, 0.232, 0.226, 0.35, 0.16, 0.168, 0.263, 0.129, 0.138, 0.117]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15147385001182556
[2m[36m(func pid=63038)[0m mae:  0.10778089612722397
[2m[36m(func pid=63038)[0m rmse_per_class: [0.098, 0.23, 0.057, 0.311, 0.06, 0.168, 0.242, 0.106, 0.146, 0.099]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17895646393299103
[2m[36m(func pid=51559)[0m mae:  0.1312950700521469
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.335, 0.106, 0.19, 0.292, 0.139, 0.144, 0.111]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3815 | Steps: 4 | Val loss: 0.2953 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3100 | Steps: 4 | Val loss: 0.2986 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3458 | Steps: 4 | Val loss: 0.2754 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6485 | Steps: 4 | Val loss: 0.5005 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:17:43 (running for 00:28:00.96)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.652 |  0.179 |                   68 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.381 |  0.164 |                   67 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.347 |  0.151 |                   20 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.274 |  0.191 |                   20 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16419675946235657
[2m[36m(func pid=52139)[0m mae:  0.11893625557422638
[2m[36m(func pid=52139)[0m rmse_per_class: [0.109, 0.251, 0.073, 0.316, 0.079, 0.179, 0.26, 0.126, 0.142, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.17331334948539734
[2m[36m(func pid=63128)[0m mae:  0.1072440966963768
[2m[36m(func pid=63128)[0m rmse_per_class: [0.093, 0.22, 0.092, 0.295, 0.153, 0.163, 0.263, 0.152, 0.2, 0.104]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15010115504264832
[2m[36m(func pid=63038)[0m mae:  0.10564043372869492
[2m[36m(func pid=63038)[0m rmse_per_class: [0.095, 0.23, 0.056, 0.31, 0.06, 0.162, 0.238, 0.108, 0.146, 0.096]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17891715466976166
[2m[36m(func pid=51559)[0m mae:  0.13125821948051453
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.335, 0.105, 0.19, 0.292, 0.139, 0.144, 0.111]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3808 | Steps: 4 | Val loss: 0.2952 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2712 | Steps: 4 | Val loss: 0.2778 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3348 | Steps: 4 | Val loss: 0.2772 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6487 | Steps: 4 | Val loss: 0.4979 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:17:48 (running for 00:28:06.43)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.649 |  0.179 |                   69 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.381 |  0.164 |                   68 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.346 |  0.15  |                   21 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.31  |  0.173 |                   21 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1641153246164322
[2m[36m(func pid=52139)[0m mae:  0.11891237646341324
[2m[36m(func pid=52139)[0m rmse_per_class: [0.109, 0.251, 0.073, 0.315, 0.078, 0.18, 0.259, 0.126, 0.142, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15556608140468597
[2m[36m(func pid=63128)[0m mae:  0.09694479405879974
[2m[36m(func pid=63128)[0m rmse_per_class: [0.086, 0.204, 0.033, 0.263, 0.115, 0.159, 0.242, 0.143, 0.227, 0.083]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15149106085300446
[2m[36m(func pid=63038)[0m mae:  0.10697565227746964
[2m[36m(func pid=63038)[0m rmse_per_class: [0.096, 0.231, 0.058, 0.31, 0.06, 0.163, 0.242, 0.105, 0.151, 0.099]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1788690835237503
[2m[36m(func pid=51559)[0m mae:  0.13128744065761566
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.104, 0.19, 0.292, 0.139, 0.144, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3826 | Steps: 4 | Val loss: 0.2947 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2678 | Steps: 4 | Val loss: 0.2811 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3380 | Steps: 4 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6476 | Steps: 4 | Val loss: 0.4956 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=52139)[0m rmse: 0.16374866664409637
[2m[36m(func pid=52139)[0m mae:  0.11851967871189117
[2m[36m(func pid=52139)[0m rmse_per_class: [0.108, 0.249, 0.072, 0.317, 0.078, 0.18, 0.258, 0.124, 0.142, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1578386127948761
[2m[36m(func pid=63128)[0m mae:  0.09881819784641266
[2m[36m(func pid=63128)[0m rmse_per_class: [0.085, 0.237, 0.046, 0.28, 0.105, 0.17, 0.237, 0.12, 0.206, 0.093]
== Status ==
Current time: 2024-01-07 20:17:54 (running for 00:28:11.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.649 |  0.179 |                   70 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.383 |  0.164 |                   69 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.335 |  0.151 |                   22 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.271 |  0.156 |                   22 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.15193918347358704
[2m[36m(func pid=63038)[0m mae:  0.10724729299545288
[2m[36m(func pid=63038)[0m rmse_per_class: [0.095, 0.23, 0.055, 0.31, 0.062, 0.164, 0.243, 0.106, 0.154, 0.101]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17873427271842957
[2m[36m(func pid=51559)[0m mae:  0.1311069130897522
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.105, 0.189, 0.291, 0.139, 0.144, 0.11]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3782 | Steps: 4 | Val loss: 0.2958 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2739 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6405 | Steps: 4 | Val loss: 0.4949 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3390 | Steps: 4 | Val loss: 0.2760 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 20:17:59 (running for 00:28:17.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.648 |  0.179 |                   71 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.378 |  0.165 |                   70 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.338 |  0.152 |                   23 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.268 |  0.158 |                   23 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.16450265049934387
[2m[36m(func pid=52139)[0m mae:  0.1193237453699112
[2m[36m(func pid=52139)[0m rmse_per_class: [0.109, 0.25, 0.073, 0.316, 0.08, 0.18, 0.263, 0.125, 0.142, 0.109]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14894434809684753
[2m[36m(func pid=63128)[0m mae:  0.09052951633930206
[2m[36m(func pid=63128)[0m rmse_per_class: [0.086, 0.22, 0.049, 0.283, 0.08, 0.163, 0.224, 0.141, 0.128, 0.115]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.1785462200641632
[2m[36m(func pid=51559)[0m mae:  0.1309366524219513
[2m[36m(func pid=51559)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.105, 0.19, 0.291, 0.139, 0.144, 0.111]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1510457694530487
[2m[36m(func pid=63038)[0m mae:  0.10672125965356827
[2m[36m(func pid=63038)[0m rmse_per_class: [0.095, 0.226, 0.056, 0.304, 0.061, 0.166, 0.244, 0.103, 0.153, 0.1]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3793 | Steps: 4 | Val loss: 0.2956 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2521 | Steps: 4 | Val loss: 0.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:18:04 (running for 00:28:22.63)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.64  |  0.179 |                   72 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.379 |  0.164 |                   71 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.339 |  0.151 |                   24 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.274 |  0.149 |                   24 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6353 | Steps: 4 | Val loss: 0.4926 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=52139)[0m rmse: 0.16440431773662567
[2m[36m(func pid=52139)[0m mae:  0.11933176219463348
[2m[36m(func pid=52139)[0m rmse_per_class: [0.11, 0.25, 0.073, 0.315, 0.078, 0.18, 0.262, 0.125, 0.142, 0.108]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15351256728172302
[2m[36m(func pid=63128)[0m mae:  0.09108555316925049
[2m[36m(func pid=63128)[0m rmse_per_class: [0.083, 0.209, 0.045, 0.296, 0.065, 0.162, 0.228, 0.157, 0.132, 0.157]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3382 | Steps: 4 | Val loss: 0.2724 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=51559)[0m rmse: 0.1787773072719574
[2m[36m(func pid=51559)[0m mae:  0.13117113709449768
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.335, 0.105, 0.19, 0.291, 0.139, 0.143, 0.111]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14845043420791626
[2m[36m(func pid=63038)[0m mae:  0.10350855439901352
[2m[36m(func pid=63038)[0m rmse_per_class: [0.094, 0.224, 0.056, 0.306, 0.063, 0.156, 0.242, 0.105, 0.144, 0.094]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2679 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3769 | Steps: 4 | Val loss: 0.2949 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 20:18:10 (running for 00:28:28.09)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.635 |  0.179 |                   73 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.379 |  0.164 |                   71 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.338 |  0.148 |                   25 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.268 |  0.143 |                   26 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14262524247169495
[2m[36m(func pid=63128)[0m mae:  0.08319108188152313
[2m[36m(func pid=63128)[0m rmse_per_class: [0.076, 0.217, 0.031, 0.271, 0.062, 0.168, 0.224, 0.159, 0.129, 0.091]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6386 | Steps: 4 | Val loss: 0.4933 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3259 | Steps: 4 | Val loss: 0.2698 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=52139)[0m rmse: 0.16399702429771423
[2m[36m(func pid=52139)[0m mae:  0.11901123821735382
[2m[36m(func pid=52139)[0m rmse_per_class: [0.109, 0.25, 0.073, 0.314, 0.078, 0.181, 0.262, 0.125, 0.141, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17897891998291016
[2m[36m(func pid=51559)[0m mae:  0.13139434158802032
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.106, 0.189, 0.291, 0.139, 0.144, 0.111]
[2m[36m(func pid=51559)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14678418636322021
[2m[36m(func pid=63038)[0m mae:  0.10241613537073135
[2m[36m(func pid=63038)[0m rmse_per_class: [0.092, 0.224, 0.052, 0.297, 0.063, 0.157, 0.242, 0.102, 0.146, 0.093]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2654 | Steps: 4 | Val loss: 0.2750 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3723 | Steps: 4 | Val loss: 0.2949 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=63128)[0m rmse: 0.15157559514045715
[2m[36m(func pid=63128)[0m mae:  0.09114101529121399
[2m[36m(func pid=63128)[0m rmse_per_class: [0.098, 0.227, 0.035, 0.289, 0.077, 0.174, 0.233, 0.13, 0.161, 0.091]
== Status ==
Current time: 2024-01-07 20:18:15 (running for 00:28:33.42)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00012 | RUNNING    | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.639 |  0.179 |                   74 |
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.377 |  0.164 |                   72 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.326 |  0.147 |                   26 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.265 |  0.152 |                   27 |
| train_84a75_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=51559)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6306 | Steps: 4 | Val loss: 0.4892 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3327 | Steps: 4 | Val loss: 0.2685 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=52139)[0m rmse: 0.16395536065101624
[2m[36m(func pid=52139)[0m mae:  0.11905290931463242
[2m[36m(func pid=52139)[0m rmse_per_class: [0.109, 0.25, 0.071, 0.314, 0.079, 0.18, 0.262, 0.125, 0.142, 0.107]
[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=51559)[0m rmse: 0.17895513772964478
[2m[36m(func pid=51559)[0m mae:  0.13135460019111633
[2m[36m(func pid=51559)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.106, 0.19, 0.291, 0.139, 0.144, 0.111]
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2611 | Steps: 4 | Val loss: 0.3105 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=63038)[0m rmse: 0.14579011499881744
[2m[36m(func pid=63038)[0m mae:  0.1012042760848999
[2m[36m(func pid=63038)[0m rmse_per_class: [0.093, 0.223, 0.049, 0.297, 0.062, 0.155, 0.237, 0.103, 0.145, 0.093]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3755 | Steps: 4 | Val loss: 0.2946 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=63128)[0m rmse: 0.17784591019153595
[2m[36m(func pid=63128)[0m mae:  0.11023616790771484
[2m[36m(func pid=63128)[0m rmse_per_class: [0.117, 0.216, 0.083, 0.327, 0.106, 0.177, 0.276, 0.135, 0.178, 0.162]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3206 | Steps: 4 | Val loss: 0.2705 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=52139)[0m rmse: 0.16370226442813873
[2m[36m(func pid=52139)[0m mae:  0.11881358921527863
[2m[36m(func pid=52139)[0m rmse_per_class: [0.109, 0.249, 0.071, 0.314, 0.08, 0.18, 0.262, 0.125, 0.141, 0.107]
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2894 | Steps: 4 | Val loss: 0.2624 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=63038)[0m rmse: 0.14757139980793
[2m[36m(func pid=63038)[0m mae:  0.10283344984054565
[2m[36m(func pid=63038)[0m rmse_per_class: [0.093, 0.226, 0.053, 0.3, 0.064, 0.159, 0.232, 0.101, 0.151, 0.096]
[2m[36m(func pid=63128)[0m rmse: 0.14187927544116974
[2m[36m(func pid=63128)[0m mae:  0.08325906097888947
[2m[36m(func pid=63128)[0m rmse_per_class: [0.076, 0.199, 0.039, 0.265, 0.079, 0.162, 0.212, 0.116, 0.181, 0.09]
[2m[36m(func pid=70360)[0m Dataloader to compute accuracy: val== Status ==
Current time: 2024-01-07 20:18:21 (running for 00:28:38.72)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.372 |  0.164 |                   73 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.333 |  0.146 |                   27 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.261 |  0.178 |                   28 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=70360)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=70360)[0m Configuration completed!
[2m[36m(func pid=70360)[0m New optimizer parameters:
[2m[36m(func pid=70360)[0m SGD (
[2m[36m(func pid=70360)[0m Parameter Group 0
[2m[36m(func pid=70360)[0m     dampening: 0
[2m[36m(func pid=70360)[0m     differentiable: False
[2m[36m(func pid=70360)[0m     foreach: None
[2m[36m(func pid=70360)[0m     lr: 0.0001
[2m[36m(func pid=70360)[0m     maximize: False
[2m[36m(func pid=70360)[0m     momentum: 0.99
[2m[36m(func pid=70360)[0m     nesterov: False
[2m[36m(func pid=70360)[0m     weight_decay: 1e-05
[2m[36m(func pid=70360)[0m )
[2m[36m(func pid=70360)[0m 
== Status ==
Current time: 2024-01-07 20:18:26 (running for 00:28:44.54)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.376 |  0.164 |                   74 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.333 |  0.146 |                   27 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.261 |  0.178 |                   28 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m 
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=52139)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3738 | Steps: 4 | Val loss: 0.2941 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3245 | Steps: 4 | Val loss: 0.2773 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2897 | Steps: 4 | Val loss: 0.2711 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8976 | Steps: 4 | Val loss: 0.6984 | Batch size: 32 | lr: 0.0001 | Duration: 4.66s
== Status ==
Current time: 2024-01-07 20:18:31 (running for 00:28:49.56)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00013 | RUNNING    | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.376 |  0.164 |                   74 |
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.321 |  0.148 |                   28 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.142 |                   29 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=52139)[0m rmse: 0.1635366976261139
[2m[36m(func pid=52139)[0m mae:  0.11866136640310287
[2m[36m(func pid=52139)[0m rmse_per_class: [0.109, 0.249, 0.071, 0.313, 0.079, 0.18, 0.262, 0.125, 0.141, 0.107]
[2m[36m(func pid=63038)[0m rmse: 0.1526772379875183
[2m[36m(func pid=63038)[0m mae:  0.106698177754879
[2m[36m(func pid=63038)[0m rmse_per_class: [0.094, 0.232, 0.065, 0.311, 0.068, 0.163, 0.234, 0.102, 0.156, 0.103]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14611777663230896
[2m[36m(func pid=63128)[0m mae:  0.08528690040111542
[2m[36m(func pid=63128)[0m rmse_per_class: [0.085, 0.203, 0.047, 0.294, 0.058, 0.155, 0.21, 0.144, 0.173, 0.092]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1821894347667694
[2m[36m(func pid=70360)[0m mae:  0.13414838910102844
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.111, 0.19, 0.294, 0.143, 0.144, 0.112]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3184 | Steps: 4 | Val loss: 0.2675 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2956 | Steps: 4 | Val loss: 0.2905 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8949 | Steps: 4 | Val loss: 0.6923 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=63128)[0m rmse: 0.15679481625556946
[2m[36m(func pid=63128)[0m mae:  0.09517407417297363
[2m[36m(func pid=63128)[0m rmse_per_class: [0.087, 0.213, 0.054, 0.316, 0.055, 0.153, 0.205, 0.114, 0.252, 0.118]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1455887258052826
[2m[36m(func pid=63038)[0m mae:  0.10086554288864136
[2m[36m(func pid=63038)[0m rmse_per_class: [0.094, 0.224, 0.058, 0.296, 0.065, 0.156, 0.231, 0.098, 0.14, 0.094]
[2m[36m(func pid=70360)[0m rmse: 0.18172979354858398
[2m[36m(func pid=70360)[0m mae:  0.1337805688381195
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.266, 0.104, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2652 | Steps: 4 | Val loss: 0.2877 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:18:37 (running for 00:28:55.47)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.325 |  0.153 |                   29 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.296 |  0.157 |                   31 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.898 |  0.182 |                    1 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=71112)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=71112)[0m Configuration completed!
[2m[36m(func pid=71112)[0m New optimizer parameters:
[2m[36m(func pid=71112)[0m SGD (
[2m[36m(func pid=71112)[0m Parameter Group 0
[2m[36m(func pid=71112)[0m     dampening: 0
[2m[36m(func pid=71112)[0m     differentiable: False
[2m[36m(func pid=71112)[0m     foreach: None
[2m[36m(func pid=71112)[0m     lr: 0.001
[2m[36m(func pid=71112)[0m     maximize: False
[2m[36m(func pid=71112)[0m     momentum: 0.99
[2m[36m(func pid=71112)[0m     nesterov: False
[2m[36m(func pid=71112)[0m     weight_decay: 1e-05
[2m[36m(func pid=71112)[0m )
[2m[36m(func pid=71112)[0m 
== Status ==
Current time: 2024-01-07 20:18:43 (running for 00:29:01.04)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.318 |  0.146 |                   30 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.265 |  0.155 |                   32 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.895 |  0.182 |                    2 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1546809822320938
[2m[36m(func pid=63128)[0m mae:  0.09585090726613998
[2m[36m(func pid=63128)[0m rmse_per_class: [0.08, 0.249, 0.044, 0.328, 0.064, 0.16, 0.219, 0.107, 0.162, 0.132]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8920 | Steps: 4 | Val loss: 0.6871 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3262 | Steps: 4 | Val loss: 0.2651 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8903 | Steps: 4 | Val loss: 0.6920 | Batch size: 32 | lr: 0.001 | Duration: 4.77s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2847 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=70360)[0m rmse: 0.18107867240905762
[2m[36m(func pid=70360)[0m mae:  0.1332244575023651
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.265, 0.102, 0.338, 0.112, 0.189, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14396926760673523
[2m[36m(func pid=63038)[0m mae:  0.09847192466259003
[2m[36m(func pid=63038)[0m rmse_per_class: [0.092, 0.22, 0.053, 0.29, 0.064, 0.159, 0.235, 0.099, 0.133, 0.094]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1824360489845276
[2m[36m(func pid=71112)[0m mae:  0.13426265120506287
[2m[36m(func pid=71112)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.339, 0.112, 0.191, 0.294, 0.143, 0.143, 0.111]
[2m[36m(func pid=71112)[0m 
== Status ==
Current time: 2024-01-07 20:18:48 (running for 00:29:06.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.326 |  0.144 |                   31 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.285 |  0.169 |                   33 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.892 |  0.181 |                    3 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.89  |  0.182 |                    1 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.16938579082489014
[2m[36m(func pid=63128)[0m mae:  0.10333085060119629
[2m[36m(func pid=63128)[0m rmse_per_class: [0.077, 0.29, 0.045, 0.329, 0.064, 0.177, 0.234, 0.125, 0.189, 0.164]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8881 | Steps: 4 | Val loss: 0.6834 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3121 | Steps: 4 | Val loss: 0.2635 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8718 | Steps: 4 | Val loss: 0.6616 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2620 | Steps: 4 | Val loss: 0.3036 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=70360)[0m rmse: 0.18082493543624878
[2m[36m(func pid=70360)[0m mae:  0.13294219970703125
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.338, 0.112, 0.189, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14275887608528137
[2m[36m(func pid=63038)[0m mae:  0.09735918790102005
[2m[36m(func pid=63038)[0m rmse_per_class: [0.089, 0.22, 0.055, 0.287, 0.064, 0.158, 0.234, 0.099, 0.131, 0.091]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.18142065405845642
[2m[36m(func pid=71112)[0m mae:  0.13346146047115326
[2m[36m(func pid=71112)[0m rmse_per_class: [0.117, 0.266, 0.104, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=71112)[0m 
== Status ==
Current time: 2024-01-07 20:18:54 (running for 00:29:11.80)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.312 |  0.143 |                   32 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.262 |  0.168 |                   34 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.888 |  0.181 |                    4 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.872 |  0.181 |                    2 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.16812197864055634
[2m[36m(func pid=63128)[0m mae:  0.10098061710596085
[2m[36m(func pid=63128)[0m rmse_per_class: [0.08, 0.287, 0.075, 0.326, 0.072, 0.171, 0.235, 0.118, 0.163, 0.155]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8772 | Steps: 4 | Val loss: 0.6772 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3274 | Steps: 4 | Val loss: 0.2611 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8331 | Steps: 4 | Val loss: 0.6256 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2883 | Steps: 4 | Val loss: 0.2727 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=70360)[0m rmse: 0.1802789270877838
[2m[36m(func pid=70360)[0m mae:  0.13243813812732697
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.264, 0.101, 0.337, 0.112, 0.19, 0.294, 0.14, 0.141, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14075374603271484
[2m[36m(func pid=63038)[0m mae:  0.09632186591625214
[2m[36m(func pid=63038)[0m rmse_per_class: [0.085, 0.218, 0.047, 0.286, 0.063, 0.153, 0.227, 0.098, 0.133, 0.096]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:18:59 (running for 00:29:16.98)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.327 |  0.141 |                   33 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.262 |  0.168 |                   34 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.877 |  0.18  |                    5 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.833 |  0.181 |                    3 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.1808241307735443
[2m[36m(func pid=71112)[0m mae:  0.13282793760299683
[2m[36m(func pid=71112)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.337, 0.113, 0.19, 0.294, 0.141, 0.141, 0.109]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15488407015800476
[2m[36m(func pid=63128)[0m mae:  0.09144316613674164
[2m[36m(func pid=63128)[0m rmse_per_class: [0.082, 0.217, 0.087, 0.291, 0.074, 0.165, 0.217, 0.117, 0.158, 0.141]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8671 | Steps: 4 | Val loss: 0.6700 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3081 | Steps: 4 | Val loss: 0.2612 | Batch size: 32 | lr: 0.01 | Duration: 3.37s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7855 | Steps: 4 | Val loss: 0.5829 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2596 | Steps: 4 | Val loss: 0.3038 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=70360)[0m rmse: 0.1800391674041748
[2m[36m(func pid=70360)[0m mae:  0.13224853575229645
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.262, 0.1, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14079825580120087
[2m[36m(func pid=63038)[0m mae:  0.09672863781452179
[2m[36m(func pid=63038)[0m rmse_per_class: [0.084, 0.217, 0.05, 0.285, 0.063, 0.152, 0.224, 0.098, 0.136, 0.1]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:04 (running for 00:29:22.26)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.308 |  0.141 |                   34 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.288 |  0.155 |                   35 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.867 |  0.18  |                    6 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.786 |  0.18  |                    4 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.18011194467544556
[2m[36m(func pid=71112)[0m mae:  0.13216832280158997
[2m[36m(func pid=71112)[0m rmse_per_class: [0.115, 0.264, 0.101, 0.336, 0.113, 0.19, 0.293, 0.14, 0.141, 0.109]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.173378586769104
[2m[36m(func pid=63128)[0m mae:  0.10412584245204926
[2m[36m(func pid=63128)[0m rmse_per_class: [0.079, 0.211, 0.186, 0.324, 0.077, 0.167, 0.232, 0.12, 0.215, 0.123]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8589 | Steps: 4 | Val loss: 0.6608 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3129 | Steps: 4 | Val loss: 0.2642 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7242 | Steps: 4 | Val loss: 0.5333 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=70360)[0m rmse: 0.17993423342704773
[2m[36m(func pid=70360)[0m mae:  0.1321784108877182
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.336, 0.111, 0.19, 0.295, 0.139, 0.142, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2658 | Steps: 4 | Val loss: 0.3337 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=63038)[0m rmse: 0.1435650885105133
[2m[36m(func pid=63038)[0m mae:  0.09855379164218903
[2m[36m(func pid=63038)[0m rmse_per_class: [0.083, 0.221, 0.056, 0.289, 0.066, 0.155, 0.223, 0.098, 0.138, 0.106]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:10 (running for 00:29:27.79)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.313 |  0.144 |                   35 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.26  |  0.173 |                   36 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.859 |  0.18  |                    7 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.724 |  0.179 |                    5 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17936302721500397
[2m[36m(func pid=71112)[0m mae:  0.1314418464899063
[2m[36m(func pid=71112)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.335, 0.111, 0.191, 0.292, 0.14, 0.14, 0.108]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.19098573923110962
[2m[36m(func pid=63128)[0m mae:  0.11651704460382462
[2m[36m(func pid=63128)[0m rmse_per_class: [0.085, 0.217, 0.163, 0.325, 0.098, 0.168, 0.272, 0.133, 0.313, 0.136]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8445 | Steps: 4 | Val loss: 0.6522 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3321 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6615 | Steps: 4 | Val loss: 0.4821 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2898 | Steps: 4 | Val loss: 0.3230 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=70360)[0m rmse: 0.17975184321403503
[2m[36m(func pid=70360)[0m mae:  0.13200849294662476
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.111, 0.19, 0.295, 0.14, 0.142, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14412404596805573
[2m[36m(func pid=63038)[0m mae:  0.09759454429149628
[2m[36m(func pid=63038)[0m rmse_per_class: [0.08, 0.227, 0.06, 0.295, 0.073, 0.15, 0.22, 0.101, 0.133, 0.102]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:15 (running for 00:29:33.20)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.332 |  0.144 |                   36 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.266 |  0.191 |                   37 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.844 |  0.18  |                    8 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.661 |  0.179 |                    6 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17860423028469086
[2m[36m(func pid=71112)[0m mae:  0.13075385987758636
[2m[36m(func pid=71112)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.334, 0.108, 0.191, 0.29, 0.139, 0.141, 0.108]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1823309361934662
[2m[36m(func pid=63128)[0m mae:  0.11011677980422974
[2m[36m(func pid=63128)[0m rmse_per_class: [0.072, 0.223, 0.173, 0.334, 0.084, 0.156, 0.241, 0.113, 0.263, 0.164]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8306 | Steps: 4 | Val loss: 0.6399 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3201 | Steps: 4 | Val loss: 0.2656 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5975 | Steps: 4 | Val loss: 0.4369 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2990 | Steps: 4 | Val loss: 0.2855 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=70360)[0m rmse: 0.1795601099729538
[2m[36m(func pid=70360)[0m mae:  0.1318809539079666
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.111, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14503887295722961
[2m[36m(func pid=63038)[0m mae:  0.09789589792490005
[2m[36m(func pid=63038)[0m rmse_per_class: [0.081, 0.23, 0.066, 0.295, 0.072, 0.15, 0.219, 0.101, 0.136, 0.1]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:20 (running for 00:29:38.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.32  |  0.145 |                   37 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.29  |  0.182 |                   38 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.831 |  0.18  |                    9 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.597 |  0.178 |                    7 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17824691534042358
[2m[36m(func pid=71112)[0m mae:  0.1305268108844757
[2m[36m(func pid=71112)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.333, 0.103, 0.191, 0.289, 0.138, 0.143, 0.11]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.16161838173866272
[2m[36m(func pid=63128)[0m mae:  0.09705330431461334
[2m[36m(func pid=63128)[0m rmse_per_class: [0.071, 0.213, 0.103, 0.307, 0.092, 0.155, 0.213, 0.12, 0.193, 0.149]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8159 | Steps: 4 | Val loss: 0.6287 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3134 | Steps: 4 | Val loss: 0.2638 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5432 | Steps: 4 | Val loss: 0.3979 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2735 | Steps: 4 | Val loss: 0.2860 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=70360)[0m rmse: 0.17958129942417145
[2m[36m(func pid=70360)[0m mae:  0.13187167048454285
[2m[36m(func pid=70360)[0m rmse_per_class: [0.114, 0.261, 0.099, 0.336, 0.111, 0.19, 0.295, 0.14, 0.142, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14389023184776306
[2m[36m(func pid=63038)[0m mae:  0.09743436425924301
[2m[36m(func pid=63038)[0m rmse_per_class: [0.079, 0.225, 0.06, 0.291, 0.071, 0.152, 0.22, 0.1, 0.138, 0.103]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:26 (running for 00:29:43.88)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.313 |  0.144 |                   38 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.299 |  0.162 |                   39 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.816 |  0.18  |                   10 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.543 |  0.178 |                    8 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17771366238594055
[2m[36m(func pid=71112)[0m mae:  0.1301528960466385
[2m[36m(func pid=71112)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.333, 0.098, 0.191, 0.287, 0.138, 0.145, 0.11]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.16197092831134796
[2m[36m(func pid=63128)[0m mae:  0.09857401251792908
[2m[36m(func pid=63128)[0m rmse_per_class: [0.089, 0.225, 0.046, 0.278, 0.188, 0.185, 0.257, 0.113, 0.128, 0.112]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8029 | Steps: 4 | Val loss: 0.6162 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3116 | Steps: 4 | Val loss: 0.2601 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4953 | Steps: 4 | Val loss: 0.3688 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2624 | Steps: 4 | Val loss: 0.2879 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=70360)[0m rmse: 0.1793973445892334
[2m[36m(func pid=70360)[0m mae:  0.13172557950019836
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14094766974449158
[2m[36m(func pid=63038)[0m mae:  0.09528549015522003
[2m[36m(func pid=63038)[0m rmse_per_class: [0.08, 0.213, 0.053, 0.282, 0.066, 0.153, 0.222, 0.099, 0.136, 0.105]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:31 (running for 00:29:49.35)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.312 |  0.141 |                   39 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.274 |  0.162 |                   40 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.803 |  0.179 |                   11 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.495 |  0.178 |                    9 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17780368030071259
[2m[36m(func pid=71112)[0m mae:  0.13028490543365479
[2m[36m(func pid=71112)[0m rmse_per_class: [0.118, 0.261, 0.097, 0.333, 0.094, 0.19, 0.287, 0.138, 0.148, 0.112]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.163302943110466
[2m[36m(func pid=63128)[0m mae:  0.09946344792842865
[2m[36m(func pid=63128)[0m rmse_per_class: [0.089, 0.241, 0.042, 0.284, 0.193, 0.186, 0.26, 0.111, 0.125, 0.102]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7846 | Steps: 4 | Val loss: 0.6033 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3046 | Steps: 4 | Val loss: 0.2578 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4580 | Steps: 4 | Val loss: 0.3459 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2548 | Steps: 4 | Val loss: 0.2873 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=70360)[0m rmse: 0.17947730422019958
[2m[36m(func pid=70360)[0m mae:  0.13174179196357727
[2m[36m(func pid=70360)[0m rmse_per_class: [0.114, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13851343095302582
[2m[36m(func pid=63038)[0m mae:  0.09326647222042084
[2m[36m(func pid=63038)[0m rmse_per_class: [0.079, 0.211, 0.044, 0.281, 0.069, 0.151, 0.224, 0.1, 0.129, 0.097]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:36 (running for 00:29:54.68)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.305 |  0.139 |                   40 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.262 |  0.163 |                   41 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.785 |  0.179 |                   12 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.177 |                   10 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17738641798496246
[2m[36m(func pid=71112)[0m mae:  0.12994447350502014
[2m[36m(func pid=71112)[0m rmse_per_class: [0.119, 0.262, 0.094, 0.333, 0.091, 0.19, 0.285, 0.137, 0.148, 0.114]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.16366228461265564
[2m[36m(func pid=63128)[0m mae:  0.10216863453388214
[2m[36m(func pid=63128)[0m rmse_per_class: [0.076, 0.234, 0.034, 0.296, 0.185, 0.193, 0.258, 0.112, 0.125, 0.123]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7704 | Steps: 4 | Val loss: 0.5917 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3132 | Steps: 4 | Val loss: 0.2599 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4280 | Steps: 4 | Val loss: 0.3295 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2771 | Steps: 4 | Val loss: 0.2877 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=70360)[0m rmse: 0.17935976386070251
[2m[36m(func pid=70360)[0m mae:  0.13163113594055176
[2m[36m(func pid=70360)[0m rmse_per_class: [0.114, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14088845252990723
[2m[36m(func pid=63038)[0m mae:  0.09469373524188995
[2m[36m(func pid=63038)[0m rmse_per_class: [0.084, 0.213, 0.05, 0.283, 0.069, 0.152, 0.225, 0.1, 0.129, 0.103]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:42 (running for 00:30:00.11)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.313 |  0.141 |                   41 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.255 |  0.164 |                   42 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.77  |  0.179 |                   13 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.428 |  0.177 |                   11 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17718346416950226
[2m[36m(func pid=71112)[0m mae:  0.1296929270029068
[2m[36m(func pid=71112)[0m rmse_per_class: [0.119, 0.262, 0.094, 0.332, 0.088, 0.191, 0.284, 0.138, 0.148, 0.116]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15838351845741272
[2m[36m(func pid=63128)[0m mae:  0.09823603928089142
[2m[36m(func pid=63128)[0m rmse_per_class: [0.076, 0.213, 0.054, 0.331, 0.106, 0.175, 0.226, 0.118, 0.161, 0.125]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7539 | Steps: 4 | Val loss: 0.5764 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2973 | Steps: 4 | Val loss: 0.2615 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4139 | Steps: 4 | Val loss: 0.3184 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2755 | Steps: 4 | Val loss: 0.2941 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=70360)[0m rmse: 0.17914414405822754
[2m[36m(func pid=70360)[0m mae:  0.13146981596946716
[2m[36m(func pid=70360)[0m rmse_per_class: [0.114, 0.261, 0.098, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1423598825931549
[2m[36m(func pid=63038)[0m mae:  0.09569146484136581
[2m[36m(func pid=63038)[0m rmse_per_class: [0.082, 0.215, 0.056, 0.288, 0.073, 0.152, 0.222, 0.098, 0.133, 0.104]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:47 (running for 00:30:05.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.297 |  0.142 |                   42 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.277 |  0.158 |                   43 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.754 |  0.179 |                   14 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.414 |  0.176 |                   12 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17595937848091125
[2m[36m(func pid=71112)[0m mae:  0.12859013676643372
[2m[36m(func pid=71112)[0m rmse_per_class: [0.119, 0.262, 0.093, 0.33, 0.084, 0.191, 0.281, 0.137, 0.148, 0.116]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1556852012872696
[2m[36m(func pid=63128)[0m mae:  0.09531703591346741
[2m[36m(func pid=63128)[0m rmse_per_class: [0.069, 0.219, 0.037, 0.334, 0.074, 0.17, 0.194, 0.142, 0.217, 0.102]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7354 | Steps: 4 | Val loss: 0.5638 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2970 | Steps: 4 | Val loss: 0.2599 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4056 | Steps: 4 | Val loss: 0.3150 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2671 | Steps: 4 | Val loss: 0.2805 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=70360)[0m rmse: 0.17905738949775696
[2m[36m(func pid=70360)[0m mae:  0.13139767944812775
[2m[36m(func pid=70360)[0m rmse_per_class: [0.114, 0.261, 0.098, 0.335, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14091184735298157
[2m[36m(func pid=63038)[0m mae:  0.09506284445524216
[2m[36m(func pid=63038)[0m rmse_per_class: [0.081, 0.214, 0.052, 0.282, 0.073, 0.153, 0.223, 0.097, 0.134, 0.1]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:53 (running for 00:30:10.80)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.297 |  0.141 |                   43 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.276 |  0.156 |                   44 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.735 |  0.179 |                   15 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.406 |  0.175 |                   13 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17507846653461456
[2m[36m(func pid=71112)[0m mae:  0.12783846259117126
[2m[36m(func pid=71112)[0m rmse_per_class: [0.118, 0.261, 0.091, 0.328, 0.081, 0.191, 0.279, 0.137, 0.148, 0.117]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15166763961315155
[2m[36m(func pid=63128)[0m mae:  0.09136998653411865
[2m[36m(func pid=63128)[0m rmse_per_class: [0.068, 0.208, 0.028, 0.314, 0.081, 0.162, 0.19, 0.146, 0.228, 0.092]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7164 | Steps: 4 | Val loss: 0.5479 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3179 | Steps: 4 | Val loss: 0.2590 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4063 | Steps: 4 | Val loss: 0.3162 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=70360)[0m rmse: 0.17873962223529816
[2m[36m(func pid=70360)[0m mae:  0.1310909390449524
[2m[36m(func pid=70360)[0m rmse_per_class: [0.114, 0.26, 0.097, 0.335, 0.109, 0.191, 0.292, 0.139, 0.142, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2563 | Steps: 4 | Val loss: 0.2701 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=63038)[0m rmse: 0.13947363197803497
[2m[36m(func pid=63038)[0m mae:  0.09432214498519897
[2m[36m(func pid=63038)[0m rmse_per_class: [0.082, 0.212, 0.051, 0.282, 0.074, 0.152, 0.224, 0.095, 0.128, 0.094]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:19:58 (running for 00:30:16.02)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.318 |  0.139 |                   44 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.267 |  0.152 |                   45 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.716 |  0.179 |                   16 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.406 |  0.174 |                   14 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.1738767921924591
[2m[36m(func pid=71112)[0m mae:  0.12675787508487701
[2m[36m(func pid=71112)[0m rmse_per_class: [0.118, 0.261, 0.089, 0.327, 0.078, 0.191, 0.276, 0.136, 0.148, 0.115]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14727869629859924
[2m[36m(func pid=63128)[0m mae:  0.08978471159934998
[2m[36m(func pid=63128)[0m rmse_per_class: [0.069, 0.201, 0.036, 0.3, 0.09, 0.168, 0.209, 0.113, 0.2, 0.086]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7038 | Steps: 4 | Val loss: 0.5316 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3122 | Steps: 4 | Val loss: 0.2654 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4038 | Steps: 4 | Val loss: 0.3201 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=70360)[0m rmse: 0.17857161164283752
[2m[36m(func pid=70360)[0m mae:  0.13091905415058136
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.335, 0.108, 0.19, 0.292, 0.139, 0.143, 0.107]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2475 | Steps: 4 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=63038)[0m rmse: 0.14469346404075623
[2m[36m(func pid=63038)[0m mae:  0.09817836433649063
[2m[36m(func pid=63038)[0m rmse_per_class: [0.082, 0.218, 0.062, 0.296, 0.076, 0.155, 0.226, 0.095, 0.136, 0.1]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:20:03 (running for 00:30:21.34)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.312 |  0.145 |                   45 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.256 |  0.147 |                   46 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.704 |  0.179 |                   17 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.404 |  0.173 |                   15 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.1725267469882965
[2m[36m(func pid=71112)[0m mae:  0.12559883296489716
[2m[36m(func pid=71112)[0m rmse_per_class: [0.117, 0.26, 0.087, 0.325, 0.075, 0.191, 0.273, 0.135, 0.146, 0.116]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15111789107322693
[2m[36m(func pid=63128)[0m mae:  0.09235037863254547
[2m[36m(func pid=63128)[0m rmse_per_class: [0.08, 0.204, 0.028, 0.279, 0.088, 0.189, 0.246, 0.125, 0.184, 0.089]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6832 | Steps: 4 | Val loss: 0.5201 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2878 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4203 | Steps: 4 | Val loss: 0.3256 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=70360)[0m rmse: 0.17863266170024872
[2m[36m(func pid=70360)[0m mae:  0.1309976726770401
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.108, 0.19, 0.292, 0.139, 0.142, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2840 | Steps: 4 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=63038)[0m rmse: 0.1439944952726364
[2m[36m(func pid=63038)[0m mae:  0.09787384420633316
[2m[36m(func pid=63038)[0m rmse_per_class: [0.081, 0.218, 0.053, 0.29, 0.071, 0.155, 0.224, 0.096, 0.142, 0.109]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:20:08 (running for 00:30:26.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.288 |  0.144 |                   46 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.247 |  0.151 |                   47 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.683 |  0.179 |                   18 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.42  |  0.17  |                   16 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.17045298218727112
[2m[36m(func pid=71112)[0m mae:  0.12384726107120514
[2m[36m(func pid=71112)[0m rmse_per_class: [0.115, 0.258, 0.082, 0.323, 0.072, 0.19, 0.269, 0.134, 0.147, 0.115]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15259140729904175
[2m[36m(func pid=63128)[0m mae:  0.09242856502532959
[2m[36m(func pid=63128)[0m rmse_per_class: [0.093, 0.204, 0.025, 0.267, 0.11, 0.183, 0.273, 0.148, 0.129, 0.094]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6689 | Steps: 4 | Val loss: 0.5069 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3015 | Steps: 4 | Val loss: 0.2563 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4247 | Steps: 4 | Val loss: 0.3343 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=70360)[0m rmse: 0.1785840392112732
[2m[36m(func pid=70360)[0m mae:  0.13095924258232117
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.107, 0.19, 0.292, 0.139, 0.142, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2626 | Steps: 4 | Val loss: 0.2768 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=63038)[0m rmse: 0.1373613029718399
[2m[36m(func pid=63038)[0m mae:  0.09164537489414215
[2m[36m(func pid=63038)[0m rmse_per_class: [0.079, 0.208, 0.037, 0.272, 0.07, 0.154, 0.229, 0.1, 0.126, 0.1]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.16997729241847992
[2m[36m(func pid=71112)[0m mae:  0.1234750896692276
[2m[36m(func pid=71112)[0m rmse_per_class: [0.115, 0.259, 0.082, 0.322, 0.069, 0.189, 0.269, 0.132, 0.147, 0.117]
[2m[36m(func pid=71112)[0m 
== Status ==
Current time: 2024-01-07 20:20:14 (running for 00:30:32.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.301 |  0.137 |                   47 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.284 |  0.153 |                   48 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.669 |  0.179 |                   19 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.425 |  0.17  |                   17 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1558023989200592
[2m[36m(func pid=63128)[0m mae:  0.0945475772023201
[2m[36m(func pid=63128)[0m rmse_per_class: [0.072, 0.2, 0.041, 0.283, 0.114, 0.169, 0.254, 0.167, 0.159, 0.098]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6483 | Steps: 4 | Val loss: 0.4922 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2935 | Steps: 4 | Val loss: 0.2550 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4337 | Steps: 4 | Val loss: 0.3434 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=70360)[0m rmse: 0.17833378911018372
[2m[36m(func pid=70360)[0m mae:  0.13071474432945251
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.106, 0.19, 0.292, 0.139, 0.143, 0.107]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2808 | Steps: 4 | Val loss: 0.2629 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=63038)[0m rmse: 0.1363784670829773
[2m[36m(func pid=63038)[0m mae:  0.09074950218200684
[2m[36m(func pid=63038)[0m rmse_per_class: [0.079, 0.209, 0.037, 0.269, 0.069, 0.15, 0.229, 0.103, 0.126, 0.093]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:20:19 (running for 00:30:37.68)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.294 |  0.136 |                   48 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.263 |  0.156 |                   49 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.648 |  0.178 |                   20 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.434 |  0.168 |                   18 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.16840039193630219
[2m[36m(func pid=71112)[0m mae:  0.12209081649780273
[2m[36m(func pid=71112)[0m rmse_per_class: [0.115, 0.258, 0.079, 0.319, 0.067, 0.188, 0.265, 0.131, 0.145, 0.116]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14661067724227905
[2m[36m(func pid=63128)[0m mae:  0.08714468032121658
[2m[36m(func pid=63128)[0m rmse_per_class: [0.066, 0.205, 0.055, 0.275, 0.111, 0.167, 0.201, 0.151, 0.151, 0.084]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6322 | Steps: 4 | Val loss: 0.4805 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2946 | Steps: 4 | Val loss: 0.2565 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=70360)[0m rmse: 0.17812006175518036
[2m[36m(func pid=70360)[0m mae:  0.13057611882686615
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.105, 0.19, 0.292, 0.139, 0.143, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4431 | Steps: 4 | Val loss: 0.3535 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2878 | Steps: 4 | Val loss: 0.2624 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=63038)[0m rmse: 0.13809734582901
[2m[36m(func pid=63038)[0m mae:  0.09207640588283539
[2m[36m(func pid=63038)[0m rmse_per_class: [0.08, 0.213, 0.046, 0.273, 0.069, 0.147, 0.224, 0.099, 0.132, 0.098]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:20:25 (running for 00:30:43.12)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.295 |  0.138 |                   49 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.281 |  0.147 |                   50 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.632 |  0.178 |                   21 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.443 |  0.168 |                   19 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.16767999529838562
[2m[36m(func pid=71112)[0m mae:  0.12150420248508453
[2m[36m(func pid=71112)[0m rmse_per_class: [0.116, 0.258, 0.077, 0.318, 0.065, 0.187, 0.264, 0.13, 0.145, 0.116]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14380314946174622
[2m[36m(func pid=63128)[0m mae:  0.08594496548175812
[2m[36m(func pid=63128)[0m rmse_per_class: [0.069, 0.213, 0.057, 0.278, 0.102, 0.163, 0.194, 0.137, 0.133, 0.092]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6153 | Steps: 4 | Val loss: 0.4692 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3017 | Steps: 4 | Val loss: 0.2614 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=70360)[0m rmse: 0.17810603976249695
[2m[36m(func pid=70360)[0m mae:  0.13059669733047485
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.259, 0.095, 0.335, 0.104, 0.19, 0.291, 0.139, 0.143, 0.107]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4493 | Steps: 4 | Val loss: 0.3593 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2547 | Steps: 4 | Val loss: 0.2810 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=63038)[0m rmse: 0.14249339699745178
[2m[36m(func pid=63038)[0m mae:  0.09504085779190063
[2m[36m(func pid=63038)[0m rmse_per_class: [0.075, 0.218, 0.057, 0.286, 0.071, 0.147, 0.222, 0.103, 0.141, 0.105]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15102483332157135
[2m[36m(func pid=63128)[0m mae:  0.09288695454597473
[2m[36m(func pid=63128)[0m rmse_per_class: [0.075, 0.211, 0.062, 0.326, 0.074, 0.155, 0.211, 0.161, 0.129, 0.107]
== Status ==
Current time: 2024-01-07 20:20:30 (running for 00:30:48.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.302 |  0.142 |                   50 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.255 |  0.151 |                   52 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.615 |  0.178 |                   22 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.443 |  0.168 |                   19 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.16532164812088013
[2m[36m(func pid=71112)[0m mae:  0.11938520520925522
[2m[36m(func pid=71112)[0m rmse_per_class: [0.114, 0.256, 0.073, 0.315, 0.063, 0.187, 0.259, 0.128, 0.144, 0.114]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6019 | Steps: 4 | Val loss: 0.4581 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2957 | Steps: 4 | Val loss: 0.2676 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=70360)[0m rmse: 0.17811641097068787
[2m[36m(func pid=70360)[0m mae:  0.13059642910957336
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.334, 0.104, 0.19, 0.292, 0.139, 0.143, 0.108]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2743 | Steps: 4 | Val loss: 0.2816 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4557 | Steps: 4 | Val loss: 0.3667 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=63038)[0m rmse: 0.14757589995861053
[2m[36m(func pid=63038)[0m mae:  0.09967706352472305
[2m[36m(func pid=63038)[0m rmse_per_class: [0.075, 0.214, 0.057, 0.284, 0.068, 0.161, 0.23, 0.102, 0.168, 0.117]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:20:36 (running for 00:30:53.82)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.296 |  0.148 |                   51 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.274 |  0.155 |                   53 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.602 |  0.178 |                   23 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.449 |  0.165 |                   20 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.15453290939331055
[2m[36m(func pid=63128)[0m mae:  0.09483976662158966
[2m[36m(func pid=63128)[0m rmse_per_class: [0.077, 0.208, 0.053, 0.323, 0.066, 0.17, 0.222, 0.173, 0.135, 0.119]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1637841761112213
[2m[36m(func pid=71112)[0m mae:  0.11803863197565079
[2m[36m(func pid=71112)[0m rmse_per_class: [0.114, 0.256, 0.07, 0.312, 0.061, 0.185, 0.257, 0.127, 0.144, 0.112]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5874 | Steps: 4 | Val loss: 0.4476 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2978 | Steps: 4 | Val loss: 0.2604 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=70360)[0m rmse: 0.1780201941728592
[2m[36m(func pid=70360)[0m mae:  0.13049717247486115
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.334, 0.101, 0.19, 0.292, 0.138, 0.145, 0.109]
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2547 | Steps: 4 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4660 | Steps: 4 | Val loss: 0.3720 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=63038)[0m rmse: 0.14121678471565247
[2m[36m(func pid=63038)[0m mae:  0.09516477584838867
[2m[36m(func pid=63038)[0m rmse_per_class: [0.076, 0.208, 0.046, 0.277, 0.068, 0.155, 0.228, 0.099, 0.154, 0.101]
[2m[36m(func pid=63038)[0m 
== Status ==
Current time: 2024-01-07 20:20:41 (running for 00:30:59.11)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.298 |  0.141 |                   52 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.255 |  0.158 |                   54 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.587 |  0.178 |                   24 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.456 |  0.164 |                   21 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.15792734920978546
[2m[36m(func pid=63128)[0m mae:  0.0949961394071579
[2m[36m(func pid=63128)[0m rmse_per_class: [0.086, 0.215, 0.067, 0.307, 0.063, 0.167, 0.223, 0.181, 0.156, 0.114]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.16223907470703125
[2m[36m(func pid=71112)[0m mae:  0.11666754633188248
[2m[36m(func pid=71112)[0m rmse_per_class: [0.113, 0.254, 0.065, 0.308, 0.06, 0.185, 0.256, 0.125, 0.145, 0.112]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5705 | Steps: 4 | Val loss: 0.4346 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2850 | Steps: 4 | Val loss: 0.2572 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2553 | Steps: 4 | Val loss: 0.2804 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=70360)[0m rmse: 0.1779102385044098
[2m[36m(func pid=70360)[0m mae:  0.13042810559272766
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.334, 0.102, 0.19, 0.291, 0.139, 0.144, 0.109]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4731 | Steps: 4 | Val loss: 0.3771 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:20:46 (running for 00:31:04.13)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.285 |  0.138 |                   53 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.255 |  0.158 |                   54 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.571 |  0.178 |                   25 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.466 |  0.162 |                   22 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m rmse: 0.13796107470989227
[2m[36m(func pid=63038)[0m mae:  0.09244836121797562
[2m[36m(func pid=63038)[0m rmse_per_class: [0.077, 0.208, 0.039, 0.278, 0.072, 0.155, 0.232, 0.096, 0.13, 0.092]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1563587188720703
[2m[36m(func pid=63128)[0m mae:  0.0934298038482666
[2m[36m(func pid=63128)[0m rmse_per_class: [0.096, 0.218, 0.067, 0.311, 0.064, 0.165, 0.216, 0.164, 0.161, 0.102]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.16089291870594025
[2m[36m(func pid=71112)[0m mae:  0.11531702429056168
[2m[36m(func pid=71112)[0m rmse_per_class: [0.112, 0.254, 0.062, 0.305, 0.059, 0.185, 0.253, 0.124, 0.145, 0.111]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5602 | Steps: 4 | Val loss: 0.4243 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2830 | Steps: 4 | Val loss: 0.2561 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2459 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=70360)[0m rmse: 0.1777530163526535
[2m[36m(func pid=70360)[0m mae:  0.13028104603290558
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.096, 0.334, 0.101, 0.19, 0.291, 0.138, 0.144, 0.11]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4775 | Steps: 4 | Val loss: 0.3791 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 20:20:52 (running for 00:31:09.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.283 |  0.137 |                   54 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.255 |  0.156 |                   55 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.56  |  0.178 |                   26 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.473 |  0.161 |                   23 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m rmse: 0.1368333101272583
[2m[36m(func pid=63038)[0m mae:  0.0912294015288353
[2m[36m(func pid=63038)[0m rmse_per_class: [0.074, 0.206, 0.04, 0.283, 0.072, 0.15, 0.223, 0.099, 0.129, 0.093]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14435210824012756
[2m[36m(func pid=63128)[0m mae:  0.08641668409109116
[2m[36m(func pid=63128)[0m rmse_per_class: [0.08, 0.21, 0.03, 0.268, 0.078, 0.178, 0.213, 0.121, 0.171, 0.095]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.15948131680488586
[2m[36m(func pid=71112)[0m mae:  0.11392940580844879
[2m[36m(func pid=71112)[0m rmse_per_class: [0.111, 0.252, 0.059, 0.303, 0.057, 0.185, 0.25, 0.123, 0.145, 0.11]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5479 | Steps: 4 | Val loss: 0.4157 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=70360)[0m rmse: 0.17782831192016602
[2m[36m(func pid=70360)[0m mae:  0.13035361468791962
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.096, 0.334, 0.101, 0.19, 0.292, 0.139, 0.144, 0.11]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2894 | Steps: 4 | Val loss: 0.2587 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2367 | Steps: 4 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4824 | Steps: 4 | Val loss: 0.3786 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5337 | Steps: 4 | Val loss: 0.4059 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 20:20:57 (running for 00:31:15.33)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.289 |  0.14  |                   55 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.246 |  0.144 |                   56 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.548 |  0.178 |                   27 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.478 |  0.159 |                   24 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m rmse: 0.13953132927417755
[2m[36m(func pid=63038)[0m mae:  0.09216384589672089
[2m[36m(func pid=63038)[0m rmse_per_class: [0.078, 0.202, 0.057, 0.294, 0.07, 0.146, 0.216, 0.104, 0.129, 0.1]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15517064929008484
[2m[36m(func pid=63128)[0m mae:  0.09248016774654388
[2m[36m(func pid=63128)[0m rmse_per_class: [0.105, 0.215, 0.053, 0.27, 0.125, 0.183, 0.23, 0.121, 0.155, 0.093]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1573575884103775
[2m[36m(func pid=71112)[0m mae:  0.11192603409290314
[2m[36m(func pid=71112)[0m rmse_per_class: [0.109, 0.25, 0.055, 0.298, 0.057, 0.183, 0.247, 0.124, 0.143, 0.108]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17779485881328583
[2m[36m(func pid=70360)[0m mae:  0.13031184673309326
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.096, 0.333, 0.1, 0.19, 0.291, 0.139, 0.144, 0.11]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2966 | Steps: 4 | Val loss: 0.2597 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2702 | Steps: 4 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4789 | Steps: 4 | Val loss: 0.3783 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 20:21:03 (running for 00:31:20.73)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.289 |  0.14  |                   55 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.27  |  0.154 |                   58 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.534 |  0.178 |                   28 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.482 |  0.157 |                   25 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1537250280380249
[2m[36m(func pid=63128)[0m mae:  0.09154842048883438
[2m[36m(func pid=63128)[0m rmse_per_class: [0.088, 0.212, 0.047, 0.285, 0.151, 0.165, 0.221, 0.114, 0.155, 0.099]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14064863324165344
[2m[36m(func pid=63038)[0m mae:  0.09322404861450195
[2m[36m(func pid=63038)[0m rmse_per_class: [0.082, 0.2, 0.061, 0.292, 0.068, 0.152, 0.216, 0.096, 0.135, 0.104]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5192 | Steps: 4 | Val loss: 0.3962 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=71112)[0m rmse: 0.15636661648750305
[2m[36m(func pid=71112)[0m mae:  0.11113104969263077
[2m[36m(func pid=71112)[0m rmse_per_class: [0.108, 0.25, 0.053, 0.296, 0.056, 0.18, 0.248, 0.123, 0.143, 0.107]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1776123195886612
[2m[36m(func pid=70360)[0m mae:  0.13022807240486145
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.095, 0.334, 0.099, 0.19, 0.29, 0.138, 0.144, 0.111]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2592 | Steps: 4 | Val loss: 0.2839 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2910 | Steps: 4 | Val loss: 0.2608 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4696 | Steps: 4 | Val loss: 0.3782 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 20:21:08 (running for 00:31:26.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.291 |  0.142 |                   57 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.27  |  0.154 |                   58 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.519 |  0.178 |                   29 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.479 |  0.156 |                   26 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m rmse: 0.14153733849525452
[2m[36m(func pid=63038)[0m mae:  0.09446068108081818
[2m[36m(func pid=63038)[0m rmse_per_class: [0.078, 0.201, 0.052, 0.282, 0.066, 0.16, 0.222, 0.095, 0.142, 0.118]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15564517676830292
[2m[36m(func pid=63128)[0m mae:  0.09366900473833084
[2m[36m(func pid=63128)[0m rmse_per_class: [0.078, 0.197, 0.04, 0.33, 0.141, 0.159, 0.218, 0.121, 0.175, 0.098]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5128 | Steps: 4 | Val loss: 0.3872 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=71112)[0m rmse: 0.15474840998649597
[2m[36m(func pid=71112)[0m mae:  0.10975436121225357
[2m[36m(func pid=71112)[0m rmse_per_class: [0.107, 0.247, 0.05, 0.291, 0.055, 0.179, 0.246, 0.123, 0.142, 0.105]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17742212116718292
[2m[36m(func pid=70360)[0m mae:  0.1300695240497589
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.257, 0.095, 0.333, 0.098, 0.19, 0.29, 0.138, 0.144, 0.111]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2734 | Steps: 4 | Val loss: 0.2550 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2557 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4797 | Steps: 4 | Val loss: 0.3777 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4999 | Steps: 4 | Val loss: 0.3809 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 20:21:13 (running for 00:31:31.67)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.136 |                   58 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.259 |  0.156 |                   59 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.513 |  0.177 |                   30 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.47  |  0.155 |                   27 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m rmse: 0.13636405766010284
[2m[36m(func pid=63038)[0m mae:  0.09025944769382477
[2m[36m(func pid=63038)[0m rmse_per_class: [0.074, 0.201, 0.041, 0.271, 0.066, 0.156, 0.216, 0.095, 0.133, 0.11]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.1700378954410553
[2m[36m(func pid=63128)[0m mae:  0.10496538877487183
[2m[36m(func pid=63128)[0m rmse_per_class: [0.077, 0.212, 0.099, 0.349, 0.134, 0.17, 0.217, 0.121, 0.21, 0.112]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.15294554829597473
[2m[36m(func pid=71112)[0m mae:  0.10797200351953506
[2m[36m(func pid=71112)[0m rmse_per_class: [0.106, 0.246, 0.048, 0.287, 0.055, 0.177, 0.244, 0.123, 0.14, 0.103]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1771484762430191
[2m[36m(func pid=70360)[0m mae:  0.12981192767620087
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.095, 0.333, 0.096, 0.19, 0.289, 0.138, 0.145, 0.11]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2592 | Steps: 4 | Val loss: 0.2841 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2965 | Steps: 4 | Val loss: 0.2520 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4771 | Steps: 4 | Val loss: 0.3709 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=63128)[0m rmse: 0.15845561027526855
[2m[36m(func pid=63128)[0m mae:  0.09753963351249695
[2m[36m(func pid=63128)[0m rmse_per_class: [0.077, 0.248, 0.056, 0.303, 0.121, 0.186, 0.207, 0.108, 0.179, 0.099]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:21:19 (running for 00:31:36.96)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.136 |                   58 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.259 |  0.158 |                   61 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.5   |  0.177 |                   31 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.48  |  0.153 |                   28 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4853 | Steps: 4 | Val loss: 0.3741 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=63038)[0m rmse: 0.13424018025398254
[2m[36m(func pid=63038)[0m mae:  0.08781126886606216
[2m[36m(func pid=63038)[0m rmse_per_class: [0.073, 0.202, 0.041, 0.268, 0.068, 0.148, 0.211, 0.102, 0.122, 0.106]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.15214236080646515
[2m[36m(func pid=71112)[0m mae:  0.10716257989406586
[2m[36m(func pid=71112)[0m rmse_per_class: [0.104, 0.245, 0.046, 0.287, 0.055, 0.176, 0.243, 0.124, 0.14, 0.103]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17715519666671753
[2m[36m(func pid=70360)[0m mae:  0.12975983321666718
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.258, 0.096, 0.333, 0.096, 0.19, 0.289, 0.137, 0.144, 0.11]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2481 | Steps: 4 | Val loss: 0.2567 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2841 | Steps: 4 | Val loss: 0.2538 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4671 | Steps: 4 | Val loss: 0.3673 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:21:24 (running for 00:31:42.22)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.297 |  0.134 |                   59 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.248 |  0.139 |                   62 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.485 |  0.177 |                   32 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.477 |  0.152 |                   29 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13862311840057373
[2m[36m(func pid=63128)[0m mae:  0.08397026360034943
[2m[36m(func pid=63128)[0m rmse_per_class: [0.071, 0.21, 0.028, 0.252, 0.109, 0.172, 0.217, 0.112, 0.139, 0.076]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4759 | Steps: 4 | Val loss: 0.3684 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=63038)[0m rmse: 0.13646773993968964
[2m[36m(func pid=63038)[0m mae:  0.08941769599914551
[2m[36m(func pid=63038)[0m rmse_per_class: [0.075, 0.203, 0.048, 0.266, 0.066, 0.153, 0.211, 0.1, 0.13, 0.113]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.15120500326156616
[2m[36m(func pid=71112)[0m mae:  0.10624042898416519
[2m[36m(func pid=71112)[0m rmse_per_class: [0.102, 0.244, 0.045, 0.286, 0.055, 0.175, 0.24, 0.123, 0.139, 0.104]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17714908719062805
[2m[36m(func pid=70360)[0m mae:  0.12981660664081573
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.259, 0.096, 0.332, 0.095, 0.19, 0.289, 0.137, 0.145, 0.111]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2607 | Steps: 4 | Val loss: 0.2655 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2895 | Steps: 4 | Val loss: 0.2509 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4618 | Steps: 4 | Val loss: 0.3645 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4714 | Steps: 4 | Val loss: 0.3626 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:21:30 (running for 00:31:47.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.284 |  0.136 |                   60 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.261 |  0.147 |                   63 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.476 |  0.177 |                   33 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.467 |  0.151 |                   30 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14650319516658783
[2m[36m(func pid=63128)[0m mae:  0.09099896997213364
[2m[36m(func pid=63128)[0m rmse_per_class: [0.073, 0.214, 0.029, 0.27, 0.106, 0.173, 0.242, 0.115, 0.147, 0.094]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1335252821445465
[2m[36m(func pid=63038)[0m mae:  0.08786804229021072
[2m[36m(func pid=63038)[0m rmse_per_class: [0.072, 0.204, 0.048, 0.261, 0.068, 0.148, 0.213, 0.095, 0.125, 0.101]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.15044234693050385
[2m[36m(func pid=71112)[0m mae:  0.10552306473255157
[2m[36m(func pid=71112)[0m rmse_per_class: [0.102, 0.241, 0.044, 0.286, 0.054, 0.176, 0.239, 0.121, 0.139, 0.103]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17725230753421783
[2m[36m(func pid=70360)[0m mae:  0.12984833121299744
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.259, 0.097, 0.331, 0.094, 0.191, 0.289, 0.137, 0.145, 0.112]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2513 | Steps: 4 | Val loss: 0.2810 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2956 | Steps: 4 | Val loss: 0.2510 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4655 | Steps: 4 | Val loss: 0.3587 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4626 | Steps: 4 | Val loss: 0.3570 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:21:35 (running for 00:31:53.31)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.296 |  0.134 |                   62 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.261 |  0.147 |                   63 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.471 |  0.177 |                   34 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.462 |  0.15  |                   31 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m rmse: 0.133775532245636
[2m[36m(func pid=63038)[0m mae:  0.08792103826999664
[2m[36m(func pid=63038)[0m rmse_per_class: [0.072, 0.21, 0.045, 0.26, 0.067, 0.15, 0.208, 0.095, 0.129, 0.102]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15708783268928528
[2m[36m(func pid=63128)[0m mae:  0.09754207730293274
[2m[36m(func pid=63128)[0m rmse_per_class: [0.079, 0.215, 0.051, 0.315, 0.103, 0.168, 0.24, 0.114, 0.151, 0.134]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14964208006858826
[2m[36m(func pid=71112)[0m mae:  0.1047564148902893
[2m[36m(func pid=71112)[0m rmse_per_class: [0.1, 0.241, 0.041, 0.286, 0.054, 0.175, 0.236, 0.122, 0.138, 0.103]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1773688793182373
[2m[36m(func pid=70360)[0m mae:  0.12996205687522888
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.26, 0.096, 0.332, 0.093, 0.19, 0.287, 0.138, 0.145, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2819 | Steps: 4 | Val loss: 0.2487 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2538 | Steps: 4 | Val loss: 0.2832 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4375 | Steps: 4 | Val loss: 0.3539 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 20:21:41 (running for 00:31:58.75)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.282 |  0.131 |                   63 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.251 |  0.157 |                   64 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.463 |  0.177 |                   35 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.466 |  0.15  |                   32 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m rmse: 0.13144537806510925
[2m[36m(func pid=63038)[0m mae:  0.0866989940404892
[2m[36m(func pid=63038)[0m rmse_per_class: [0.071, 0.202, 0.04, 0.252, 0.068, 0.152, 0.213, 0.093, 0.125, 0.098]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.15551526844501495
[2m[36m(func pid=63128)[0m mae:  0.09347172826528549
[2m[36m(func pid=63128)[0m rmse_per_class: [0.076, 0.231, 0.039, 0.329, 0.087, 0.164, 0.215, 0.113, 0.155, 0.146]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4570 | Steps: 4 | Val loss: 0.3527 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=71112)[0m rmse: 0.14908215403556824
[2m[36m(func pid=71112)[0m mae:  0.1042703241109848
[2m[36m(func pid=71112)[0m rmse_per_class: [0.1, 0.239, 0.04, 0.287, 0.054, 0.174, 0.235, 0.12, 0.138, 0.102]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17750194668769836
[2m[36m(func pid=70360)[0m mae:  0.13006970286369324
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.261, 0.096, 0.332, 0.092, 0.19, 0.287, 0.138, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2402 | Steps: 4 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2829 | Steps: 4 | Val loss: 0.2475 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4492 | Steps: 4 | Val loss: 0.3479 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:21:46 (running for 00:32:04.06)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.282 |  0.131 |                   63 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.15  |                   66 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.457 |  0.178 |                   36 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.438 |  0.149 |                   33 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.15007171034812927
[2m[36m(func pid=63128)[0m mae:  0.08977150171995163
[2m[36m(func pid=63128)[0m rmse_per_class: [0.068, 0.257, 0.028, 0.322, 0.081, 0.159, 0.209, 0.119, 0.142, 0.117]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4519 | Steps: 4 | Val loss: 0.3482 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=63038)[0m rmse: 0.1301613748073578
[2m[36m(func pid=63038)[0m mae:  0.08526618778705597
[2m[36m(func pid=63038)[0m rmse_per_class: [0.071, 0.202, 0.038, 0.251, 0.068, 0.151, 0.211, 0.094, 0.12, 0.095]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14810672402381897
[2m[36m(func pid=71112)[0m mae:  0.10316240787506104
[2m[36m(func pid=71112)[0m rmse_per_class: [0.099, 0.239, 0.039, 0.286, 0.054, 0.173, 0.234, 0.119, 0.137, 0.1]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1771572381258011
[2m[36m(func pid=70360)[0m mae:  0.12983056902885437
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.261, 0.094, 0.331, 0.092, 0.19, 0.287, 0.138, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2558 | Steps: 4 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2893 | Steps: 4 | Val loss: 0.2513 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4366 | Steps: 4 | Val loss: 0.3399 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:21:51 (running for 00:32:09.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.283 |  0.13  |                   64 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.256 |  0.146 |                   67 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.452 |  0.177 |                   37 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.449 |  0.148 |                   34 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1455235630273819
[2m[36m(func pid=63128)[0m mae:  0.08707897365093231
[2m[36m(func pid=63128)[0m rmse_per_class: [0.068, 0.258, 0.024, 0.312, 0.071, 0.161, 0.206, 0.11, 0.145, 0.1]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4455 | Steps: 4 | Val loss: 0.3425 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=63038)[0m rmse: 0.13405141234397888
[2m[36m(func pid=63038)[0m mae:  0.08781719952821732
[2m[36m(func pid=63038)[0m rmse_per_class: [0.075, 0.202, 0.044, 0.256, 0.066, 0.153, 0.215, 0.095, 0.129, 0.105]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14805260300636292
[2m[36m(func pid=71112)[0m mae:  0.10298329591751099
[2m[36m(func pid=71112)[0m rmse_per_class: [0.098, 0.239, 0.037, 0.287, 0.054, 0.173, 0.232, 0.121, 0.137, 0.101]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17698454856872559
[2m[36m(func pid=70360)[0m mae:  0.1297546923160553
[2m[36m(func pid=70360)[0m rmse_per_class: [0.12, 0.26, 0.092, 0.332, 0.091, 0.19, 0.286, 0.138, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2492 | Steps: 4 | Val loss: 0.2601 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2865 | Steps: 4 | Val loss: 0.2519 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4259 | Steps: 4 | Val loss: 0.3330 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:21:57 (running for 00:32:15.02)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.289 |  0.134 |                   65 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.249 |  0.14  |                   68 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.445 |  0.177 |                   38 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.437 |  0.148 |                   35 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14023004472255707
[2m[36m(func pid=63128)[0m mae:  0.08421797305345535
[2m[36m(func pid=63128)[0m rmse_per_class: [0.066, 0.215, 0.024, 0.29, 0.065, 0.16, 0.207, 0.123, 0.157, 0.095]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4407 | Steps: 4 | Val loss: 0.3385 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=71112)[0m rmse: 0.1477240025997162
[2m[36m(func pid=71112)[0m mae:  0.10260721296072006
[2m[36m(func pid=71112)[0m rmse_per_class: [0.098, 0.238, 0.036, 0.29, 0.054, 0.173, 0.23, 0.12, 0.137, 0.102]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1348438262939453
[2m[36m(func pid=63038)[0m mae:  0.08829494565725327
[2m[36m(func pid=63038)[0m rmse_per_class: [0.08, 0.206, 0.051, 0.265, 0.071, 0.149, 0.213, 0.094, 0.123, 0.097]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1767960488796234
[2m[36m(func pid=70360)[0m mae:  0.1295214593410492
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.261, 0.093, 0.331, 0.091, 0.19, 0.285, 0.138, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2441 | Steps: 4 | Val loss: 0.2564 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4231 | Steps: 4 | Val loss: 0.3248 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2782 | Steps: 4 | Val loss: 0.2571 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:22:02 (running for 00:32:20.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.286 |  0.135 |                   66 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.244 |  0.138 |                   69 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.441 |  0.177 |                   39 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.426 |  0.148 |                   36 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13833864033222198
[2m[36m(func pid=63128)[0m mae:  0.0828416645526886
[2m[36m(func pid=63128)[0m rmse_per_class: [0.075, 0.204, 0.025, 0.278, 0.081, 0.156, 0.203, 0.135, 0.141, 0.086]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4358 | Steps: 4 | Val loss: 0.3354 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=63038)[0m rmse: 0.1396758109331131
[2m[36m(func pid=63038)[0m mae:  0.09072761982679367
[2m[36m(func pid=63038)[0m rmse_per_class: [0.078, 0.211, 0.055, 0.28, 0.074, 0.147, 0.212, 0.099, 0.127, 0.112]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14654788374900818
[2m[36m(func pid=71112)[0m mae:  0.10138577222824097
[2m[36m(func pid=71112)[0m rmse_per_class: [0.095, 0.236, 0.035, 0.289, 0.054, 0.174, 0.227, 0.119, 0.137, 0.099]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1768859326839447
[2m[36m(func pid=70360)[0m mae:  0.12960362434387207
[2m[36m(func pid=70360)[0m rmse_per_class: [0.12, 0.261, 0.094, 0.331, 0.09, 0.19, 0.285, 0.138, 0.147, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2599 | Steps: 4 | Val loss: 0.2531 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4046 | Steps: 4 | Val loss: 0.3160 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2813 | Steps: 4 | Val loss: 0.2575 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4288 | Steps: 4 | Val loss: 0.3315 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 20:22:08 (running for 00:32:26.07)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.278 |  0.14  |                   67 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.26  |  0.137 |                   70 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.436 |  0.177 |                   40 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.423 |  0.147 |                   37 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13705530762672424
[2m[36m(func pid=63128)[0m mae:  0.0807400643825531
[2m[36m(func pid=63128)[0m rmse_per_class: [0.067, 0.207, 0.027, 0.261, 0.092, 0.153, 0.204, 0.137, 0.126, 0.096]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1464683562517166
[2m[36m(func pid=71112)[0m mae:  0.10103362798690796
[2m[36m(func pid=71112)[0m rmse_per_class: [0.094, 0.236, 0.035, 0.292, 0.054, 0.173, 0.227, 0.119, 0.136, 0.1]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13972529768943787
[2m[36m(func pid=63038)[0m mae:  0.09089086949825287
[2m[36m(func pid=63038)[0m rmse_per_class: [0.072, 0.208, 0.056, 0.279, 0.075, 0.147, 0.213, 0.098, 0.131, 0.118]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17655114829540253
[2m[36m(func pid=70360)[0m mae:  0.12929227948188782
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.26, 0.093, 0.331, 0.088, 0.19, 0.284, 0.138, 0.147, 0.115]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2621 | Steps: 4 | Val loss: 0.2584 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4020 | Steps: 4 | Val loss: 0.3108 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2752 | Steps: 4 | Val loss: 0.2570 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4233 | Steps: 4 | Val loss: 0.3286 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=63128)[0m rmse: 0.14242856204509735
[2m[36m(func pid=63128)[0m mae:  0.08378331363201141
[2m[36m(func pid=63128)[0m rmse_per_class: [0.065, 0.212, 0.035, 0.267, 0.103, 0.156, 0.209, 0.135, 0.126, 0.117]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:22:13 (running for 00:32:31.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.281 |  0.14  |                   68 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.262 |  0.142 |                   71 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.429 |  0.177 |                   41 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.405 |  0.146 |                   38 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.1459382325410843
[2m[36m(func pid=71112)[0m mae:  0.10019706189632416
[2m[36m(func pid=71112)[0m rmse_per_class: [0.093, 0.233, 0.034, 0.292, 0.054, 0.175, 0.226, 0.117, 0.135, 0.099]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13892370462417603
[2m[36m(func pid=63038)[0m mae:  0.0903981477022171
[2m[36m(func pid=63038)[0m rmse_per_class: [0.066, 0.206, 0.05, 0.276, 0.076, 0.149, 0.215, 0.098, 0.134, 0.119]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.176367849111557
[2m[36m(func pid=70360)[0m mae:  0.12907859683036804
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.26, 0.093, 0.33, 0.088, 0.19, 0.284, 0.138, 0.147, 0.115]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2402 | Steps: 4 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3847 | Steps: 4 | Val loss: 0.3058 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2750 | Steps: 4 | Val loss: 0.2561 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4219 | Steps: 4 | Val loss: 0.3260 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:22:19 (running for 00:32:36.88)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.275 |  0.139 |                   69 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.146 |                   72 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.423 |  0.176 |                   42 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.402 |  0.146 |                   39 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14640776813030243
[2m[36m(func pid=63128)[0m mae:  0.0875006765127182
[2m[36m(func pid=63128)[0m rmse_per_class: [0.064, 0.215, 0.041, 0.299, 0.084, 0.157, 0.204, 0.12, 0.136, 0.144]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14578020572662354
[2m[36m(func pid=71112)[0m mae:  0.09987466037273407
[2m[36m(func pid=71112)[0m rmse_per_class: [0.092, 0.232, 0.033, 0.294, 0.055, 0.175, 0.227, 0.116, 0.135, 0.099]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13783514499664307
[2m[36m(func pid=63038)[0m mae:  0.08920978009700775
[2m[36m(func pid=63038)[0m rmse_per_class: [0.066, 0.204, 0.047, 0.275, 0.075, 0.148, 0.213, 0.102, 0.131, 0.118]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17606839537620544
[2m[36m(func pid=70360)[0m mae:  0.12887223064899445
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.26, 0.091, 0.33, 0.088, 0.19, 0.283, 0.138, 0.146, 0.115]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2873 | Steps: 4 | Val loss: 0.2580 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2362 | Steps: 4 | Val loss: 0.2631 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3918 | Steps: 4 | Val loss: 0.2978 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4164 | Steps: 4 | Val loss: 0.3240 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=63128)[0m rmse: 0.14357778429985046
[2m[36m(func pid=63128)[0m mae:  0.08597934991121292
[2m[36m(func pid=63128)[0m rmse_per_class: [0.062, 0.208, 0.07, 0.295, 0.071, 0.158, 0.199, 0.108, 0.138, 0.128]
[2m[36m(func pid=71112)[0m rmse: 0.14562901854515076
[2m[36m(func pid=71112)[0m mae:  0.09937800467014313
[2m[36m(func pid=71112)[0m rmse_per_class: [0.09, 0.232, 0.034, 0.296, 0.055, 0.173, 0.226, 0.118, 0.135, 0.098]
[2m[36m(func pid=71112)[0m 
== Status ==
Current time: 2024-01-07 20:22:24 (running for 00:32:42.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.275 |  0.138 |                   70 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.146 |                   72 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.422 |  0.176 |                   43 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.392 |  0.146 |                   41 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13986852765083313
[2m[36m(func pid=63038)[0m mae:  0.09027047455310822
[2m[36m(func pid=63038)[0m rmse_per_class: [0.068, 0.207, 0.05, 0.283, 0.075, 0.149, 0.214, 0.106, 0.131, 0.116]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17588505148887634
[2m[36m(func pid=70360)[0m mae:  0.12874993681907654
[2m[36m(func pid=70360)[0m rmse_per_class: [0.12, 0.26, 0.09, 0.33, 0.087, 0.189, 0.282, 0.138, 0.145, 0.115]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3790 | Steps: 4 | Val loss: 0.2909 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2308 | Steps: 4 | Val loss: 0.2641 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2930 | Steps: 4 | Val loss: 0.2607 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4173 | Steps: 4 | Val loss: 0.3220 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:22:30 (running for 00:32:47.92)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.287 |  0.14  |                   71 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.144 |                   73 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.416 |  0.176 |                   44 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.379 |  0.145 |                   42 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.14546938240528107
[2m[36m(func pid=71112)[0m mae:  0.09888021647930145
[2m[36m(func pid=71112)[0m rmse_per_class: [0.089, 0.231, 0.033, 0.297, 0.055, 0.173, 0.223, 0.121, 0.135, 0.099]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.14279025793075562
[2m[36m(func pid=63128)[0m mae:  0.08630726486444473
[2m[36m(func pid=63128)[0m rmse_per_class: [0.068, 0.214, 0.073, 0.296, 0.069, 0.162, 0.204, 0.108, 0.148, 0.087]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14256122708320618
[2m[36m(func pid=63038)[0m mae:  0.09186579287052155
[2m[36m(func pid=63038)[0m rmse_per_class: [0.075, 0.211, 0.058, 0.293, 0.078, 0.147, 0.21, 0.107, 0.131, 0.115]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17572160065174103
[2m[36m(func pid=70360)[0m mae:  0.12854884564876556
[2m[36m(func pid=70360)[0m rmse_per_class: [0.12, 0.26, 0.091, 0.33, 0.087, 0.19, 0.281, 0.137, 0.145, 0.115]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3730 | Steps: 4 | Val loss: 0.2865 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2400 | Steps: 4 | Val loss: 0.2578 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2793 | Steps: 4 | Val loss: 0.2562 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4177 | Steps: 4 | Val loss: 0.3194 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 20:22:35 (running for 00:32:53.29)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.293 |  0.143 |                   72 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.231 |  0.143 |                   74 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.417 |  0.176 |                   45 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.373 |  0.145 |                   43 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.14502769708633423
[2m[36m(func pid=71112)[0m mae:  0.09838245809078217
[2m[36m(func pid=71112)[0m rmse_per_class: [0.089, 0.23, 0.032, 0.298, 0.055, 0.174, 0.223, 0.119, 0.135, 0.098]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63128)[0m rmse: 0.13767123222351074
[2m[36m(func pid=63128)[0m mae:  0.0842311754822731
[2m[36m(func pid=63128)[0m rmse_per_class: [0.065, 0.214, 0.04, 0.278, 0.074, 0.164, 0.207, 0.107, 0.148, 0.078]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13841086626052856
[2m[36m(func pid=63038)[0m mae:  0.089338518679142
[2m[36m(func pid=63038)[0m rmse_per_class: [0.073, 0.206, 0.048, 0.285, 0.072, 0.147, 0.208, 0.107, 0.129, 0.108]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17539770901203156
[2m[36m(func pid=70360)[0m mae:  0.12821850180625916
[2m[36m(func pid=70360)[0m rmse_per_class: [0.119, 0.26, 0.091, 0.329, 0.086, 0.19, 0.281, 0.136, 0.146, 0.116]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3610 | Steps: 4 | Val loss: 0.2831 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2560 | Steps: 4 | Val loss: 0.2594 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2817 | Steps: 4 | Val loss: 0.2531 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4121 | Steps: 4 | Val loss: 0.3170 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:22:40 (running for 00:32:58.62)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14549999684095383
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.279 |  0.138 |                   73 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.256 |  0.139 |                   76 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.418 |  0.175 |                   46 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.373 |  0.145 |                   43 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13900287449359894
[2m[36m(func pid=63128)[0m mae:  0.08398494124412537
[2m[36m(func pid=63128)[0m rmse_per_class: [0.067, 0.224, 0.03, 0.272, 0.067, 0.174, 0.214, 0.111, 0.155, 0.075]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1450529396533966
[2m[36m(func pid=71112)[0m mae:  0.09864553064107895
[2m[36m(func pid=71112)[0m rmse_per_class: [0.089, 0.23, 0.03, 0.299, 0.055, 0.174, 0.222, 0.116, 0.135, 0.1]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1354009211063385
[2m[36m(func pid=63038)[0m mae:  0.08696992695331573
[2m[36m(func pid=63038)[0m rmse_per_class: [0.069, 0.201, 0.045, 0.28, 0.073, 0.147, 0.21, 0.11, 0.122, 0.097]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17464753985404968
[2m[36m(func pid=70360)[0m mae:  0.12756270170211792
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.259, 0.09, 0.329, 0.084, 0.19, 0.28, 0.135, 0.147, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2616 | Steps: 4 | Val loss: 0.2595 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3581 | Steps: 4 | Val loss: 0.2790 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2790 | Steps: 4 | Val loss: 0.2528 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4102 | Steps: 4 | Val loss: 0.3147 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 20:22:46 (running for 00:33:04.01)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14549999684095383
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.282 |  0.135 |                   74 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.262 |  0.14  |                   77 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.412 |  0.175 |                   47 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.361 |  0.145 |                   44 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13983163237571716
[2m[36m(func pid=63128)[0m mae:  0.08299849182367325
[2m[36m(func pid=63128)[0m rmse_per_class: [0.071, 0.229, 0.032, 0.267, 0.074, 0.178, 0.213, 0.113, 0.151, 0.07]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14495354890823364
[2m[36m(func pid=71112)[0m mae:  0.09900065511465073
[2m[36m(func pid=71112)[0m rmse_per_class: [0.09, 0.229, 0.031, 0.3, 0.055, 0.173, 0.222, 0.114, 0.135, 0.101]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13490840792655945
[2m[36m(func pid=63038)[0m mae:  0.08696642518043518
[2m[36m(func pid=63038)[0m rmse_per_class: [0.069, 0.198, 0.047, 0.275, 0.066, 0.148, 0.209, 0.104, 0.129, 0.104]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17396530508995056
[2m[36m(func pid=70360)[0m mae:  0.1270030438899994
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.088, 0.329, 0.084, 0.19, 0.279, 0.135, 0.147, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2504 | Steps: 4 | Val loss: 0.2591 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3670 | Steps: 4 | Val loss: 0.2761 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2810 | Steps: 4 | Val loss: 0.2648 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4020 | Steps: 4 | Val loss: 0.3141 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 20:22:51 (running for 00:33:09.34)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.279 |  0.135 |                   75 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.25  |  0.142 |                   78 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.41  |  0.174 |                   48 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.358 |  0.145 |                   45 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14189472794532776
[2m[36m(func pid=63128)[0m mae:  0.08456752449274063
[2m[36m(func pid=63128)[0m rmse_per_class: [0.071, 0.208, 0.035, 0.253, 0.107, 0.163, 0.221, 0.135, 0.156, 0.069]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14500442147254944
[2m[36m(func pid=71112)[0m mae:  0.09885969012975693
[2m[36m(func pid=71112)[0m rmse_per_class: [0.091, 0.23, 0.03, 0.301, 0.055, 0.174, 0.219, 0.114, 0.136, 0.101]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14411328732967377
[2m[36m(func pid=63038)[0m mae:  0.09429343044757843
[2m[36m(func pid=63038)[0m rmse_per_class: [0.076, 0.204, 0.038, 0.272, 0.067, 0.161, 0.23, 0.108, 0.168, 0.119]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1737673431634903
[2m[36m(func pid=70360)[0m mae:  0.1268600970506668
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.087, 0.328, 0.083, 0.191, 0.278, 0.135, 0.147, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2645 | Steps: 4 | Val loss: 0.2829 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3518 | Steps: 4 | Val loss: 0.2726 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2781 | Steps: 4 | Val loss: 0.2544 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4073 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 20:22:56 (running for 00:33:14.63)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.281 |  0.144 |                   76 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.264 |  0.158 |                   79 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.402 |  0.174 |                   49 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.367 |  0.145 |                   46 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.15781143307685852
[2m[36m(func pid=63128)[0m mae:  0.09687169641256332
[2m[36m(func pid=63128)[0m rmse_per_class: [0.079, 0.213, 0.044, 0.276, 0.127, 0.192, 0.239, 0.118, 0.223, 0.068]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.144407719373703
[2m[36m(func pid=71112)[0m mae:  0.0983181744813919
[2m[36m(func pid=71112)[0m rmse_per_class: [0.091, 0.229, 0.03, 0.3, 0.054, 0.172, 0.218, 0.114, 0.136, 0.099]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1359221637248993
[2m[36m(func pid=63038)[0m mae:  0.08813585340976715
[2m[36m(func pid=63038)[0m rmse_per_class: [0.069, 0.2, 0.04, 0.27, 0.07, 0.16, 0.224, 0.099, 0.13, 0.097]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17325401306152344
[2m[36m(func pid=70360)[0m mae:  0.12643864750862122
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.087, 0.328, 0.083, 0.19, 0.276, 0.135, 0.146, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2930 | Steps: 4 | Val loss: 0.2772 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3647 | Steps: 4 | Val loss: 0.2703 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2825 | Steps: 4 | Val loss: 0.2528 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4018 | Steps: 4 | Val loss: 0.3120 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:23:02 (running for 00:33:19.99)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.278 |  0.136 |                   77 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.293 |  0.154 |                   80 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.407 |  0.173 |                   50 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.352 |  0.144 |                   47 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.15430893003940582
[2m[36m(func pid=63128)[0m mae:  0.09430807828903198
[2m[36m(func pid=63128)[0m rmse_per_class: [0.082, 0.203, 0.041, 0.278, 0.126, 0.196, 0.243, 0.12, 0.184, 0.069]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14368905127048492
[2m[36m(func pid=71112)[0m mae:  0.0979621410369873
[2m[36m(func pid=71112)[0m rmse_per_class: [0.091, 0.226, 0.029, 0.298, 0.055, 0.173, 0.217, 0.111, 0.135, 0.101]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13447526097297668
[2m[36m(func pid=63038)[0m mae:  0.08675043284893036
[2m[36m(func pid=63038)[0m rmse_per_class: [0.065, 0.199, 0.035, 0.268, 0.072, 0.168, 0.225, 0.1, 0.123, 0.09]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17328839004039764
[2m[36m(func pid=70360)[0m mae:  0.1265139877796173
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.259, 0.087, 0.327, 0.082, 0.19, 0.276, 0.135, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2495 | Steps: 4 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3613 | Steps: 4 | Val loss: 0.2690 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2626 | Steps: 4 | Val loss: 0.2496 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4037 | Steps: 4 | Val loss: 0.3117 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=63128)[0m rmse: 0.14804963767528534
[2m[36m(func pid=63128)[0m mae:  0.09004237502813339
[2m[36m(func pid=63128)[0m rmse_per_class: [0.076, 0.204, 0.032, 0.303, 0.107, 0.168, 0.225, 0.141, 0.144, 0.081]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:23:07 (running for 00:33:25.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.282 |  0.134 |                   78 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.249 |  0.148 |                   81 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.402 |  0.173 |                   51 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.365 |  0.144 |                   48 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.1438191682100296
[2m[36m(func pid=71112)[0m mae:  0.09818921238183975
[2m[36m(func pid=71112)[0m rmse_per_class: [0.093, 0.225, 0.029, 0.299, 0.055, 0.173, 0.219, 0.111, 0.135, 0.1]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1310882866382599
[2m[36m(func pid=63038)[0m mae:  0.08518493175506592
[2m[36m(func pid=63038)[0m rmse_per_class: [0.065, 0.197, 0.032, 0.267, 0.07, 0.158, 0.213, 0.096, 0.127, 0.086]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17336252331733704
[2m[36m(func pid=70360)[0m mae:  0.12655119597911835
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.259, 0.087, 0.326, 0.082, 0.19, 0.276, 0.135, 0.146, 0.115]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2589 | Steps: 4 | Val loss: 0.2889 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3523 | Steps: 4 | Val loss: 0.2672 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2728 | Steps: 4 | Val loss: 0.2488 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4004 | Steps: 4 | Val loss: 0.3102 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:23:13 (running for 00:33:30.90)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.263 |  0.131 |                   79 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.259 |  0.158 |                   82 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.404 |  0.173 |                   52 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.361 |  0.144 |                   49 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.15751178562641144
[2m[36m(func pid=63128)[0m mae:  0.09582962095737457
[2m[36m(func pid=63128)[0m rmse_per_class: [0.073, 0.216, 0.026, 0.32, 0.101, 0.174, 0.23, 0.178, 0.148, 0.108]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1435551941394806
[2m[36m(func pid=71112)[0m mae:  0.09816113114356995
[2m[36m(func pid=71112)[0m rmse_per_class: [0.096, 0.225, 0.029, 0.298, 0.055, 0.171, 0.216, 0.109, 0.136, 0.1]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13093800842761993
[2m[36m(func pid=63038)[0m mae:  0.08493338525295258
[2m[36m(func pid=63038)[0m rmse_per_class: [0.064, 0.197, 0.034, 0.268, 0.07, 0.149, 0.205, 0.097, 0.13, 0.095]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17264637351036072
[2m[36m(func pid=70360)[0m mae:  0.12594687938690186
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.258, 0.086, 0.325, 0.082, 0.19, 0.275, 0.135, 0.145, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2460 | Steps: 4 | Val loss: 0.2829 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3480 | Steps: 4 | Val loss: 0.2659 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2832 | Steps: 4 | Val loss: 0.2555 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3988 | Steps: 4 | Val loss: 0.3109 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:23:18 (running for 00:33:36.36)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.131 |                   80 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.246 |  0.158 |                   83 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.4   |  0.173 |                   53 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.352 |  0.144 |                   50 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1577254831790924
[2m[36m(func pid=63128)[0m mae:  0.09644649922847748
[2m[36m(func pid=63128)[0m rmse_per_class: [0.074, 0.225, 0.025, 0.305, 0.1, 0.166, 0.239, 0.148, 0.137, 0.157]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14335110783576965
[2m[36m(func pid=71112)[0m mae:  0.09815321117639542
[2m[36m(func pid=71112)[0m rmse_per_class: [0.096, 0.226, 0.029, 0.296, 0.055, 0.17, 0.216, 0.108, 0.136, 0.101]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13734307885169983
[2m[36m(func pid=63038)[0m mae:  0.08908571302890778
[2m[36m(func pid=63038)[0m rmse_per_class: [0.065, 0.203, 0.069, 0.281, 0.073, 0.148, 0.208, 0.097, 0.133, 0.096]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1730060875415802
[2m[36m(func pid=70360)[0m mae:  0.12629923224449158
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.259, 0.086, 0.326, 0.082, 0.189, 0.276, 0.135, 0.146, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2556 | Steps: 4 | Val loss: 0.2604 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3569 | Steps: 4 | Val loss: 0.2658 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2691 | Steps: 4 | Val loss: 0.2543 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4007 | Steps: 4 | Val loss: 0.3105 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:23:24 (running for 00:33:41.71)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.283 |  0.137 |                   81 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.256 |  0.14  |                   84 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.399 |  0.173 |                   54 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.348 |  0.143 |                   51 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1401466429233551
[2m[36m(func pid=63128)[0m mae:  0.08534737676382065
[2m[36m(func pid=63128)[0m rmse_per_class: [0.071, 0.218, 0.027, 0.274, 0.085, 0.174, 0.225, 0.105, 0.135, 0.088]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14358919858932495
[2m[36m(func pid=71112)[0m mae:  0.09844326227903366
[2m[36m(func pid=71112)[0m rmse_per_class: [0.095, 0.226, 0.03, 0.297, 0.054, 0.17, 0.217, 0.108, 0.137, 0.102]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13688670098781586
[2m[36m(func pid=63038)[0m mae:  0.0880388468503952
[2m[36m(func pid=63038)[0m rmse_per_class: [0.071, 0.205, 0.07, 0.276, 0.074, 0.146, 0.208, 0.099, 0.125, 0.094]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17279818654060364
[2m[36m(func pid=70360)[0m mae:  0.126128688454628
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.258, 0.086, 0.326, 0.081, 0.189, 0.276, 0.135, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3516 | Steps: 4 | Val loss: 0.2660 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2378 | Steps: 4 | Val loss: 0.2623 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2917 | Steps: 4 | Val loss: 0.2479 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4018 | Steps: 4 | Val loss: 0.3093 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=71112)[0m rmse: 0.1439119130373001
[2m[36m(func pid=71112)[0m mae:  0.0990932509303093
[2m[36m(func pid=71112)[0m rmse_per_class: [0.096, 0.226, 0.029, 0.296, 0.054, 0.168, 0.22, 0.106, 0.138, 0.105]
[2m[36m(func pid=71112)[0m 
== Status ==
Current time: 2024-01-07 20:23:29 (running for 00:33:47.00)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.269 |  0.137 |                   82 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.256 |  0.14  |                   84 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.401 |  0.173 |                   55 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.352 |  0.144 |                   53 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14142261445522308
[2m[36m(func pid=63128)[0m mae:  0.0864839106798172
[2m[36m(func pid=63128)[0m rmse_per_class: [0.068, 0.207, 0.03, 0.265, 0.071, 0.194, 0.243, 0.104, 0.155, 0.076]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1303086280822754
[2m[36m(func pid=63038)[0m mae:  0.08391186594963074
[2m[36m(func pid=63038)[0m rmse_per_class: [0.07, 0.2, 0.036, 0.255, 0.069, 0.151, 0.207, 0.094, 0.125, 0.097]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1718607395887375
[2m[36m(func pid=70360)[0m mae:  0.12524983286857605
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.084, 0.325, 0.08, 0.189, 0.273, 0.134, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3420 | Steps: 4 | Val loss: 0.2664 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2627 | Steps: 4 | Val loss: 0.2617 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2815 | Steps: 4 | Val loss: 0.2511 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3984 | Steps: 4 | Val loss: 0.3088 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 20:23:34 (running for 00:33:52.46)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.292 |  0.13  |                   83 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.238 |  0.141 |                   85 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.402 |  0.172 |                   56 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.342 |  0.144 |                   54 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14193056523799896
[2m[36m(func pid=63128)[0m mae:  0.0870518609881401
[2m[36m(func pid=63128)[0m rmse_per_class: [0.065, 0.206, 0.039, 0.272, 0.082, 0.185, 0.24, 0.104, 0.152, 0.074]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14409661293029785
[2m[36m(func pid=71112)[0m mae:  0.09939642250537872
[2m[36m(func pid=71112)[0m rmse_per_class: [0.096, 0.226, 0.029, 0.295, 0.054, 0.167, 0.222, 0.106, 0.138, 0.106]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13344720005989075
[2m[36m(func pid=63038)[0m mae:  0.08596442639827728
[2m[36m(func pid=63038)[0m rmse_per_class: [0.073, 0.199, 0.03, 0.262, 0.066, 0.157, 0.208, 0.097, 0.145, 0.097]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17150411009788513
[2m[36m(func pid=70360)[0m mae:  0.12486337125301361
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.257, 0.084, 0.325, 0.079, 0.189, 0.272, 0.133, 0.147, 0.111]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2519 | Steps: 4 | Val loss: 0.2698 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3495 | Steps: 4 | Val loss: 0.2657 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2898 | Steps: 4 | Val loss: 0.2493 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4025 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=63128)[0m rmse: 0.14963354170322418
[2m[36m(func pid=63128)[0m mae:  0.09085465222597122
[2m[36m(func pid=63128)[0m rmse_per_class: [0.065, 0.216, 0.062, 0.31, 0.104, 0.159, 0.224, 0.129, 0.144, 0.083]
[2m[36m(func pid=63128)[0m 
== Status ==
Current time: 2024-01-07 20:23:40 (running for 00:33:57.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.281 |  0.133 |                   84 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.252 |  0.15  |                   87 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.398 |  0.172 |                   57 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.342 |  0.144 |                   54 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m rmse: 0.14355185627937317
[2m[36m(func pid=71112)[0m mae:  0.09843523800373077
[2m[36m(func pid=71112)[0m rmse_per_class: [0.095, 0.226, 0.029, 0.292, 0.054, 0.166, 0.222, 0.109, 0.137, 0.106]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13217344880104065
[2m[36m(func pid=63038)[0m mae:  0.0847931057214737
[2m[36m(func pid=63038)[0m rmse_per_class: [0.065, 0.198, 0.032, 0.256, 0.066, 0.154, 0.209, 0.097, 0.138, 0.106]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17180876433849335
[2m[36m(func pid=70360)[0m mae:  0.12525007128715515
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.084, 0.324, 0.079, 0.189, 0.274, 0.134, 0.146, 0.112]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2705 | Steps: 4 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3525 | Steps: 4 | Val loss: 0.2655 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3991 | Steps: 4 | Val loss: 0.3099 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2785 | Steps: 4 | Val loss: 0.2499 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:23:45 (running for 00:34:03.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.29  |  0.132 |                   85 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.271 |  0.147 |                   88 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.403 |  0.172 |                   58 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.349 |  0.144 |                   55 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14732126891613007
[2m[36m(func pid=63128)[0m mae:  0.08911456167697906
[2m[36m(func pid=63128)[0m rmse_per_class: [0.067, 0.212, 0.056, 0.298, 0.099, 0.158, 0.219, 0.127, 0.15, 0.086]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14306023716926575
[2m[36m(func pid=71112)[0m mae:  0.09841076284646988
[2m[36m(func pid=71112)[0m rmse_per_class: [0.096, 0.225, 0.028, 0.29, 0.054, 0.165, 0.224, 0.106, 0.137, 0.105]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17156758904457092
[2m[36m(func pid=70360)[0m mae:  0.1250610649585724
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.258, 0.083, 0.324, 0.078, 0.189, 0.273, 0.134, 0.146, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1327313631772995
[2m[36m(func pid=63038)[0m mae:  0.08508089184761047
[2m[36m(func pid=63038)[0m rmse_per_class: [0.063, 0.197, 0.044, 0.258, 0.069, 0.151, 0.213, 0.098, 0.126, 0.109]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2618 | Steps: 4 | Val loss: 0.2636 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3478 | Steps: 4 | Val loss: 0.2685 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4012 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2741 | Steps: 4 | Val loss: 0.2521 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:23:51 (running for 00:34:08.78)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.278 |  0.133 |                   86 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.262 |  0.144 |                   89 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.399 |  0.172 |                   59 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.352 |  0.143 |                   56 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14410728216171265
[2m[36m(func pid=63128)[0m mae:  0.08709181100130081
[2m[36m(func pid=63128)[0m rmse_per_class: [0.066, 0.211, 0.044, 0.288, 0.103, 0.166, 0.218, 0.106, 0.158, 0.081]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14486601948738098
[2m[36m(func pid=71112)[0m mae:  0.10040032863616943
[2m[36m(func pid=71112)[0m rmse_per_class: [0.103, 0.227, 0.03, 0.295, 0.054, 0.164, 0.23, 0.103, 0.138, 0.104]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17123878002166748
[2m[36m(func pid=70360)[0m mae:  0.12480749189853668
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.257, 0.083, 0.324, 0.078, 0.189, 0.273, 0.133, 0.146, 0.112]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1352173089981079
[2m[36m(func pid=63038)[0m mae:  0.08618751168251038
[2m[36m(func pid=63038)[0m rmse_per_class: [0.065, 0.198, 0.049, 0.258, 0.068, 0.157, 0.22, 0.096, 0.126, 0.113]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2655 | Steps: 4 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3393 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3979 | Steps: 4 | Val loss: 0.3093 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2795 | Steps: 4 | Val loss: 0.2525 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=71112)[0m rmse: 0.14399908483028412
[2m[36m(func pid=71112)[0m mae:  0.09961619228124619
[2m[36m(func pid=71112)[0m rmse_per_class: [0.101, 0.228, 0.029, 0.292, 0.054, 0.164, 0.229, 0.103, 0.138, 0.102]
[2m[36m(func pid=71112)[0m 
== Status ==
Current time: 2024-01-07 20:23:56 (running for 00:34:14.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.274 |  0.135 |                   87 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.262 |  0.144 |                   89 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.401 |  0.171 |                   60 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.339 |  0.144 |                   58 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14557020366191864
[2m[36m(func pid=63128)[0m mae:  0.08843456953763962
[2m[36m(func pid=63128)[0m rmse_per_class: [0.067, 0.211, 0.039, 0.296, 0.116, 0.17, 0.223, 0.107, 0.146, 0.082]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17087377607822418
[2m[36m(func pid=70360)[0m mae:  0.12445791065692902
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.257, 0.082, 0.323, 0.077, 0.189, 0.272, 0.134, 0.146, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13589076697826385
[2m[36m(func pid=63038)[0m mae:  0.08608920872211456
[2m[36m(func pid=63038)[0m rmse_per_class: [0.064, 0.203, 0.044, 0.256, 0.067, 0.157, 0.218, 0.098, 0.13, 0.121]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2460 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3473 | Steps: 4 | Val loss: 0.2698 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3979 | Steps: 4 | Val loss: 0.3102 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2736 | Steps: 4 | Val loss: 0.2524 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 20:24:02 (running for 00:34:19.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.28  |  0.136 |                   88 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.246 |  0.156 |                   91 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.398 |  0.171 |                   61 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.339 |  0.144 |                   58 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.15570296347141266
[2m[36m(func pid=63128)[0m mae:  0.09566951543092728
[2m[36m(func pid=63128)[0m rmse_per_class: [0.082, 0.21, 0.035, 0.31, 0.17, 0.166, 0.233, 0.11, 0.164, 0.078]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14490282535552979
[2m[36m(func pid=71112)[0m mae:  0.10041399300098419
[2m[36m(func pid=71112)[0m rmse_per_class: [0.102, 0.228, 0.03, 0.291, 0.054, 0.164, 0.232, 0.103, 0.14, 0.104]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.17117013037204742
[2m[36m(func pid=70360)[0m mae:  0.12472771108150482
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.257, 0.083, 0.324, 0.077, 0.189, 0.272, 0.134, 0.146, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13552366197109222
[2m[36m(func pid=63038)[0m mae:  0.08668000996112823
[2m[36m(func pid=63038)[0m rmse_per_class: [0.063, 0.204, 0.047, 0.257, 0.069, 0.151, 0.214, 0.096, 0.135, 0.12]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2312 | Steps: 4 | Val loss: 0.2875 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3710 | Steps: 4 | Val loss: 0.2704 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3939 | Steps: 4 | Val loss: 0.3106 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2597 | Steps: 4 | Val loss: 0.2517 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 20:24:07 (running for 00:34:25.01)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.274 |  0.136 |                   89 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.231 |  0.166 |                   92 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.398 |  0.171 |                   62 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.347 |  0.145 |                   59 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.16629411280155182
[2m[36m(func pid=63128)[0m mae:  0.10289455950260162
[2m[36m(func pid=63128)[0m rmse_per_class: [0.115, 0.201, 0.078, 0.307, 0.166, 0.159, 0.247, 0.112, 0.18, 0.098]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14521059393882751
[2m[36m(func pid=71112)[0m mae:  0.10002301633358002
[2m[36m(func pid=71112)[0m rmse_per_class: [0.102, 0.228, 0.03, 0.29, 0.054, 0.165, 0.23, 0.106, 0.141, 0.106]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1709248274564743
[2m[36m(func pid=70360)[0m mae:  0.12454740703105927
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.256, 0.082, 0.324, 0.076, 0.188, 0.273, 0.134, 0.145, 0.114]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.135422021150589
[2m[36m(func pid=63038)[0m mae:  0.08608191460371017
[2m[36m(func pid=63038)[0m rmse_per_class: [0.066, 0.202, 0.063, 0.264, 0.067, 0.15, 0.209, 0.096, 0.131, 0.106]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2400 | Steps: 4 | Val loss: 0.2864 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3467 | Steps: 4 | Val loss: 0.2693 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4006 | Steps: 4 | Val loss: 0.3097 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2786 | Steps: 4 | Val loss: 0.2537 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:24:12 (running for 00:34:30.43)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.26  |  0.135 |                   90 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.166 |                   93 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.394 |  0.171 |                   63 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.371 |  0.145 |                   60 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.16596880555152893
[2m[36m(func pid=63128)[0m mae:  0.10169259458780289
[2m[36m(func pid=63128)[0m rmse_per_class: [0.115, 0.209, 0.103, 0.302, 0.137, 0.163, 0.242, 0.107, 0.185, 0.097]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14460715651512146
[2m[36m(func pid=71112)[0m mae:  0.09946796298027039
[2m[36m(func pid=71112)[0m rmse_per_class: [0.098, 0.227, 0.03, 0.287, 0.054, 0.164, 0.231, 0.107, 0.141, 0.108]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1700085699558258
[2m[36m(func pid=70360)[0m mae:  0.12378843128681183
[2m[36m(func pid=70360)[0m rmse_per_class: [0.118, 0.256, 0.079, 0.322, 0.075, 0.188, 0.27, 0.133, 0.145, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1373736560344696
[2m[36m(func pid=63038)[0m mae:  0.08671123534440994
[2m[36m(func pid=63038)[0m rmse_per_class: [0.069, 0.202, 0.068, 0.27, 0.066, 0.152, 0.206, 0.1, 0.132, 0.109]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2416 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3650 | Steps: 4 | Val loss: 0.2675 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2676 | Steps: 4 | Val loss: 0.2515 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3943 | Steps: 4 | Val loss: 0.3092 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 20:24:18 (running for 00:34:35.89)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.279 |  0.137 |                   91 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.242 |  0.148 |                   94 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.401 |  0.17  |                   64 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.347 |  0.145 |                   61 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1481429487466812
[2m[36m(func pid=63128)[0m mae:  0.08833038061857224
[2m[36m(func pid=63128)[0m rmse_per_class: [0.089, 0.204, 0.076, 0.276, 0.101, 0.156, 0.218, 0.122, 0.141, 0.098]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14257583022117615
[2m[36m(func pid=71112)[0m mae:  0.09737519919872284
[2m[36m(func pid=71112)[0m rmse_per_class: [0.091, 0.225, 0.027, 0.284, 0.054, 0.161, 0.229, 0.111, 0.138, 0.106]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.135146826505661
[2m[36m(func pid=63038)[0m mae:  0.08580495417118073
[2m[36m(func pid=63038)[0m rmse_per_class: [0.072, 0.199, 0.055, 0.264, 0.069, 0.154, 0.206, 0.098, 0.129, 0.105]
[2m[36m(func pid=70360)[0m rmse: 0.17004410922527313
[2m[36m(func pid=70360)[0m mae:  0.12377746403217316
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.256, 0.08, 0.322, 0.076, 0.188, 0.271, 0.132, 0.145, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2506 | Steps: 4 | Val loss: 0.2578 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3374 | Steps: 4 | Val loss: 0.2685 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2541 | Steps: 4 | Val loss: 0.2486 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3974 | Steps: 4 | Val loss: 0.3083 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:24:23 (running for 00:34:41.37)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.268 |  0.135 |                   92 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.251 |  0.14  |                   95 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.394 |  0.17  |                   65 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.365 |  0.143 |                   62 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13952399790287018
[2m[36m(func pid=63128)[0m mae:  0.08254063874483109
[2m[36m(func pid=63128)[0m rmse_per_class: [0.081, 0.205, 0.062, 0.275, 0.074, 0.157, 0.202, 0.109, 0.13, 0.101]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14360299706459045
[2m[36m(func pid=71112)[0m mae:  0.09847008436918259
[2m[36m(func pid=71112)[0m rmse_per_class: [0.094, 0.225, 0.028, 0.284, 0.054, 0.162, 0.233, 0.109, 0.141, 0.108]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1310460865497589
[2m[36m(func pid=63038)[0m mae:  0.0836627185344696
[2m[36m(func pid=63038)[0m rmse_per_class: [0.07, 0.193, 0.039, 0.255, 0.067, 0.155, 0.202, 0.096, 0.132, 0.101]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.16912822425365448
[2m[36m(func pid=70360)[0m mae:  0.1229456290602684
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.255, 0.078, 0.321, 0.075, 0.187, 0.268, 0.133, 0.145, 0.112]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2591 | Steps: 4 | Val loss: 0.2515 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3361 | Steps: 4 | Val loss: 0.2695 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4008 | Steps: 4 | Val loss: 0.3078 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2677 | Steps: 4 | Val loss: 0.2517 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:24:29 (running for 00:34:46.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.254 |  0.131 |                   93 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.259 |  0.133 |                   96 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.397 |  0.169 |                   66 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.337 |  0.144 |                   63 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13332629203796387
[2m[36m(func pid=63128)[0m mae:  0.0794997438788414
[2m[36m(func pid=63128)[0m rmse_per_class: [0.081, 0.198, 0.048, 0.269, 0.063, 0.158, 0.199, 0.101, 0.127, 0.09]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1442592591047287
[2m[36m(func pid=71112)[0m mae:  0.09922625124454498
[2m[36m(func pid=71112)[0m rmse_per_class: [0.096, 0.225, 0.029, 0.283, 0.053, 0.162, 0.236, 0.109, 0.141, 0.108]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13412845134735107
[2m[36m(func pid=63038)[0m mae:  0.0851486548781395
[2m[36m(func pid=63038)[0m rmse_per_class: [0.077, 0.194, 0.04, 0.256, 0.062, 0.158, 0.208, 0.098, 0.148, 0.1]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.16876640915870667
[2m[36m(func pid=70360)[0m mae:  0.12259819358587265
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.254, 0.079, 0.322, 0.074, 0.187, 0.268, 0.131, 0.145, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2559 | Steps: 4 | Val loss: 0.2568 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3502 | Steps: 4 | Val loss: 0.2671 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3964 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2725 | Steps: 4 | Val loss: 0.2523 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 20:24:34 (running for 00:34:52.10)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.268 |  0.134 |                   94 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.256 |  0.135 |                   97 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.401 |  0.169 |                   67 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.336 |  0.144 |                   64 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.13543812930583954
[2m[36m(func pid=63128)[0m mae:  0.08242206275463104
[2m[36m(func pid=63128)[0m rmse_per_class: [0.067, 0.197, 0.037, 0.292, 0.062, 0.161, 0.208, 0.115, 0.13, 0.086]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14292261004447937
[2m[36m(func pid=71112)[0m mae:  0.09773074090480804
[2m[36m(func pid=71112)[0m rmse_per_class: [0.095, 0.224, 0.028, 0.279, 0.053, 0.16, 0.235, 0.11, 0.138, 0.106]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.1687176525592804
[2m[36m(func pid=70360)[0m mae:  0.12257260084152222
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.255, 0.078, 0.322, 0.074, 0.186, 0.268, 0.131, 0.145, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.1357426941394806
[2m[36m(func pid=63038)[0m mae:  0.08590550720691681
[2m[36m(func pid=63038)[0m rmse_per_class: [0.074, 0.195, 0.065, 0.27, 0.064, 0.149, 0.208, 0.098, 0.135, 0.1]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2610 | Steps: 4 | Val loss: 0.2632 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3535 | Steps: 4 | Val loss: 0.2675 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3997 | Steps: 4 | Val loss: 0.3090 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2579 | Steps: 4 | Val loss: 0.2588 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:24:39 (running for 00:34:57.41)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.136 |                   95 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.261 |  0.142 |                   98 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.169 |                   68 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.35  |  0.143 |                   65 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.1422373205423355
[2m[36m(func pid=63128)[0m mae:  0.08656690269708633
[2m[36m(func pid=63128)[0m rmse_per_class: [0.069, 0.198, 0.027, 0.279, 0.067, 0.197, 0.232, 0.115, 0.157, 0.083]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14305716753005981
[2m[36m(func pid=71112)[0m mae:  0.09768251329660416
[2m[36m(func pid=71112)[0m rmse_per_class: [0.093, 0.223, 0.027, 0.278, 0.053, 0.16, 0.236, 0.114, 0.139, 0.107]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.16862671077251434
[2m[36m(func pid=70360)[0m mae:  0.12253433465957642
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.255, 0.077, 0.322, 0.074, 0.186, 0.267, 0.131, 0.145, 0.113]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.14170409739017487
[2m[36m(func pid=63038)[0m mae:  0.08999181538820267
[2m[36m(func pid=63038)[0m rmse_per_class: [0.076, 0.199, 0.087, 0.284, 0.068, 0.15, 0.214, 0.096, 0.145, 0.098]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2343 | Steps: 4 | Val loss: 0.2717 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3476 | Steps: 4 | Val loss: 0.2665 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3983 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2658 | Steps: 4 | Val loss: 0.2557 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:24:44 (running for 00:35:02.67)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.258 |  0.142 |                   96 |
| train_84a75_00015 | RUNNING    | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.234 |  0.149 |                   99 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.4   |  0.169 |                   69 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.353 |  0.143 |                   66 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63128)[0m rmse: 0.14861193299293518
[2m[36m(func pid=63128)[0m mae:  0.09125348925590515
[2m[36m(func pid=63128)[0m rmse_per_class: [0.08, 0.207, 0.025, 0.281, 0.074, 0.177, 0.235, 0.115, 0.21, 0.081]
[2m[36m(func pid=63128)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.16812774538993835
[2m[36m(func pid=70360)[0m mae:  0.1220998615026474
[2m[36m(func pid=70360)[0m rmse_per_class: [0.115, 0.255, 0.077, 0.321, 0.073, 0.186, 0.266, 0.131, 0.144, 0.112]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14304417371749878
[2m[36m(func pid=71112)[0m mae:  0.09753739833831787
[2m[36m(func pid=71112)[0m rmse_per_class: [0.095, 0.223, 0.027, 0.275, 0.053, 0.161, 0.238, 0.114, 0.14, 0.104]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13925574719905853
[2m[36m(func pid=63038)[0m mae:  0.08827479928731918
[2m[36m(func pid=63038)[0m rmse_per_class: [0.065, 0.196, 0.078, 0.271, 0.071, 0.155, 0.216, 0.094, 0.143, 0.101]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=63128)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2356 | Steps: 4 | Val loss: 0.2643 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3152 | Steps: 4 | Val loss: 0.2673 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3935 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2552 | Steps: 4 | Val loss: 0.2487 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=63128)[0m rmse: 0.1420217752456665
[2m[36m(func pid=63128)[0m mae:  0.08660247176885605
[2m[36m(func pid=63128)[0m rmse_per_class: [0.073, 0.224, 0.025, 0.292, 0.079, 0.168, 0.205, 0.11, 0.167, 0.075]
== Status ==
Current time: 2024-01-07 20:24:50 (running for 00:35:08.09)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 3 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.266 |  0.139 |                   97 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.398 |  0.168 |                   70 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.348 |  0.143 |                   67 |
| train_84a75_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70360)[0m rmse: 0.1685817539691925
[2m[36m(func pid=70360)[0m mae:  0.1225190982222557
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.255, 0.078, 0.322, 0.074, 0.186, 0.268, 0.131, 0.145, 0.113]
[2m[36m(func pid=71112)[0m rmse: 0.14378248155117035
[2m[36m(func pid=71112)[0m mae:  0.09773558378219604
[2m[36m(func pid=71112)[0m rmse_per_class: [0.094, 0.224, 0.028, 0.279, 0.053, 0.161, 0.238, 0.117, 0.14, 0.105]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13275673985481262
[2m[36m(func pid=63038)[0m mae:  0.08388032019138336
[2m[36m(func pid=63038)[0m rmse_per_class: [0.062, 0.194, 0.048, 0.253, 0.076, 0.154, 0.214, 0.095, 0.13, 0.103]
[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3315 | Steps: 4 | Val loss: 0.2693 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3964 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2746 | Steps: 4 | Val loss: 0.2488 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=71112)[0m rmse: 0.14576919376850128
[2m[36m(func pid=71112)[0m mae:  0.09971225261688232
[2m[36m(func pid=71112)[0m rmse_per_class: [0.098, 0.224, 0.03, 0.282, 0.053, 0.163, 0.245, 0.111, 0.142, 0.109]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m rmse: 0.16821709275245667
[2m[36m(func pid=70360)[0m mae:  0.12224072217941284
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.255, 0.077, 0.321, 0.073, 0.186, 0.267, 0.131, 0.144, 0.112]
[2m[36m(func pid=63038)[0m rmse: 0.13249437510967255
[2m[36m(func pid=63038)[0m mae:  0.0829877182841301
[2m[36m(func pid=63038)[0m rmse_per_class: [0.065, 0.197, 0.034, 0.248, 0.071, 0.156, 0.214, 0.096, 0.136, 0.109]
== Status ==
Current time: 2024-01-07 20:24:56 (running for 00:35:14.09)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.255 |  0.133 |                   98 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.394 |  0.169 |                   71 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.331 |  0.146 |                   69 |
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63038)[0m 
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=87283)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=87283)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=87283)[0m Configuration completed!
[2m[36m(func pid=87283)[0m New optimizer parameters:
[2m[36m(func pid=87283)[0m SGD (
[2m[36m(func pid=87283)[0m Parameter Group 0
[2m[36m(func pid=87283)[0m     dampening: 0
[2m[36m(func pid=87283)[0m     differentiable: False
[2m[36m(func pid=87283)[0m     foreach: None
[2m[36m(func pid=87283)[0m     lr: 0.01
[2m[36m(func pid=87283)[0m     maximize: False
[2m[36m(func pid=87283)[0m     momentum: 0.99
[2m[36m(func pid=87283)[0m     nesterov: False
[2m[36m(func pid=87283)[0m     weight_decay: 1e-05
[2m[36m(func pid=87283)[0m )
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3249 | Steps: 4 | Val loss: 0.2699 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=71112)[0m rmse: 0.14658823609352112
[2m[36m(func pid=71112)[0m mae:  0.10053522884845734
[2m[36m(func pid=71112)[0m rmse_per_class: [0.1, 0.224, 0.032, 0.282, 0.053, 0.164, 0.248, 0.108, 0.144, 0.11]
== Status ==
Current time: 2024-01-07 20:25:01 (running for 00:35:19.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00014 | RUNNING    | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.275 |  0.132 |                   99 |
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.168 |                   72 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.325 |  0.147 |                   70 |
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=63038)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2649 | Steps: 4 | Val loss: 0.2496 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3979 | Steps: 4 | Val loss: 0.3102 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8666 | Steps: 4 | Val loss: 0.6216 | Batch size: 32 | lr: 0.01 | Duration: 4.74s
[2m[36m(func pid=70360)[0m rmse: 0.16833776235580444
[2m[36m(func pid=70360)[0m mae:  0.12239988148212433
[2m[36m(func pid=70360)[0m rmse_per_class: [0.117, 0.255, 0.077, 0.32, 0.073, 0.186, 0.267, 0.132, 0.144, 0.112]
[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=63038)[0m rmse: 0.13326454162597656
[2m[36m(func pid=63038)[0m mae:  0.08355884999036789
[2m[36m(func pid=63038)[0m rmse_per_class: [0.068, 0.2, 0.032, 0.25, 0.077, 0.154, 0.209, 0.096, 0.136, 0.111]
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3538 | Steps: 4 | Val loss: 0.2673 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=87283)[0m rmse: 0.18273241817951202
[2m[36m(func pid=87283)[0m mae:  0.13406169414520264
[2m[36m(func pid=87283)[0m rmse_per_class: [0.116, 0.268, 0.11, 0.339, 0.114, 0.191, 0.295, 0.145, 0.139, 0.11]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.14516347646713257
[2m[36m(func pid=71112)[0m mae:  0.09882508218288422
[2m[36m(func pid=71112)[0m rmse_per_class: [0.1, 0.222, 0.033, 0.28, 0.053, 0.162, 0.244, 0.109, 0.141, 0.108]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3984 | Steps: 4 | Val loss: 0.3099 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7011 | Steps: 4 | Val loss: 0.4614 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=70360)[0m rmse: 0.16725927591323853
[2m[36m(func pid=70360)[0m mae:  0.12144167721271515
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.254, 0.075, 0.319, 0.072, 0.185, 0.265, 0.131, 0.144, 0.11]
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3231 | Steps: 4 | Val loss: 0.2667 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=87283)[0m rmse: 0.17953351140022278
[2m[36m(func pid=87283)[0m mae:  0.13144716620445251
[2m[36m(func pid=87283)[0m rmse_per_class: [0.113, 0.263, 0.103, 0.332, 0.11, 0.192, 0.294, 0.141, 0.14, 0.108]
[2m[36m(func pid=71112)[0m rmse: 0.1448369026184082
[2m[36m(func pid=71112)[0m mae:  0.09865882992744446
[2m[36m(func pid=71112)[0m rmse_per_class: [0.103, 0.222, 0.032, 0.273, 0.053, 0.162, 0.246, 0.108, 0.142, 0.107]
== Status ==
Current time: 2024-01-07 20:25:07 (running for 00:35:24.91)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.398 |  0.168 |                   73 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.354 |  0.145 |                   71 |
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.867 |  0.183 |                    1 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 20:25:13 (running for 00:35:30.92)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.398 |  0.168 |                   73 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.323 |  0.145 |                   72 |
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.867 |  0.183 |                    1 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70360)[0m 
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88115)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=88115)[0m Configuration completed!
[2m[36m(func pid=88115)[0m New optimizer parameters:
[2m[36m(func pid=88115)[0m SGD (
[2m[36m(func pid=88115)[0m Parameter Group 0
[2m[36m(func pid=88115)[0m     dampening: 0
[2m[36m(func pid=88115)[0m     differentiable: False
[2m[36m(func pid=88115)[0m     foreach: None
[2m[36m(func pid=88115)[0m     lr: 0.1
[2m[36m(func pid=88115)[0m     maximize: False
[2m[36m(func pid=88115)[0m     momentum: 0.99
[2m[36m(func pid=88115)[0m     nesterov: False
[2m[36m(func pid=88115)[0m     weight_decay: 1e-05
[2m[36m(func pid=88115)[0m )
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=70360)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3964 | Steps: 4 | Val loss: 0.3093 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5117 | Steps: 4 | Val loss: 0.3465 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3184 | Steps: 4 | Val loss: 0.2696 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 20:25:18 (running for 00:35:35.96)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1432499960064888
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00016 | RUNNING    | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.398 |  0.167 |                   74 |
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.323 |  0.145 |                   72 |
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.701 |  0.18  |                    2 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_84a75_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6884 | Steps: 4 | Val loss: 0.3395 | Batch size: 32 | lr: 0.1 | Duration: 5.05s
[2m[36m(func pid=70360)[0m rmse: 0.16692575812339783
[2m[36m(func pid=70360)[0m mae:  0.1211254820227623
[2m[36m(func pid=70360)[0m rmse_per_class: [0.116, 0.253, 0.074, 0.319, 0.072, 0.185, 0.264, 0.132, 0.143, 0.11]
[2m[36m(func pid=71112)[0m rmse: 0.14729145169258118
[2m[36m(func pid=71112)[0m mae:  0.10064560174942017
[2m[36m(func pid=71112)[0m rmse_per_class: [0.106, 0.222, 0.035, 0.279, 0.052, 0.164, 0.249, 0.107, 0.146, 0.112]
[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.17666539549827576
[2m[36m(func pid=87283)[0m mae:  0.1295044869184494
[2m[36m(func pid=87283)[0m rmse_per_class: [0.111, 0.258, 0.099, 0.33, 0.094, 0.191, 0.292, 0.136, 0.144, 0.11]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.17862488329410553
[2m[36m(func pid=88115)[0m mae:  0.13088369369506836
[2m[36m(func pid=88115)[0m rmse_per_class: [0.111, 0.266, 0.118, 0.336, 0.08, 0.19, 0.284, 0.146, 0.143, 0.112]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4144 | Steps: 4 | Val loss: 0.3190 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3270 | Steps: 4 | Val loss: 0.2676 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4555 | Steps: 4 | Val loss: 0.3816 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=87283)[0m rmse: 0.17570476233959198
[2m[36m(func pid=87283)[0m mae:  0.1288277804851532
[2m[36m(func pid=87283)[0m rmse_per_class: [0.112, 0.256, 0.101, 0.335, 0.077, 0.188, 0.289, 0.133, 0.152, 0.114]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=71112)[0m rmse: 0.1466454267501831
[2m[36m(func pid=71112)[0m mae:  0.09916606545448303
[2m[36m(func pid=71112)[0m rmse_per_class: [0.102, 0.224, 0.038, 0.279, 0.052, 0.162, 0.243, 0.112, 0.145, 0.11]
[2m[36m(func pid=88115)[0m rmse: 0.1612345576286316
[2m[36m(func pid=88115)[0m mae:  0.11584792286157608
[2m[36m(func pid=88115)[0m rmse_per_class: [0.111, 0.252, 0.065, 0.295, 0.057, 0.183, 0.268, 0.144, 0.135, 0.102]
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4195 | Steps: 4 | Val loss: 0.3447 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:25:24 (running for 00:35:42.05)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.14499999582767487
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.318 |  0.147 |                   73 |
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.414 |  0.176 |                    4 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.688 |  0.179 |                    1 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=71112)[0m 
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88955)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=88955)[0m Configuration completed!
[2m[36m(func pid=88955)[0m New optimizer parameters:
[2m[36m(func pid=88955)[0m SGD (
[2m[36m(func pid=88955)[0m Parameter Group 0
[2m[36m(func pid=88955)[0m     dampening: 0
[2m[36m(func pid=88955)[0m     differentiable: False
[2m[36m(func pid=88955)[0m     foreach: None
[2m[36m(func pid=88955)[0m     lr: 0.0001
[2m[36m(func pid=88955)[0m     maximize: False
[2m[36m(func pid=88955)[0m     momentum: 0.9
[2m[36m(func pid=88955)[0m     nesterov: False
[2m[36m(func pid=88955)[0m     weight_decay: 1e-05
[2m[36m(func pid=88955)[0m )
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:25:29 (running for 00:35:47.59)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.14499999582767487
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00017 | RUNNING    | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.327 |  0.147 |                   74 |
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.419 |  0.173 |                    5 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.455 |  0.161 |                    2 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=87283)[0m rmse: 0.17321479320526123
[2m[36m(func pid=87283)[0m mae:  0.12647408246994019
[2m[36m(func pid=87283)[0m rmse_per_class: [0.111, 0.255, 0.094, 0.336, 0.067, 0.186, 0.281, 0.131, 0.158, 0.114]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=71112)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3285 | Steps: 4 | Val loss: 0.2699 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5887 | Steps: 4 | Val loss: 0.4997 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8928 | Steps: 4 | Val loss: 0.7070 | Batch size: 32 | lr: 0.0001 | Duration: 4.83s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4612 | Steps: 4 | Val loss: 0.3856 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=71112)[0m rmse: 0.14844608306884766
[2m[36m(func pid=71112)[0m mae:  0.10046806186437607
[2m[36m(func pid=71112)[0m rmse_per_class: [0.104, 0.224, 0.039, 0.284, 0.052, 0.163, 0.246, 0.112, 0.145, 0.115]
[2m[36m(func pid=88115)[0m rmse: 0.15992829203605652
[2m[36m(func pid=88115)[0m mae:  0.1097249761223793
[2m[36m(func pid=88115)[0m rmse_per_class: [0.099, 0.25, 0.05, 0.331, 0.055, 0.164, 0.282, 0.131, 0.133, 0.104]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.18272803723812103
[2m[36m(func pid=88955)[0m mae:  0.13453897833824158
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.267, 0.106, 0.339, 0.113, 0.19, 0.295, 0.143, 0.144, 0.113]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1696452796459198
[2m[36m(func pid=87283)[0m mae:  0.12279979884624481
[2m[36m(func pid=87283)[0m rmse_per_class: [0.112, 0.251, 0.086, 0.335, 0.061, 0.182, 0.272, 0.131, 0.157, 0.11]
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5344 | Steps: 4 | Val loss: 1.2421 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8968 | Steps: 4 | Val loss: 0.6946 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=88115)[0m rmse: 0.19594071805477142
[2m[36m(func pid=88115)[0m mae:  0.1226097121834755
[2m[36m(func pid=88115)[0m rmse_per_class: [0.107, 0.279, 0.05, 0.369, 0.056, 0.179, 0.56, 0.13, 0.135, 0.094]
[2m[36m(func pid=88955)[0m rmse: 0.18166275322437286
[2m[36m(func pid=88955)[0m mae:  0.13375982642173767
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.266, 0.103, 0.339, 0.113, 0.19, 0.294, 0.141, 0.143, 0.111]
== Status ==
Current time: 2024-01-07 20:25:35 (running for 00:35:52.70)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.419 |  0.173 |                    5 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.589 |  0.16  |                    3 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.893 |  0.183 |                    1 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 20:25:41 (running for 00:35:59.63)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.419 |  0.173 |                    5 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.534 |  0.196 |                    4 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.893 |  0.183 |                    1 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=89738)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=89738)[0m Configuration completed!
[2m[36m(func pid=89738)[0m New optimizer parameters:
[2m[36m(func pid=89738)[0m SGD (
[2m[36m(func pid=89738)[0m Parameter Group 0
[2m[36m(func pid=89738)[0m     dampening: 0
[2m[36m(func pid=89738)[0m     differentiable: False
[2m[36m(func pid=89738)[0m     foreach: None
[2m[36m(func pid=89738)[0m     lr: 0.001
[2m[36m(func pid=89738)[0m     maximize: False
[2m[36m(func pid=89738)[0m     momentum: 0.9
[2m[36m(func pid=89738)[0m     nesterov: False
[2m[36m(func pid=89738)[0m     weight_decay: 1e-05
[2m[36m(func pid=89738)[0m )
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4114 | Steps: 4 | Val loss: 2.0288 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5088 | Steps: 4 | Val loss: 0.4142 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8885 | Steps: 4 | Val loss: 0.6877 | Batch size: 32 | lr: 0.0001 | Duration: 3.27s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8928 | Steps: 4 | Val loss: 0.6944 | Batch size: 32 | lr: 0.001 | Duration: 4.65s
== Status ==
Current time: 2024-01-07 20:25:46 (running for 00:36:04.65)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.461 |  0.17  |                    6 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.534 |  0.196 |                    4 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.897 |  0.182 |                    2 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.2067938596010208
[2m[36m(func pid=88115)[0m mae:  0.12934033572673798
[2m[36m(func pid=88115)[0m rmse_per_class: [0.11, 0.285, 0.05, 0.379, 0.056, 0.18, 0.651, 0.126, 0.137, 0.094]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1623992621898651
[2m[36m(func pid=87283)[0m mae:  0.1159825325012207
[2m[36m(func pid=87283)[0m rmse_per_class: [0.107, 0.237, 0.07, 0.323, 0.058, 0.18, 0.261, 0.134, 0.152, 0.103]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1809757649898529
[2m[36m(func pid=88955)[0m mae:  0.13318614661693573
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.189, 0.294, 0.14, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.18242567777633667
[2m[36m(func pid=89738)[0m mae:  0.13427641987800598
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4422 | Steps: 4 | Val loss: 0.7135 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8898 | Steps: 4 | Val loss: 0.6862 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5380 | Steps: 4 | Val loss: 0.4364 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8752 | Steps: 4 | Val loss: 0.6724 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=88115)[0m rmse: 0.19407279789447784
[2m[36m(func pid=88115)[0m mae:  0.12065216153860092
[2m[36m(func pid=88115)[0m rmse_per_class: [0.126, 0.274, 0.104, 0.379, 0.056, 0.173, 0.478, 0.121, 0.135, 0.094]
== Status ==
Current time: 2024-01-07 20:25:53 (running for 00:36:10.76)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.509 |  0.162 |                    7 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.442 |  0.194 |                    6 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.889 |  0.181 |                    3 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.893 |  0.182 |                    1 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=87283)[0m rmse: 0.1542629599571228
[2m[36m(func pid=87283)[0m mae:  0.1085689514875412
[2m[36m(func pid=87283)[0m rmse_per_class: [0.101, 0.231, 0.052, 0.301, 0.055, 0.18, 0.246, 0.135, 0.147, 0.096]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.18062643706798553
[2m[36m(func pid=88955)[0m mae:  0.13287630677223206
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.338, 0.111, 0.189, 0.294, 0.14, 0.142, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.18180319666862488
[2m[36m(func pid=89738)[0m mae:  0.1337825357913971
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8822 | Steps: 4 | Val loss: 0.6842 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4148 | Steps: 4 | Val loss: 0.3560 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5465 | Steps: 4 | Val loss: 0.4453 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8516 | Steps: 4 | Val loss: 0.6449 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:25:58 (running for 00:36:16.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.538 |  0.154 |                    8 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.415 |  0.173 |                    7 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.89  |  0.181 |                    4 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.875 |  0.182 |                    2 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17271672189235687
[2m[36m(func pid=88115)[0m mae:  0.11281236261129379
[2m[36m(func pid=88115)[0m rmse_per_class: [0.109, 0.26, 0.07, 0.371, 0.056, 0.192, 0.268, 0.163, 0.137, 0.101]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.18040446937084198
[2m[36m(func pid=88955)[0m mae:  0.1326867640018463
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.338, 0.11, 0.19, 0.295, 0.14, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15037287771701813
[2m[36m(func pid=87283)[0m mae:  0.10407184064388275
[2m[36m(func pid=87283)[0m rmse_per_class: [0.098, 0.227, 0.047, 0.297, 0.054, 0.181, 0.235, 0.129, 0.144, 0.092]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1809554547071457
[2m[36m(func pid=89738)[0m mae:  0.1330726444721222
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3990 | Steps: 4 | Val loss: 0.3750 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5506 | Steps: 4 | Val loss: 0.4366 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8818 | Steps: 4 | Val loss: 0.6817 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8160 | Steps: 4 | Val loss: 0.6161 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:26:04 (running for 00:36:21.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.547 |  0.15  |                    9 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.399 |  0.177 |                    8 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.882 |  0.18  |                    5 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.852 |  0.181 |                    3 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1773175746202469
[2m[36m(func pid=88115)[0m mae:  0.11602681875228882
[2m[36m(func pid=88115)[0m rmse_per_class: [0.117, 0.26, 0.062, 0.367, 0.056, 0.178, 0.246, 0.224, 0.149, 0.116]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1498737633228302
[2m[36m(func pid=87283)[0m mae:  0.10155818611383438
[2m[36m(func pid=87283)[0m rmse_per_class: [0.095, 0.227, 0.047, 0.303, 0.055, 0.184, 0.234, 0.128, 0.138, 0.088]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.18038848042488098
[2m[36m(func pid=88955)[0m mae:  0.13265936076641083
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.338, 0.11, 0.189, 0.295, 0.141, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.18028321862220764
[2m[36m(func pid=89738)[0m mae:  0.13248249888420105
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.264, 0.101, 0.336, 0.111, 0.19, 0.294, 0.139, 0.142, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3978 | Steps: 4 | Val loss: 0.4141 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8762 | Steps: 4 | Val loss: 0.6791 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5175 | Steps: 4 | Val loss: 0.4213 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7804 | Steps: 4 | Val loss: 0.5869 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 20:26:09 (running for 00:36:27.05)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.551 |  0.15  |                   10 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.399 |  0.177 |                    8 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.876 |  0.18  |                    7 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.816 |  0.18  |                    4 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m rmse: 0.18037322163581848
[2m[36m(func pid=88955)[0m mae:  0.13263428211212158
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.263, 0.1, 0.338, 0.11, 0.19, 0.295, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.18587246537208557
[2m[36m(func pid=88115)[0m mae:  0.11946340650320053
[2m[36m(func pid=88115)[0m rmse_per_class: [0.115, 0.254, 0.047, 0.362, 0.056, 0.199, 0.231, 0.263, 0.217, 0.115]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15413105487823486
[2m[36m(func pid=87283)[0m mae:  0.10209748893976212
[2m[36m(func pid=87283)[0m rmse_per_class: [0.098, 0.228, 0.048, 0.319, 0.055, 0.188, 0.262, 0.122, 0.136, 0.085]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1798720806837082
[2m[36m(func pid=89738)[0m mae:  0.13205108046531677
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.335, 0.111, 0.19, 0.295, 0.139, 0.141, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8733 | Steps: 4 | Val loss: 0.6769 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3967 | Steps: 4 | Val loss: 0.4093 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4996 | Steps: 4 | Val loss: 0.4142 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7449 | Steps: 4 | Val loss: 0.5593 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:26:14 (running for 00:36:32.35)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.517 |  0.154 |                   11 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.398 |  0.186 |                    9 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.873 |  0.181 |                    8 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.78  |  0.18  |                    5 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m rmse: 0.18050292134284973
[2m[36m(func pid=88955)[0m mae:  0.13271573185920715
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.263, 0.1, 0.338, 0.111, 0.19, 0.295, 0.14, 0.144, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.1847669631242752
[2m[36m(func pid=88115)[0m mae:  0.11869041621685028
[2m[36m(func pid=88115)[0m rmse_per_class: [0.142, 0.256, 0.047, 0.348, 0.056, 0.202, 0.221, 0.296, 0.15, 0.13]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1611027717590332
[2m[36m(func pid=87283)[0m mae:  0.10303554683923721
[2m[36m(func pid=87283)[0m rmse_per_class: [0.1, 0.234, 0.049, 0.329, 0.055, 0.192, 0.314, 0.12, 0.134, 0.084]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17960302531719208
[2m[36m(func pid=89738)[0m mae:  0.13177339732646942
[2m[36m(func pid=89738)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.335, 0.11, 0.19, 0.294, 0.138, 0.142, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3755 | Steps: 4 | Val loss: 0.3710 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8647 | Steps: 4 | Val loss: 0.6739 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4740 | Steps: 4 | Val loss: 0.4299 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7123 | Steps: 4 | Val loss: 0.5340 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:26:20 (running for 00:36:37.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.5   |  0.161 |                   12 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.397 |  0.185 |                   10 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.865 |  0.18  |                    9 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.745 |  0.18  |                    6 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1930805742740631
[2m[36m(func pid=88115)[0m mae:  0.11965646594762802
[2m[36m(func pid=88115)[0m rmse_per_class: [0.115, 0.267, 0.049, 0.325, 0.056, 0.178, 0.244, 0.258, 0.323, 0.115]
[2m[36m(func pid=88955)[0m rmse: 0.1802459955215454
[2m[36m(func pid=88955)[0m mae:  0.13249556720256805
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.11, 0.19, 0.295, 0.14, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1697460114955902
[2m[36m(func pid=87283)[0m mae:  0.10669173300266266
[2m[36m(func pid=87283)[0m rmse_per_class: [0.099, 0.227, 0.049, 0.343, 0.055, 0.198, 0.386, 0.122, 0.134, 0.084]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17923036217689514
[2m[36m(func pid=89738)[0m mae:  0.13144537806510925
[2m[36m(func pid=89738)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.334, 0.109, 0.191, 0.293, 0.138, 0.142, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3843 | Steps: 4 | Val loss: 0.4071 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8611 | Steps: 4 | Val loss: 0.6705 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3998 | Steps: 4 | Val loss: 0.4825 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6817 | Steps: 4 | Val loss: 0.5096 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=88955)[0m rmse: 0.180079847574234
[2m[36m(func pid=88955)[0m mae:  0.1323082000017166
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.295, 0.14, 0.143, 0.109]
== Status ==
Current time: 2024-01-07 20:26:25 (running for 00:36:43.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.474 |  0.17  |                   13 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.375 |  0.193 |                   11 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.861 |  0.18  |                   10 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.712 |  0.179 |                    7 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.19732043147087097
[2m[36m(func pid=88115)[0m mae:  0.12123920768499374
[2m[36m(func pid=88115)[0m rmse_per_class: [0.111, 0.255, 0.049, 0.357, 0.055, 0.203, 0.247, 0.259, 0.323, 0.114]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.17595812678337097
[2m[36m(func pid=87283)[0m mae:  0.11115182936191559
[2m[36m(func pid=87283)[0m rmse_per_class: [0.099, 0.227, 0.049, 0.359, 0.056, 0.204, 0.415, 0.123, 0.144, 0.084]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1787949949502945
[2m[36m(func pid=89738)[0m mae:  0.1311221718788147
[2m[36m(func pid=89738)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.334, 0.108, 0.191, 0.292, 0.138, 0.142, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4468 | Steps: 4 | Val loss: 0.5094 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8527 | Steps: 4 | Val loss: 0.6675 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3854 | Steps: 4 | Val loss: 0.5060 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6527 | Steps: 4 | Val loss: 0.4919 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:26:30 (running for 00:36:48.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.4   |  0.176 |                   14 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.384 |  0.197 |                   12 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.853 |  0.18  |                   11 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.682 |  0.179 |                    8 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m rmse: 0.18020425736904144
[2m[36m(func pid=88955)[0m mae:  0.13241906464099884
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.262, 0.099, 0.338, 0.111, 0.19, 0.295, 0.14, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.22418789565563202
[2m[36m(func pid=88115)[0m mae:  0.13726277649402618
[2m[36m(func pid=88115)[0m rmse_per_class: [0.11, 0.261, 0.071, 0.377, 0.055, 0.212, 0.269, 0.242, 0.395, 0.251]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.17905578017234802
[2m[36m(func pid=87283)[0m mae:  0.11358191072940826
[2m[36m(func pid=87283)[0m rmse_per_class: [0.103, 0.229, 0.049, 0.366, 0.056, 0.204, 0.387, 0.122, 0.189, 0.085]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1788214147090912
[2m[36m(func pid=89738)[0m mae:  0.1312185525894165
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.334, 0.106, 0.19, 0.292, 0.138, 0.143, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8489 | Steps: 4 | Val loss: 0.6626 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3631 | Steps: 4 | Val loss: 1.1286 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3943 | Steps: 4 | Val loss: 0.4098 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6263 | Steps: 4 | Val loss: 0.4772 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=88955)[0m rmse: 0.18003588914871216
[2m[36m(func pid=88955)[0m mae:  0.13230949640274048
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.262, 0.098, 0.338, 0.111, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:26:36 (running for 00:36:54.14)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.385 |  0.179 |                   15 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.447 |  0.224 |                   13 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.849 |  0.18  |                   12 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.653 |  0.179 |                    9 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.22497864067554474
[2m[36m(func pid=88115)[0m mae:  0.14165356755256653
[2m[36m(func pid=88115)[0m rmse_per_class: [0.11, 0.27, 0.049, 0.384, 0.055, 0.206, 0.244, 0.297, 0.54, 0.094]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.17498409748077393
[2m[36m(func pid=87283)[0m mae:  0.1106233149766922
[2m[36m(func pid=87283)[0m rmse_per_class: [0.101, 0.228, 0.049, 0.36, 0.056, 0.202, 0.347, 0.138, 0.186, 0.084]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17882004380226135
[2m[36m(func pid=89738)[0m mae:  0.13125482201576233
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.26, 0.098, 0.335, 0.103, 0.191, 0.291, 0.138, 0.145, 0.111]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3890 | Steps: 4 | Val loss: 1.4436 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8479 | Steps: 4 | Val loss: 0.6582 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6024 | Steps: 4 | Val loss: 0.4610 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3873 | Steps: 4 | Val loss: 0.3731 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:26:41 (running for 00:36:59.49)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.394 |  0.175 |                   16 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.363 |  0.225 |                   14 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.848 |  0.18  |                   13 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.626 |  0.179 |                   10 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m rmse: 0.17986683547496796
[2m[36m(func pid=88955)[0m mae:  0.132188081741333
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.338, 0.111, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.2151404321193695
[2m[36m(func pid=88115)[0m mae:  0.13573762774467468
[2m[36m(func pid=88115)[0m rmse_per_class: [0.11, 0.257, 0.049, 0.384, 0.056, 0.202, 0.234, 0.286, 0.482, 0.093]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17902526259422302
[2m[36m(func pid=89738)[0m mae:  0.13136126101016998
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.261, 0.099, 0.335, 0.101, 0.19, 0.29, 0.139, 0.145, 0.113]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.17314355075359344
[2m[36m(func pid=87283)[0m mae:  0.10886245965957642
[2m[36m(func pid=87283)[0m rmse_per_class: [0.099, 0.234, 0.049, 0.356, 0.056, 0.196, 0.317, 0.152, 0.19, 0.083]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3629 | Steps: 4 | Val loss: 1.4833 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8416 | Steps: 4 | Val loss: 0.6514 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5812 | Steps: 4 | Val loss: 0.4466 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3722 | Steps: 4 | Val loss: 0.3512 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:26:47 (running for 00:37:04.90)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.387 |  0.173 |                   17 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.363 |  0.207 |                   16 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.848 |  0.18  |                   13 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.602 |  0.179 |                   11 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.2065122127532959
[2m[36m(func pid=88115)[0m mae:  0.13265126943588257
[2m[36m(func pid=88115)[0m rmse_per_class: [0.11, 0.262, 0.049, 0.381, 0.06, 0.21, 0.24, 0.296, 0.289, 0.169]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17977169156074524
[2m[36m(func pid=88955)[0m mae:  0.13210037350654602
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.097, 0.338, 0.111, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.178846076130867
[2m[36m(func pid=89738)[0m mae:  0.13121435046195984
[2m[36m(func pid=89738)[0m rmse_per_class: [0.119, 0.261, 0.098, 0.335, 0.098, 0.19, 0.289, 0.139, 0.145, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.17109587788581848
[2m[36m(func pid=87283)[0m mae:  0.10776285827159882
[2m[36m(func pid=87283)[0m rmse_per_class: [0.096, 0.237, 0.049, 0.349, 0.056, 0.19, 0.284, 0.163, 0.203, 0.085]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3421 | Steps: 4 | Val loss: 0.9395 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8356 | Steps: 4 | Val loss: 0.6498 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5610 | Steps: 4 | Val loss: 0.4328 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3979 | Steps: 4 | Val loss: 0.3692 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 20:26:52 (running for 00:37:10.38)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.372 |  0.171 |                   18 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.342 |  0.207 |                   17 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.842 |  0.18  |                   14 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.581 |  0.179 |                   12 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.20694871246814728
[2m[36m(func pid=88115)[0m mae:  0.13208474218845367
[2m[36m(func pid=88115)[0m rmse_per_class: [0.109, 0.27, 0.05, 0.372, 0.069, 0.223, 0.239, 0.333, 0.25, 0.153]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17977026104927063
[2m[36m(func pid=88955)[0m mae:  0.13210129737854004
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.262, 0.098, 0.337, 0.11, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17889511585235596
[2m[36m(func pid=89738)[0m mae:  0.1312149167060852
[2m[36m(func pid=89738)[0m rmse_per_class: [0.12, 0.261, 0.097, 0.335, 0.098, 0.19, 0.289, 0.14, 0.145, 0.115]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1740475594997406
[2m[36m(func pid=87283)[0m mae:  0.10935620218515396
[2m[36m(func pid=87283)[0m rmse_per_class: [0.096, 0.239, 0.048, 0.35, 0.056, 0.18, 0.273, 0.186, 0.225, 0.087]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3580 | Steps: 4 | Val loss: 0.7010 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8311 | Steps: 4 | Val loss: 0.6464 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5461 | Steps: 4 | Val loss: 0.4201 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3530 | Steps: 4 | Val loss: 0.3488 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 20:26:58 (running for 00:37:15.84)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.398 |  0.174 |                   19 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.358 |  0.204 |                   18 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.836 |  0.18  |                   15 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.561 |  0.179 |                   13 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.203639954328537
[2m[36m(func pid=88115)[0m mae:  0.1294650137424469
[2m[36m(func pid=88115)[0m rmse_per_class: [0.109, 0.274, 0.051, 0.365, 0.076, 0.219, 0.238, 0.341, 0.248, 0.115]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1797613799571991
[2m[36m(func pid=88955)[0m mae:  0.13205182552337646
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.111, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17861256003379822
[2m[36m(func pid=89738)[0m mae:  0.1309545338153839
[2m[36m(func pid=89738)[0m rmse_per_class: [0.12, 0.26, 0.096, 0.334, 0.098, 0.19, 0.288, 0.139, 0.145, 0.115]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1691380739212036
[2m[36m(func pid=87283)[0m mae:  0.10648924112319946
[2m[36m(func pid=87283)[0m rmse_per_class: [0.099, 0.233, 0.048, 0.343, 0.055, 0.171, 0.252, 0.171, 0.229, 0.09]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3841 | Steps: 4 | Val loss: 0.5176 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8259 | Steps: 4 | Val loss: 0.6410 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5283 | Steps: 4 | Val loss: 0.4085 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3521 | Steps: 4 | Val loss: 0.3166 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:27:03 (running for 00:37:21.10)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.353 |  0.169 |                   20 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.384 |  0.198 |                   19 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.831 |  0.18  |                   16 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.546 |  0.179 |                   14 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1982090324163437
[2m[36m(func pid=88115)[0m mae:  0.12500889599323273
[2m[36m(func pid=88115)[0m rmse_per_class: [0.114, 0.268, 0.054, 0.356, 0.08, 0.204, 0.24, 0.333, 0.246, 0.088]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17964769899845123
[2m[36m(func pid=88955)[0m mae:  0.1319832056760788
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.11, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17848394811153412
[2m[36m(func pid=89738)[0m mae:  0.1308036893606186
[2m[36m(func pid=89738)[0m rmse_per_class: [0.121, 0.26, 0.095, 0.334, 0.098, 0.191, 0.288, 0.139, 0.145, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.16105644404888153
[2m[36m(func pid=87283)[0m mae:  0.1018955335021019
[2m[36m(func pid=87283)[0m rmse_per_class: [0.101, 0.227, 0.044, 0.326, 0.056, 0.168, 0.232, 0.161, 0.202, 0.094]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4038 | Steps: 4 | Val loss: 0.3634 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8205 | Steps: 4 | Val loss: 0.6346 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5127 | Steps: 4 | Val loss: 0.3989 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3612 | Steps: 4 | Val loss: 0.2980 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:27:08 (running for 00:37:26.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.352 |  0.161 |                   21 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.404 |  0.18  |                   20 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.826 |  0.18  |                   17 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.528 |  0.178 |                   15 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.179914653301239
[2m[36m(func pid=88115)[0m mae:  0.11234550178050995
[2m[36m(func pid=88115)[0m rmse_per_class: [0.124, 0.266, 0.056, 0.337, 0.088, 0.16, 0.24, 0.224, 0.218, 0.085]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17935778200626373
[2m[36m(func pid=88955)[0m mae:  0.13172170519828796
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.11, 0.189, 0.293, 0.14, 0.143, 0.107]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17866623401641846
[2m[36m(func pid=89738)[0m mae:  0.13092273473739624
[2m[36m(func pid=89738)[0m rmse_per_class: [0.122, 0.26, 0.096, 0.334, 0.098, 0.191, 0.288, 0.139, 0.145, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1574571281671524
[2m[36m(func pid=87283)[0m mae:  0.10040895640850067
[2m[36m(func pid=87283)[0m rmse_per_class: [0.104, 0.226, 0.037, 0.32, 0.056, 0.162, 0.227, 0.153, 0.189, 0.101]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3332 | Steps: 4 | Val loss: 0.3115 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8181 | Steps: 4 | Val loss: 0.6310 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5011 | Steps: 4 | Val loss: 0.3899 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3416 | Steps: 4 | Val loss: 0.2874 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:27:13 (running for 00:37:31.56)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.361 |  0.157 |                   22 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.333 |  0.178 |                   21 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.82  |  0.179 |                   18 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.513 |  0.179 |                   16 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17774401605129242
[2m[36m(func pid=88115)[0m mae:  0.10860776901245117
[2m[36m(func pid=88115)[0m rmse_per_class: [0.136, 0.248, 0.102, 0.317, 0.108, 0.16, 0.249, 0.195, 0.164, 0.099]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17945894598960876
[2m[36m(func pid=88955)[0m mae:  0.1318417489528656
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.11, 0.19, 0.294, 0.139, 0.143, 0.108]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1782699078321457
[2m[36m(func pid=89738)[0m mae:  0.13060131669044495
[2m[36m(func pid=89738)[0m rmse_per_class: [0.121, 0.26, 0.095, 0.333, 0.097, 0.191, 0.287, 0.139, 0.145, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15549348294734955
[2m[36m(func pid=87283)[0m mae:  0.0997777134180069
[2m[36m(func pid=87283)[0m rmse_per_class: [0.106, 0.225, 0.031, 0.317, 0.056, 0.157, 0.223, 0.146, 0.183, 0.111]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3722 | Steps: 4 | Val loss: 0.3175 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4922 | Steps: 4 | Val loss: 0.3803 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8112 | Steps: 4 | Val loss: 0.6287 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3601 | Steps: 4 | Val loss: 0.2854 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=88115)[0m rmse: 0.18405663967132568
[2m[36m(func pid=88115)[0m mae:  0.11125414073467255
[2m[36m(func pid=88115)[0m rmse_per_class: [0.118, 0.24, 0.11, 0.31, 0.112, 0.176, 0.256, 0.234, 0.162, 0.122]
[2m[36m(func pid=88115)[0m 
== Status ==
Current time: 2024-01-07 20:27:19 (running for 00:37:37.02)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.342 |  0.155 |                   23 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.372 |  0.184 |                   22 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.818 |  0.179 |                   19 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.501 |  0.178 |                   17 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m rmse: 0.17775414884090424
[2m[36m(func pid=89738)[0m mae:  0.13016772270202637
[2m[36m(func pid=89738)[0m rmse_per_class: [0.121, 0.26, 0.094, 0.333, 0.096, 0.19, 0.285, 0.139, 0.144, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17954418063163757
[2m[36m(func pid=88955)[0m mae:  0.13190311193466187
[2m[36m(func pid=88955)[0m rmse_per_class: [0.114, 0.262, 0.098, 0.336, 0.109, 0.19, 0.294, 0.139, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15325722098350525
[2m[36m(func pid=87283)[0m mae:  0.09796586632728577
[2m[36m(func pid=87283)[0m rmse_per_class: [0.1, 0.226, 0.029, 0.313, 0.056, 0.152, 0.225, 0.144, 0.179, 0.109]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3583 | Steps: 4 | Val loss: 0.3631 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4836 | Steps: 4 | Val loss: 0.3734 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8103 | Steps: 4 | Val loss: 0.6255 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3505 | Steps: 4 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:27:24 (running for 00:37:42.33)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.36  |  0.153 |                   24 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.358 |  0.198 |                   23 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.811 |  0.18  |                   20 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.492 |  0.178 |                   18 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.19843995571136475
[2m[36m(func pid=88115)[0m mae:  0.121212899684906
[2m[36m(func pid=88115)[0m rmse_per_class: [0.117, 0.246, 0.073, 0.319, 0.089, 0.209, 0.285, 0.262, 0.155, 0.229]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.177586629986763
[2m[36m(func pid=89738)[0m mae:  0.13005682826042175
[2m[36m(func pid=89738)[0m rmse_per_class: [0.121, 0.261, 0.095, 0.332, 0.095, 0.19, 0.285, 0.139, 0.144, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17953777313232422
[2m[36m(func pid=88955)[0m mae:  0.13187231123447418
[2m[36m(func pid=88955)[0m rmse_per_class: [0.114, 0.261, 0.099, 0.336, 0.108, 0.19, 0.294, 0.139, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15634292364120483
[2m[36m(func pid=87283)[0m mae:  0.09921251237392426
[2m[36m(func pid=87283)[0m rmse_per_class: [0.096, 0.227, 0.023, 0.317, 0.055, 0.154, 0.229, 0.182, 0.166, 0.115]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3444 | Steps: 4 | Val loss: 0.3778 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4742 | Steps: 4 | Val loss: 0.3667 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8059 | Steps: 4 | Val loss: 0.6230 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3535 | Steps: 4 | Val loss: 0.3051 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 20:27:29 (running for 00:37:47.52)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.35  |  0.156 |                   25 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.344 |  0.189 |                   24 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.81  |  0.18  |                   21 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.484 |  0.178 |                   19 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.18870408833026886
[2m[36m(func pid=88115)[0m mae:  0.11731831729412079
[2m[36m(func pid=88115)[0m rmse_per_class: [0.117, 0.249, 0.039, 0.338, 0.079, 0.206, 0.273, 0.259, 0.147, 0.181]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17735877633094788
[2m[36m(func pid=89738)[0m mae:  0.12982317805290222
[2m[36m(func pid=89738)[0m rmse_per_class: [0.12, 0.26, 0.096, 0.332, 0.095, 0.19, 0.284, 0.138, 0.144, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1795213669538498
[2m[36m(func pid=88955)[0m mae:  0.13183195888996124
[2m[36m(func pid=88955)[0m rmse_per_class: [0.114, 0.261, 0.099, 0.336, 0.109, 0.19, 0.294, 0.139, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.16336365044116974
[2m[36m(func pid=87283)[0m mae:  0.10353769361972809
[2m[36m(func pid=87283)[0m rmse_per_class: [0.094, 0.228, 0.023, 0.328, 0.055, 0.16, 0.238, 0.218, 0.16, 0.13]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3321 | Steps: 4 | Val loss: 0.3989 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4664 | Steps: 4 | Val loss: 0.3626 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8031 | Steps: 4 | Val loss: 0.6201 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3416 | Steps: 4 | Val loss: 0.3147 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:27:35 (running for 00:37:52.77)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.354 |  0.163 |                   26 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.332 |  0.183 |                   25 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.806 |  0.18  |                   22 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.474 |  0.177 |                   20 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.18267300724983215
[2m[36m(func pid=88115)[0m mae:  0.11403077840805054
[2m[36m(func pid=88115)[0m rmse_per_class: [0.142, 0.242, 0.034, 0.353, 0.071, 0.199, 0.243, 0.251, 0.143, 0.149]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17734715342521667
[2m[36m(func pid=89738)[0m mae:  0.1298145204782486
[2m[36m(func pid=89738)[0m rmse_per_class: [0.12, 0.26, 0.096, 0.331, 0.095, 0.19, 0.285, 0.138, 0.144, 0.114]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17958834767341614
[2m[36m(func pid=88955)[0m mae:  0.1318555623292923
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.16802141070365906
[2m[36m(func pid=87283)[0m mae:  0.1060885414481163
[2m[36m(func pid=87283)[0m rmse_per_class: [0.097, 0.231, 0.023, 0.33, 0.055, 0.163, 0.242, 0.247, 0.16, 0.131]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3391 | Steps: 4 | Val loss: 0.3959 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4615 | Steps: 4 | Val loss: 0.3565 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7979 | Steps: 4 | Val loss: 0.6187 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3418 | Steps: 4 | Val loss: 0.3280 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 20:27:40 (running for 00:37:58.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.342 |  0.168 |                   27 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.339 |  0.183 |                   26 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.803 |  0.18  |                   23 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.466 |  0.177 |                   21 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.18275359272956848
[2m[36m(func pid=88115)[0m mae:  0.11413279920816422
[2m[36m(func pid=88115)[0m rmse_per_class: [0.19, 0.233, 0.036, 0.361, 0.073, 0.182, 0.223, 0.232, 0.142, 0.156]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1768893450498581
[2m[36m(func pid=89738)[0m mae:  0.12944331765174866
[2m[36m(func pid=89738)[0m rmse_per_class: [0.12, 0.26, 0.095, 0.33, 0.094, 0.189, 0.285, 0.138, 0.144, 0.113]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17968714237213135
[2m[36m(func pid=88955)[0m mae:  0.13192257285118103
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.109, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.17314089834690094
[2m[36m(func pid=87283)[0m mae:  0.1088443249464035
[2m[36m(func pid=87283)[0m rmse_per_class: [0.096, 0.228, 0.024, 0.331, 0.054, 0.169, 0.248, 0.299, 0.153, 0.13]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3301 | Steps: 4 | Val loss: 0.4043 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4493 | Steps: 4 | Val loss: 0.3508 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7913 | Steps: 4 | Val loss: 0.6155 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3595 | Steps: 4 | Val loss: 0.3431 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 20:27:45 (running for 00:38:03.46)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.342 |  0.173 |                   28 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.33  |  0.185 |                   27 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.798 |  0.18  |                   24 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.462 |  0.177 |                   22 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.18505947291851044
[2m[36m(func pid=88115)[0m mae:  0.11484072357416153
[2m[36m(func pid=88115)[0m rmse_per_class: [0.181, 0.223, 0.039, 0.368, 0.079, 0.168, 0.245, 0.223, 0.143, 0.182]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17602898180484772
[2m[36m(func pid=89738)[0m mae:  0.12871046364307404
[2m[36m(func pid=89738)[0m rmse_per_class: [0.119, 0.259, 0.094, 0.329, 0.093, 0.189, 0.284, 0.137, 0.144, 0.112]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17953696846961975
[2m[36m(func pid=88955)[0m mae:  0.13181693851947784
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1769506335258484
[2m[36m(func pid=87283)[0m mae:  0.11102256923913956
[2m[36m(func pid=87283)[0m rmse_per_class: [0.096, 0.226, 0.025, 0.333, 0.053, 0.177, 0.247, 0.328, 0.15, 0.133]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3661 | Steps: 4 | Val loss: 0.3841 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4444 | Steps: 4 | Val loss: 0.3445 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3171 | Steps: 4 | Val loss: 0.3105 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7857 | Steps: 4 | Val loss: 0.6115 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:27:51 (running for 00:38:08.85)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.359 |  0.177 |                   29 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.366 |  0.189 |                   28 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.791 |  0.18  |                   25 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.449 |  0.176 |                   23 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.18946360051631927
[2m[36m(func pid=88115)[0m mae:  0.11644963175058365
[2m[36m(func pid=88115)[0m rmse_per_class: [0.207, 0.223, 0.036, 0.365, 0.085, 0.162, 0.242, 0.23, 0.156, 0.188]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17547312378883362
[2m[36m(func pid=89738)[0m mae:  0.1282343864440918
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.259, 0.093, 0.329, 0.093, 0.188, 0.283, 0.137, 0.143, 0.111]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1794462502002716
[2m[36m(func pid=88955)[0m mae:  0.13177093863487244
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.16264185309410095
[2m[36m(func pid=87283)[0m mae:  0.10083724558353424
[2m[36m(func pid=87283)[0m rmse_per_class: [0.092, 0.216, 0.025, 0.299, 0.052, 0.175, 0.234, 0.264, 0.15, 0.117]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3217 | Steps: 4 | Val loss: 0.3523 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4384 | Steps: 4 | Val loss: 0.3407 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7807 | Steps: 4 | Val loss: 0.6066 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3225 | Steps: 4 | Val loss: 0.3029 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:27:56 (running for 00:38:14.23)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.317 |  0.163 |                   30 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.322 |  0.193 |                   29 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.786 |  0.179 |                   26 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.444 |  0.175 |                   24 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.19332441687583923
[2m[36m(func pid=88115)[0m mae:  0.11695325374603271
[2m[36m(func pid=88115)[0m rmse_per_class: [0.226, 0.217, 0.073, 0.351, 0.089, 0.163, 0.25, 0.232, 0.156, 0.176]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17523103952407837
[2m[36m(func pid=89738)[0m mae:  0.128006249666214
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.259, 0.093, 0.328, 0.092, 0.189, 0.282, 0.137, 0.143, 0.111]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1794862598180771
[2m[36m(func pid=88955)[0m mae:  0.13178758323192596
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15920403599739075
[2m[36m(func pid=87283)[0m mae:  0.09783470630645752
[2m[36m(func pid=87283)[0m rmse_per_class: [0.085, 0.213, 0.024, 0.294, 0.052, 0.177, 0.225, 0.239, 0.152, 0.132]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3447 | Steps: 4 | Val loss: 0.3590 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4387 | Steps: 4 | Val loss: 0.3362 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3161 | Steps: 4 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7798 | Steps: 4 | Val loss: 0.6041 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:28:02 (running for 00:38:19.80)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.323 |  0.159 |                   31 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.345 |  0.195 |                   30 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.781 |  0.179 |                   27 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.438 |  0.175 |                   25 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1945461481809616
[2m[36m(func pid=88115)[0m mae:  0.11855298280715942
[2m[36m(func pid=88115)[0m rmse_per_class: [0.231, 0.218, 0.068, 0.35, 0.089, 0.172, 0.252, 0.269, 0.151, 0.145]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17439210414886475
[2m[36m(func pid=89738)[0m mae:  0.127317875623703
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.258, 0.09, 0.328, 0.093, 0.189, 0.28, 0.137, 0.143, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15438388288021088
[2m[36m(func pid=87283)[0m mae:  0.09402139484882355
[2m[36m(func pid=87283)[0m rmse_per_class: [0.079, 0.216, 0.024, 0.286, 0.052, 0.175, 0.22, 0.203, 0.148, 0.14]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17943081259727478
[2m[36m(func pid=88955)[0m mae:  0.1317729353904724
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3053 | Steps: 4 | Val loss: 0.3606 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4326 | Steps: 4 | Val loss: 0.3323 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3165 | Steps: 4 | Val loss: 0.2872 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7747 | Steps: 4 | Val loss: 0.6019 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 20:28:07 (running for 00:38:25.36)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.316 |  0.154 |                   32 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.305 |  0.189 |                   31 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.78  |  0.179 |                   28 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.439 |  0.174 |                   26 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1892048865556717
[2m[36m(func pid=88115)[0m mae:  0.11612342298030853
[2m[36m(func pid=88115)[0m rmse_per_class: [0.203, 0.229, 0.057, 0.336, 0.088, 0.184, 0.248, 0.283, 0.147, 0.118]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1739848107099533
[2m[36m(func pid=89738)[0m mae:  0.12701596319675446
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.257, 0.088, 0.327, 0.093, 0.188, 0.279, 0.137, 0.143, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15286016464233398
[2m[36m(func pid=87283)[0m mae:  0.09234447777271271
[2m[36m(func pid=87283)[0m rmse_per_class: [0.078, 0.23, 0.025, 0.282, 0.053, 0.176, 0.215, 0.183, 0.144, 0.144]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17943954467773438
[2m[36m(func pid=88955)[0m mae:  0.13174405694007874
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3563 | Steps: 4 | Val loss: 0.3328 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4321 | Steps: 4 | Val loss: 0.3300 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3185 | Steps: 4 | Val loss: 0.2883 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7747 | Steps: 4 | Val loss: 0.5981 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 20:28:12 (running for 00:38:30.59)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.316 |  0.153 |                   33 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.356 |  0.176 |                   32 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.775 |  0.179 |                   29 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.433 |  0.174 |                   27 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1763196438550949
[2m[36m(func pid=88115)[0m mae:  0.10932116210460663
[2m[36m(func pid=88115)[0m rmse_per_class: [0.159, 0.23, 0.046, 0.323, 0.087, 0.177, 0.242, 0.245, 0.142, 0.113]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17381498217582703
[2m[36m(func pid=89738)[0m mae:  0.12688112258911133
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.257, 0.088, 0.328, 0.091, 0.188, 0.28, 0.136, 0.143, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15435372292995453
[2m[36m(func pid=87283)[0m mae:  0.09228911995887756
[2m[36m(func pid=87283)[0m rmse_per_class: [0.079, 0.248, 0.028, 0.277, 0.054, 0.178, 0.212, 0.157, 0.148, 0.163]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17950472235679626
[2m[36m(func pid=88955)[0m mae:  0.13182495534420013
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3165 | Steps: 4 | Val loss: 0.3056 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4304 | Steps: 4 | Val loss: 0.3270 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2991 | Steps: 4 | Val loss: 0.2929 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7692 | Steps: 4 | Val loss: 0.5953 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:28:18 (running for 00:38:35.85)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.318 |  0.154 |                   34 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.317 |  0.164 |                   33 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.775 |  0.18  |                   30 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.432 |  0.174 |                   28 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16355225443840027
[2m[36m(func pid=88115)[0m mae:  0.10343901067972183
[2m[36m(func pid=88115)[0m rmse_per_class: [0.133, 0.225, 0.044, 0.323, 0.09, 0.164, 0.253, 0.169, 0.134, 0.101]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17331239581108093
[2m[36m(func pid=89738)[0m mae:  0.12638609111309052
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.256, 0.088, 0.328, 0.09, 0.188, 0.278, 0.135, 0.143, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1579723060131073
[2m[36m(func pid=87283)[0m mae:  0.09486272186040878
[2m[36m(func pid=87283)[0m rmse_per_class: [0.082, 0.256, 0.032, 0.28, 0.06, 0.175, 0.219, 0.118, 0.154, 0.204]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1795368194580078
[2m[36m(func pid=88955)[0m mae:  0.13184760510921478
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.11, 0.19, 0.294, 0.14, 0.142, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3237 | Steps: 4 | Val loss: 0.2963 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4218 | Steps: 4 | Val loss: 0.3247 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3017 | Steps: 4 | Val loss: 0.2906 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7634 | Steps: 4 | Val loss: 0.5931 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:28:23 (running for 00:38:41.46)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.299 |  0.158 |                   35 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.324 |  0.158 |                   34 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.769 |  0.18  |                   31 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.43  |  0.173 |                   29 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.15843252837657928
[2m[36m(func pid=88115)[0m mae:  0.1005374938249588
[2m[36m(func pid=88115)[0m rmse_per_class: [0.125, 0.213, 0.048, 0.32, 0.092, 0.175, 0.238, 0.145, 0.139, 0.09]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1731356680393219
[2m[36m(func pid=89738)[0m mae:  0.12624184787273407
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.257, 0.088, 0.327, 0.088, 0.188, 0.277, 0.135, 0.143, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1592341959476471
[2m[36m(func pid=87283)[0m mae:  0.09620852768421173
[2m[36m(func pid=87283)[0m rmse_per_class: [0.081, 0.245, 0.038, 0.279, 0.066, 0.172, 0.225, 0.111, 0.152, 0.224]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17932260036468506
[2m[36m(func pid=88955)[0m mae:  0.13169696927070618
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.108, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3364 | Steps: 4 | Val loss: 0.3006 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4203 | Steps: 4 | Val loss: 0.3224 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3031 | Steps: 4 | Val loss: 0.2933 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7593 | Steps: 4 | Val loss: 0.5898 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 20:28:28 (running for 00:38:46.62)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.302 |  0.159 |                   36 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.336 |  0.161 |                   35 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.763 |  0.179 |                   32 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.422 |  0.173 |                   30 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16050338745117188
[2m[36m(func pid=88115)[0m mae:  0.0996263325214386
[2m[36m(func pid=88115)[0m rmse_per_class: [0.144, 0.213, 0.063, 0.328, 0.091, 0.178, 0.23, 0.138, 0.138, 0.082]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17300257086753845
[2m[36m(func pid=89738)[0m mae:  0.12614065408706665
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.257, 0.087, 0.327, 0.089, 0.188, 0.277, 0.135, 0.143, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17935112118721008
[2m[36m(func pid=88955)[0m mae:  0.13170917332172394
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.11, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=87283)[0m rmse: 0.16141648590564728
[2m[36m(func pid=87283)[0m mae:  0.09627998620271683
[2m[36m(func pid=87283)[0m rmse_per_class: [0.08, 0.226, 0.045, 0.284, 0.074, 0.176, 0.22, 0.128, 0.136, 0.244]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3472 | Steps: 4 | Val loss: 0.3567 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4173 | Steps: 4 | Val loss: 0.3215 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3022 | Steps: 4 | Val loss: 0.3027 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7533 | Steps: 4 | Val loss: 0.5869 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=88115)[0m rmse: 0.17950613796710968
[2m[36m(func pid=88115)[0m mae:  0.10751531273126602
[2m[36m(func pid=88115)[0m rmse_per_class: [0.229, 0.233, 0.097, 0.354, 0.088, 0.16, 0.226, 0.157, 0.149, 0.102]
[2m[36m(func pid=88115)[0m 
== Status ==
Current time: 2024-01-07 20:28:34 (running for 00:38:52.02)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.303 |  0.161 |                   37 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.347 |  0.18  |                   36 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.759 |  0.179 |                   33 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.42  |  0.173 |                   31 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m rmse: 0.1726798117160797
[2m[36m(func pid=89738)[0m mae:  0.12589892745018005
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.257, 0.086, 0.326, 0.09, 0.187, 0.276, 0.135, 0.142, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17946913838386536
[2m[36m(func pid=88955)[0m mae:  0.1318390667438507
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.337, 0.109, 0.19, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.1661059558391571
[2m[36m(func pid=87283)[0m mae:  0.09955312311649323
[2m[36m(func pid=87283)[0m rmse_per_class: [0.078, 0.213, 0.046, 0.299, 0.08, 0.185, 0.215, 0.144, 0.14, 0.261]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3368 | Steps: 4 | Val loss: 0.3987 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4175 | Steps: 4 | Val loss: 0.3199 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3053 | Steps: 4 | Val loss: 0.2912 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7514 | Steps: 4 | Val loss: 0.5839 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 20:28:39 (running for 00:38:57.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.302 |  0.166 |                   38 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.337 |  0.192 |                   37 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.753 |  0.179 |                   34 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.417 |  0.173 |                   32 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.19161692261695862
[2m[36m(func pid=88115)[0m mae:  0.11324061453342438
[2m[36m(func pid=88115)[0m rmse_per_class: [0.225, 0.234, 0.099, 0.353, 0.093, 0.176, 0.228, 0.243, 0.15, 0.115]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17223525047302246
[2m[36m(func pid=89738)[0m mae:  0.12550970911979675
[2m[36m(func pid=89738)[0m rmse_per_class: [0.119, 0.256, 0.085, 0.326, 0.089, 0.186, 0.274, 0.136, 0.142, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.16300351917743683
[2m[36m(func pid=87283)[0m mae:  0.0963887944817543
[2m[36m(func pid=87283)[0m rmse_per_class: [0.086, 0.208, 0.064, 0.31, 0.095, 0.171, 0.203, 0.149, 0.14, 0.204]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1795440912246704
[2m[36m(func pid=88955)[0m mae:  0.13187158107757568
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.337, 0.109, 0.19, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3575 | Steps: 4 | Val loss: 0.3926 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4133 | Steps: 4 | Val loss: 0.3178 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3057 | Steps: 4 | Val loss: 0.2854 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7473 | Steps: 4 | Val loss: 0.5802 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:28:44 (running for 00:39:02.63)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.305 |  0.163 |                   39 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.358 |  0.195 |                   38 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.751 |  0.18  |                   35 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.418 |  0.172 |                   33 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.19488483667373657
[2m[36m(func pid=88115)[0m mae:  0.1153099313378334
[2m[36m(func pid=88115)[0m rmse_per_class: [0.198, 0.226, 0.076, 0.336, 0.11, 0.213, 0.233, 0.234, 0.18, 0.143]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.17172671854496002
[2m[36m(func pid=89738)[0m mae:  0.12506137788295746
[2m[36m(func pid=89738)[0m rmse_per_class: [0.118, 0.256, 0.084, 0.326, 0.088, 0.186, 0.273, 0.135, 0.142, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.16124457120895386
[2m[36m(func pid=87283)[0m mae:  0.09338442981243134
[2m[36m(func pid=87283)[0m rmse_per_class: [0.099, 0.207, 0.086, 0.311, 0.114, 0.161, 0.199, 0.144, 0.14, 0.15]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17942704260349274
[2m[36m(func pid=88955)[0m mae:  0.1317538470029831
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.109, 0.189, 0.294, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3173 | Steps: 4 | Val loss: 0.3806 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4140 | Steps: 4 | Val loss: 0.3161 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3580 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:28:50 (running for 00:39:07.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.306 |  0.161 |                   40 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.317 |  0.196 |                   39 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.747 |  0.179 |                   36 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.413 |  0.172 |                   34 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.19577078521251678
[2m[36m(func pid=88115)[0m mae:  0.11701637506484985
[2m[36m(func pid=88115)[0m rmse_per_class: [0.196, 0.224, 0.061, 0.327, 0.119, 0.216, 0.254, 0.174, 0.209, 0.178]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7486 | Steps: 4 | Val loss: 0.5792 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=89738)[0m rmse: 0.1714206486940384
[2m[36m(func pid=89738)[0m mae:  0.12474679946899414
[2m[36m(func pid=89738)[0m rmse_per_class: [0.117, 0.255, 0.085, 0.326, 0.088, 0.187, 0.271, 0.135, 0.142, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15946291387081146
[2m[36m(func pid=87283)[0m mae:  0.09173458814620972
[2m[36m(func pid=87283)[0m rmse_per_class: [0.102, 0.207, 0.081, 0.302, 0.133, 0.156, 0.198, 0.159, 0.139, 0.117]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1795063316822052
[2m[36m(func pid=88955)[0m mae:  0.1318151354789734
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.294, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3556 | Steps: 4 | Val loss: 0.3416 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4110 | Steps: 4 | Val loss: 0.3143 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2998 | Steps: 4 | Val loss: 0.2817 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:28:55 (running for 00:39:13.42)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.358 |  0.159 |                   41 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.356 |  0.185 |                   40 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.749 |  0.18  |                   37 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.414 |  0.171 |                   35 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.18454690277576447
[2m[36m(func pid=88115)[0m mae:  0.10856448113918304
[2m[36m(func pid=88115)[0m rmse_per_class: [0.154, 0.219, 0.043, 0.307, 0.111, 0.205, 0.258, 0.228, 0.171, 0.15]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7418 | Steps: 4 | Val loss: 0.5749 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=89738)[0m rmse: 0.17109480500221252
[2m[36m(func pid=89738)[0m mae:  0.12439151853322983
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.256, 0.085, 0.324, 0.088, 0.186, 0.269, 0.134, 0.142, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.16185793280601501
[2m[36m(func pid=87283)[0m mae:  0.09293016046285629
[2m[36m(func pid=87283)[0m rmse_per_class: [0.118, 0.204, 0.106, 0.311, 0.149, 0.154, 0.198, 0.136, 0.134, 0.109]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17936185002326965
[2m[36m(func pid=88955)[0m mae:  0.13172002136707306
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3298 | Steps: 4 | Val loss: 0.3140 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4101 | Steps: 4 | Val loss: 0.3137 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2982 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 20:29:01 (running for 00:39:19.11)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.3   |  0.162 |                   42 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.33  |  0.172 |                   41 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.742 |  0.179 |                   38 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.411 |  0.171 |                   36 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1717972755432129
[2m[36m(func pid=88115)[0m mae:  0.10114969313144684
[2m[36m(func pid=88115)[0m rmse_per_class: [0.146, 0.206, 0.042, 0.302, 0.098, 0.176, 0.271, 0.2, 0.146, 0.133]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7401 | Steps: 4 | Val loss: 0.5720 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=89738)[0m rmse: 0.17101360857486725
[2m[36m(func pid=89738)[0m mae:  0.1244535893201828
[2m[36m(func pid=89738)[0m rmse_per_class: [0.117, 0.257, 0.084, 0.325, 0.087, 0.186, 0.27, 0.134, 0.142, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15961840748786926
[2m[36m(func pid=87283)[0m mae:  0.0923171192407608
[2m[36m(func pid=87283)[0m rmse_per_class: [0.12, 0.207, 0.093, 0.309, 0.146, 0.154, 0.198, 0.11, 0.144, 0.115]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1793326735496521
[2m[36m(func pid=88955)[0m mae:  0.1317157745361328
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.108]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3187 | Steps: 4 | Val loss: 0.3045 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4080 | Steps: 4 | Val loss: 0.3123 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2875 | Steps: 4 | Val loss: 0.2778 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=88115)[0m rmse: 0.16620656847953796
[2m[36m(func pid=88115)[0m mae:  0.09980063885450363
[2m[36m(func pid=88115)[0m rmse_per_class: [0.15, 0.209, 0.063, 0.306, 0.084, 0.177, 0.269, 0.146, 0.14, 0.119]
[2m[36m(func pid=88115)[0m 
== Status ==
Current time: 2024-01-07 20:29:06 (running for 00:39:24.67)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.298 |  0.16  |                   43 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.319 |  0.166 |                   42 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.74  |  0.179 |                   39 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.41  |  0.171 |                   37 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7395 | Steps: 4 | Val loss: 0.5690 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=89738)[0m rmse: 0.17057369649410248
[2m[36m(func pid=89738)[0m mae:  0.12402186542749405
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.256, 0.083, 0.324, 0.086, 0.186, 0.269, 0.133, 0.142, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15822406113147736
[2m[36m(func pid=87283)[0m mae:  0.09294288605451584
[2m[36m(func pid=87283)[0m rmse_per_class: [0.104, 0.212, 0.087, 0.303, 0.143, 0.161, 0.206, 0.099, 0.148, 0.121]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17931678891181946
[2m[36m(func pid=88955)[0m mae:  0.1317184865474701
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.108, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3234 | Steps: 4 | Val loss: 0.3020 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4067 | Steps: 4 | Val loss: 0.3118 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3171 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 20:29:12 (running for 00:39:30.04)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.288 |  0.158 |                   44 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.323 |  0.161 |                   43 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.739 |  0.179 |                   40 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.408 |  0.171 |                   38 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1614733785390854
[2m[36m(func pid=88115)[0m mae:  0.09783202409744263
[2m[36m(func pid=88115)[0m rmse_per_class: [0.135, 0.213, 0.043, 0.294, 0.07, 0.188, 0.274, 0.129, 0.133, 0.137]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7334 | Steps: 4 | Val loss: 0.5674 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=89738)[0m rmse: 0.1703283041715622
[2m[36m(func pid=89738)[0m mae:  0.12393069267272949
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.255, 0.082, 0.324, 0.085, 0.186, 0.269, 0.134, 0.141, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15905030071735382
[2m[36m(func pid=87283)[0m mae:  0.09387202560901642
[2m[36m(func pid=87283)[0m rmse_per_class: [0.098, 0.218, 0.071, 0.298, 0.131, 0.163, 0.207, 0.101, 0.164, 0.141]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17924091219902039
[2m[36m(func pid=88955)[0m mae:  0.13161876797676086
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.109, 0.19, 0.294, 0.14, 0.143, 0.108]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3089 | Steps: 4 | Val loss: 0.3049 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4028 | Steps: 4 | Val loss: 0.3101 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 20:29:17 (running for 00:39:35.34)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.317 |  0.159 |                   45 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.309 |  0.159 |                   44 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.733 |  0.179 |                   41 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.407 |  0.17  |                   39 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.15881666541099548
[2m[36m(func pid=88115)[0m mae:  0.0968196913599968
[2m[36m(func pid=88115)[0m rmse_per_class: [0.119, 0.216, 0.033, 0.283, 0.062, 0.187, 0.269, 0.125, 0.132, 0.163]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2891 | Steps: 4 | Val loss: 0.2766 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7239 | Steps: 4 | Val loss: 0.5633 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=89738)[0m rmse: 0.1700034886598587
[2m[36m(func pid=89738)[0m mae:  0.12364427745342255
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.255, 0.082, 0.324, 0.085, 0.185, 0.269, 0.133, 0.141, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m rmse: 0.15716169774532318
[2m[36m(func pid=87283)[0m mae:  0.09329038858413696
[2m[36m(func pid=87283)[0m rmse_per_class: [0.083, 0.216, 0.057, 0.287, 0.143, 0.166, 0.212, 0.104, 0.17, 0.135]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17938517034053802
[2m[36m(func pid=88955)[0m mae:  0.13171923160552979
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.109, 0.189, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3571 | Steps: 4 | Val loss: 0.3121 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4047 | Steps: 4 | Val loss: 0.3096 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 20:29:22 (running for 00:39:40.67)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.289 |  0.157 |                   46 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.357 |  0.161 |                   45 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.724 |  0.179 |                   42 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.403 |  0.17  |                   40 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16093717515468597
[2m[36m(func pid=88115)[0m mae:  0.09708222001791
[2m[36m(func pid=88115)[0m rmse_per_class: [0.112, 0.22, 0.035, 0.281, 0.059, 0.185, 0.255, 0.144, 0.134, 0.185]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2710 | Steps: 4 | Val loss: 0.2715 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=89738)[0m rmse: 0.17012588679790497
[2m[36m(func pid=89738)[0m mae:  0.12377133220434189
[2m[36m(func pid=89738)[0m rmse_per_class: [0.116, 0.255, 0.083, 0.323, 0.086, 0.185, 0.269, 0.134, 0.141, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7233 | Steps: 4 | Val loss: 0.5604 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=87283)[0m rmse: 0.15321063995361328
[2m[36m(func pid=87283)[0m mae:  0.09088756889104843
[2m[36m(func pid=87283)[0m rmse_per_class: [0.076, 0.212, 0.05, 0.275, 0.138, 0.168, 0.213, 0.104, 0.158, 0.138]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3412 | Steps: 4 | Val loss: 0.3262 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=88955)[0m rmse: 0.17945194244384766
[2m[36m(func pid=88955)[0m mae:  0.13176248967647552
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.337, 0.108, 0.189, 0.294, 0.14, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4014 | Steps: 4 | Val loss: 0.3080 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:29:28 (running for 00:39:46.14)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.271 |  0.153 |                   47 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.341 |  0.162 |                   46 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.723 |  0.179 |                   43 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.405 |  0.17  |                   41 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16198858618736267
[2m[36m(func pid=88115)[0m mae:  0.09820674359798431
[2m[36m(func pid=88115)[0m rmse_per_class: [0.115, 0.242, 0.031, 0.292, 0.067, 0.18, 0.253, 0.135, 0.137, 0.169]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3183 | Steps: 4 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=89738)[0m rmse: 0.1697063148021698
[2m[36m(func pid=89738)[0m mae:  0.12345483154058456
[2m[36m(func pid=89738)[0m rmse_per_class: [0.115, 0.254, 0.082, 0.322, 0.085, 0.185, 0.269, 0.134, 0.141, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7252 | Steps: 4 | Val loss: 0.5559 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=87283)[0m rmse: 0.1523744910955429
[2m[36m(func pid=87283)[0m mae:  0.08913624286651611
[2m[36m(func pid=87283)[0m rmse_per_class: [0.073, 0.212, 0.047, 0.272, 0.134, 0.172, 0.213, 0.108, 0.156, 0.138]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3083 | Steps: 4 | Val loss: 0.3103 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=88955)[0m rmse: 0.17925921082496643
[2m[36m(func pid=88955)[0m mae:  0.13160064816474915
[2m[36m(func pid=88955)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.189, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4054 | Steps: 4 | Val loss: 0.3065 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:29:33 (running for 00:39:51.55)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.318 |  0.152 |                   48 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.308 |  0.165 |                   47 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.725 |  0.179 |                   44 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.401 |  0.17  |                   42 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16457226872444153
[2m[36m(func pid=88115)[0m mae:  0.10023149102926254
[2m[36m(func pid=88115)[0m rmse_per_class: [0.124, 0.25, 0.033, 0.307, 0.095, 0.172, 0.245, 0.143, 0.142, 0.136]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3129 | Steps: 4 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=89738)[0m rmse: 0.16912229359149933
[2m[36m(func pid=89738)[0m mae:  0.12293367087841034
[2m[36m(func pid=89738)[0m rmse_per_class: [0.115, 0.255, 0.08, 0.322, 0.085, 0.185, 0.267, 0.133, 0.141, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7158 | Steps: 4 | Val loss: 0.5526 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=87283)[0m rmse: 0.1592683643102646
[2m[36m(func pid=87283)[0m mae:  0.09355601668357849
[2m[36m(func pid=87283)[0m rmse_per_class: [0.073, 0.211, 0.051, 0.281, 0.139, 0.179, 0.224, 0.108, 0.188, 0.14]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3089 | Steps: 4 | Val loss: 0.3193 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=88955)[0m rmse: 0.17920419573783875
[2m[36m(func pid=88955)[0m mae:  0.13155806064605713
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.108, 0.189, 0.294, 0.139, 0.142, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4022 | Steps: 4 | Val loss: 0.3055 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 20:29:39 (running for 00:39:57.18)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.313 |  0.159 |                   49 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.309 |  0.171 |                   48 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.716 |  0.179 |                   45 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.405 |  0.169 |                   43 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17060670256614685
[2m[36m(func pid=88115)[0m mae:  0.10323905944824219
[2m[36m(func pid=88115)[0m rmse_per_class: [0.124, 0.273, 0.039, 0.315, 0.126, 0.167, 0.237, 0.183, 0.14, 0.101]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2919 | Steps: 4 | Val loss: 0.2903 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=89738)[0m rmse: 0.16846652328968048
[2m[36m(func pid=89738)[0m mae:  0.12238399684429169
[2m[36m(func pid=89738)[0m rmse_per_class: [0.114, 0.254, 0.078, 0.322, 0.086, 0.184, 0.266, 0.132, 0.141, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7137 | Steps: 4 | Val loss: 0.5503 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=87283)[0m rmse: 0.16175352036952972
[2m[36m(func pid=87283)[0m mae:  0.0945647805929184
[2m[36m(func pid=87283)[0m rmse_per_class: [0.073, 0.212, 0.046, 0.286, 0.131, 0.178, 0.226, 0.111, 0.203, 0.152]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3152 | Steps: 4 | Val loss: 0.3152 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=88955)[0m rmse: 0.17922472953796387
[2m[36m(func pid=88955)[0m mae:  0.13161584734916687
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.109, 0.189, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3989 | Steps: 4 | Val loss: 0.3051 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:29:44 (running for 00:40:02.66)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.292 |  0.162 |                   50 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.315 |  0.171 |                   49 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.714 |  0.179 |                   46 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.402 |  0.168 |                   44 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17060433328151703
[2m[36m(func pid=88115)[0m mae:  0.10441603511571884
[2m[36m(func pid=88115)[0m rmse_per_class: [0.139, 0.27, 0.055, 0.322, 0.124, 0.163, 0.234, 0.173, 0.141, 0.084]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16846224665641785
[2m[36m(func pid=89738)[0m mae:  0.12244771420955658
[2m[36m(func pid=89738)[0m rmse_per_class: [0.115, 0.253, 0.077, 0.322, 0.087, 0.184, 0.268, 0.132, 0.141, 0.107]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2767 | Steps: 4 | Val loss: 0.2899 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7096 | Steps: 4 | Val loss: 0.5464 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m rmse: 0.16042283177375793
[2m[36m(func pid=87283)[0m mae:  0.09305739402770996
[2m[36m(func pid=87283)[0m rmse_per_class: [0.07, 0.209, 0.044, 0.293, 0.138, 0.174, 0.218, 0.118, 0.202, 0.137]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3140 | Steps: 4 | Val loss: 0.3024 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=88955)[0m rmse: 0.17919839918613434
[2m[36m(func pid=88955)[0m mae:  0.13158848881721497
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.109, 0.189, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3944 | Steps: 4 | Val loss: 0.3045 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:29:50 (running for 00:40:08.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.277 |  0.16  |                   51 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.314 |  0.169 |                   50 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.71  |  0.179 |                   47 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.399 |  0.168 |                   45 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1687990128993988
[2m[36m(func pid=88115)[0m mae:  0.10252617299556732
[2m[36m(func pid=88115)[0m rmse_per_class: [0.148, 0.243, 0.061, 0.314, 0.122, 0.162, 0.237, 0.172, 0.138, 0.091]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16843339800834656
[2m[36m(func pid=89738)[0m mae:  0.12244439125061035
[2m[36m(func pid=89738)[0m rmse_per_class: [0.115, 0.254, 0.078, 0.321, 0.086, 0.184, 0.268, 0.132, 0.141, 0.107]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2773 | Steps: 4 | Val loss: 0.2912 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7075 | Steps: 4 | Val loss: 0.5459 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3200 | Steps: 4 | Val loss: 0.3035 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=87283)[0m rmse: 0.15993034839630127
[2m[36m(func pid=87283)[0m mae:  0.09234543889760971
[2m[36m(func pid=87283)[0m rmse_per_class: [0.069, 0.206, 0.046, 0.306, 0.146, 0.166, 0.212, 0.134, 0.194, 0.121]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3966 | Steps: 4 | Val loss: 0.3038 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=88955)[0m rmse: 0.17918047308921814
[2m[36m(func pid=88955)[0m mae:  0.1315820813179016
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.107, 0.19, 0.294, 0.138, 0.144, 0.11]
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:29:55 (running for 00:40:13.65)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.277 |  0.16  |                   52 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.32  |  0.17  |                   51 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.708 |  0.179 |                   48 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.394 |  0.168 |                   46 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17021773755550385
[2m[36m(func pid=88115)[0m mae:  0.10240115225315094
[2m[36m(func pid=88115)[0m rmse_per_class: [0.143, 0.223, 0.065, 0.309, 0.11, 0.173, 0.25, 0.181, 0.138, 0.109]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3020 | Steps: 4 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=89738)[0m rmse: 0.16822369396686554
[2m[36m(func pid=89738)[0m mae:  0.12226127088069916
[2m[36m(func pid=89738)[0m rmse_per_class: [0.114, 0.254, 0.08, 0.32, 0.084, 0.184, 0.267, 0.131, 0.141, 0.107]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7008 | Steps: 4 | Val loss: 0.5433 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=87283)[0m rmse: 0.16066370904445648
[2m[36m(func pid=87283)[0m mae:  0.09345810860395432
[2m[36m(func pid=87283)[0m rmse_per_class: [0.07, 0.205, 0.06, 0.314, 0.148, 0.162, 0.213, 0.135, 0.189, 0.11]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3015 | Steps: 4 | Val loss: 0.3074 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=88955)[0m rmse: 0.17914018034934998
[2m[36m(func pid=88955)[0m mae:  0.13153786957263947
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.108, 0.19, 0.293, 0.139, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3948 | Steps: 4 | Val loss: 0.3040 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=88115)[0m rmse: 0.1718997061252594
[2m[36m(func pid=88115)[0m mae:  0.10340740531682968
[2m[36m(func pid=88115)[0m rmse_per_class: [0.179, 0.223, 0.055, 0.3, 0.109, 0.189, 0.259, 0.148, 0.137, 0.119]
== Status ==
Current time: 2024-01-07 20:30:01 (running for 00:40:18.94)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.302 |  0.161 |                   53 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.302 |  0.172 |                   52 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.701 |  0.179 |                   49 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.397 |  0.168 |                   47 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2718 | Steps: 4 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=89738)[0m rmse: 0.1684103161096573
[2m[36m(func pid=89738)[0m mae:  0.12241313606500626
[2m[36m(func pid=89738)[0m rmse_per_class: [0.114, 0.253, 0.08, 0.32, 0.084, 0.184, 0.268, 0.131, 0.141, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6996 | Steps: 4 | Val loss: 0.5398 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=87283)[0m rmse: 0.160697340965271
[2m[36m(func pid=87283)[0m mae:  0.09458792209625244
[2m[36m(func pid=87283)[0m rmse_per_class: [0.07, 0.209, 0.075, 0.314, 0.157, 0.163, 0.213, 0.111, 0.194, 0.102]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3293 | Steps: 4 | Val loss: 0.3047 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3907 | Steps: 4 | Val loss: 0.3018 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=88955)[0m rmse: 0.17930373549461365
[2m[36m(func pid=88955)[0m mae:  0.1316472291946411
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.336, 0.108, 0.19, 0.294, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.16793867945671082
[2m[36m(func pid=88115)[0m mae:  0.10019715130329132
[2m[36m(func pid=88115)[0m rmse_per_class: [0.15, 0.218, 0.043, 0.29, 0.1, 0.183, 0.263, 0.165, 0.136, 0.132]
== Status ==
Current time: 2024-01-07 20:30:06 (running for 00:40:24.35)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.272 |  0.161 |                   54 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.329 |  0.168 |                   53 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.7   |  0.179 |                   50 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.395 |  0.168 |                   48 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2931 | Steps: 4 | Val loss: 0.2944 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=89738)[0m rmse: 0.16751423478126526
[2m[36m(func pid=89738)[0m mae:  0.12167952954769135
[2m[36m(func pid=89738)[0m rmse_per_class: [0.114, 0.253, 0.078, 0.319, 0.084, 0.184, 0.266, 0.131, 0.141, 0.107]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6984 | Steps: 4 | Val loss: 0.5379 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=87283)[0m rmse: 0.16248874366283417
[2m[36m(func pid=87283)[0m mae:  0.09669110924005508
[2m[36m(func pid=87283)[0m rmse_per_class: [0.08, 0.209, 0.085, 0.317, 0.143, 0.166, 0.215, 0.104, 0.207, 0.099]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3273 | Steps: 4 | Val loss: 0.2981 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3899 | Steps: 4 | Val loss: 0.3013 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=88955)[0m rmse: 0.1791745126247406
[2m[36m(func pid=88955)[0m mae:  0.13154014945030212
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:30:12 (running for 00:40:29.84)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.293 |  0.162 |                   55 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.327 |  0.168 |                   54 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.698 |  0.179 |                   51 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.391 |  0.168 |                   49 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16763223707675934
[2m[36m(func pid=88115)[0m mae:  0.09895308315753937
[2m[36m(func pid=88115)[0m rmse_per_class: [0.161, 0.213, 0.046, 0.296, 0.084, 0.178, 0.249, 0.132, 0.146, 0.17]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16730958223342896
[2m[36m(func pid=89738)[0m mae:  0.12149367481470108
[2m[36m(func pid=89738)[0m rmse_per_class: [0.114, 0.252, 0.078, 0.318, 0.083, 0.183, 0.266, 0.13, 0.141, 0.107]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2809 | Steps: 4 | Val loss: 0.3010 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6947 | Steps: 4 | Val loss: 0.5362 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=87283)[0m rmse: 0.1689833700656891
[2m[36m(func pid=87283)[0m mae:  0.100925013422966
[2m[36m(func pid=87283)[0m rmse_per_class: [0.093, 0.204, 0.098, 0.316, 0.144, 0.172, 0.225, 0.11, 0.225, 0.104]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3132 | Steps: 4 | Val loss: 0.3062 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3889 | Steps: 4 | Val loss: 0.3010 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=88955)[0m rmse: 0.17916187644004822
[2m[36m(func pid=88955)[0m mae:  0.13154786825180054
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:30:17 (running for 00:40:35.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.281 |  0.169 |                   56 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.313 |  0.175 |                   55 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.695 |  0.179 |                   52 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.39  |  0.167 |                   50 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17493002116680145
[2m[36m(func pid=88115)[0m mae:  0.10135219991207123
[2m[36m(func pid=88115)[0m rmse_per_class: [0.176, 0.22, 0.084, 0.306, 0.084, 0.166, 0.239, 0.168, 0.15, 0.156]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16722071170806885
[2m[36m(func pid=89738)[0m mae:  0.12137148529291153
[2m[36m(func pid=89738)[0m rmse_per_class: [0.113, 0.252, 0.078, 0.317, 0.083, 0.183, 0.265, 0.131, 0.141, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2765 | Steps: 4 | Val loss: 0.2999 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6939 | Steps: 4 | Val loss: 0.5351 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3094 | Steps: 4 | Val loss: 0.2931 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=87283)[0m rmse: 0.17192193865776062
[2m[36m(func pid=87283)[0m mae:  0.10180302709341049
[2m[36m(func pid=87283)[0m rmse_per_class: [0.106, 0.202, 0.102, 0.306, 0.148, 0.176, 0.233, 0.114, 0.214, 0.116]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3895 | Steps: 4 | Val loss: 0.3010 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=88955)[0m rmse: 0.17919595539569855
[2m[36m(func pid=88955)[0m mae:  0.1315918266773224
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.138, 0.144, 0.11]
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:30:22 (running for 00:40:40.67)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.276 |  0.172 |                   57 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.309 |  0.168 |                   56 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.694 |  0.179 |                   53 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.389 |  0.167 |                   51 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16777542233467102
[2m[36m(func pid=88115)[0m mae:  0.09981489181518555
[2m[36m(func pid=88115)[0m rmse_per_class: [0.153, 0.23, 0.079, 0.301, 0.101, 0.167, 0.238, 0.156, 0.139, 0.113]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16713660955429077
[2m[36m(func pid=89738)[0m mae:  0.12137242406606674
[2m[36m(func pid=89738)[0m rmse_per_class: [0.114, 0.252, 0.078, 0.316, 0.082, 0.183, 0.267, 0.131, 0.141, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2967 | Steps: 4 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6914 | Steps: 4 | Val loss: 0.5323 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3452 | Steps: 4 | Val loss: 0.3047 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3924 | Steps: 4 | Val loss: 0.2994 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=87283)[0m rmse: 0.16592124104499817
[2m[36m(func pid=87283)[0m mae:  0.09867547452449799
[2m[36m(func pid=87283)[0m rmse_per_class: [0.11, 0.198, 0.081, 0.294, 0.125, 0.178, 0.228, 0.113, 0.203, 0.129]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17904825508594513
[2m[36m(func pid=88955)[0m mae:  0.13145031034946442
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1663506180047989
[2m[36m(func pid=89738)[0m mae:  0.1206078752875328
[2m[36m(func pid=89738)[0m rmse_per_class: [0.113, 0.252, 0.077, 0.316, 0.082, 0.183, 0.263, 0.129, 0.141, 0.108]
== Status ==
Current time: 2024-01-07 20:30:28 (running for 00:40:46.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.297 |  0.166 |                   58 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.309 |  0.168 |                   56 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.691 |  0.179 |                   54 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.392 |  0.166 |                   53 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17088009417057037
[2m[36m(func pid=88115)[0m mae:  0.10223815590143204
[2m[36m(func pid=88115)[0m rmse_per_class: [0.168, 0.237, 0.113, 0.311, 0.115, 0.168, 0.239, 0.135, 0.138, 0.084]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2734 | Steps: 4 | Val loss: 0.2770 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6888 | Steps: 4 | Val loss: 0.5300 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3206 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3888 | Steps: 4 | Val loss: 0.2987 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m rmse: 0.15623438358306885
[2m[36m(func pid=87283)[0m mae:  0.09194451570510864
[2m[36m(func pid=87283)[0m rmse_per_class: [0.105, 0.196, 0.089, 0.292, 0.115, 0.175, 0.213, 0.102, 0.152, 0.122]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17893224954605103
[2m[36m(func pid=88955)[0m mae:  0.13129368424415588
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.336, 0.108, 0.19, 0.292, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16601376235485077
[2m[36m(func pid=89738)[0m mae:  0.12032053619623184
[2m[36m(func pid=89738)[0m rmse_per_class: [0.113, 0.252, 0.076, 0.316, 0.081, 0.183, 0.262, 0.129, 0.14, 0.107]
[2m[36m(func pid=89738)[0m 
== Status ==
Current time: 2024-01-07 20:30:33 (running for 00:40:51.43)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.273 |  0.156 |                   59 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.345 |  0.171 |                   57 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.689 |  0.179 |                   55 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.389 |  0.166 |                   54 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.17096813023090363
[2m[36m(func pid=88115)[0m mae:  0.10332606732845306
[2m[36m(func pid=88115)[0m rmse_per_class: [0.154, 0.272, 0.094, 0.306, 0.116, 0.169, 0.239, 0.143, 0.142, 0.074]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2660 | Steps: 4 | Val loss: 0.2749 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6850 | Steps: 4 | Val loss: 0.5284 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=87283)[0m rmse: 0.15183906257152557
[2m[36m(func pid=87283)[0m mae:  0.0891847014427185
[2m[36m(func pid=87283)[0m rmse_per_class: [0.095, 0.202, 0.082, 0.29, 0.092, 0.171, 0.204, 0.102, 0.135, 0.146]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3883 | Steps: 4 | Val loss: 0.2987 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3170 | Steps: 4 | Val loss: 0.3213 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=88955)[0m rmse: 0.17900167405605316
[2m[36m(func pid=88955)[0m mae:  0.1313660442829132
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.259, 0.097, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.109]
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:30:39 (running for 00:40:56.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.266 |  0.152 |                   60 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.321 |  0.171 |                   58 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.685 |  0.179 |                   56 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.388 |  0.166 |                   55 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m rmse: 0.166053906083107
[2m[36m(func pid=89738)[0m mae:  0.12043174356222153
[2m[36m(func pid=89738)[0m rmse_per_class: [0.113, 0.251, 0.076, 0.317, 0.08, 0.182, 0.263, 0.129, 0.14, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.16972880065441132
[2m[36m(func pid=88115)[0m mae:  0.10215004533529282
[2m[36m(func pid=88115)[0m rmse_per_class: [0.133, 0.313, 0.069, 0.3, 0.126, 0.167, 0.239, 0.13, 0.143, 0.078]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3186 | Steps: 4 | Val loss: 0.2719 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6819 | Steps: 4 | Val loss: 0.5283 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3844 | Steps: 4 | Val loss: 0.2988 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m rmse: 0.1464720517396927
[2m[36m(func pid=87283)[0m mae:  0.0860314890742302
[2m[36m(func pid=87283)[0m rmse_per_class: [0.072, 0.211, 0.056, 0.275, 0.077, 0.157, 0.199, 0.132, 0.137, 0.149]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3238 | Steps: 4 | Val loss: 0.3149 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=88955)[0m rmse: 0.17880703508853912
[2m[36m(func pid=88955)[0m mae:  0.13117672502994537
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.259, 0.097, 0.335, 0.108, 0.19, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1661141812801361
[2m[36m(func pid=89738)[0m mae:  0.1204482913017273
[2m[36m(func pid=89738)[0m rmse_per_class: [0.112, 0.252, 0.077, 0.316, 0.081, 0.182, 0.264, 0.128, 0.14, 0.108]
[2m[36m(func pid=89738)[0m 
== Status ==
Current time: 2024-01-07 20:30:44 (running for 00:41:02.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.319 |  0.146 |                   61 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.317 |  0.17  |                   59 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.682 |  0.179 |                   57 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.384 |  0.166 |                   56 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16154882311820984
[2m[36m(func pid=88115)[0m mae:  0.09740038216114044
[2m[36m(func pid=88115)[0m rmse_per_class: [0.109, 0.303, 0.037, 0.292, 0.135, 0.161, 0.235, 0.124, 0.14, 0.081]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2966 | Steps: 4 | Val loss: 0.2788 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6788 | Steps: 4 | Val loss: 0.5235 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3898 | Steps: 4 | Val loss: 0.2985 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=87283)[0m rmse: 0.15165403485298157
[2m[36m(func pid=87283)[0m mae:  0.08903245627880096
[2m[36m(func pid=87283)[0m rmse_per_class: [0.078, 0.227, 0.067, 0.278, 0.073, 0.166, 0.208, 0.113, 0.144, 0.163]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3013 | Steps: 4 | Val loss: 0.2970 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=88955)[0m rmse: 0.17884786427021027
[2m[36m(func pid=88955)[0m mae:  0.1312321126461029
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.336, 0.108, 0.19, 0.293, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
== Status ==
Current time: 2024-01-07 20:30:49 (running for 00:41:07.57)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.297 |  0.152 |                   62 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.324 |  0.162 |                   60 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.679 |  0.179 |                   58 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.39  |  0.166 |                   57 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m rmse: 0.16600322723388672
[2m[36m(func pid=89738)[0m mae:  0.12041523307561874
[2m[36m(func pid=89738)[0m rmse_per_class: [0.112, 0.252, 0.077, 0.316, 0.081, 0.182, 0.264, 0.128, 0.14, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.15618185698986053
[2m[36m(func pid=88115)[0m mae:  0.09452936798334122
[2m[36m(func pid=88115)[0m rmse_per_class: [0.102, 0.273, 0.029, 0.291, 0.126, 0.166, 0.234, 0.109, 0.145, 0.085]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2768 | Steps: 4 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6756 | Steps: 4 | Val loss: 0.5210 | Batch size: 32 | lr: 0.0001 | Duration: 3.30s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3862 | Steps: 4 | Val loss: 0.2979 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=87283)[0m rmse: 0.15652057528495789
[2m[36m(func pid=87283)[0m mae:  0.0932096317410469
[2m[36m(func pid=87283)[0m rmse_per_class: [0.076, 0.234, 0.079, 0.284, 0.071, 0.171, 0.215, 0.113, 0.154, 0.169]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3144 | Steps: 4 | Val loss: 0.2861 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:30:55 (running for 00:41:12.76)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.277 |  0.157 |                   63 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.301 |  0.156 |                   61 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.676 |  0.179 |                   59 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.39  |  0.166 |                   57 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m rmse: 0.17879173159599304
[2m[36m(func pid=88955)[0m mae:  0.13118687272071838
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.107, 0.19, 0.293, 0.138, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16559427976608276
[2m[36m(func pid=89738)[0m mae:  0.12003608047962189
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.251, 0.076, 0.316, 0.08, 0.182, 0.263, 0.128, 0.14, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.15381594002246857
[2m[36m(func pid=88115)[0m mae:  0.0930500403046608
[2m[36m(func pid=88115)[0m rmse_per_class: [0.103, 0.246, 0.029, 0.286, 0.108, 0.172, 0.235, 0.104, 0.148, 0.107]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2751 | Steps: 4 | Val loss: 0.2954 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6692 | Steps: 4 | Val loss: 0.5179 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3796 | Steps: 4 | Val loss: 0.2971 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=87283)[0m rmse: 0.1635959893465042
[2m[36m(func pid=87283)[0m mae:  0.09787929803133011
[2m[36m(func pid=87283)[0m rmse_per_class: [0.077, 0.235, 0.094, 0.308, 0.066, 0.17, 0.215, 0.117, 0.189, 0.166]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3023 | Steps: 4 | Val loss: 0.2841 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 20:31:00 (running for 00:41:18.06)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.275 |  0.164 |                   64 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.314 |  0.154 |                   62 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.669 |  0.179 |                   60 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.386 |  0.166 |                   58 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m rmse: 0.17884708940982819
[2m[36m(func pid=88955)[0m mae:  0.1312447041273117
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.107, 0.19, 0.293, 0.138, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1652865707874298
[2m[36m(func pid=89738)[0m mae:  0.11977583169937134
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.252, 0.076, 0.315, 0.079, 0.181, 0.262, 0.128, 0.14, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.15255871415138245
[2m[36m(func pid=88115)[0m mae:  0.0925883948802948
[2m[36m(func pid=88115)[0m rmse_per_class: [0.116, 0.221, 0.03, 0.285, 0.09, 0.173, 0.237, 0.109, 0.149, 0.115]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3134 | Steps: 4 | Val loss: 0.2930 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6753 | Steps: 4 | Val loss: 0.5164 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=87283)[0m rmse: 0.1613810956478119
[2m[36m(func pid=87283)[0m mae:  0.0968293696641922
[2m[36m(func pid=87283)[0m rmse_per_class: [0.072, 0.227, 0.075, 0.308, 0.066, 0.169, 0.215, 0.124, 0.201, 0.158]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3888 | Steps: 4 | Val loss: 0.2964 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3556 | Steps: 4 | Val loss: 0.2899 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 20:31:05 (running for 00:41:23.59)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.313 |  0.161 |                   65 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.302 |  0.153 |                   63 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.675 |  0.179 |                   61 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.38  |  0.165 |                   59 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m rmse: 0.17889121174812317
[2m[36m(func pid=88955)[0m mae:  0.13131490349769592
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.336, 0.107, 0.19, 0.293, 0.139, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16489477455615997
[2m[36m(func pid=89738)[0m mae:  0.11941864341497421
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.251, 0.075, 0.314, 0.079, 0.181, 0.261, 0.128, 0.14, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.16121932864189148
[2m[36m(func pid=88115)[0m mae:  0.09727199375629425
[2m[36m(func pid=88115)[0m rmse_per_class: [0.145, 0.208, 0.038, 0.301, 0.08, 0.171, 0.243, 0.147, 0.147, 0.133]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2976 | Steps: 4 | Val loss: 0.2912 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6635 | Steps: 4 | Val loss: 0.5148 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3827 | Steps: 4 | Val loss: 0.2964 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=87283)[0m rmse: 0.16054150462150574
[2m[36m(func pid=87283)[0m mae:  0.09654062241315842
[2m[36m(func pid=87283)[0m rmse_per_class: [0.073, 0.224, 0.095, 0.312, 0.067, 0.164, 0.213, 0.122, 0.205, 0.13]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3206 | Steps: 4 | Val loss: 0.2923 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 20:31:11 (running for 00:41:29.20)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.298 |  0.161 |                   66 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.356 |  0.161 |                   64 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.664 |  0.179 |                   62 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.389 |  0.165 |                   60 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.16257277131080627
[2m[36m(func pid=88115)[0m mae:  0.09844993054866791
[2m[36m(func pid=88115)[0m rmse_per_class: [0.135, 0.201, 0.041, 0.308, 0.079, 0.167, 0.245, 0.179, 0.14, 0.131]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17896835505962372
[2m[36m(func pid=88955)[0m mae:  0.13140295445919037
[2m[36m(func pid=88955)[0m rmse_per_class: [0.118, 0.26, 0.096, 0.336, 0.106, 0.189, 0.292, 0.139, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16491097211837769
[2m[36m(func pid=89738)[0m mae:  0.11951504647731781
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.251, 0.074, 0.314, 0.079, 0.18, 0.263, 0.128, 0.14, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2834 | Steps: 4 | Val loss: 0.2959 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3044 | Steps: 4 | Val loss: 0.2923 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6608 | Steps: 4 | Val loss: 0.5123 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3860 | Steps: 4 | Val loss: 0.2967 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=87283)[0m rmse: 0.16339750587940216
[2m[36m(func pid=87283)[0m mae:  0.09732894599437714
[2m[36m(func pid=87283)[0m rmse_per_class: [0.074, 0.223, 0.102, 0.316, 0.064, 0.159, 0.211, 0.133, 0.215, 0.138]
[2m[36m(func pid=87283)[0m 
== Status ==
Current time: 2024-01-07 20:31:16 (running for 00:41:34.61)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.283 |  0.163 |                   67 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.304 |  0.162 |                   66 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.664 |  0.179 |                   62 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.383 |  0.165 |                   61 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1615702360868454
[2m[36m(func pid=88115)[0m mae:  0.09754025191068649
[2m[36m(func pid=88115)[0m rmse_per_class: [0.126, 0.197, 0.049, 0.31, 0.088, 0.163, 0.236, 0.19, 0.136, 0.121]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17915156483650208
[2m[36m(func pid=88955)[0m mae:  0.13154096901416779
[2m[36m(func pid=88955)[0m rmse_per_class: [0.118, 0.26, 0.096, 0.336, 0.106, 0.189, 0.293, 0.14, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16509217023849487
[2m[36m(func pid=89738)[0m mae:  0.11963106691837311
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.25, 0.075, 0.316, 0.078, 0.181, 0.262, 0.128, 0.14, 0.11]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2790 | Steps: 4 | Val loss: 0.3049 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3341 | Steps: 4 | Val loss: 0.2914 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3793 | Steps: 4 | Val loss: 0.2961 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6611 | Steps: 4 | Val loss: 0.5091 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=87283)[0m rmse: 0.167318657040596
[2m[36m(func pid=87283)[0m mae:  0.09848184883594513
[2m[36m(func pid=87283)[0m rmse_per_class: [0.076, 0.231, 0.086, 0.309, 0.062, 0.165, 0.217, 0.158, 0.226, 0.143]
[2m[36m(func pid=87283)[0m 
== Status ==
Current time: 2024-01-07 20:31:22 (running for 00:41:40.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.279 |  0.167 |                   68 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.334 |  0.159 |                   67 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.661 |  0.179 |                   63 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.386 |  0.165 |                   62 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1593681126832962
[2m[36m(func pid=88115)[0m mae:  0.09615526348352432
[2m[36m(func pid=88115)[0m rmse_per_class: [0.108, 0.198, 0.049, 0.302, 0.097, 0.162, 0.23, 0.2, 0.136, 0.112]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17909929156303406
[2m[36m(func pid=88955)[0m mae:  0.13147391378879547
[2m[36m(func pid=88955)[0m rmse_per_class: [0.118, 0.26, 0.096, 0.336, 0.106, 0.189, 0.292, 0.14, 0.144, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16470184922218323
[2m[36m(func pid=89738)[0m mae:  0.11930862814188004
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.249, 0.074, 0.315, 0.077, 0.181, 0.261, 0.128, 0.14, 0.111]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2723 | Steps: 4 | Val loss: 0.3134 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3147 | Steps: 4 | Val loss: 0.2811 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6609 | Steps: 4 | Val loss: 0.5077 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3798 | Steps: 4 | Val loss: 0.2954 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=87283)[0m rmse: 0.16830460727214813
[2m[36m(func pid=87283)[0m mae:  0.09942597895860672
[2m[36m(func pid=87283)[0m rmse_per_class: [0.077, 0.228, 0.08, 0.307, 0.064, 0.17, 0.221, 0.148, 0.255, 0.132]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88115)[0m rmse: 0.153954416513443
[2m[36m(func pid=88115)[0m mae:  0.09328688681125641
[2m[36m(func pid=88115)[0m rmse_per_class: [0.09, 0.199, 0.035, 0.295, 0.107, 0.159, 0.234, 0.165, 0.136, 0.12]
== Status ==
Current time: 2024-01-07 20:31:27 (running for 00:41:45.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.272 |  0.168 |                   69 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.315 |  0.154 |                   68 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.661 |  0.179 |                   64 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.379 |  0.165 |                   63 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=88115)[0m 

[2m[36m(func pid=88955)[0m rmse: 0.17886167764663696
[2m[36m(func pid=88955)[0m mae:  0.13122454285621643
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.336, 0.107, 0.19, 0.292, 0.139, 0.144, 0.109]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16442933678627014
[2m[36m(func pid=89738)[0m mae:  0.11922068893909454
[2m[36m(func pid=89738)[0m rmse_per_class: [0.113, 0.25, 0.072, 0.313, 0.077, 0.181, 0.262, 0.127, 0.14, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2855 | Steps: 4 | Val loss: 0.3032 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3297 | Steps: 4 | Val loss: 0.2798 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6568 | Steps: 4 | Val loss: 0.5058 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=87283)[0m rmse: 0.16426366567611694
[2m[36m(func pid=87283)[0m mae:  0.09655372053384781
[2m[36m(func pid=87283)[0m rmse_per_class: [0.077, 0.222, 0.092, 0.295, 0.073, 0.175, 0.225, 0.128, 0.248, 0.107]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3903 | Steps: 4 | Val loss: 0.2938 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 20:31:33 (running for 00:41:50.95)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.285 |  0.164 |                   70 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.33  |  0.151 |                   69 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.661 |  0.179 |                   65 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.38  |  0.164 |                   64 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.1514964997768402
[2m[36m(func pid=88115)[0m mae:  0.09238302707672119
[2m[36m(func pid=88115)[0m rmse_per_class: [0.092, 0.204, 0.028, 0.289, 0.127, 0.165, 0.237, 0.117, 0.141, 0.115]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1788840889930725
[2m[36m(func pid=88955)[0m mae:  0.13124589622020721
[2m[36m(func pid=88955)[0m rmse_per_class: [0.118, 0.259, 0.096, 0.336, 0.107, 0.19, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1634979397058487
[2m[36m(func pid=89738)[0m mae:  0.11828253418207169
[2m[36m(func pid=89738)[0m rmse_per_class: [0.112, 0.249, 0.071, 0.312, 0.077, 0.181, 0.258, 0.127, 0.14, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2819 | Steps: 4 | Val loss: 0.2994 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3332 | Steps: 4 | Val loss: 0.2848 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=87283)[0m rmse: 0.1637950837612152
[2m[36m(func pid=87283)[0m mae:  0.09614436328411102
[2m[36m(func pid=87283)[0m rmse_per_class: [0.079, 0.219, 0.085, 0.31, 0.076, 0.173, 0.217, 0.126, 0.24, 0.113]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6523 | Steps: 4 | Val loss: 0.5028 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3844 | Steps: 4 | Val loss: 0.2928 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:31:38 (running for 00:41:56.40)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.282 |  0.164 |                   71 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.333 |  0.152 |                   70 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.657 |  0.179 |                   66 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.39  |  0.163 |                   65 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.15164640545845032
[2m[36m(func pid=88115)[0m mae:  0.09330827742815018
[2m[36m(func pid=88115)[0m rmse_per_class: [0.097, 0.211, 0.031, 0.28, 0.136, 0.17, 0.24, 0.1, 0.147, 0.103]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16281957924365997
[2m[36m(func pid=89738)[0m mae:  0.11768513917922974
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.248, 0.069, 0.312, 0.076, 0.181, 0.256, 0.126, 0.14, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1788811832666397
[2m[36m(func pid=88955)[0m mae:  0.13128571212291718
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.336, 0.106, 0.19, 0.292, 0.139, 0.144, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2787 | Steps: 4 | Val loss: 0.3076 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3048 | Steps: 4 | Val loss: 0.2867 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=87283)[0m rmse: 0.1683986634016037
[2m[36m(func pid=87283)[0m mae:  0.09963804483413696
[2m[36m(func pid=87283)[0m rmse_per_class: [0.082, 0.221, 0.085, 0.327, 0.079, 0.183, 0.221, 0.105, 0.267, 0.113]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6488 | Steps: 4 | Val loss: 0.5001 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3831 | Steps: 4 | Val loss: 0.2927 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:31:44 (running for 00:42:01.70)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.279 |  0.168 |                   72 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.305 |  0.152 |                   71 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.652 |  0.179 |                   67 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.384 |  0.163 |                   66 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.15173758566379547
[2m[36m(func pid=88115)[0m mae:  0.09419034421443939
[2m[36m(func pid=88115)[0m rmse_per_class: [0.099, 0.226, 0.034, 0.271, 0.139, 0.17, 0.235, 0.102, 0.149, 0.091]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16283389925956726
[2m[36m(func pid=89738)[0m mae:  0.11767452955245972
[2m[36m(func pid=89738)[0m rmse_per_class: [0.112, 0.249, 0.069, 0.311, 0.076, 0.181, 0.255, 0.125, 0.14, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.1788589060306549
[2m[36m(func pid=88955)[0m mae:  0.13124267756938934
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.259, 0.096, 0.335, 0.107, 0.19, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2746 | Steps: 4 | Val loss: 0.3153 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3047 | Steps: 4 | Val loss: 0.2845 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=87283)[0m rmse: 0.17075133323669434
[2m[36m(func pid=87283)[0m mae:  0.10165475308895111
[2m[36m(func pid=87283)[0m rmse_per_class: [0.082, 0.219, 0.08, 0.333, 0.075, 0.199, 0.225, 0.094, 0.281, 0.119]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3781 | Steps: 4 | Val loss: 0.2928 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6437 | Steps: 4 | Val loss: 0.4992 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:31:49 (running for 00:42:06.99)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.275 |  0.171 |                   73 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.305 |  0.153 |                   72 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.649 |  0.179 |                   68 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.383 |  0.163 |                   67 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.15338289737701416
[2m[36m(func pid=88115)[0m mae:  0.09619225561618805
[2m[36m(func pid=88115)[0m rmse_per_class: [0.099, 0.231, 0.037, 0.27, 0.139, 0.173, 0.231, 0.106, 0.162, 0.085]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1627928763628006
[2m[36m(func pid=89738)[0m mae:  0.11765895783901215
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.249, 0.069, 0.312, 0.076, 0.182, 0.256, 0.125, 0.14, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3001 | Steps: 4 | Val loss: 0.3277 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=88955)[0m rmse: 0.17879453301429749
[2m[36m(func pid=88955)[0m mae:  0.13114610314369202
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.335, 0.107, 0.189, 0.292, 0.139, 0.143, 0.11]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3039 | Steps: 4 | Val loss: 0.2850 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=87283)[0m rmse: 0.17734719812870026
[2m[36m(func pid=87283)[0m mae:  0.10680576413869858
[2m[36m(func pid=87283)[0m rmse_per_class: [0.082, 0.219, 0.091, 0.343, 0.081, 0.209, 0.233, 0.101, 0.287, 0.127]
[2m[36m(func pid=87283)[0m 
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3795 | Steps: 4 | Val loss: 0.2925 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6471 | Steps: 4 | Val loss: 0.4988 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 20:31:54 (running for 00:42:12.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14524999633431435
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00018 | RUNNING    | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.3   |  0.177 |                   74 |
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.304 |  0.154 |                   73 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.644 |  0.179 |                   69 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.378 |  0.163 |                   68 |
| train_84a75_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88115)[0m rmse: 0.15435878932476044
[2m[36m(func pid=88115)[0m mae:  0.09678259491920471
[2m[36m(func pid=88115)[0m rmse_per_class: [0.099, 0.239, 0.044, 0.269, 0.129, 0.163, 0.234, 0.117, 0.167, 0.083]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16264408826828003
[2m[36m(func pid=89738)[0m mae:  0.11758208274841309
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.248, 0.069, 0.312, 0.075, 0.182, 0.256, 0.125, 0.14, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=87283)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2905 | Steps: 4 | Val loss: 0.3220 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=88955)[0m rmse: 0.17894403636455536
[2m[36m(func pid=88955)[0m mae:  0.1312519609928131
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.335, 0.106, 0.19, 0.292, 0.138, 0.144, 0.112]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3129 | Steps: 4 | Val loss: 0.2838 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=87283)[0m rmse: 0.17842373251914978
[2m[36m(func pid=87283)[0m mae:  0.10793878883123398
[2m[36m(func pid=87283)[0m rmse_per_class: [0.082, 0.219, 0.101, 0.341, 0.088, 0.214, 0.237, 0.108, 0.252, 0.142]
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3800 | Steps: 4 | Val loss: 0.2920 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6425 | Steps: 4 | Val loss: 0.4963 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=88115)[0m rmse: 0.15895198285579681
[2m[36m(func pid=88115)[0m mae:  0.09952287375926971
[2m[36m(func pid=88115)[0m rmse_per_class: [0.096, 0.229, 0.057, 0.274, 0.122, 0.163, 0.241, 0.137, 0.182, 0.088]
[2m[36m(func pid=88115)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.1624506413936615
[2m[36m(func pid=89738)[0m mae:  0.1173829585313797
[2m[36m(func pid=89738)[0m rmse_per_class: [0.11, 0.248, 0.07, 0.311, 0.075, 0.181, 0.255, 0.125, 0.14, 0.109]
[2m[36m(func pid=88955)[0m rmse: 0.17881807684898376
[2m[36m(func pid=88955)[0m mae:  0.13115623593330383
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.106, 0.189, 0.291, 0.139, 0.143, 0.111]
[2m[36m(func pid=88115)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3373 | Steps: 4 | Val loss: 0.2926 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=88115)[0m rmse: 0.16759368777275085
[2m[36m(func pid=88115)[0m mae:  0.1038830503821373
[2m[36m(func pid=88115)[0m rmse_per_class: [0.099, 0.224, 0.075, 0.285, 0.118, 0.168, 0.251, 0.157, 0.203, 0.096]
== Status ==
Current time: 2024-01-07 20:31:59 (running for 00:42:17.57)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14549999684095383
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00019 | RUNNING    | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.313 |  0.159 |                   74 |
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.647 |  0.179 |                   70 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.379 |  0.163 |                   69 |
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 20:32:06 (running for 00:42:24.50)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 3 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.647 |  0.179 |                   70 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.379 |  0.163 |                   69 |
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=106295)[0m Configuration completed!
[2m[36m(func pid=106295)[0m New optimizer parameters:
[2m[36m(func pid=106295)[0m SGD (
[2m[36m(func pid=106295)[0m Parameter Group 0
[2m[36m(func pid=106295)[0m     dampening: 0
[2m[36m(func pid=106295)[0m     differentiable: False
[2m[36m(func pid=106295)[0m     foreach: None
[2m[36m(func pid=106295)[0m     lr: 0.01
[2m[36m(func pid=106295)[0m     maximize: False
[2m[36m(func pid=106295)[0m     momentum: 0.9
[2m[36m(func pid=106295)[0m     nesterov: False
[2m[36m(func pid=106295)[0m     weight_decay: 1e-05
[2m[36m(func pid=106295)[0m )
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6438 | Steps: 4 | Val loss: 0.4927 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3812 | Steps: 4 | Val loss: 0.2922 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8676 | Steps: 4 | Val loss: 0.6286 | Batch size: 32 | lr: 0.01 | Duration: 5.03s
[2m[36m(func pid=89738)[0m rmse: 0.16252240538597107
[2m[36m(func pid=89738)[0m mae:  0.11754930019378662
[2m[36m(func pid=89738)[0m rmse_per_class: [0.11, 0.248, 0.07, 0.311, 0.075, 0.181, 0.257, 0.125, 0.14, 0.109]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m rmse: 0.17883571982383728
[2m[36m(func pid=88955)[0m mae:  0.13122710585594177
[2m[36m(func pid=88955)[0m rmse_per_class: [0.118, 0.26, 0.096, 0.335, 0.106, 0.189, 0.291, 0.139, 0.143, 0.111]
[2m[36m(func pid=106295)[0m rmse: 0.18250903487205505
[2m[36m(func pid=106295)[0m mae:  0.1341501921415329
[2m[36m(func pid=106295)[0m rmse_per_class: [0.117, 0.267, 0.109, 0.34, 0.112, 0.191, 0.294, 0.144, 0.141, 0.11]
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3811 | Steps: 4 | Val loss: 0.2925 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:32:12 (running for 00:42:30.17)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.643 |  0.179 |                   71 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.381 |  0.163 |                   71 |
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106828)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=106828)[0m Configuration completed!
[2m[36m(func pid=106828)[0m New optimizer parameters:
[2m[36m(func pid=106828)[0m SGD (
[2m[36m(func pid=106828)[0m Parameter Group 0
[2m[36m(func pid=106828)[0m     dampening: 0
[2m[36m(func pid=106828)[0m     differentiable: False
[2m[36m(func pid=106828)[0m     foreach: None
[2m[36m(func pid=106828)[0m     lr: 0.1
[2m[36m(func pid=106828)[0m     maximize: False
[2m[36m(func pid=106828)[0m     momentum: 0.9
[2m[36m(func pid=106828)[0m     nesterov: False
[2m[36m(func pid=106828)[0m     weight_decay: 1e-05
[2m[36m(func pid=106828)[0m )
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=89738)[0m rmse: 0.16271640360355377
[2m[36m(func pid=89738)[0m mae:  0.1177036389708519
[2m[36m(func pid=89738)[0m rmse_per_class: [0.111, 0.248, 0.071, 0.31, 0.075, 0.181, 0.258, 0.125, 0.139, 0.109]
== Status ==
Current time: 2024-01-07 20:32:17 (running for 00:42:35.54)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.644 |  0.179 |                   72 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.381 |  0.163 |                   72 |
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.868 |  0.183 |                    1 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6336 | Steps: 4 | Val loss: 0.4915 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7237 | Steps: 4 | Val loss: 0.4976 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6936 | Steps: 4 | Val loss: 0.3499 | Batch size: 32 | lr: 0.1 | Duration: 4.62s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3755 | Steps: 4 | Val loss: 0.2908 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=88955)[0m rmse: 0.17883947491645813
[2m[36m(func pid=88955)[0m mae:  0.13122449815273285
[2m[36m(func pid=88955)[0m rmse_per_class: [0.118, 0.26, 0.096, 0.335, 0.106, 0.189, 0.291, 0.139, 0.143, 0.111]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.18092282116413116
[2m[36m(func pid=106295)[0m mae:  0.13282588124275208
[2m[36m(func pid=106295)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.337, 0.11, 0.191, 0.294, 0.14, 0.14, 0.108]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.1812378168106079
[2m[36m(func pid=106828)[0m mae:  0.13313549757003784
[2m[36m(func pid=106828)[0m rmse_per_class: [0.115, 0.265, 0.126, 0.336, 0.087, 0.19, 0.293, 0.143, 0.144, 0.113]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:32:23 (running for 00:42:40.93)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.634 |  0.179 |                   73 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.376 |  0.162 |                   73 |
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.724 |  0.181 |                    2 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.694 |  0.181 |                    1 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m rmse: 0.16167959570884705
[2m[36m(func pid=89738)[0m mae:  0.1168968677520752
[2m[36m(func pid=89738)[0m rmse_per_class: [0.11, 0.247, 0.068, 0.309, 0.075, 0.179, 0.256, 0.125, 0.139, 0.108]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6349 | Steps: 4 | Val loss: 0.4900 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5594 | Steps: 4 | Val loss: 0.4011 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4313 | Steps: 4 | Val loss: 0.3442 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3785 | Steps: 4 | Val loss: 0.2903 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=88955)[0m rmse: 0.17890289425849915
[2m[36m(func pid=88955)[0m mae:  0.13124585151672363
[2m[36m(func pid=88955)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.335, 0.106, 0.19, 0.291, 0.139, 0.144, 0.111]
[2m[36m(func pid=88955)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.17933912575244904
[2m[36m(func pid=106295)[0m mae:  0.13183847069740295
[2m[36m(func pid=106295)[0m rmse_per_class: [0.118, 0.264, 0.104, 0.337, 0.099, 0.19, 0.291, 0.136, 0.143, 0.111]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.17215780913829803
[2m[36m(func pid=106828)[0m mae:  0.12546448409557343
[2m[36m(func pid=106828)[0m rmse_per_class: [0.116, 0.262, 0.106, 0.323, 0.066, 0.188, 0.277, 0.128, 0.145, 0.11]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:32:28 (running for 00:42:46.53)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00020 | RUNNING    | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.635 |  0.179 |                   74 |
| train_84a75_00021 | RUNNING    | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.379 |  0.161 |                   74 |
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.559 |  0.179 |                    3 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.431 |  0.172 |                    2 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m rmse: 0.1613500416278839
[2m[36m(func pid=89738)[0m mae:  0.11655330657958984
[2m[36m(func pid=89738)[0m rmse_per_class: [0.109, 0.247, 0.069, 0.31, 0.074, 0.179, 0.254, 0.125, 0.139, 0.107]
[2m[36m(func pid=89738)[0m 
[2m[36m(func pid=88955)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6325 | Steps: 4 | Val loss: 0.4887 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4568 | Steps: 4 | Val loss: 0.3447 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4810 | Steps: 4 | Val loss: 0.3466 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=88955)[0m rmse: 0.1790478378534317
[2m[36m(func pid=88955)[0m mae:  0.1313290297985077
[2m[36m(func pid=88955)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.106, 0.189, 0.291, 0.139, 0.144, 0.112]
[2m[36m(func pid=89738)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3772 | Steps: 4 | Val loss: 0.2897 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=106295)[0m rmse: 0.17691491544246674
[2m[36m(func pid=106295)[0m mae:  0.12954017519950867
[2m[36m(func pid=106295)[0m rmse_per_class: [0.122, 0.264, 0.095, 0.335, 0.093, 0.189, 0.283, 0.135, 0.143, 0.111]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.15313009917736053
[2m[36m(func pid=106828)[0m mae:  0.10785160958766937
[2m[36m(func pid=106828)[0m rmse_per_class: [0.113, 0.24, 0.052, 0.297, 0.056, 0.166, 0.253, 0.119, 0.134, 0.101]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:32:34 (running for 00:42:51.85)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.457 |  0.177 |                    4 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.481 |  0.153 |                    3 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89738)[0m rmse: 0.16096404194831848
[2m[36m(func pid=89738)[0m mae:  0.11625760793685913
[2m[36m(func pid=89738)[0m rmse_per_class: [0.109, 0.246, 0.068, 0.309, 0.074, 0.179, 0.254, 0.125, 0.139, 0.107]
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4130 | Steps: 4 | Val loss: 0.3211 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4287 | Steps: 4 | Val loss: 0.3023 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=106295)[0m rmse: 0.17551779747009277
[2m[36m(func pid=106295)[0m mae:  0.12835684418678284
[2m[36m(func pid=106295)[0m rmse_per_class: [0.125, 0.263, 0.091, 0.333, 0.087, 0.19, 0.28, 0.133, 0.142, 0.112]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.1557699292898178
[2m[36m(func pid=106828)[0m mae:  0.10626008361577988
[2m[36m(func pid=106828)[0m rmse_per_class: [0.108, 0.245, 0.052, 0.329, 0.054, 0.163, 0.251, 0.121, 0.134, 0.099]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4003 | Steps: 4 | Val loss: 0.3124 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.3625 | Steps: 4 | Val loss: 0.2901 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=106295)[0m rmse: 0.1734331101179123
[2m[36m(func pid=106295)[0m mae:  0.12660571932792664
[2m[36m(func pid=106295)[0m rmse_per_class: [0.123, 0.263, 0.087, 0.331, 0.081, 0.189, 0.275, 0.131, 0.141, 0.113]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:32:43 (running for 00:43:01.33)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.4   |  0.173 |                    6 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.429 |  0.156 |                    4 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15729686617851257
[2m[36m(func pid=106828)[0m mae:  0.10359156131744385
[2m[36m(func pid=106828)[0m rmse_per_class: [0.101, 0.248, 0.05, 0.328, 0.053, 0.17, 0.274, 0.117, 0.137, 0.095]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3980 | Steps: 4 | Val loss: 0.3089 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.3486 | Steps: 4 | Val loss: 0.2826 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 20:32:49 (running for 00:43:06.74)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.398 |  0.169 |                    7 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.362 |  0.157 |                    5 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=106295)[0m rmse: 0.16933093965053558

[2m[36m(func pid=106295)[0m mae:  0.12324122339487076
[2m[36m(func pid=106295)[0m rmse_per_class: [0.117, 0.257, 0.08, 0.332, 0.073, 0.183, 0.271, 0.128, 0.141, 0.113]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.15298637747764587
[2m[36m(func pid=106828)[0m mae:  0.0980825424194336
[2m[36m(func pid=106828)[0m rmse_per_class: [0.094, 0.244, 0.058, 0.284, 0.051, 0.166, 0.239, 0.152, 0.132, 0.109]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3959 | Steps: 4 | Val loss: 0.3066 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3340 | Steps: 4 | Val loss: 0.2931 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:32:54 (running for 00:43:12.42)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.396 |  0.166 |                    8 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.349 |  0.153 |                    6 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1661205142736435
[2m[36m(func pid=106295)[0m mae:  0.12082662433385849
[2m[36m(func pid=106295)[0m rmse_per_class: [0.113, 0.252, 0.073, 0.329, 0.069, 0.179, 0.27, 0.126, 0.14, 0.111]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.1613626480102539
[2m[36m(func pid=106828)[0m mae:  0.10161926597356796
[2m[36m(func pid=106828)[0m rmse_per_class: [0.092, 0.233, 0.058, 0.286, 0.051, 0.19, 0.222, 0.232, 0.133, 0.117]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3983 | Steps: 4 | Val loss: 0.3017 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3290 | Steps: 4 | Val loss: 0.2904 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:33:00 (running for 00:43:18.06)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.396 |  0.166 |                    8 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.334 |  0.161 |                    7 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.16224491596221924
[2m[36m(func pid=106295)[0m mae:  0.11748973280191422
[2m[36m(func pid=106295)[0m rmse_per_class: [0.108, 0.247, 0.066, 0.325, 0.065, 0.174, 0.266, 0.124, 0.138, 0.109]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.165054589509964
[2m[36m(func pid=106828)[0m mae:  0.10518290847539902
[2m[36m(func pid=106828)[0m rmse_per_class: [0.088, 0.224, 0.049, 0.298, 0.056, 0.201, 0.215, 0.257, 0.147, 0.115]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3933 | Steps: 4 | Val loss: 0.2951 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3381 | Steps: 4 | Val loss: 0.2823 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 20:33:05 (running for 00:43:23.41)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.398 |  0.162 |                    9 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.329 |  0.165 |                    8 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15640893578529358
[2m[36m(func pid=106828)[0m mae:  0.10004087537527084
[2m[36m(func pid=106828)[0m rmse_per_class: [0.086, 0.226, 0.084, 0.309, 0.084, 0.172, 0.201, 0.142, 0.161, 0.099]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.15879473090171814
[2m[36m(func pid=106295)[0m mae:  0.11445774883031845
[2m[36m(func pid=106295)[0m rmse_per_class: [0.104, 0.242, 0.061, 0.319, 0.064, 0.173, 0.261, 0.122, 0.136, 0.106]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3227 | Steps: 4 | Val loss: 0.2859 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3771 | Steps: 4 | Val loss: 0.2910 | Batch size: 32 | lr: 0.01 | Duration: 3.33s
== Status ==
Current time: 2024-01-07 20:33:11 (running for 00:43:29.08)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.393 |  0.159 |                   10 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.338 |  0.156 |                    9 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15771767497062683
[2m[36m(func pid=106828)[0m mae:  0.10084602981805801
[2m[36m(func pid=106828)[0m rmse_per_class: [0.084, 0.226, 0.089, 0.32, 0.092, 0.175, 0.2, 0.108, 0.185, 0.098]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.15696460008621216
[2m[36m(func pid=106295)[0m mae:  0.1127782091498375
[2m[36m(func pid=106295)[0m rmse_per_class: [0.102, 0.24, 0.059, 0.315, 0.062, 0.174, 0.257, 0.12, 0.136, 0.105]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3186 | Steps: 4 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3725 | Steps: 4 | Val loss: 0.2860 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 20:33:17 (running for 00:43:35.13)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.377 |  0.157 |                   11 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.323 |  0.158 |                   10 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1503310352563858
[2m[36m(func pid=106828)[0m mae:  0.0966826006770134
[2m[36m(func pid=106828)[0m rmse_per_class: [0.077, 0.216, 0.063, 0.313, 0.088, 0.163, 0.196, 0.1, 0.182, 0.105]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.15480566024780273
[2m[36m(func pid=106295)[0m mae:  0.11076366901397705
[2m[36m(func pid=106295)[0m rmse_per_class: [0.101, 0.238, 0.057, 0.31, 0.062, 0.172, 0.252, 0.119, 0.134, 0.102]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2888 | Steps: 4 | Val loss: 0.2712 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3662 | Steps: 4 | Val loss: 0.2825 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 20:33:22 (running for 00:43:40.46)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.373 |  0.155 |                   12 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.289 |  0.152 |                   12 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1515326052904129
[2m[36m(func pid=106828)[0m mae:  0.09698966890573502
[2m[36m(func pid=106828)[0m rmse_per_class: [0.08, 0.217, 0.043, 0.305, 0.078, 0.195, 0.226, 0.103, 0.163, 0.105]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.1534578502178192
[2m[36m(func pid=106295)[0m mae:  0.10939860343933105
[2m[36m(func pid=106295)[0m rmse_per_class: [0.101, 0.238, 0.055, 0.308, 0.061, 0.172, 0.249, 0.117, 0.134, 0.101]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3021 | Steps: 4 | Val loss: 0.3158 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3649 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 20:33:28 (running for 00:43:45.95)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.366 |  0.153 |                   13 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.302 |  0.179 |                   13 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.17921535670757294
[2m[36m(func pid=106828)[0m mae:  0.11639194190502167
[2m[36m(func pid=106828)[0m rmse_per_class: [0.082, 0.222, 0.047, 0.301, 0.05, 0.192, 0.267, 0.131, 0.163, 0.337]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.15234389901161194
[2m[36m(func pid=106295)[0m mae:  0.10819575935602188
[2m[36m(func pid=106295)[0m rmse_per_class: [0.1, 0.239, 0.053, 0.305, 0.06, 0.172, 0.244, 0.116, 0.134, 0.1]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2960 | Steps: 4 | Val loss: 0.3285 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3654 | Steps: 4 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:33:33 (running for 00:43:51.54)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.365 |  0.152 |                   14 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.296 |  0.183 |                   14 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.18304060399532318
[2m[36m(func pid=106828)[0m mae:  0.11767584085464478
[2m[36m(func pid=106828)[0m rmse_per_class: [0.079, 0.211, 0.043, 0.303, 0.05, 0.207, 0.263, 0.131, 0.158, 0.384]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.15110912919044495
[2m[36m(func pid=106295)[0m mae:  0.10725928843021393
[2m[36m(func pid=106295)[0m rmse_per_class: [0.1, 0.239, 0.052, 0.301, 0.06, 0.168, 0.244, 0.114, 0.135, 0.097]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3226 | Steps: 4 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3520 | Steps: 4 | Val loss: 0.2764 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 20:33:39 (running for 00:43:56.89)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.365 |  0.151 |                   15 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.323 |  0.149 |                   15 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1486676037311554
[2m[36m(func pid=106828)[0m mae:  0.09158747643232346
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.2, 0.05, 0.287, 0.06, 0.179, 0.208, 0.195, 0.135, 0.103]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.1505970060825348
[2m[36m(func pid=106295)[0m mae:  0.10710849612951279
[2m[36m(func pid=106295)[0m rmse_per_class: [0.099, 0.239, 0.051, 0.299, 0.06, 0.166, 0.245, 0.112, 0.135, 0.099]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3089 | Steps: 4 | Val loss: 0.2667 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3528 | Steps: 4 | Val loss: 0.2759 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:33:44 (running for 00:44:02.44)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.352 |  0.151 |                   16 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.309 |  0.146 |                   16 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14646199345588684
[2m[36m(func pid=106828)[0m mae:  0.09289446473121643
[2m[36m(func pid=106828)[0m rmse_per_class: [0.072, 0.213, 0.059, 0.278, 0.071, 0.167, 0.205, 0.104, 0.171, 0.126]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.15044711530208588
[2m[36m(func pid=106295)[0m mae:  0.10705683380365372
[2m[36m(func pid=106295)[0m rmse_per_class: [0.097, 0.238, 0.052, 0.301, 0.06, 0.164, 0.245, 0.11, 0.135, 0.103]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3040 | Steps: 4 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3512 | Steps: 4 | Val loss: 0.2753 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 20:33:50 (running for 00:44:07.71)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.353 |  0.15  |                   17 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.304 |  0.146 |                   17 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14555545151233673
[2m[36m(func pid=106828)[0m mae:  0.0923013985157013
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.225, 0.038, 0.268, 0.09, 0.159, 0.197, 0.106, 0.179, 0.125]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.150308758020401
[2m[36m(func pid=106295)[0m mae:  0.10670576989650726
[2m[36m(func pid=106295)[0m rmse_per_class: [0.096, 0.238, 0.053, 0.3, 0.06, 0.162, 0.245, 0.108, 0.135, 0.107]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3047 | Steps: 4 | Val loss: 0.2646 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3504 | Steps: 4 | Val loss: 0.2736 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 20:33:55 (running for 00:44:13.13)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.351 |  0.15  |                   18 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.305 |  0.147 |                   18 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1466701328754425
[2m[36m(func pid=106828)[0m mae:  0.09263327717781067
[2m[36m(func pid=106828)[0m rmse_per_class: [0.071, 0.222, 0.038, 0.272, 0.094, 0.157, 0.201, 0.098, 0.19, 0.124]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.1495705246925354
[2m[36m(func pid=106295)[0m mae:  0.1057235598564148
[2m[36m(func pid=106295)[0m rmse_per_class: [0.093, 0.239, 0.052, 0.295, 0.06, 0.161, 0.243, 0.108, 0.135, 0.11]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2835 | Steps: 4 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 20:34:00 (running for 00:44:18.52)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.35  |  0.15  |                   19 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.284 |  0.154 |                   19 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15386666357517242
[2m[36m(func pid=106828)[0m mae:  0.0970158576965332
[2m[36m(func pid=106828)[0m rmse_per_class: [0.08, 0.215, 0.057, 0.299, 0.089, 0.159, 0.217, 0.129, 0.149, 0.143]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3465 | Steps: 4 | Val loss: 0.2709 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=106295)[0m rmse: 0.1477942168712616
[2m[36m(func pid=106295)[0m mae:  0.10471899807453156
[2m[36m(func pid=106295)[0m rmse_per_class: [0.091, 0.236, 0.049, 0.284, 0.061, 0.163, 0.245, 0.106, 0.135, 0.108]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2918 | Steps: 4 | Val loss: 0.2634 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 20:34:06 (running for 00:44:24.17)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.347 |  0.148 |                   20 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.292 |  0.147 |                   20 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1466657668352127
[2m[36m(func pid=106828)[0m mae:  0.09204018861055374
[2m[36m(func pid=106828)[0m rmse_per_class: [0.088, 0.225, 0.05, 0.294, 0.088, 0.159, 0.208, 0.118, 0.126, 0.111]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3439 | Steps: 4 | Val loss: 0.2697 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=106295)[0m rmse: 0.14644838869571686
[2m[36m(func pid=106295)[0m mae:  0.1034676805138588
[2m[36m(func pid=106295)[0m rmse_per_class: [0.089, 0.235, 0.048, 0.281, 0.061, 0.16, 0.249, 0.104, 0.134, 0.103]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2745 | Steps: 4 | Val loss: 0.2711 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 20:34:11 (running for 00:44:29.69)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.344 |  0.146 |                   21 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.274 |  0.152 |                   21 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15191012620925903
[2m[36m(func pid=106828)[0m mae:  0.0953884944319725
[2m[36m(func pid=106828)[0m rmse_per_class: [0.083, 0.224, 0.052, 0.3, 0.097, 0.162, 0.223, 0.123, 0.131, 0.123]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3431 | Steps: 4 | Val loss: 0.2697 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2827 | Steps: 4 | Val loss: 0.2641 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=106295)[0m rmse: 0.14618539810180664
[2m[36m(func pid=106295)[0m mae:  0.10281851142644882
[2m[36m(func pid=106295)[0m rmse_per_class: [0.087, 0.235, 0.05, 0.281, 0.061, 0.16, 0.251, 0.105, 0.133, 0.1]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:34:17 (running for 00:44:35.13)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.343 |  0.146 |                   22 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.283 |  0.148 |                   22 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14818881452083588
[2m[36m(func pid=106828)[0m mae:  0.09087160974740982
[2m[36m(func pid=106828)[0m rmse_per_class: [0.076, 0.219, 0.048, 0.277, 0.093, 0.172, 0.237, 0.118, 0.14, 0.102]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3387 | Steps: 4 | Val loss: 0.2696 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2777 | Steps: 4 | Val loss: 0.2651 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=106295)[0m rmse: 0.14616353809833527
[2m[36m(func pid=106295)[0m mae:  0.10276999324560165
[2m[36m(func pid=106295)[0m rmse_per_class: [0.086, 0.234, 0.055, 0.281, 0.062, 0.16, 0.249, 0.103, 0.132, 0.099]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:34:22 (running for 00:44:40.68)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.339 |  0.146 |                   23 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.278 |  0.149 |                   23 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14866314828395844
[2m[36m(func pid=106828)[0m mae:  0.08915633708238602
[2m[36m(func pid=106828)[0m rmse_per_class: [0.068, 0.215, 0.058, 0.273, 0.1, 0.175, 0.236, 0.104, 0.143, 0.115]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3347 | Steps: 4 | Val loss: 0.2680 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2703 | Steps: 4 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=106295)[0m rmse: 0.14499308168888092
[2m[36m(func pid=106295)[0m mae:  0.1016467809677124
[2m[36m(func pid=106295)[0m rmse_per_class: [0.085, 0.232, 0.058, 0.28, 0.062, 0.16, 0.24, 0.102, 0.132, 0.099]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:34:28 (running for 00:44:46.08)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.335 |  0.145 |                   24 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.27  |  0.152 |                   24 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15220747888088226
[2m[36m(func pid=106828)[0m mae:  0.09158723056316376
[2m[36m(func pid=106828)[0m rmse_per_class: [0.066, 0.211, 0.078, 0.284, 0.116, 0.172, 0.24, 0.104, 0.158, 0.092]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3309 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2738 | Steps: 4 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=106295)[0m rmse: 0.14472100138664246
[2m[36m(func pid=106295)[0m mae:  0.10147535800933838
[2m[36m(func pid=106295)[0m rmse_per_class: [0.085, 0.232, 0.055, 0.282, 0.064, 0.16, 0.236, 0.101, 0.131, 0.101]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.15024469792842865
[2m[36m(func pid=106828)[0m mae:  0.09049997478723526
[2m[36m(func pid=106828)[0m rmse_per_class: [0.067, 0.21, 0.042, 0.28, 0.112, 0.169, 0.232, 0.115, 0.191, 0.084]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:34:33 (running for 00:44:51.68)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.331 |  0.145 |                   25 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.274 |  0.15  |                   25 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3362 | Steps: 4 | Val loss: 0.2691 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2710 | Steps: 4 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=106295)[0m rmse: 0.14585627615451813
[2m[36m(func pid=106295)[0m mae:  0.10188080370426178
[2m[36m(func pid=106295)[0m rmse_per_class: [0.086, 0.233, 0.057, 0.289, 0.064, 0.16, 0.236, 0.099, 0.132, 0.102]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:34:39 (running for 00:44:56.98)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.336 |  0.146 |                   26 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.271 |  0.146 |                   26 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1458561271429062
[2m[36m(func pid=106828)[0m mae:  0.08790072053670883
[2m[36m(func pid=106828)[0m rmse_per_class: [0.084, 0.211, 0.046, 0.28, 0.1, 0.161, 0.23, 0.115, 0.151, 0.08]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3279 | Steps: 4 | Val loss: 0.2681 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2616 | Steps: 4 | Val loss: 0.2595 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=106295)[0m rmse: 0.14497752487659454
[2m[36m(func pid=106295)[0m mae:  0.10123374313116074
[2m[36m(func pid=106295)[0m rmse_per_class: [0.086, 0.232, 0.053, 0.287, 0.066, 0.158, 0.237, 0.099, 0.132, 0.1]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.1420983374118805
[2m[36m(func pid=106828)[0m mae:  0.08535045385360718
[2m[36m(func pid=106828)[0m rmse_per_class: [0.077, 0.207, 0.051, 0.276, 0.087, 0.15, 0.226, 0.132, 0.142, 0.074]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:34:44 (running for 00:45:02.41)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.328 |  0.145 |                   27 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.262 |  0.142 |                   27 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3290 | Steps: 4 | Val loss: 0.2665 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2762 | Steps: 4 | Val loss: 0.3018 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=106295)[0m rmse: 0.14410939812660217
[2m[36m(func pid=106295)[0m mae:  0.10019458830356598
[2m[36m(func pid=106295)[0m rmse_per_class: [0.086, 0.232, 0.051, 0.284, 0.067, 0.157, 0.237, 0.099, 0.131, 0.097]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:34:50 (running for 00:45:07.86)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.329 |  0.144 |                   28 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.276 |  0.168 |                   28 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1681521236896515
[2m[36m(func pid=106828)[0m mae:  0.1032533273100853
[2m[36m(func pid=106828)[0m rmse_per_class: [0.079, 0.235, 0.051, 0.34, 0.087, 0.167, 0.213, 0.137, 0.171, 0.203]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3234 | Steps: 4 | Val loss: 0.2669 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2675 | Steps: 4 | Val loss: 0.3346 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=106295)[0m rmse: 0.14465513825416565
[2m[36m(func pid=106295)[0m mae:  0.10061956942081451
[2m[36m(func pid=106295)[0m rmse_per_class: [0.087, 0.232, 0.056, 0.284, 0.068, 0.159, 0.239, 0.098, 0.13, 0.094]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:34:55 (running for 00:45:13.02)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.323 |  0.145 |                   29 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.267 |  0.18  |                   29 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.18026632070541382
[2m[36m(func pid=106828)[0m mae:  0.1111350879073143
[2m[36m(func pid=106828)[0m rmse_per_class: [0.081, 0.246, 0.04, 0.36, 0.07, 0.181, 0.205, 0.114, 0.206, 0.3]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3150 | Steps: 4 | Val loss: 0.2650 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2672 | Steps: 4 | Val loss: 0.2863 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=106295)[0m rmse: 0.14333809912204742
[2m[36m(func pid=106295)[0m mae:  0.09938445687294006
[2m[36m(func pid=106295)[0m rmse_per_class: [0.085, 0.23, 0.053, 0.281, 0.068, 0.16, 0.235, 0.098, 0.129, 0.095]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:35:00 (running for 00:45:18.54)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.315 |  0.143 |                   30 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.267 |  0.157 |                   30 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15727761387825012
[2m[36m(func pid=106828)[0m mae:  0.09527955949306488
[2m[36m(func pid=106828)[0m rmse_per_class: [0.072, 0.236, 0.035, 0.33, 0.079, 0.162, 0.204, 0.113, 0.184, 0.158]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3260 | Steps: 4 | Val loss: 0.2632 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2583 | Steps: 4 | Val loss: 0.2658 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=106295)[0m rmse: 0.14211905002593994
[2m[36m(func pid=106295)[0m mae:  0.09797049313783646
[2m[36m(func pid=106295)[0m rmse_per_class: [0.084, 0.229, 0.054, 0.281, 0.066, 0.157, 0.227, 0.098, 0.13, 0.095]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.14930716156959534
[2m[36m(func pid=106828)[0m mae:  0.08919163048267365
[2m[36m(func pid=106828)[0m rmse_per_class: [0.07, 0.232, 0.049, 0.256, 0.113, 0.165, 0.226, 0.141, 0.155, 0.086]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:35:06 (running for 00:45:23.87)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.326 |  0.142 |                   31 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.258 |  0.149 |                   31 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3132 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2690 | Steps: 4 | Val loss: 0.2753 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=106295)[0m rmse: 0.14325430989265442
[2m[36m(func pid=106295)[0m mae:  0.09913495182991028
[2m[36m(func pid=106295)[0m rmse_per_class: [0.084, 0.228, 0.051, 0.281, 0.065, 0.159, 0.227, 0.099, 0.136, 0.102]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:35:11 (running for 00:45:29.36)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.313 |  0.143 |                   32 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.269 |  0.157 |                   32 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15709932148456573
[2m[36m(func pid=106828)[0m mae:  0.09412477910518646
[2m[36m(func pid=106828)[0m rmse_per_class: [0.072, 0.22, 0.093, 0.273, 0.127, 0.164, 0.247, 0.124, 0.172, 0.077]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3157 | Steps: 4 | Val loss: 0.2643 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2693 | Steps: 4 | Val loss: 0.2840 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=106295)[0m rmse: 0.14283767342567444
[2m[36m(func pid=106295)[0m mae:  0.09899153560400009
[2m[36m(func pid=106295)[0m rmse_per_class: [0.085, 0.226, 0.048, 0.279, 0.066, 0.159, 0.23, 0.098, 0.136, 0.101]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.16109858453273773
[2m[36m(func pid=106828)[0m mae:  0.09719531983137131
[2m[36m(func pid=106828)[0m rmse_per_class: [0.077, 0.223, 0.13, 0.302, 0.118, 0.167, 0.243, 0.123, 0.156, 0.072]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:35:17 (running for 00:45:35.00)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.316 |  0.143 |                   33 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.269 |  0.161 |                   33 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3103 | Steps: 4 | Val loss: 0.2643 | Batch size: 32 | lr: 0.01 | Duration: 3.31s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2715 | Steps: 4 | Val loss: 0.2624 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=106295)[0m rmse: 0.14245294034481049
[2m[36m(func pid=106295)[0m mae:  0.098264679312706
[2m[36m(func pid=106295)[0m rmse_per_class: [0.087, 0.227, 0.05, 0.282, 0.066, 0.156, 0.232, 0.097, 0.132, 0.096]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.14442896842956543
[2m[36m(func pid=106828)[0m mae:  0.08836622536182404
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.22, 0.074, 0.263, 0.108, 0.16, 0.231, 0.112, 0.138, 0.068]
== Status ==
Current time: 2024-01-07 20:35:22 (running for 00:45:40.59)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.31  |  0.142 |                   34 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.271 |  0.144 |                   34 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3244 | Steps: 4 | Val loss: 0.2634 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2655 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.1 | Duration: 3.31s
[2m[36m(func pid=106295)[0m rmse: 0.14193223416805267
[2m[36m(func pid=106295)[0m mae:  0.09691122174263
[2m[36m(func pid=106295)[0m rmse_per_class: [0.086, 0.227, 0.047, 0.283, 0.067, 0.152, 0.234, 0.102, 0.126, 0.096]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:35:28 (running for 00:45:46.38)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.324 |  0.142 |                   35 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.265 |  0.142 |                   35 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14151020348072052
[2m[36m(func pid=106828)[0m mae:  0.08508403599262238
[2m[36m(func pid=106828)[0m rmse_per_class: [0.081, 0.238, 0.048, 0.276, 0.087, 0.155, 0.216, 0.108, 0.136, 0.07]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3049 | Steps: 4 | Val loss: 0.2659 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2697 | Steps: 4 | Val loss: 0.2624 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=106295)[0m rmse: 0.14376752078533173
[2m[36m(func pid=106295)[0m mae:  0.09886614978313446
[2m[36m(func pid=106295)[0m rmse_per_class: [0.089, 0.228, 0.052, 0.283, 0.069, 0.157, 0.241, 0.095, 0.126, 0.098]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:35:33 (running for 00:45:51.64)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.305 |  0.144 |                   36 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.27  |  0.142 |                   36 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14205320179462433
[2m[36m(func pid=106828)[0m mae:  0.08419381082057953
[2m[36m(func pid=106828)[0m rmse_per_class: [0.071, 0.234, 0.04, 0.282, 0.077, 0.16, 0.219, 0.126, 0.127, 0.084]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3188 | Steps: 4 | Val loss: 0.2644 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2398 | Steps: 4 | Val loss: 0.2980 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=106295)[0m rmse: 0.1429503858089447
[2m[36m(func pid=106295)[0m mae:  0.09772985428571701
[2m[36m(func pid=106295)[0m rmse_per_class: [0.086, 0.228, 0.049, 0.282, 0.066, 0.156, 0.234, 0.097, 0.126, 0.106]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:35:39 (running for 00:45:56.93)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.319 |  0.143 |                   37 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.24  |  0.164 |                   37 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1643616110086441
[2m[36m(func pid=106828)[0m mae:  0.09859317541122437
[2m[36m(func pid=106828)[0m rmse_per_class: [0.078, 0.23, 0.048, 0.322, 0.072, 0.161, 0.246, 0.184, 0.141, 0.163]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3146 | Steps: 4 | Val loss: 0.2624 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2604 | Steps: 4 | Val loss: 0.2815 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=106295)[0m rmse: 0.1416381150484085
[2m[36m(func pid=106295)[0m mae:  0.09683695435523987
[2m[36m(func pid=106295)[0m rmse_per_class: [0.085, 0.225, 0.05, 0.275, 0.066, 0.156, 0.234, 0.095, 0.125, 0.106]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:35:44 (running for 00:46:02.69)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.315 |  0.142 |                   38 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.26  |  0.158 |                   38 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.158193439245224
[2m[36m(func pid=106828)[0m mae:  0.09179608523845673
[2m[36m(func pid=106828)[0m rmse_per_class: [0.075, 0.217, 0.06, 0.292, 0.083, 0.159, 0.235, 0.176, 0.148, 0.137]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3108 | Steps: 4 | Val loss: 0.2617 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2428 | Steps: 4 | Val loss: 0.2858 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=106295)[0m rmse: 0.14144019782543182
[2m[36m(func pid=106295)[0m mae:  0.09596818685531616
[2m[36m(func pid=106295)[0m rmse_per_class: [0.085, 0.225, 0.052, 0.274, 0.064, 0.155, 0.225, 0.097, 0.127, 0.112]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:35:50 (running for 00:46:07.88)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.311 |  0.141 |                   39 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.243 |  0.161 |                   39 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.16109612584114075
[2m[36m(func pid=106828)[0m mae:  0.09413503110408783
[2m[36m(func pid=106828)[0m rmse_per_class: [0.078, 0.215, 0.085, 0.297, 0.099, 0.166, 0.204, 0.131, 0.238, 0.097]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3075 | Steps: 4 | Val loss: 0.2595 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2687 | Steps: 4 | Val loss: 0.3009 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 20:35:55 (running for 00:46:12.89)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.311 |  0.141 |                   39 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.243 |  0.161 |                   39 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13994385302066803
[2m[36m(func pid=106295)[0m mae:  0.09446670114994049
[2m[36m(func pid=106295)[0m rmse_per_class: [0.08, 0.224, 0.045, 0.268, 0.064, 0.155, 0.22, 0.101, 0.129, 0.115]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.16940852999687195
[2m[36m(func pid=106828)[0m mae:  0.10126571357250214
[2m[36m(func pid=106828)[0m rmse_per_class: [0.092, 0.21, 0.086, 0.293, 0.096, 0.172, 0.227, 0.138, 0.291, 0.088]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3090 | Steps: 4 | Val loss: 0.2586 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2381 | Steps: 4 | Val loss: 0.2959 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:36:00 (running for 00:46:18.49)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.308 |  0.14  |                   40 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.269 |  0.169 |                   40 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13908399641513824
[2m[36m(func pid=106295)[0m mae:  0.09379364550113678
[2m[36m(func pid=106295)[0m rmse_per_class: [0.075, 0.224, 0.04, 0.263, 0.065, 0.156, 0.217, 0.103, 0.135, 0.113]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.16467642784118652
[2m[36m(func pid=106828)[0m mae:  0.10016515105962753
[2m[36m(func pid=106828)[0m rmse_per_class: [0.081, 0.216, 0.04, 0.271, 0.097, 0.174, 0.244, 0.135, 0.288, 0.101]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2938 | Steps: 4 | Val loss: 0.2580 | Batch size: 32 | lr: 0.01 | Duration: 3.44s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2778 | Steps: 4 | Val loss: 0.2969 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 20:36:06 (running for 00:46:23.91)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.309 |  0.139 |                   41 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.238 |  0.165 |                   41 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.16411259770393372
[2m[36m(func pid=106828)[0m mae:  0.10076133161783218
[2m[36m(func pid=106828)[0m rmse_per_class: [0.076, 0.214, 0.034, 0.289, 0.084, 0.17, 0.238, 0.146, 0.288, 0.102]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13858941197395325
[2m[36m(func pid=106295)[0m mae:  0.09383602440357208
[2m[36m(func pid=106295)[0m rmse_per_class: [0.077, 0.224, 0.044, 0.26, 0.065, 0.155, 0.219, 0.099, 0.138, 0.103]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2610 | Steps: 4 | Val loss: 0.2931 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3067 | Steps: 4 | Val loss: 0.2596 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 20:36:12 (running for 00:46:29.71)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.294 |  0.139 |                   42 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.278 |  0.164 |                   42 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.16172686219215393
[2m[36m(func pid=106828)[0m mae:  0.09897495806217194
[2m[36m(func pid=106828)[0m rmse_per_class: [0.076, 0.211, 0.028, 0.287, 0.105, 0.172, 0.236, 0.138, 0.284, 0.08]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13978561758995056
[2m[36m(func pid=106295)[0m mae:  0.09461464732885361
[2m[36m(func pid=106295)[0m rmse_per_class: [0.082, 0.221, 0.049, 0.27, 0.069, 0.155, 0.225, 0.095, 0.131, 0.102]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2548 | Steps: 4 | Val loss: 0.2843 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3056 | Steps: 4 | Val loss: 0.2591 | Batch size: 32 | lr: 0.01 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 20:36:17 (running for 00:46:35.20)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.307 |  0.14  |                   43 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.261 |  0.162 |                   43 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1569085419178009
[2m[36m(func pid=106828)[0m mae:  0.09566640108823776
[2m[36m(func pid=106828)[0m rmse_per_class: [0.082, 0.214, 0.029, 0.28, 0.093, 0.183, 0.245, 0.115, 0.252, 0.076]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13907289505004883
[2m[36m(func pid=106295)[0m mae:  0.09365474432706833
[2m[36m(func pid=106295)[0m rmse_per_class: [0.081, 0.22, 0.05, 0.272, 0.069, 0.155, 0.227, 0.093, 0.123, 0.099]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2586 | Steps: 4 | Val loss: 0.2839 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3008 | Steps: 4 | Val loss: 0.2596 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 20:36:23 (running for 00:46:41.01)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.306 |  0.139 |                   44 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.259 |  0.154 |                   45 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15429271757602692
[2m[36m(func pid=106828)[0m mae:  0.09175989776849747
[2m[36m(func pid=106828)[0m rmse_per_class: [0.082, 0.22, 0.027, 0.279, 0.072, 0.179, 0.24, 0.116, 0.241, 0.086]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13950420916080475
[2m[36m(func pid=106295)[0m mae:  0.09369461238384247
[2m[36m(func pid=106295)[0m rmse_per_class: [0.082, 0.219, 0.048, 0.274, 0.071, 0.156, 0.228, 0.093, 0.122, 0.101]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2702 | Steps: 4 | Val loss: 0.2761 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3124 | Steps: 4 | Val loss: 0.2571 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 20:36:28 (running for 00:46:46.16)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.301 |  0.14  |                   45 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.27  |  0.151 |                   46 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1514568030834198
[2m[36m(func pid=106828)[0m mae:  0.0887184590101242
[2m[36m(func pid=106828)[0m rmse_per_class: [0.091, 0.221, 0.04, 0.284, 0.079, 0.178, 0.233, 0.108, 0.176, 0.104]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.1375085413455963
[2m[36m(func pid=106295)[0m mae:  0.09208925068378448
[2m[36m(func pid=106295)[0m rmse_per_class: [0.079, 0.218, 0.046, 0.27, 0.073, 0.153, 0.224, 0.092, 0.121, 0.101]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2697 | Steps: 4 | Val loss: 0.2901 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3003 | Steps: 4 | Val loss: 0.2560 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 20:36:33 (running for 00:46:51.59)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.312 |  0.138 |                   46 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.27  |  0.16  |                   47 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15952926874160767
[2m[36m(func pid=106828)[0m mae:  0.09252236038446426
[2m[36m(func pid=106828)[0m rmse_per_class: [0.11, 0.225, 0.117, 0.318, 0.083, 0.169, 0.214, 0.108, 0.141, 0.11]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13676291704177856
[2m[36m(func pid=106295)[0m mae:  0.09099005162715912
[2m[36m(func pid=106295)[0m rmse_per_class: [0.078, 0.218, 0.046, 0.272, 0.07, 0.149, 0.216, 0.096, 0.122, 0.099]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2650 | Steps: 4 | Val loss: 0.2711 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2943 | Steps: 4 | Val loss: 0.2567 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 20:36:39 (running for 00:46:57.20)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.3   |  0.137 |                   47 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.265 |  0.149 |                   48 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14862987399101257
[2m[36m(func pid=106828)[0m mae:  0.08719305694103241
[2m[36m(func pid=106828)[0m rmse_per_class: [0.081, 0.236, 0.056, 0.295, 0.082, 0.157, 0.201, 0.134, 0.135, 0.109]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13747286796569824
[2m[36m(func pid=106295)[0m mae:  0.09158946573734283
[2m[36m(func pid=106295)[0m rmse_per_class: [0.077, 0.218, 0.046, 0.274, 0.07, 0.152, 0.214, 0.096, 0.126, 0.102]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2778 | Steps: 4 | Val loss: 0.2747 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2978 | Steps: 4 | Val loss: 0.2573 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=106828)[0m rmse: 0.1544368863105774
[2m[36m(func pid=106828)[0m mae:  0.09187997877597809
[2m[36m(func pid=106828)[0m rmse_per_class: [0.079, 0.262, 0.036, 0.29, 0.104, 0.168, 0.207, 0.161, 0.144, 0.093]
== Status ==
Current time: 2024-01-07 20:36:44 (running for 00:47:02.63)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.294 |  0.137 |                   48 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.278 |  0.154 |                   49 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13811460137367249
[2m[36m(func pid=106295)[0m mae:  0.09247631579637527
[2m[36m(func pid=106295)[0m rmse_per_class: [0.079, 0.217, 0.045, 0.27, 0.068, 0.154, 0.219, 0.095, 0.132, 0.102]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2528 | Steps: 4 | Val loss: 0.2757 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2958 | Steps: 4 | Val loss: 0.2549 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 20:36:50 (running for 00:47:08.27)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.298 |  0.138 |                   49 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.154 |                   50 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15399421751499176
[2m[36m(func pid=106828)[0m mae:  0.09428147971630096
[2m[36m(func pid=106828)[0m rmse_per_class: [0.08, 0.244, 0.03, 0.289, 0.101, 0.168, 0.237, 0.13, 0.178, 0.083]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13560576736927032
[2m[36m(func pid=106295)[0m mae:  0.09083744138479233
[2m[36m(func pid=106295)[0m rmse_per_class: [0.076, 0.214, 0.044, 0.266, 0.067, 0.153, 0.22, 0.093, 0.128, 0.095]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2457 | Steps: 4 | Val loss: 0.2646 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2916 | Steps: 4 | Val loss: 0.2556 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=106828)[0m rmse: 0.1429671049118042
[2m[36m(func pid=106828)[0m mae:  0.08752204477787018
[2m[36m(func pid=106828)[0m rmse_per_class: [0.071, 0.227, 0.024, 0.252, 0.08, 0.173, 0.237, 0.109, 0.178, 0.079]
== Status ==
Current time: 2024-01-07 20:36:56 (running for 00:47:13.91)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.296 |  0.136 |                   50 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.246 |  0.143 |                   51 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.1362885683774948
[2m[36m(func pid=106295)[0m mae:  0.0910734012722969
[2m[36m(func pid=106295)[0m rmse_per_class: [0.078, 0.214, 0.042, 0.268, 0.066, 0.153, 0.218, 0.094, 0.129, 0.1]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2556 | Steps: 4 | Val loss: 0.2595 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 20:37:01 (running for 00:47:19.51)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.292 |  0.136 |                   51 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.256 |  0.138 |                   52 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3017 | Steps: 4 | Val loss: 0.2553 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
[2m[36m(func pid=106828)[0m rmse: 0.13838331401348114
[2m[36m(func pid=106828)[0m mae:  0.08472304046154022
[2m[36m(func pid=106828)[0m rmse_per_class: [0.067, 0.232, 0.024, 0.249, 0.076, 0.164, 0.217, 0.103, 0.174, 0.077]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13556641340255737
[2m[36m(func pid=106295)[0m mae:  0.09020092338323593
[2m[36m(func pid=106295)[0m rmse_per_class: [0.078, 0.213, 0.039, 0.273, 0.065, 0.152, 0.216, 0.095, 0.125, 0.1]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2727 | Steps: 4 | Val loss: 0.2779 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:37:07 (running for 00:47:24.92)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.302 |  0.136 |                   52 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.273 |  0.154 |                   53 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1540505290031433
[2m[36m(func pid=106828)[0m mae:  0.09534601867198944
[2m[36m(func pid=106828)[0m rmse_per_class: [0.076, 0.239, 0.042, 0.293, 0.114, 0.171, 0.22, 0.104, 0.193, 0.087]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2987 | Steps: 4 | Val loss: 0.2572 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=106295)[0m rmse: 0.13745008409023285
[2m[36m(func pid=106295)[0m mae:  0.09168130904436111
[2m[36m(func pid=106295)[0m rmse_per_class: [0.081, 0.215, 0.042, 0.276, 0.067, 0.152, 0.218, 0.095, 0.131, 0.099]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2501 | Steps: 4 | Val loss: 0.2933 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 20:37:12 (running for 00:47:30.29)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.299 |  0.137 |                   53 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.25  |  0.164 |                   54 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.163908451795578
[2m[36m(func pid=106828)[0m mae:  0.09891794621944427
[2m[36m(func pid=106828)[0m rmse_per_class: [0.099, 0.226, 0.096, 0.334, 0.126, 0.17, 0.218, 0.107, 0.181, 0.083]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2965 | Steps: 4 | Val loss: 0.2611 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=106295)[0m rmse: 0.141170933842659
[2m[36m(func pid=106295)[0m mae:  0.0934331938624382
[2m[36m(func pid=106295)[0m rmse_per_class: [0.086, 0.216, 0.052, 0.287, 0.066, 0.153, 0.216, 0.101, 0.131, 0.105]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2721 | Steps: 4 | Val loss: 0.2635 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=106828)[0m rmse: 0.14131394028663635
[2m[36m(func pid=106828)[0m mae:  0.0835898146033287
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.215, 0.043, 0.281, 0.088, 0.174, 0.202, 0.105, 0.152, 0.085]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:37:18 (running for 00:47:35.76)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.297 |  0.141 |                   54 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.272 |  0.141 |                   55 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3048 | Steps: 4 | Val loss: 0.2605 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=106295)[0m rmse: 0.14141851663589478
[2m[36m(func pid=106295)[0m mae:  0.0926898866891861
[2m[36m(func pid=106295)[0m rmse_per_class: [0.087, 0.216, 0.049, 0.287, 0.065, 0.154, 0.209, 0.11, 0.126, 0.111]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2680 | Steps: 4 | Val loss: 0.2632 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:37:23 (running for 00:47:41.51)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.305 |  0.141 |                   55 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.268 |  0.139 |                   56 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.13913647830486298
[2m[36m(func pid=106828)[0m mae:  0.0803905576467514
[2m[36m(func pid=106828)[0m rmse_per_class: [0.063, 0.22, 0.039, 0.256, 0.066, 0.169, 0.206, 0.137, 0.147, 0.088]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2975 | Steps: 4 | Val loss: 0.2581 | Batch size: 32 | lr: 0.01 | Duration: 3.26s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2491 | Steps: 4 | Val loss: 0.2764 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=106295)[0m rmse: 0.13880349695682526
[2m[36m(func pid=106295)[0m mae:  0.0912029892206192
[2m[36m(func pid=106295)[0m rmse_per_class: [0.078, 0.215, 0.046, 0.283, 0.066, 0.154, 0.207, 0.101, 0.127, 0.112]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.15165457129478455
[2m[36m(func pid=106828)[0m mae:  0.08770022541284561
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.222, 0.067, 0.285, 0.058, 0.166, 0.209, 0.165, 0.185, 0.09]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:37:29 (running for 00:47:47.08)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.298 |  0.139 |                   56 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.249 |  0.152 |                   57 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2864 | Steps: 4 | Val loss: 0.2555 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2423 | Steps: 4 | Val loss: 0.2822 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=106295)[0m rmse: 0.13647766411304474
[2m[36m(func pid=106295)[0m mae:  0.09017987549304962
[2m[36m(func pid=106295)[0m rmse_per_class: [0.075, 0.212, 0.044, 0.274, 0.069, 0.152, 0.208, 0.096, 0.126, 0.108]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:37:34 (running for 00:47:52.61)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.286 |  0.136 |                   57 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.242 |  0.154 |                   58 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15352676808834076
[2m[36m(func pid=106828)[0m mae:  0.089198537170887
[2m[36m(func pid=106828)[0m rmse_per_class: [0.066, 0.217, 0.037, 0.296, 0.058, 0.155, 0.203, 0.158, 0.259, 0.087]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2855 | Steps: 4 | Val loss: 0.2548 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2474 | Steps: 4 | Val loss: 0.2932 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=106295)[0m rmse: 0.13587045669555664
[2m[36m(func pid=106295)[0m mae:  0.08963368088006973
[2m[36m(func pid=106295)[0m rmse_per_class: [0.072, 0.212, 0.036, 0.271, 0.07, 0.152, 0.21, 0.096, 0.13, 0.11]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:37:40 (running for 00:47:58.26)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.286 |  0.136 |                   58 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.247 |  0.154 |                   59 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1540285050868988
[2m[36m(func pid=106828)[0m mae:  0.09064830839633942
[2m[36m(func pid=106828)[0m rmse_per_class: [0.064, 0.248, 0.027, 0.323, 0.064, 0.155, 0.212, 0.128, 0.237, 0.083]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2780 | Steps: 4 | Val loss: 0.2540 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2535 | Steps: 4 | Val loss: 0.3004 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=106295)[0m rmse: 0.13503319025039673
[2m[36m(func pid=106295)[0m mae:  0.08896312117576599
[2m[36m(func pid=106295)[0m rmse_per_class: [0.071, 0.213, 0.038, 0.269, 0.069, 0.152, 0.213, 0.095, 0.128, 0.101]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:37:46 (running for 00:48:03.71)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.278 |  0.135 |                   59 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.157 |                   60 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15689291059970856
[2m[36m(func pid=106828)[0m mae:  0.09344391524791718
[2m[36m(func pid=106828)[0m rmse_per_class: [0.064, 0.265, 0.03, 0.334, 0.078, 0.161, 0.217, 0.118, 0.224, 0.079]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2826 | Steps: 4 | Val loss: 0.2563 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2916 | Steps: 4 | Val loss: 0.2870 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=106295)[0m rmse: 0.13711825013160706
[2m[36m(func pid=106295)[0m mae:  0.09023860096931458
[2m[36m(func pid=106295)[0m rmse_per_class: [0.074, 0.217, 0.04, 0.273, 0.071, 0.149, 0.216, 0.096, 0.131, 0.102]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.15283969044685364
[2m[36m(func pid=106828)[0m mae:  0.09197254478931427
[2m[36m(func pid=106828)[0m rmse_per_class: [0.066, 0.238, 0.035, 0.318, 0.07, 0.172, 0.227, 0.111, 0.209, 0.082]
[2m[36m(func pid=106828)[0m 
== Status ==
Current time: 2024-01-07 20:37:51 (running for 00:48:09.30)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.283 |  0.137 |                   60 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.292 |  0.153 |                   61 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2917 | Steps: 4 | Val loss: 0.2564 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2715 | Steps: 4 | Val loss: 0.2764 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=106295)[0m rmse: 0.13763123750686646
[2m[36m(func pid=106295)[0m mae:  0.08990058302879333
[2m[36m(func pid=106295)[0m rmse_per_class: [0.073, 0.218, 0.041, 0.269, 0.07, 0.15, 0.211, 0.099, 0.133, 0.111]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:37:57 (running for 00:48:14.74)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.292 |  0.138 |                   61 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.271 |  0.149 |                   62 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14877240359783173
[2m[36m(func pid=106828)[0m mae:  0.0892985612154007
[2m[36m(func pid=106828)[0m rmse_per_class: [0.077, 0.233, 0.046, 0.302, 0.072, 0.164, 0.234, 0.115, 0.167, 0.077]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2779 | Steps: 4 | Val loss: 0.2550 | Batch size: 32 | lr: 0.01 | Duration: 3.35s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2746 | Steps: 4 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=106295)[0m rmse: 0.13588398694992065
[2m[36m(func pid=106295)[0m mae:  0.08895290642976761
[2m[36m(func pid=106295)[0m rmse_per_class: [0.073, 0.213, 0.041, 0.264, 0.071, 0.151, 0.208, 0.097, 0.134, 0.107]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:38:02 (running for 00:48:20.30)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.278 |  0.136 |                   62 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.275 |  0.148 |                   63 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14846158027648926
[2m[36m(func pid=106828)[0m mae:  0.08653281629085541
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.225, 0.034, 0.272, 0.093, 0.166, 0.226, 0.176, 0.128, 0.094]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2950 | Steps: 4 | Val loss: 0.2554 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2482 | Steps: 4 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=106295)[0m rmse: 0.1360231339931488
[2m[36m(func pid=106295)[0m mae:  0.08885721117258072
[2m[36m(func pid=106295)[0m rmse_per_class: [0.072, 0.213, 0.044, 0.266, 0.069, 0.151, 0.211, 0.097, 0.129, 0.108]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:38:08 (running for 00:48:25.76)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.295 |  0.136 |                   63 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.248 |  0.145 |                   64 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14520283043384552
[2m[36m(func pid=106828)[0m mae:  0.08516259491443634
[2m[36m(func pid=106828)[0m rmse_per_class: [0.064, 0.225, 0.027, 0.279, 0.109, 0.161, 0.223, 0.146, 0.127, 0.091]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2791 | Steps: 4 | Val loss: 0.2566 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2566 | Steps: 4 | Val loss: 0.2622 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=106295)[0m rmse: 0.13727381825447083
[2m[36m(func pid=106295)[0m mae:  0.08933945000171661
[2m[36m(func pid=106295)[0m rmse_per_class: [0.073, 0.214, 0.05, 0.273, 0.074, 0.152, 0.212, 0.096, 0.123, 0.107]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:38:13 (running for 00:48:31.30)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.279 |  0.137 |                   64 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.257 |  0.143 |                   65 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14339736104011536
[2m[36m(func pid=106828)[0m mae:  0.08470018953084946
[2m[36m(func pid=106828)[0m rmse_per_class: [0.067, 0.233, 0.023, 0.266, 0.119, 0.157, 0.225, 0.123, 0.145, 0.078]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2909 | Steps: 4 | Val loss: 0.2557 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2696 | Steps: 4 | Val loss: 0.2622 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=106295)[0m rmse: 0.1361963301897049
[2m[36m(func pid=106295)[0m mae:  0.08840509504079819
[2m[36m(func pid=106295)[0m rmse_per_class: [0.072, 0.212, 0.053, 0.274, 0.075, 0.153, 0.212, 0.096, 0.117, 0.099]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:38:19 (running for 00:48:36.98)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.291 |  0.136 |                   65 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.27  |  0.145 |                   66 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14489372074604034
[2m[36m(func pid=106828)[0m mae:  0.08666824549436569
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.227, 0.025, 0.26, 0.117, 0.157, 0.234, 0.13, 0.142, 0.088]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2756 | Steps: 4 | Val loss: 0.2532 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2509 | Steps: 4 | Val loss: 0.2605 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=106295)[0m rmse: 0.13450074195861816
[2m[36m(func pid=106295)[0m mae:  0.08704336732625961
[2m[36m(func pid=106295)[0m rmse_per_class: [0.069, 0.211, 0.048, 0.267, 0.076, 0.15, 0.211, 0.097, 0.115, 0.102]
[2m[36m(func pid=106295)[0m 
== Status ==
Current time: 2024-01-07 20:38:24 (running for 00:48:42.35)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.276 |  0.135 |                   66 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.251 |  0.142 |                   67 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14200778305530548
[2m[36m(func pid=106828)[0m mae:  0.08692523837089539
[2m[36m(func pid=106828)[0m rmse_per_class: [0.075, 0.229, 0.027, 0.27, 0.089, 0.162, 0.233, 0.113, 0.136, 0.085]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2867 | Steps: 4 | Val loss: 0.2530 | Batch size: 32 | lr: 0.01 | Duration: 3.34s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2459 | Steps: 4 | Val loss: 0.2629 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 20:38:29 (running for 00:48:47.36)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.276 |  0.135 |                   66 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.251 |  0.142 |                   67 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13430669903755188
[2m[36m(func pid=106295)[0m mae:  0.08679626882076263
[2m[36m(func pid=106295)[0m rmse_per_class: [0.071, 0.209, 0.051, 0.268, 0.072, 0.151, 0.205, 0.096, 0.115, 0.104]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.14318716526031494
[2m[36m(func pid=106828)[0m mae:  0.08812837302684784
[2m[36m(func pid=106828)[0m rmse_per_class: [0.085, 0.244, 0.031, 0.284, 0.085, 0.158, 0.229, 0.103, 0.128, 0.084]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2917 | Steps: 4 | Val loss: 0.2545 | Batch size: 32 | lr: 0.01 | Duration: 3.33s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2615 | Steps: 4 | Val loss: 0.2632 | Batch size: 32 | lr: 0.1 | Duration: 3.30s
== Status ==
Current time: 2024-01-07 20:38:35 (running for 00:48:53.02)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.287 |  0.134 |                   67 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.246 |  0.143 |                   68 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1363430917263031
[2m[36m(func pid=106295)[0m mae:  0.08857659250497818
[2m[36m(func pid=106295)[0m rmse_per_class: [0.072, 0.206, 0.052, 0.265, 0.076, 0.161, 0.213, 0.099, 0.118, 0.103]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.14439885318279266
[2m[36m(func pid=106828)[0m mae:  0.08837153017520905
[2m[36m(func pid=106828)[0m rmse_per_class: [0.075, 0.246, 0.044, 0.283, 0.09, 0.153, 0.226, 0.127, 0.122, 0.078]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2734 | Steps: 4 | Val loss: 0.2543 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2377 | Steps: 4 | Val loss: 0.2689 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 20:38:41 (running for 00:48:58.84)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.292 |  0.136 |                   68 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.262 |  0.144 |                   69 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13640207052230835
[2m[36m(func pid=106295)[0m mae:  0.08871198445558548
[2m[36m(func pid=106295)[0m rmse_per_class: [0.075, 0.209, 0.052, 0.266, 0.073, 0.156, 0.211, 0.098, 0.126, 0.099]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m rmse: 0.14913561940193176
[2m[36m(func pid=106828)[0m mae:  0.0905071496963501
[2m[36m(func pid=106828)[0m rmse_per_class: [0.07, 0.239, 0.058, 0.289, 0.09, 0.155, 0.222, 0.167, 0.131, 0.072]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2566 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2806 | Steps: 4 | Val loss: 0.2548 | Batch size: 32 | lr: 0.01 | Duration: 3.35s
== Status ==
Current time: 2024-01-07 20:38:46 (running for 00:49:04.49)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.273 |  0.136 |                   69 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.238 |  0.149 |                   70 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.14976873993873596
[2m[36m(func pid=106828)[0m mae:  0.09152325987815857
[2m[36m(func pid=106828)[0m rmse_per_class: [0.071, 0.232, 0.067, 0.292, 0.089, 0.158, 0.221, 0.136, 0.161, 0.07]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13708634674549103
[2m[36m(func pid=106295)[0m mae:  0.08912207186222076
[2m[36m(func pid=106295)[0m rmse_per_class: [0.078, 0.205, 0.052, 0.268, 0.075, 0.152, 0.216, 0.097, 0.13, 0.098]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2614 | Steps: 4 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2810 | Steps: 4 | Val loss: 0.2546 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 20:38:52 (running for 00:49:10.10)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.281 |  0.137 |                   70 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.257 |  0.15  |                   71 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.15644358098506927
[2m[36m(func pid=106828)[0m mae:  0.0959865003824234
[2m[36m(func pid=106828)[0m rmse_per_class: [0.069, 0.225, 0.06, 0.294, 0.094, 0.166, 0.236, 0.129, 0.158, 0.134]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.13765618205070496
[2m[36m(func pid=106295)[0m mae:  0.08868773281574249
[2m[36m(func pid=106295)[0m rmse_per_class: [0.077, 0.206, 0.051, 0.263, 0.071, 0.152, 0.213, 0.103, 0.138, 0.103]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2760 | Steps: 4 | Val loss: 0.2883 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2765 | Steps: 4 | Val loss: 0.2522 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 20:38:57 (running for 00:49:15.37)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.281 |  0.138 |                   71 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.276 |  0.162 |                   73 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1617347151041031
[2m[36m(func pid=106828)[0m mae:  0.100407674908638
[2m[36m(func pid=106828)[0m rmse_per_class: [0.072, 0.225, 0.074, 0.31, 0.109, 0.178, 0.253, 0.124, 0.152, 0.121]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.1349256932735443
[2m[36m(func pid=106295)[0m mae:  0.08681488037109375
[2m[36m(func pid=106295)[0m rmse_per_class: [0.073, 0.203, 0.047, 0.258, 0.07, 0.149, 0.209, 0.1, 0.134, 0.105]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2542 | Steps: 4 | Val loss: 0.2879 | Batch size: 32 | lr: 0.1 | Duration: 3.55s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2850 | Steps: 4 | Val loss: 0.2489 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 20:39:03 (running for 00:49:21.34)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.276 |  0.135 |                   72 |
| train_84a75_00023 | RUNNING    | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.254 |  0.162 |                   74 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.16235482692718506
[2m[36m(func pid=106828)[0m mae:  0.10062700510025024
[2m[36m(func pid=106828)[0m rmse_per_class: [0.092, 0.224, 0.098, 0.307, 0.103, 0.176, 0.259, 0.125, 0.146, 0.095]
[2m[36m(func pid=106828)[0m 
[2m[36m(func pid=106295)[0m rmse: 0.1308751255273819
[2m[36m(func pid=106295)[0m mae:  0.08490587025880814
[2m[36m(func pid=106295)[0m rmse_per_class: [0.07, 0.198, 0.042, 0.255, 0.071, 0.152, 0.207, 0.096, 0.124, 0.094]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106828)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2477 | Steps: 4 | Val loss: 0.2709 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2937 | Steps: 4 | Val loss: 0.2514 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 20:39:09 (running for 00:49:26.85)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.285 |  0.131 |                   73 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106828)[0m rmse: 0.1506601870059967
[2m[36m(func pid=106828)[0m mae:  0.09294857084751129
[2m[36m(func pid=106828)[0m rmse_per_class: [0.094, 0.22, 0.074, 0.284, 0.097, 0.17, 0.238, 0.122, 0.132, 0.076]
[2m[36m(func pid=106295)[0m rmse: 0.13270814716815948
[2m[36m(func pid=106295)[0m mae:  0.08640773594379425
[2m[36m(func pid=106295)[0m rmse_per_class: [0.071, 0.197, 0.038, 0.264, 0.07, 0.162, 0.208, 0.103, 0.119, 0.096]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2885 | Steps: 4 | Val loss: 0.2537 | Batch size: 32 | lr: 0.01 | Duration: 3.30s
== Status ==
Current time: 2024-01-07 20:39:15 (running for 00:49:32.69)
Memory usage on this node: 17.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.294 |  0.133 |                   74 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13454711437225342
[2m[36m(func pid=106295)[0m mae:  0.08693763613700867
[2m[36m(func pid=106295)[0m rmse_per_class: [0.073, 0.199, 0.04, 0.272, 0.072, 0.16, 0.21, 0.101, 0.118, 0.1]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2865 | Steps: 4 | Val loss: 0.2552 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 20:39:20 (running for 00:49:38.45)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.289 |  0.135 |                   75 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1371665745973587
[2m[36m(func pid=106295)[0m mae:  0.08663924783468246
[2m[36m(func pid=106295)[0m rmse_per_class: [0.073, 0.206, 0.046, 0.269, 0.064, 0.149, 0.204, 0.12, 0.124, 0.117]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2741 | Steps: 4 | Val loss: 0.2533 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 20:39:26 (running for 00:49:44.07)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.286 |  0.137 |                   76 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13707661628723145
[2m[36m(func pid=106295)[0m mae:  0.0862695723772049
[2m[36m(func pid=106295)[0m rmse_per_class: [0.073, 0.209, 0.054, 0.258, 0.066, 0.15, 0.204, 0.124, 0.123, 0.109]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2873 | Steps: 4 | Val loss: 0.2552 | Batch size: 32 | lr: 0.01 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 20:39:32 (running for 00:49:49.95)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.274 |  0.137 |                   77 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13939891755580902
[2m[36m(func pid=106295)[0m mae:  0.08776085078716278
[2m[36m(func pid=106295)[0m rmse_per_class: [0.074, 0.209, 0.064, 0.258, 0.07, 0.152, 0.215, 0.111, 0.128, 0.113]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2837 | Steps: 4 | Val loss: 0.2551 | Batch size: 32 | lr: 0.01 | Duration: 3.33s
== Status ==
Current time: 2024-01-07 20:39:38 (running for 00:49:55.82)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.287 |  0.139 |                   78 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13849341869354248
[2m[36m(func pid=106295)[0m mae:  0.0877186506986618
[2m[36m(func pid=106295)[0m rmse_per_class: [0.077, 0.208, 0.065, 0.26, 0.072, 0.151, 0.216, 0.104, 0.126, 0.107]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2675 | Steps: 4 | Val loss: 0.2520 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 20:39:44 (running for 00:50:01.78)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.284 |  0.138 |                   79 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1352868378162384
[2m[36m(func pid=106295)[0m mae:  0.08572658896446228
[2m[36m(func pid=106295)[0m rmse_per_class: [0.07, 0.208, 0.056, 0.26, 0.076, 0.148, 0.21, 0.103, 0.118, 0.102]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2610 | Steps: 4 | Val loss: 0.2506 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 20:39:49 (running for 00:50:07.63)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.267 |  0.135 |                   80 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.133540540933609
[2m[36m(func pid=106295)[0m mae:  0.08511051535606384
[2m[36m(func pid=106295)[0m rmse_per_class: [0.068, 0.206, 0.049, 0.261, 0.078, 0.149, 0.208, 0.1, 0.118, 0.098]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2907 | Steps: 4 | Val loss: 0.2515 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 20:39:55 (running for 00:50:13.42)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.261 |  0.134 |                   81 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13387827575206757
[2m[36m(func pid=106295)[0m mae:  0.08528372645378113
[2m[36m(func pid=106295)[0m rmse_per_class: [0.066, 0.205, 0.047, 0.267, 0.075, 0.149, 0.198, 0.1, 0.129, 0.102]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2842 | Steps: 4 | Val loss: 0.2596 | Batch size: 32 | lr: 0.01 | Duration: 3.36s
== Status ==
Current time: 2024-01-07 20:40:01 (running for 00:50:19.38)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.291 |  0.134 |                   82 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.14037823677062988
[2m[36m(func pid=106295)[0m mae:  0.09037386626005173
[2m[36m(func pid=106295)[0m rmse_per_class: [0.067, 0.205, 0.04, 0.274, 0.073, 0.154, 0.202, 0.109, 0.166, 0.113]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2879 | Steps: 4 | Val loss: 0.2628 | Batch size: 32 | lr: 0.01 | Duration: 3.45s
== Status ==
Current time: 2024-01-07 20:40:07 (running for 00:50:25.37)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.284 |  0.14  |                   83 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1425418108701706
[2m[36m(func pid=106295)[0m mae:  0.09142819046974182
[2m[36m(func pid=106295)[0m rmse_per_class: [0.068, 0.205, 0.035, 0.275, 0.068, 0.164, 0.203, 0.113, 0.186, 0.108]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2809 | Steps: 4 | Val loss: 0.2593 | Batch size: 32 | lr: 0.01 | Duration: 3.54s
== Status ==
Current time: 2024-01-07 20:40:13 (running for 00:50:31.45)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.288 |  0.143 |                   84 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1402740329504013
[2m[36m(func pid=106295)[0m mae:  0.08905763179063797
[2m[36m(func pid=106295)[0m rmse_per_class: [0.067, 0.205, 0.032, 0.278, 0.076, 0.161, 0.208, 0.112, 0.16, 0.103]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2760 | Steps: 4 | Val loss: 0.2546 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 20:40:19 (running for 00:50:37.50)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.281 |  0.14  |                   85 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1360541135072708
[2m[36m(func pid=106295)[0m mae:  0.08622565120458603
[2m[36m(func pid=106295)[0m rmse_per_class: [0.068, 0.204, 0.041, 0.275, 0.075, 0.152, 0.204, 0.113, 0.13, 0.1]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2764 | Steps: 4 | Val loss: 0.2523 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 20:40:25 (running for 00:50:43.11)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.276 |  0.136 |                   86 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13346554338932037
[2m[36m(func pid=106295)[0m mae:  0.08520369231700897
[2m[36m(func pid=106295)[0m rmse_per_class: [0.071, 0.201, 0.038, 0.272, 0.077, 0.15, 0.205, 0.102, 0.128, 0.09]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2790 | Steps: 4 | Val loss: 0.2514 | Batch size: 32 | lr: 0.01 | Duration: 3.34s
== Status ==
Current time: 2024-01-07 20:40:31 (running for 00:50:49.09)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.276 |  0.133 |                   87 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13171494007110596
[2m[36m(func pid=106295)[0m mae:  0.08512529730796814
[2m[36m(func pid=106295)[0m rmse_per_class: [0.071, 0.2, 0.029, 0.272, 0.077, 0.154, 0.207, 0.098, 0.123, 0.087]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2766 | Steps: 4 | Val loss: 0.2494 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 20:40:37 (running for 00:50:55.06)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.279 |  0.132 |                   88 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13080012798309326
[2m[36m(func pid=106295)[0m mae:  0.08389696478843689
[2m[36m(func pid=106295)[0m rmse_per_class: [0.069, 0.202, 0.034, 0.265, 0.073, 0.15, 0.201, 0.097, 0.129, 0.088]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3037 | Steps: 4 | Val loss: 0.2506 | Batch size: 32 | lr: 0.01 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 20:40:43 (running for 00:51:00.77)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.277 |  0.131 |                   89 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13407287001609802
[2m[36m(func pid=106295)[0m mae:  0.08459137380123138
[2m[36m(func pid=106295)[0m rmse_per_class: [0.064, 0.213, 0.039, 0.264, 0.077, 0.146, 0.2, 0.109, 0.132, 0.097]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2738 | Steps: 4 | Val loss: 0.2524 | Batch size: 32 | lr: 0.01 | Duration: 3.52s
== Status ==
Current time: 2024-01-07 20:40:48 (running for 00:51:06.62)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.304 |  0.134 |                   90 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13599756360054016
[2m[36m(func pid=106295)[0m mae:  0.08570920675992966
[2m[36m(func pid=106295)[0m rmse_per_class: [0.066, 0.212, 0.044, 0.265, 0.076, 0.151, 0.203, 0.105, 0.138, 0.1]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2762 | Steps: 4 | Val loss: 0.2504 | Batch size: 32 | lr: 0.01 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 20:40:55 (running for 00:51:13.03)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.274 |  0.136 |                   91 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13398024439811707
[2m[36m(func pid=106295)[0m mae:  0.08472675830125809
[2m[36m(func pid=106295)[0m rmse_per_class: [0.064, 0.209, 0.046, 0.255, 0.073, 0.156, 0.207, 0.101, 0.133, 0.097]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2705 | Steps: 4 | Val loss: 0.2503 | Batch size: 32 | lr: 0.01 | Duration: 3.37s
== Status ==
Current time: 2024-01-07 20:41:00 (running for 00:51:18.53)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.276 |  0.134 |                   92 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1336548626422882
[2m[36m(func pid=106295)[0m mae:  0.08462165296077728
[2m[36m(func pid=106295)[0m rmse_per_class: [0.064, 0.205, 0.045, 0.255, 0.077, 0.159, 0.212, 0.099, 0.125, 0.095]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2737 | Steps: 4 | Val loss: 0.2498 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 20:41:06 (running for 00:51:24.45)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.271 |  0.134 |                   93 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13268432021141052
[2m[36m(func pid=106295)[0m mae:  0.08391670137643814
[2m[36m(func pid=106295)[0m rmse_per_class: [0.066, 0.202, 0.044, 0.256, 0.079, 0.157, 0.212, 0.099, 0.118, 0.094]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2641 | Steps: 4 | Val loss: 0.2496 | Batch size: 32 | lr: 0.01 | Duration: 3.42s
== Status ==
Current time: 2024-01-07 20:41:12 (running for 00:51:30.02)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.274 |  0.133 |                   94 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13243621587753296
[2m[36m(func pid=106295)[0m mae:  0.08333431929349899
[2m[36m(func pid=106295)[0m rmse_per_class: [0.067, 0.203, 0.048, 0.255, 0.077, 0.149, 0.212, 0.104, 0.112, 0.097]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2513 | Steps: 4 | Val loss: 0.2495 | Batch size: 32 | lr: 0.01 | Duration: 3.51s
== Status ==
Current time: 2024-01-07 20:41:18 (running for 00:51:36.15)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.264 |  0.132 |                   95 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.1323705017566681
[2m[36m(func pid=106295)[0m mae:  0.08353245258331299
[2m[36m(func pid=106295)[0m rmse_per_class: [0.071, 0.204, 0.053, 0.254, 0.075, 0.146, 0.209, 0.104, 0.114, 0.095]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2634 | Steps: 4 | Val loss: 0.2499 | Batch size: 32 | lr: 0.01 | Duration: 3.40s
== Status ==
Current time: 2024-01-07 20:41:24 (running for 00:51:42.25)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.251 |  0.132 |                   96 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13248366117477417
[2m[36m(func pid=106295)[0m mae:  0.0838891938328743
[2m[36m(func pid=106295)[0m rmse_per_class: [0.072, 0.206, 0.053, 0.253, 0.076, 0.147, 0.206, 0.099, 0.119, 0.095]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2570 | Steps: 4 | Val loss: 0.2529 | Batch size: 32 | lr: 0.01 | Duration: 3.33s
== Status ==
Current time: 2024-01-07 20:41:30 (running for 00:51:48.20)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.263 |  0.132 |                   97 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13518235087394714
[2m[36m(func pid=106295)[0m mae:  0.08579392731189728
[2m[36m(func pid=106295)[0m rmse_per_class: [0.074, 0.205, 0.056, 0.257, 0.076, 0.152, 0.206, 0.097, 0.127, 0.101]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3098 | Steps: 4 | Val loss: 0.2529 | Batch size: 32 | lr: 0.01 | Duration: 3.34s
== Status ==
Current time: 2024-01-07 20:41:36 (running for 00:51:54.00)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.257 |  0.135 |                   98 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=106295)[0m rmse: 0.13559436798095703
[2m[36m(func pid=106295)[0m mae:  0.08586879819631577
[2m[36m(func pid=106295)[0m rmse_per_class: [0.069, 0.206, 0.051, 0.259, 0.077, 0.15, 0.207, 0.101, 0.127, 0.11]
[2m[36m(func pid=106295)[0m 
[2m[36m(func pid=106295)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2681 | Steps: 4 | Val loss: 0.2572 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 20:41:42 (running for 00:51:59.99)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00022 | RUNNING    | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.31  |  0.136 |                   99 |
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 20:41:43 (running for 00:52:00.69)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/119.98 GiB heap, 0.0/55.41 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_84a75_00000 | TERMINATED | 192.168.7.53:182810 | 0.0001 |       0.99 |         0      |  0.393 |  0.167 |                   75 |
| train_84a75_00001 | TERMINATED | 192.168.7.53:183191 | 0.001  |       0.99 |         0      |  0.302 |  0.14  |                  100 |
| train_84a75_00002 | TERMINATED | 192.168.7.53:183612 | 0.01   |       0.99 |         0      |  0.25  |  0.145 |                  100 |
| train_84a75_00003 | TERMINATED | 192.168.7.53:184036 | 0.1    |       0.99 |         0      |  0.314 |  0.159 |                   75 |
| train_84a75_00004 | TERMINATED | 192.168.7.53:14018  | 0.0001 |       0.9  |         0      |  0.633 |  0.179 |                   75 |
| train_84a75_00005 | TERMINATED | 192.168.7.53:14601  | 0.001  |       0.9  |         0      |  0.374 |  0.162 |                   75 |
| train_84a75_00006 | TERMINATED | 192.168.7.53:19964  | 0.01   |       0.9  |         0      |  0.258 |  0.131 |                  100 |
| train_84a75_00007 | TERMINATED | 192.168.7.53:20486  | 0.1    |       0.9  |         0      |  0.247 |  0.16  |                  100 |
| train_84a75_00008 | TERMINATED | 192.168.7.53:32639  | 0.0001 |       0.99 |         0.0001 |  0.401 |  0.166 |                   75 |
| train_84a75_00009 | TERMINATED | 192.168.7.53:33359  | 0.001  |       0.99 |         0.0001 |  0.324 |  0.146 |                   75 |
| train_84a75_00010 | TERMINATED | 192.168.7.53:44519  | 0.01   |       0.99 |         0.0001 |  0.275 |  0.16  |                   75 |
| train_84a75_00011 | TERMINATED | 192.168.7.53:45122  | 0.1    |       0.99 |         0.0001 |  0.306 |  0.148 |                   75 |
| train_84a75_00012 | TERMINATED | 192.168.7.53:51559  | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.179 |                   75 |
| train_84a75_00013 | TERMINATED | 192.168.7.53:52139  | 0.001  |       0.9  |         0.0001 |  0.374 |  0.164 |                   75 |
| train_84a75_00014 | TERMINATED | 192.168.7.53:63038  | 0.01   |       0.9  |         0.0001 |  0.265 |  0.133 |                  100 |
| train_84a75_00015 | TERMINATED | 192.168.7.53:63128  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.142 |                  100 |
| train_84a75_00016 | TERMINATED | 192.168.7.53:70360  | 0.0001 |       0.99 |         1e-05  |  0.396 |  0.167 |                   75 |
| train_84a75_00017 | TERMINATED | 192.168.7.53:71112  | 0.001  |       0.99 |         1e-05  |  0.329 |  0.148 |                   75 |
| train_84a75_00018 | TERMINATED | 192.168.7.53:87283  | 0.01   |       0.99 |         1e-05  |  0.29  |  0.178 |                   75 |
| train_84a75_00019 | TERMINATED | 192.168.7.53:88115  | 0.1    |       0.99 |         1e-05  |  0.337 |  0.168 |                   75 |
| train_84a75_00020 | TERMINATED | 192.168.7.53:88955  | 0.0001 |       0.9  |         1e-05  |  0.632 |  0.179 |                   75 |
| train_84a75_00021 | TERMINATED | 192.168.7.53:89738  | 0.001  |       0.9  |         1e-05  |  0.377 |  0.161 |                   75 |
| train_84a75_00022 | TERMINATED | 192.168.7.53:106295 | 0.01   |       0.9  |         1e-05  |  0.268 |  0.14  |                  100 |
| train_84a75_00023 | TERMINATED | 192.168.7.53:106828 | 0.1    |       0.9  |         1e-05  |  0.248 |  0.151 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+


2024-01-07 20:41:43,009	INFO tune.py:798 -- Total run time: 3121.76 seconds (3120.67 seconds for the tuning loop).
[2m[36m(func pid=106295)[0m rmse: 0.1397199183702469
[2m[36m(func pid=106295)[0m mae:  0.08864381164312363
[2m[36m(func pid=106295)[0m rmse_per_class: [0.076, 0.205, 0.059, 0.263, 0.069, 0.157, 0.207, 0.102, 0.138, 0.122]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341356.1 ON aap04 CANCELLED AT 2024-01-07T20:41:50 ***
srun: error: aap04: task 0: Exited with exit code 1

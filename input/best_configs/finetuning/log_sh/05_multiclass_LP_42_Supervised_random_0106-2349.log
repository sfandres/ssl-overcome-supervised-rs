IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-06 23:52:04,907	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-06 23:52:04,908	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-06 23:52:07,438	SUCC scripts.py:747 -- --------------------
2024-01-06 23:52:07,438	SUCC scripts.py:748 -- Ray runtime started.
2024-01-06 23:52:07,438	SUCC scripts.py:749 -- --------------------
2024-01-06 23:52:07,438	INFO scripts.py:751 -- Next steps
2024-01-06 23:52:07,439	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-06 23:52:07,439	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-06 23:52:07,439	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-06 23:52:07,439	INFO scripts.py:773 -- import ray
2024-01-06 23:52:07,439	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-06 23:52:07,439	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-06 23:52:07,439	INFO scripts.py:791 --   ray status
2024-01-06 23:52:07,439	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-06 23:52:07,439	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-06 23:52:07,439	INFO scripts.py:810 --   ray stop
2024-01-06 23:52:07,440	INFO scripts.py:891 -- --block
2024-01-06 23:52:07,440	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-06 23:52:07,440	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              10535778911845952319
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f9b85d93160>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [5 5 5 5 5 5 5 5 5 5]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.93
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [5 5 5 5 5 5 5 5 5 5]
Done!

Supervised model resnet18 with random weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-06 23:52:50,066	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-06 23:52:50,076	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-06 23:53:13,559	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-06 23:53:13 (running for 00:00:22.64)
Memory usage on this node: 11.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |
| train_51d3e_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_51d3e_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_51d3e_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68418)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=68418)[0m Configuration completed!
[2m[36m(func pid=68418)[0m New optimizer parameters:
[2m[36m(func pid=68418)[0m SGD (
[2m[36m(func pid=68418)[0m Parameter Group 0
[2m[36m(func pid=68418)[0m     dampening: 0
[2m[36m(func pid=68418)[0m     differentiable: False
[2m[36m(func pid=68418)[0m     foreach: None
[2m[36m(func pid=68418)[0m     lr: 0.0001
[2m[36m(func pid=68418)[0m     maximize: False
[2m[36m(func pid=68418)[0m     momentum: 0.99
[2m[36m(func pid=68418)[0m     nesterov: False
[2m[36m(func pid=68418)[0m     weight_decay: 0
[2m[36m(func pid=68418)[0m )
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:53:29 (running for 00:00:38.66)
Memory usage on this node: 13.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |
| train_51d3e_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_51d3e_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68792)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=68792)[0m Configuration completed!
[2m[36m(func pid=68792)[0m New optimizer parameters:
[2m[36m(func pid=68792)[0m SGD (
[2m[36m(func pid=68792)[0m Parameter Group 0
[2m[36m(func pid=68792)[0m     dampening: 0
[2m[36m(func pid=68792)[0m     differentiable: False
[2m[36m(func pid=68792)[0m     foreach: None
[2m[36m(func pid=68792)[0m     lr: 0.001
[2m[36m(func pid=68792)[0m     maximize: False
[2m[36m(func pid=68792)[0m     momentum: 0.99
[2m[36m(func pid=68792)[0m     nesterov: False
[2m[36m(func pid=68792)[0m     weight_decay: 0
[2m[36m(func pid=68792)[0m )
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0808 | Steps: 2 | Val loss: 2.3707 | Batch size: 32 | lr: 0.001 | Duration: 6.17s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1603 | Steps: 2 | Val loss: 2.3717 | Batch size: 32 | lr: 0.0001 | Duration: 15.43s
== Status ==
Current time: 2024-01-06 23:53:38 (running for 00:00:47.63)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |
| train_51d3e_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.06669776119402986
[2m[36m(func pid=68418)[0m top5: 0.38526119402985076
[2m[36m(func pid=68418)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=68418)[0m f1_macro: 0.017161023453840043
[2m[36m(func pid=68418)[0m f1_weighted: 0.021400963612654256
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.005, 0.0, 0.049, 0.0, 0.0, 0.0, 0.117, 0.0, 0.0]
[2m[36m(func pid=68792)[0m top1: 0.07136194029850747
[2m[36m(func pid=68792)[0m top5: 0.3199626865671642
[2m[36m(func pid=68792)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=68792)[0m f1_macro: 0.020054756895767016
[2m[36m(func pid=68792)[0m f1_weighted: 0.0284427302708785
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.005, 0.0, 0.073, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0]
[2m[36m(func pid=69189)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69189)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=69189)[0m Configuration completed!
[2m[36m(func pid=69189)[0m New optimizer parameters:
[2m[36m(func pid=69189)[0m SGD (
[2m[36m(func pid=69189)[0m Parameter Group 0
[2m[36m(func pid=69189)[0m     dampening: 0
[2m[36m(func pid=69189)[0m     differentiable: False
[2m[36m(func pid=69189)[0m     foreach: None
[2m[36m(func pid=69189)[0m     lr: 0.01
[2m[36m(func pid=69189)[0m     maximize: False
[2m[36m(func pid=69189)[0m     momentum: 0.99
[2m[36m(func pid=69189)[0m     nesterov: False
[2m[36m(func pid=69189)[0m     weight_decay: 0
[2m[36m(func pid=69189)[0m )
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0361 | Steps: 2 | Val loss: 2.4062 | Batch size: 32 | lr: 0.01 | Duration: 4.06s
[2m[36m(func pid=69189)[0m top1: 0.006063432835820896
[2m[36m(func pid=69189)[0m top5: 0.3871268656716418
[2m[36m(func pid=69189)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=69189)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=69189)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-06 23:53:47 (running for 00:00:56.21)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=69627)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=69627)[0m Configuration completed!
[2m[36m(func pid=69627)[0m New optimizer parameters:
[2m[36m(func pid=69627)[0m SGD (
[2m[36m(func pid=69627)[0m Parameter Group 0
[2m[36m(func pid=69627)[0m     dampening: 0
[2m[36m(func pid=69627)[0m     differentiable: False
[2m[36m(func pid=69627)[0m     foreach: None
[2m[36m(func pid=69627)[0m     lr: 0.1
[2m[36m(func pid=69627)[0m     maximize: False
[2m[36m(func pid=69627)[0m     momentum: 0.99
[2m[36m(func pid=69627)[0m     nesterov: False== Status ==
Current time: 2024-01-06 23:53:55 (running for 00:01:04.22)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |        |            |                      |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  3.081 |       0.02 |                    1 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |        |            |                      |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |        |            |                      |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=69627)[0m     weight_decay: 0
[2m[36m(func pid=69627)[0m )
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0190 | Steps: 2 | Val loss: 2.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8221 | Steps: 2 | Val loss: 2.3858 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0720 | Steps: 2 | Val loss: 2.3266 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.3590 | Steps: 2 | Val loss: 2.8151 | Batch size: 32 | lr: 0.1 | Duration: 4.36s
== Status ==
Current time: 2024-01-06 23:54:00 (running for 00:01:09.27)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  3.16  |      0.017 |                    1 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  3.081 |      0.02  |                    1 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  3.036 |      0.001 |                    1 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |        |            |                      |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.11520522388059702
[2m[36m(func pid=68792)[0m top5: 0.5107276119402985
[2m[36m(func pid=68792)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=68792)[0m f1_macro: 0.061555562622119045
[2m[36m(func pid=68792)[0m f1_weighted: 0.08468851827400134
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.0, 0.025, 0.228, 0.0, 0.0, 0.0, 0.363, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.006063432835820896
[2m[36m(func pid=69189)[0m top5: 0.5265858208955224
[2m[36m(func pid=69189)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=69189)[0m f1_macro: 0.0013612565445026178
[2m[36m(func pid=69189)[0m f1_weighted: 8.253887629913261e-05
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.14039179104477612
[2m[36m(func pid=68418)[0m top5: 0.5251865671641791
[2m[36m(func pid=68418)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=68418)[0m f1_macro: 0.04174423103323475
[2m[36m(func pid=68418)[0m f1_weighted: 0.07916159309782289
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.0, 0.249, 0.0, 0.0, 0.0, 0.169, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.020522388059701493
[2m[36m(func pid=69627)[0m top5: 0.6380597014925373
[2m[36m(func pid=69627)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=69627)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=69627)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=69627)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8704 | Steps: 2 | Val loss: 2.3167 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7814 | Steps: 2 | Val loss: 2.2490 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0734 | Steps: 2 | Val loss: 2.3162 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 7.4169 | Steps: 2 | Val loss: 3.1473 | Batch size: 32 | lr: 0.1 | Duration: 2.57s
== Status ==
Current time: 2024-01-06 23:54:05 (running for 00:01:14.30)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  3.072 |      0.042 |                    2 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.87  |      0.007 |                    3 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.822 |      0.001 |                    2 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  4.359 |      0.004 |                    1 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.014925373134328358
[2m[36m(func pid=68792)[0m top5: 0.5634328358208955
[2m[36m(func pid=68792)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=68792)[0m f1_macro: 0.006753465402098884
[2m[36m(func pid=68792)[0m f1_weighted: 0.01539283020472027
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.0, 0.013, 0.055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m top1: 0.18330223880597016
[2m[36m(func pid=68418)[0m top5: 0.5638992537313433
[2m[36m(func pid=68418)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=68418)[0m f1_macro: 0.04782897862232779
[2m[36m(func pid=68418)[0m f1_weighted: 0.09803045343354487
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.0, 0.318, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0]
[2m[36m(func pid=69189)[0m top1: 0.24766791044776118
[2m[36m(func pid=69189)[0m top5: 0.6380597014925373
[2m[36m(func pid=69189)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=69189)[0m f1_macro: 0.08757154077214921
[2m[36m(func pid=69189)[0m f1_weighted: 0.15102528738852491
[2m[36m(func pid=69189)[0m f1_per_class: [0.088, 0.0, 0.293, 0.0, 0.0, 0.0, 0.495, 0.0, 0.0, 0.0]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.11893656716417911
[2m[36m(func pid=69627)[0m top5: 0.7322761194029851
[2m[36m(func pid=69627)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=69627)[0m f1_macro: 0.034790264441257206
[2m[36m(func pid=69627)[0m f1_weighted: 0.032679915284445714
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.261, 0.0, 0.0, 0.087, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7779 | Steps: 2 | Val loss: 2.3168 | Batch size: 32 | lr: 0.001 | Duration: 2.54s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6853 | Steps: 2 | Val loss: 2.1061 | Batch size: 32 | lr: 0.01 | Duration: 2.53s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0610 | Steps: 2 | Val loss: 2.3147 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 13.5822 | Steps: 2 | Val loss: 5.1628 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=68792)[0m top1: 0.006063432835820896
[2m[36m(func pid=68792)[0m top5: 0.5541044776119403
[2m[36m(func pid=68792)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68792)[0m f1_macro: 0.0012064965197215777
[2m[36m(func pid=68792)[0m f1_weighted: 7.315510613983447e-05
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-06 23:54:10 (running for 00:01:19.52)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  3.073 |      0.048 |                    3 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.778 |      0.001 |                    4 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.685 |      0.107 |                    4 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  7.417 |      0.035 |                    2 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.31716417910447764
[2m[36m(func pid=69189)[0m top5: 0.8736007462686567
[2m[36m(func pid=69189)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=69189)[0m f1_macro: 0.10745118938102174
[2m[36m(func pid=69189)[0m f1_weighted: 0.18420838446234095
[2m[36m(func pid=69189)[0m f1_per_class: [0.044, 0.151, 0.296, 0.0, 0.0, 0.099, 0.484, 0.0, 0.0, 0.0]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.19496268656716417
[2m[36m(func pid=68418)[0m top5: 0.5746268656716418
[2m[36m(func pid=68418)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=68418)[0m f1_macro: 0.040332712270428527
[2m[36m(func pid=68418)[0m f1_weighted: 0.10059073808492804
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.0, 0.349, 0.0, 0.0, 0.0, 0.054, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.0625
[2m[36m(func pid=69627)[0m top5: 0.7635261194029851
[2m[36m(func pid=69627)[0m f1_micro: 0.0625
[2m[36m(func pid=69627)[0m f1_macro: 0.0430216258802557
[2m[36m(func pid=69627)[0m f1_weighted: 0.008980866155404173
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.0, 0.264, 0.0, 0.0, 0.0, 0.0, 0.118, 0.0, 0.048]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7337 | Steps: 2 | Val loss: 2.3157 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.4201 | Steps: 2 | Val loss: 2.0685 | Batch size: 32 | lr: 0.01 | Duration: 2.49s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0078 | Steps: 2 | Val loss: 2.3172 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 20.6011 | Steps: 2 | Val loss: 6.4667 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=68792)[0m top1: 0.006063432835820896
[2m[36m(func pid=68792)[0m top5: 0.5634328358208955
[2m[36m(func pid=68792)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68792)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=68792)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.300839552238806
[2m[36m(func pid=69189)[0m top5: 0.7877798507462687
[2m[36m(func pid=69189)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=69189)[0m f1_macro: 0.09182466757197086
[2m[36m(func pid=69189)[0m f1_weighted: 0.16790869315574988
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.066, 0.0, 0.448, 0.0, 0.261, 0.0, 0.0, 0.0, 0.143]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:54:16 (running for 00:01:25.13)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  3.008 |      0.052 |                    5 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.734 |      0.001 |                    5 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.42  |      0.092 |                    5 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 13.582 |      0.043 |                    3 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.20848880597014927
[2m[36m(func pid=68418)[0m top5: 0.574160447761194
[2m[36m(func pid=68418)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=68418)[0m f1_macro: 0.05197119462005343
[2m[36m(func pid=68418)[0m f1_weighted: 0.10439397096375855
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.118, 0.364, 0.0, 0.0, 0.0, 0.038, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.300839552238806
[2m[36m(func pid=69627)[0m top5: 0.7639925373134329
[2m[36m(func pid=69627)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=69627)[0m f1_macro: 0.08156493589470545
[2m[36m(func pid=69627)[0m f1_weighted: 0.14004606525027033
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.0, 0.353, 0.0, 0.0, 0.0, 0.463, 0.0, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7588 | Steps: 2 | Val loss: 2.3041 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.4874 | Steps: 2 | Val loss: 2.2139 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9892 | Steps: 2 | Val loss: 2.3218 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 28.1983 | Steps: 2 | Val loss: 10.7923 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=68792)[0m top1: 0.006063432835820896
[2m[36m(func pid=68792)[0m top5: 0.6315298507462687
[2m[36m(func pid=68792)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68792)[0m f1_macro: 0.0012064965197215777
[2m[36m(func pid=68792)[0m f1_weighted: 7.315510613983447e-05
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.06902985074626866
[2m[36m(func pid=69189)[0m top5: 0.5083955223880597
[2m[36m(func pid=69189)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=69189)[0m f1_macro: 0.05268429618983237
[2m[36m(func pid=69189)[0m f1_weighted: 0.08382472962098411
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.041, 0.0, 0.254, 0.125, 0.0, 0.0, 0.082, 0.0, 0.025]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:54:21 (running for 00:01:30.21)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.989 |      0.051 |                    6 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.759 |      0.001 |                    6 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.487 |      0.053 |                    6 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 20.601 |      0.082 |                    4 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.2196828358208955
[2m[36m(func pid=68418)[0m top5: 0.5732276119402985
[2m[36m(func pid=68418)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=68418)[0m f1_macro: 0.05139045988925626
[2m[36m(func pid=68418)[0m f1_weighted: 0.10946569654582478
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.07, 0.384, 0.0, 0.0, 0.0, 0.029, 0.0, 0.031]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.11520522388059702
[2m[36m(func pid=69627)[0m top5: 0.784981343283582
[2m[36m(func pid=69627)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=69627)[0m f1_macro: 0.036614219604543544
[2m[36m(func pid=69627)[0m f1_weighted: 0.0245265102757144
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.0, 0.16, 0.0, 0.0, 0.206, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8760 | Steps: 2 | Val loss: 2.2808 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3697 | Steps: 2 | Val loss: 2.4360 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9356 | Steps: 2 | Val loss: 2.3267 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 21.2767 | Steps: 2 | Val loss: 11.3224 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=68792)[0m top1: 0.015391791044776119
[2m[36m(func pid=68792)[0m top5: 0.7038246268656716
[2m[36m(func pid=68792)[0m f1_micro: 0.015391791044776119
[2m[36m(func pid=68792)[0m f1_macro: 0.009826047582401265
[2m[36m(func pid=68792)[0m f1_weighted: 0.010780123562599985
[2m[36m(func pid=68792)[0m f1_per_class: [0.049, 0.0, 0.015, 0.035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.02751865671641791
[2m[36m(func pid=69189)[0m top5: 0.4771455223880597
[2m[36m(func pid=69189)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=69189)[0m f1_macro: 0.02913463998646641
[2m[36m(func pid=69189)[0m f1_weighted: 0.010837557963157952
[2m[36m(func pid=69189)[0m f1_per_class: [0.075, 0.021, 0.0, 0.0, 0.083, 0.0, 0.0, 0.083, 0.0, 0.03]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.20988805970149255
[2m[36m(func pid=68418)[0m top5: 0.5638992537313433
[2m[36m(func pid=68418)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=68418)[0m f1_macro: 0.04758490491829069
[2m[36m(func pid=68418)[0m f1_weighted: 0.11374565529024364
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.043, 0.4, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0]
== Status ==
Current time: 2024-01-06 23:54:26 (running for 00:01:35.51)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.936 |      0.048 |                    7 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.876 |      0.01  |                    7 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.37  |      0.029 |                    7 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 28.198 |      0.037 |                    5 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.03591417910447761
[2m[36m(func pid=69627)[0m top5: 0.42257462686567165
[2m[36m(func pid=69627)[0m f1_micro: 0.03591417910447761
[2m[36m(func pid=69627)[0m f1_macro: 0.021567128557944563
[2m[36m(func pid=69627)[0m f1_weighted: 0.005219460760989034
[2m[36m(func pid=69627)[0m f1_per_class: [0.153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7306 | Steps: 2 | Val loss: 2.2385 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2616 | Steps: 2 | Val loss: 2.4960 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8833 | Steps: 2 | Val loss: 2.3327 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 24.9577 | Steps: 2 | Val loss: 12.7071 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=68792)[0m top1: 0.10167910447761194
[2m[36m(func pid=68792)[0m top5: 0.71875
[2m[36m(func pid=68792)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.05645909353542222
[2m[36m(func pid=68792)[0m f1_weighted: 0.11357183335488519
[2m[36m(func pid=68792)[0m f1_per_class: [0.058, 0.034, 0.109, 0.129, 0.0, 0.0, 0.235, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.03917910447761194
[2m[36m(func pid=69189)[0m top5: 0.5018656716417911
[2m[36m(func pid=69189)[0m f1_micro: 0.03917910447761194
[2m[36m(func pid=69189)[0m f1_macro: 0.04430230289435189
[2m[36m(func pid=69189)[0m f1_weighted: 0.02440148857355359
[2m[36m(func pid=69189)[0m f1_per_class: [0.149, 0.0, 0.0, 0.0, 0.022, 0.143, 0.0, 0.071, 0.0, 0.057]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:54:31 (running for 00:01:40.67)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.883 |      0.041 |                    8 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.731 |      0.056 |                    8 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.262 |      0.044 |                    8 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 21.277 |      0.022 |                    6 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.15345149253731344
[2m[36m(func pid=68418)[0m top5: 0.5485074626865671
[2m[36m(func pid=68418)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=68418)[0m f1_macro: 0.04135235160683253
[2m[36m(func pid=68418)[0m f1_weighted: 0.1058975543761544
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.023, 0.376, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.06529850746268656
[2m[36m(func pid=69627)[0m top5: 0.310634328358209
[2m[36m(func pid=69627)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=69627)[0m f1_macro: 0.029501271606534762
[2m[36m(func pid=69627)[0m f1_weighted: 0.010339161556757787
[2m[36m(func pid=69627)[0m f1_per_class: [0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6355 | Steps: 2 | Val loss: 2.1937 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.1919 | Steps: 2 | Val loss: 2.4167 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8602 | Steps: 2 | Val loss: 2.3408 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 22.6498 | Steps: 2 | Val loss: 16.9912 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=68792)[0m top1: 0.23880597014925373
[2m[36m(func pid=68792)[0m top5: 0.8017723880597015
[2m[36m(func pid=68792)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=68792)[0m f1_macro: 0.10455008585294064
[2m[36m(func pid=68792)[0m f1_weighted: 0.18081073238006434
[2m[36m(func pid=68792)[0m f1_per_class: [0.082, 0.005, 0.364, 0.067, 0.0, 0.0, 0.528, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.05457089552238806
[2m[36m(func pid=69189)[0m top5: 0.5209888059701493
[2m[36m(func pid=69189)[0m f1_micro: 0.05457089552238806
[2m[36m(func pid=69189)[0m f1_macro: 0.04323781141249106
[2m[36m(func pid=69189)[0m f1_weighted: 0.035876160472845905
[2m[36m(func pid=69189)[0m f1_per_class: [0.124, 0.0, 0.0, 0.0, 0.024, 0.195, 0.03, 0.0, 0.059, 0.0]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:54:36 (running for 00:01:45.80)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.86  |      0.025 |                    9 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.636 |      0.105 |                    9 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.192 |      0.043 |                    9 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 24.958 |      0.03  |                    7 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.06669776119402986
[2m[36m(func pid=68418)[0m top5: 0.5368470149253731
[2m[36m(func pid=68418)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=68418)[0m f1_macro: 0.025107081667618502
[2m[36m(func pid=68418)[0m f1_weighted: 0.06572340458970141
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.016, 0.235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.1501865671641791
[2m[36m(func pid=69627)[0m top5: 0.8064365671641791
[2m[36m(func pid=69627)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=69627)[0m f1_macro: 0.04082835321974725
[2m[36m(func pid=69627)[0m f1_weighted: 0.06452585087487031
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6348 | Steps: 2 | Val loss: 2.1527 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0660 | Steps: 2 | Val loss: 2.2728 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7881 | Steps: 2 | Val loss: 2.3467 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 24.5325 | Steps: 2 | Val loss: 16.7979 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=68792)[0m top1: 0.27611940298507465
[2m[36m(func pid=68792)[0m top5: 0.8414179104477612
[2m[36m(func pid=68792)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=68792)[0m f1_macro: 0.06220994695129125
[2m[36m(func pid=68792)[0m f1_weighted: 0.15472894654327604
[2m[36m(func pid=68792)[0m f1_per_class: [0.11, 0.0, 0.0, 0.003, 0.0, 0.0, 0.509, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.03824626865671642
[2m[36m(func pid=69189)[0m top5: 0.5447761194029851
[2m[36m(func pid=69189)[0m f1_micro: 0.03824626865671642
[2m[36m(func pid=69189)[0m f1_macro: 0.022228109625927018
[2m[36m(func pid=69189)[0m f1_weighted: 0.03544344233217306
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.021, 0.0, 0.108, 0.0, 0.093, 0.0]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.025652985074626867
[2m[36m(func pid=68418)[0m top5: 0.5401119402985075
[2m[36m(func pid=68418)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=68418)[0m f1_macro: 0.01206014639738954
[2m[36m(func pid=68418)[0m f1_weighted: 0.030040435499984863
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.013, 0.107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:54:42 (running for 00:01:51.14)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.788 |      0.012 |                   10 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.635 |      0.062 |                   10 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.066 |      0.022 |                   10 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 24.533 |      0.048 |                    9 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m top1: 0.18563432835820895
[2m[36m(func pid=69627)[0m top5: 0.7541977611940298
[2m[36m(func pid=69627)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=69627)[0m f1_macro: 0.04814201132874143
[2m[36m(func pid=69627)[0m f1_weighted: 0.1266433174731064
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.0, 0.0, 0.041, 0.0, 0.0, 0.383, 0.0, 0.024, 0.034]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6826 | Steps: 2 | Val loss: 2.1175 | Batch size: 32 | lr: 0.001 | Duration: 2.54s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1330 | Steps: 2 | Val loss: 2.0666 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8223 | Steps: 2 | Val loss: 2.3536 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 31.9983 | Steps: 2 | Val loss: 24.3872 | Batch size: 32 | lr: 0.1 | Duration: 2.58s
[2m[36m(func pid=68792)[0m top1: 0.2887126865671642
[2m[36m(func pid=68792)[0m top5: 0.8619402985074627
[2m[36m(func pid=68792)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=68792)[0m f1_macro: 0.061973652581078745
[2m[36m(func pid=68792)[0m f1_weighted: 0.14801023975871264
[2m[36m(func pid=68792)[0m f1_per_class: [0.132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.488, 0.0, 0.0, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.16184701492537312
[2m[36m(func pid=69189)[0m top5: 0.8143656716417911
[2m[36m(func pid=69189)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=69189)[0m f1_macro: 0.11620816970004574
[2m[36m(func pid=69189)[0m f1_weighted: 0.19268089687688356
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.089, 0.308, 0.44, 0.025, 0.0, 0.168, 0.0, 0.048, 0.085]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.00792910447761194
[2m[36m(func pid=68418)[0m top5: 0.5373134328358209
[2m[36m(func pid=68418)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=68418)[0m f1_macro: 0.0024834601524200877
[2m[36m(func pid=68418)[0m f1_weighted: 0.0035773267702074694
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.11986940298507463
[2m[36m(func pid=69627)[0m top5: 0.6324626865671642
[2m[36m(func pid=69627)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=69627)[0m f1_macro: 0.05570107258761412
[2m[36m(func pid=69627)[0m f1_weighted: 0.12331411029966365
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.0, 0.0, 0.429, 0.025, 0.0, 0.0, 0.0, 0.102, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5461 | Steps: 2 | Val loss: 2.0862 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9244 | Steps: 2 | Val loss: 2.0510 | Batch size: 32 | lr: 0.01 | Duration: 2.60s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8193 | Steps: 2 | Val loss: 2.3601 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 34.9959 | Steps: 2 | Val loss: 39.7028 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-06 23:54:49 (running for 00:01:58.78)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.822 |      0.002 |                   11 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.546 |      0.076 |                   12 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.133 |      0.116 |                   11 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 31.998 |      0.056 |                   10 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.30223880597014924
[2m[36m(func pid=68792)[0m top5: 0.8852611940298507
[2m[36m(func pid=68792)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=68792)[0m f1_macro: 0.07563772877514771
[2m[36m(func pid=68792)[0m f1_weighted: 0.15931693314947704
[2m[36m(func pid=68792)[0m f1_per_class: [0.179, 0.036, 0.0, 0.0, 0.0, 0.032, 0.487, 0.0, 0.022, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.18003731343283583
[2m[36m(func pid=69189)[0m top5: 0.8208955223880597
[2m[36m(func pid=69189)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=69189)[0m f1_macro: 0.07339548521287535
[2m[36m(func pid=69189)[0m f1_weighted: 0.164640130502156
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.1, 0.0, 0.497, 0.041, 0.0, 0.024, 0.016, 0.0, 0.057]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.5387126865671642
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001209865053513262
[2m[36m(func pid=68418)[0m f1_weighted: 7.335935492384517e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.02751865671641791
[2m[36m(func pid=69627)[0m top5: 0.6310634328358209
[2m[36m(func pid=69627)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=69627)[0m f1_macro: 0.014002908440463235
[2m[36m(func pid=69627)[0m f1_weighted: 0.03482957964148434
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.0, 0.0, 0.124, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5021 | Steps: 2 | Val loss: 2.0655 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9589 | Steps: 2 | Val loss: 2.1855 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 33.5706 | Steps: 2 | Val loss: 33.7761 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=68792)[0m top1: 0.3148320895522388
[2m[36m(func pid=68792)[0m top5: 0.8857276119402985
[2m[36m(func pid=68792)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=68792)[0m f1_macro: 0.11419902156935244
[2m[36m(func pid=68792)[0m f1_weighted: 0.2119806313083471
[2m[36m(func pid=68792)[0m f1_per_class: [0.143, 0.165, 0.0, 0.027, 0.043, 0.204, 0.494, 0.0, 0.066, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8120 | Steps: 2 | Val loss: 2.3657 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-06 23:54:55 (running for 00:02:04.44)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.819 |      0.001 |                   12 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.502 |      0.114 |                   13 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.959 |      0.097 |                   13 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 34.996 |      0.014 |                   11 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.10914179104477612
[2m[36m(func pid=69189)[0m top5: 0.7196828358208955
[2m[36m(func pid=69189)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=69189)[0m f1_macro: 0.09668647938464767
[2m[36m(func pid=69189)[0m f1_weighted: 0.1077938562204251
[2m[36m(func pid=69189)[0m f1_per_class: [0.111, 0.238, 0.0, 0.152, 0.056, 0.0, 0.0, 0.364, 0.0, 0.045]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.01958955223880597
[2m[36m(func pid=69627)[0m top5: 0.6030783582089553
[2m[36m(func pid=69627)[0m f1_micro: 0.01958955223880597
[2m[36m(func pid=69627)[0m f1_macro: 0.028766262149425015
[2m[36m(func pid=69627)[0m f1_weighted: 0.009093490478608555
[2m[36m(func pid=69627)[0m f1_per_class: [0.125, 0.0, 0.0, 0.0, 0.022, 0.0, 0.0, 0.102, 0.0, 0.039]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.5377798507462687
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=68418)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4129 | Steps: 2 | Val loss: 2.0552 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.7189 | Steps: 2 | Val loss: 2.4001 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 35.4246 | Steps: 2 | Val loss: 29.3964 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7980 | Steps: 2 | Val loss: 2.3713 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=68792)[0m top1: 0.2943097014925373
[2m[36m(func pid=68792)[0m top5: 0.8955223880597015
[2m[36m(func pid=68792)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=68792)[0m f1_macro: 0.13799003241473579
[2m[36m(func pid=68792)[0m f1_weighted: 0.268844790656093
[2m[36m(func pid=68792)[0m f1_per_class: [0.043, 0.236, 0.0, 0.202, 0.081, 0.348, 0.434, 0.0, 0.035, 0.0]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.08395522388059702
[2m[36m(func pid=69189)[0m top5: 0.7327425373134329
[2m[36m(func pid=69189)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=69189)[0m f1_macro: 0.10135813344258023
[2m[36m(func pid=69189)[0m f1_weighted: 0.06863650522746025
[2m[36m(func pid=69189)[0m f1_per_class: [0.266, 0.198, 0.0, 0.007, 0.037, 0.0, 0.0, 0.456, 0.0, 0.05]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:55:01 (running for 00:02:10.82)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.812 |      0.001 |                   13 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.413 |      0.138 |                   14 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.719 |      0.101 |                   14 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 35.425 |      0.032 |                   13 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m top1: 0.05830223880597015
[2m[36m(func pid=69627)[0m top5: 0.5293843283582089
[2m[36m(func pid=69627)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=69627)[0m f1_macro: 0.03154053648849604
[2m[36m(func pid=69627)[0m f1_weighted: 0.0422070886476015
[2m[36m(func pid=69627)[0m f1_per_class: [0.036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.138, 0.0, 0.03]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.5265858208955224
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=68418)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3657 | Steps: 2 | Val loss: 2.0590 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0799 | Steps: 2 | Val loss: 2.5563 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 32.0243 | Steps: 2 | Val loss: 24.5537 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=68792)[0m top1: 0.2271455223880597
[2m[36m(func pid=68792)[0m top5: 0.886660447761194
[2m[36m(func pid=68792)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=68792)[0m f1_macro: 0.11205487988803024
[2m[36m(func pid=68792)[0m f1_weighted: 0.19999484630002506
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.219, 0.0, 0.351, 0.064, 0.289, 0.098, 0.0, 0.022, 0.077]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8215 | Steps: 2 | Val loss: 2.3781 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=69189)[0m top1: 0.05503731343283582
[2m[36m(func pid=69189)[0m top5: 0.695429104477612
[2m[36m(func pid=69189)[0m f1_micro: 0.05503731343283582
[2m[36m(func pid=69189)[0m f1_macro: 0.09102195139759815
[2m[36m(func pid=69189)[0m f1_weighted: 0.0416881476089467
[2m[36m(func pid=69189)[0m f1_per_class: [0.248, 0.046, 0.0, 0.0, 0.029, 0.0, 0.0, 0.415, 0.118, 0.055]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.0732276119402985
[2m[36m(func pid=69627)[0m top5: 0.7658582089552238
[2m[36m(func pid=69627)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=69627)[0m f1_macro: 0.04148438702334783
[2m[36m(func pid=69627)[0m f1_weighted: 0.0812812838273686
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.073, 0.0, 0.0, 0.0, 0.0, 0.219, 0.0, 0.092, 0.031]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:55:07 (running for 00:02:16.49)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.821 |      0.001 |                   15 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.366 |      0.112 |                   15 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.08  |      0.091 |                   15 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 32.024 |      0.041 |                   14 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.5111940298507462
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=68418)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3377 | Steps: 2 | Val loss: 2.0716 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.4564 | Steps: 2 | Val loss: 2.5430 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 31.2581 | Steps: 2 | Val loss: 27.3342 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=68792)[0m top1: 0.2150186567164179
[2m[36m(func pid=68792)[0m top5: 0.8292910447761194
[2m[36m(func pid=68792)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=68792)[0m f1_macro: 0.142257245211668
[2m[36m(func pid=68792)[0m f1_weighted: 0.18090426966610704
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.182, 0.308, 0.382, 0.054, 0.27, 0.003, 0.126, 0.027, 0.071]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7485 | Steps: 2 | Val loss: 2.3815 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=69189)[0m top1: 0.05690298507462686
[2m[36m(func pid=69189)[0m top5: 0.7075559701492538
[2m[36m(func pid=69189)[0m f1_micro: 0.05690298507462686
[2m[36m(func pid=69189)[0m f1_macro: 0.0926565195326051
[2m[36m(func pid=69189)[0m f1_weighted: 0.04281978453577243
[2m[36m(func pid=69189)[0m f1_per_class: [0.129, 0.0, 0.211, 0.0, 0.037, 0.101, 0.027, 0.259, 0.095, 0.068]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.16557835820895522
[2m[36m(func pid=69627)[0m top5: 0.8003731343283582
[2m[36m(func pid=69627)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=69627)[0m f1_macro: 0.041223985691043775
[2m[36m(func pid=69627)[0m f1_weighted: 0.06455900964488084
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.028]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3137 | Steps: 2 | Val loss: 2.0943 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-06 23:55:12 (running for 00:02:21.62)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.748 |      0.001 |                   16 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.338 |      0.142 |                   16 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.456 |      0.093 |                   16 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 31.258 |      0.041 |                   15 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.5013992537313433
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=68418)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7987 | Steps: 2 | Val loss: 2.3820 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 29.0848 | Steps: 2 | Val loss: 26.9415 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=68792)[0m top1: 0.22108208955223882
[2m[36m(func pid=68792)[0m top5: 0.6044776119402985
[2m[36m(func pid=68792)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=68792)[0m f1_macro: 0.16186888148974815
[2m[36m(func pid=68792)[0m f1_weighted: 0.18415296319769794
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.135, 0.345, 0.386, 0.058, 0.277, 0.0, 0.307, 0.0, 0.111]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7574 | Steps: 2 | Val loss: 2.3826 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=69189)[0m top1: 0.15485074626865672
[2m[36m(func pid=69189)[0m top5: 0.804570895522388
[2m[36m(func pid=69189)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=69189)[0m f1_macro: 0.1307526932859692
[2m[36m(func pid=69189)[0m f1_weighted: 0.1733560136558239
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.026, 0.32, 0.389, 0.035, 0.153, 0.111, 0.047, 0.109, 0.118]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.26725746268656714
[2m[36m(func pid=69627)[0m top5: 0.7527985074626866
[2m[36m(func pid=69627)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=69627)[0m f1_macro: 0.08791323417040206
[2m[36m(func pid=69627)[0m f1_weighted: 0.14610058428127481
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.005, 0.323, 0.512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.3129 | Steps: 2 | Val loss: 2.1350 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
== Status ==
Current time: 2024-01-06 23:55:17 (running for 00:02:26.81)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.757 |      0.001 |                   17 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.314 |      0.162 |                   17 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.799 |      0.131 |                   17 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 29.085 |      0.088 |                   16 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.49906716417910446
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=68418)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4834 | Steps: 2 | Val loss: 2.2369 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 24.2474 | Steps: 2 | Val loss: 24.5329 | Batch size: 32 | lr: 0.1 | Duration: 2.59s
[2m[36m(func pid=68792)[0m top1: 0.16184701492537312
[2m[36m(func pid=68792)[0m top5: 0.5629664179104478
[2m[36m(func pid=68792)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=68792)[0m f1_macro: 0.13135942480449622
[2m[36m(func pid=68792)[0m f1_weighted: 0.12402191334609583
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.082, 0.293, 0.198, 0.07, 0.358, 0.0, 0.168, 0.0, 0.144]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7521 | Steps: 2 | Val loss: 2.3835 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=69189)[0m top1: 0.20708955223880596
[2m[36m(func pid=69189)[0m top5: 0.8647388059701493
[2m[36m(func pid=69189)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=69189)[0m f1_macro: 0.1398071085302731
[2m[36m(func pid=69189)[0m f1_weighted: 0.2298261885009734
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.047, 0.32, 0.477, 0.032, 0.099, 0.236, 0.062, 0.0, 0.126]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.045242537313432835
[2m[36m(func pid=69627)[0m top5: 0.6595149253731343
[2m[36m(func pid=69627)[0m f1_micro: 0.045242537313432835
[2m[36m(func pid=69627)[0m f1_macro: 0.0885588748607623
[2m[36m(func pid=69627)[0m f1_weighted: 0.03152193270394041
[2m[36m(func pid=69627)[0m f1_per_class: [0.188, 0.082, 0.378, 0.0, 0.0, 0.0, 0.0, 0.163, 0.045, 0.029]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3101 | Steps: 2 | Val loss: 2.1750 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
== Status ==
Current time: 2024-01-06 23:55:22 (running for 00:02:31.84)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.752 |      0.001 |                   18 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.313 |      0.131 |                   18 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.483 |      0.14  |                   18 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 24.247 |      0.089 |                   17 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.5163246268656716
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=68418)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5126 | Steps: 2 | Val loss: 2.2142 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 17.9717 | Steps: 2 | Val loss: 20.4422 | Batch size: 32 | lr: 0.1 | Duration: 2.59s
[2m[36m(func pid=68792)[0m top1: 0.1333955223880597
[2m[36m(func pid=68792)[0m top5: 0.542910447761194
[2m[36m(func pid=68792)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=68792)[0m f1_macro: 0.1253131072232065
[2m[36m(func pid=68792)[0m f1_weighted: 0.099259922645515
[2m[36m(func pid=68792)[0m f1_per_class: [0.0, 0.054, 0.327, 0.112, 0.111, 0.406, 0.0, 0.147, 0.0, 0.097]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7373 | Steps: 2 | Val loss: 2.3817 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=69189)[0m top1: 0.19496268656716417
[2m[36m(func pid=69189)[0m top5: 0.8680037313432836
[2m[36m(func pid=69189)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=69189)[0m f1_macro: 0.15843869304848762
[2m[36m(func pid=69189)[0m f1_weighted: 0.24485988061618347
[2m[36m(func pid=69189)[0m f1_per_class: [0.0, 0.202, 0.32, 0.348, 0.038, 0.007, 0.309, 0.295, 0.0, 0.065]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.11520522388059702
[2m[36m(func pid=69627)[0m top5: 0.5615671641791045
[2m[36m(func pid=69627)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=69627)[0m f1_macro: 0.162774224778732
[2m[36m(func pid=69627)[0m f1_weighted: 0.10474127960392184
[2m[36m(func pid=69627)[0m f1_per_class: [0.176, 0.246, 0.372, 0.0, 0.074, 0.0, 0.071, 0.541, 0.077, 0.07]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2390 | Steps: 2 | Val loss: 2.2068 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-06 23:55:27 (running for 00:02:36.89)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.737 |      0.001 |                   19 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.31  |      0.125 |                   19 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.513 |      0.158 |                   19 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 17.972 |      0.163 |                   18 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.006063432835820896
[2m[36m(func pid=68418)[0m top5: 0.5261194029850746
[2m[36m(func pid=68418)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=68418)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=68418)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.5437 | Steps: 2 | Val loss: 2.3216 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 12.0929 | Steps: 2 | Val loss: 20.9034 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=68792)[0m top1: 0.1021455223880597
[2m[36m(func pid=68792)[0m top5: 0.5438432835820896
[2m[36m(func pid=68792)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=68792)[0m f1_macro: 0.11226553605266601
[2m[36m(func pid=68792)[0m f1_weighted: 0.06760859626130743
[2m[36m(func pid=68792)[0m f1_per_class: [0.077, 0.047, 0.339, 0.071, 0.141, 0.228, 0.0, 0.139, 0.0, 0.081]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.11380597014925373
[2m[36m(func pid=69189)[0m top5: 0.8288246268656716
[2m[36m(func pid=69189)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=69189)[0m f1_macro: 0.1406503950735049
[2m[36m(func pid=69189)[0m f1_weighted: 0.10303994999595081
[2m[36m(func pid=69189)[0m f1_per_class: [0.219, 0.205, 0.32, 0.032, 0.065, 0.0, 0.087, 0.437, 0.0, 0.041]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8141 | Steps: 2 | Val loss: 2.3813 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=69627)[0m top1: 0.17817164179104478
[2m[36m(func pid=69627)[0m top5: 0.6814365671641791
[2m[36m(func pid=69627)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=69627)[0m f1_macro: 0.18423865913589973
[2m[36m(func pid=69627)[0m f1_weighted: 0.1903272697991392
[2m[36m(func pid=69627)[0m f1_per_class: [0.286, 0.279, 0.324, 0.0, 0.03, 0.0, 0.349, 0.458, 0.118, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.2722 | Steps: 2 | Val loss: 2.2332 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
== Status ==
Current time: 2024-01-06 23:55:33 (running for 00:02:42.15)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.814 |      0.002 |                   20 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.239 |      0.112 |                   20 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.544 |      0.141 |                   20 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 12.093 |      0.184 |                   19 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.0065298507462686565
[2m[36m(func pid=68418)[0m top5: 0.5499067164179104
[2m[36m(func pid=68418)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=68418)[0m f1_macro: 0.0017515326225153894
[2m[36m(func pid=68418)[0m f1_weighted: 0.0010037423345388642
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.3349 | Steps: 2 | Val loss: 2.4011 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 13.0711 | Steps: 2 | Val loss: 29.6924 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=68792)[0m top1: 0.09001865671641791
[2m[36m(func pid=68792)[0m top5: 0.5471082089552238
[2m[36m(func pid=68792)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=68792)[0m f1_macro: 0.11156170428310466
[2m[36m(func pid=68792)[0m f1_weighted: 0.04498262784448842
[2m[36m(func pid=68792)[0m f1_per_class: [0.28, 0.03, 0.355, 0.054, 0.132, 0.061, 0.0, 0.142, 0.0, 0.062]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.11893656716417911
[2m[36m(func pid=69189)[0m top5: 0.8148320895522388
[2m[36m(func pid=69189)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=69189)[0m f1_macro: 0.15436141949298215
[2m[36m(func pid=69189)[0m f1_weighted: 0.08864983766228361
[2m[36m(func pid=69189)[0m f1_per_class: [0.274, 0.226, 0.308, 0.033, 0.075, 0.024, 0.003, 0.401, 0.159, 0.042]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8318 | Steps: 2 | Val loss: 2.3806 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=69627)[0m top1: 0.1730410447761194
[2m[36m(func pid=69627)[0m top5: 0.6814365671641791
[2m[36m(func pid=69627)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=69627)[0m f1_macro: 0.11694907987627337
[2m[36m(func pid=69627)[0m f1_weighted: 0.09431725330361476
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.245, 0.272, 0.0, 0.112, 0.272, 0.015, 0.231, 0.023, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.1825 | Steps: 2 | Val loss: 2.2597 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.7348 | Steps: 2 | Val loss: 2.4613 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
== Status ==
Current time: 2024-01-06 23:55:38 (running for 00:02:47.27)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.832 |      0.002 |                   21 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.272 |      0.112 |                   21 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.335 |      0.154 |                   21 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 13.071 |      0.117 |                   20 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.0065298507462686565
[2m[36m(func pid=68418)[0m top5: 0.5494402985074627
[2m[36m(func pid=68418)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=68418)[0m f1_macro: 0.0017733522996680891
[2m[36m(func pid=68418)[0m f1_weighted: 0.0010050653560080113
[2m[36m(func pid=68418)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 21.6570 | Steps: 2 | Val loss: 36.9197 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=68792)[0m top1: 0.08488805970149253
[2m[36m(func pid=68792)[0m top5: 0.5629664179104478
[2m[36m(func pid=68792)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=68792)[0m f1_macro: 0.10097601276115882
[2m[36m(func pid=68792)[0m f1_weighted: 0.030240350834931585
[2m[36m(func pid=68792)[0m f1_per_class: [0.207, 0.0, 0.391, 0.042, 0.158, 0.008, 0.0, 0.16, 0.0, 0.043]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.134794776119403
[2m[36m(func pid=69189)[0m top5: 0.7971082089552238
[2m[36m(func pid=69189)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=69189)[0m f1_macro: 0.18270913193556743
[2m[36m(func pid=69189)[0m f1_weighted: 0.09238395257424298
[2m[36m(func pid=69189)[0m f1_per_class: [0.304, 0.061, 0.368, 0.013, 0.08, 0.315, 0.0, 0.496, 0.108, 0.083]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6983 | Steps: 2 | Val loss: 2.3725 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=69627)[0m top1: 0.17024253731343283
[2m[36m(func pid=69627)[0m top5: 0.6767723880597015
[2m[36m(func pid=69627)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=69627)[0m f1_macro: 0.11201059822026993
[2m[36m(func pid=69627)[0m f1_weighted: 0.10912544649345766
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.201, 0.333, 0.0, 0.095, 0.247, 0.123, 0.121, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2344 | Steps: 2 | Val loss: 2.2770 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.2413 | Steps: 2 | Val loss: 2.3649 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=68418)[0m top1: 0.012126865671641791
[2m[36m(func pid=68418)[0m top5: 0.5419776119402985
[2m[36m(func pid=68418)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=68418)[0m f1_macro: 0.007532448620191659
[2m[36m(func pid=68418)[0m f1_weighted: 0.01045166767137857
[2m[36m(func pid=68418)[0m f1_per_class: [0.017, 0.026, 0.013, 0.0, 0.0, 0.0, 0.018, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-06 23:55:43 (running for 00:02:52.34)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.698 |      0.008 |                   22 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.183 |      0.101 |                   22 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.735 |      0.183 |                   22 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 21.657 |      0.112 |                   21 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 31.9267 | Steps: 2 | Val loss: 30.5724 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=68792)[0m top1: 0.08582089552238806
[2m[36m(func pid=68792)[0m top5: 0.5881529850746269
[2m[36m(func pid=68792)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=68792)[0m f1_macro: 0.10016017802422161
[2m[36m(func pid=68792)[0m f1_weighted: 0.029885565243368134
[2m[36m(func pid=68792)[0m f1_per_class: [0.186, 0.0, 0.41, 0.039, 0.133, 0.0, 0.0, 0.195, 0.0, 0.038]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.18190298507462688
[2m[36m(func pid=69189)[0m top5: 0.8330223880597015
[2m[36m(func pid=69189)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=69189)[0m f1_macro: 0.22146415260188418
[2m[36m(func pid=69189)[0m f1_weighted: 0.16466432020644367
[2m[36m(func pid=69189)[0m f1_per_class: [0.308, 0.052, 0.355, 0.184, 0.08, 0.365, 0.057, 0.535, 0.112, 0.167]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7752 | Steps: 2 | Val loss: 2.3657 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=69627)[0m top1: 0.18983208955223882
[2m[36m(func pid=69627)[0m top5: 0.6823694029850746
[2m[36m(func pid=69627)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=69627)[0m f1_macro: 0.11764459459486905
[2m[36m(func pid=69627)[0m f1_weighted: 0.16708637764167755
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.176, 0.286, 0.0, 0.041, 0.33, 0.324, 0.0, 0.02, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0776 | Steps: 2 | Val loss: 2.2810 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.0970 | Steps: 2 | Val loss: 2.1573 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-06 23:55:48 (running for 00:02:57.65)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.775 |      0.025 |                   23 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.234 |      0.1   |                   23 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.241 |      0.221 |                   23 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 31.927 |      0.118 |                   22 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.04197761194029851
[2m[36m(func pid=68418)[0m top5: 0.5345149253731343
[2m[36m(func pid=68418)[0m f1_micro: 0.04197761194029851
[2m[36m(func pid=68418)[0m f1_macro: 0.025192048161850222
[2m[36m(func pid=68418)[0m f1_weighted: 0.05878209657906651
[2m[36m(func pid=68418)[0m f1_per_class: [0.01, 0.064, 0.019, 0.0, 0.0, 0.0, 0.159, 0.0, 0.0, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 22.8878 | Steps: 2 | Val loss: 33.6554 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=68792)[0m top1: 0.08675373134328358
[2m[36m(func pid=68792)[0m top5: 0.6254664179104478
[2m[36m(func pid=68792)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=68792)[0m f1_macro: 0.10347513379771796
[2m[36m(func pid=68792)[0m f1_weighted: 0.0414470375270901
[2m[36m(func pid=68792)[0m f1_per_class: [0.147, 0.0, 0.387, 0.067, 0.123, 0.0, 0.0, 0.278, 0.0, 0.033]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.35261194029850745
[2m[36m(func pid=69189)[0m top5: 0.8889925373134329
[2m[36m(func pid=69189)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=69189)[0m f1_macro: 0.2929927022594916
[2m[36m(func pid=69189)[0m f1_weighted: 0.3610531076246364
[2m[36m(func pid=69189)[0m f1_per_class: [0.244, 0.305, 0.268, 0.517, 0.087, 0.364, 0.269, 0.46, 0.174, 0.241]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6896 | Steps: 2 | Val loss: 2.3537 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=69627)[0m top1: 0.07042910447761194
[2m[36m(func pid=69627)[0m top5: 0.7513992537313433
[2m[36m(func pid=69627)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=69627)[0m f1_macro: 0.1011565866539071
[2m[36m(func pid=69627)[0m f1_weighted: 0.08503464670112794
[2m[36m(func pid=69627)[0m f1_per_class: [0.0, 0.095, 0.32, 0.0, 0.021, 0.0, 0.149, 0.321, 0.105, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1514 | Steps: 2 | Val loss: 2.2810 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.9856 | Steps: 2 | Val loss: 2.2028 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
== Status ==
Current time: 2024-01-06 23:55:53 (running for 00:03:02.69)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.69  |      0.046 |                   24 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.078 |      0.103 |                   24 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.097 |      0.293 |                   24 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 22.888 |      0.101 |                   23 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.08675373134328358
[2m[36m(func pid=68418)[0m top5: 0.5359141791044776
[2m[36m(func pid=68418)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=68418)[0m f1_macro: 0.04614439684080128
[2m[36m(func pid=68418)[0m f1_weighted: 0.10801919925319625
[2m[36m(func pid=68418)[0m f1_per_class: [0.014, 0.097, 0.037, 0.0, 0.0, 0.0, 0.303, 0.0, 0.01, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 18.6236 | Steps: 2 | Val loss: 27.6075 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=68792)[0m top1: 0.08442164179104478
[2m[36m(func pid=68792)[0m top5: 0.6585820895522388
[2m[36m(func pid=68792)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=68792)[0m f1_macro: 0.11297315291465218
[2m[36m(func pid=68792)[0m f1_weighted: 0.05169343745691132
[2m[36m(func pid=68792)[0m f1_per_class: [0.128, 0.0, 0.387, 0.078, 0.095, 0.0, 0.0, 0.411, 0.0, 0.03]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.365205223880597
[2m[36m(func pid=69189)[0m top5: 0.917910447761194
[2m[36m(func pid=69189)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=69189)[0m f1_macro: 0.2628146508739785
[2m[36m(func pid=69189)[0m f1_weighted: 0.3721884547687703
[2m[36m(func pid=69189)[0m f1_per_class: [0.181, 0.486, 0.232, 0.37, 0.105, 0.377, 0.39, 0.314, 0.0, 0.174]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7046 | Steps: 2 | Val loss: 2.3395 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=69627)[0m top1: 0.20942164179104478
[2m[36m(func pid=69627)[0m top5: 0.835820895522388
[2m[36m(func pid=69627)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=69627)[0m f1_macro: 0.1336660467049844
[2m[36m(func pid=69627)[0m f1_weighted: 0.18105858285086576
[2m[36m(func pid=69627)[0m f1_per_class: [0.333, 0.0, 0.0, 0.531, 0.025, 0.0, 0.0, 0.447, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1223 | Steps: 2 | Val loss: 2.2630 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.3817 | Steps: 2 | Val loss: 2.3790 | Batch size: 32 | lr: 0.01 | Duration: 2.61s
== Status ==
Current time: 2024-01-06 23:55:58 (running for 00:03:07.83)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.705 |      0.055 |                   25 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.151 |      0.113 |                   25 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.986 |      0.263 |                   25 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 18.624 |      0.134 |                   24 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.11194029850746269
[2m[36m(func pid=68418)[0m top5: 0.5578358208955224
[2m[36m(func pid=68418)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=68418)[0m f1_macro: 0.05534513048343923
[2m[36m(func pid=68418)[0m f1_weighted: 0.12894439507177588
[2m[36m(func pid=68418)[0m f1_per_class: [0.016, 0.094, 0.06, 0.0, 0.0, 0.0, 0.375, 0.0, 0.009, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 16.8331 | Steps: 2 | Val loss: 22.5646 | Batch size: 32 | lr: 0.1 | Duration: 2.56s
[2m[36m(func pid=68792)[0m top1: 0.08162313432835822
[2m[36m(func pid=68792)[0m top5: 0.7047574626865671
[2m[36m(func pid=68792)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=68792)[0m f1_macro: 0.1186357217056084
[2m[36m(func pid=68792)[0m f1_weighted: 0.056182693200100504
[2m[36m(func pid=68792)[0m f1_per_class: [0.121, 0.005, 0.387, 0.077, 0.08, 0.0, 0.0, 0.483, 0.0, 0.033]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3246268656716418
[2m[36m(func pid=69189)[0m top5: 0.9221082089552238
[2m[36m(func pid=69189)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=69189)[0m f1_macro: 0.2240275954355797
[2m[36m(func pid=69189)[0m f1_weighted: 0.2938024954852088
[2m[36m(func pid=69189)[0m f1_per_class: [0.179, 0.42, 0.214, 0.086, 0.12, 0.376, 0.442, 0.264, 0.0, 0.14]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.2658582089552239
[2m[36m(func pid=69627)[0m top5: 0.7532649253731343
[2m[36m(func pid=69627)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=69627)[0m f1_macro: 0.17045562734771444
[2m[36m(func pid=69627)[0m f1_weighted: 0.20190226730418384
[2m[36m(func pid=69627)[0m f1_per_class: [0.38, 0.0, 0.0, 0.583, 0.189, 0.0, 0.0, 0.514, 0.0, 0.039]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6596 | Steps: 2 | Val loss: 2.3278 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0481 | Steps: 2 | Val loss: 2.2254 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.4008 | Steps: 2 | Val loss: 2.4885 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 18.5683 | Steps: 2 | Val loss: 27.4554 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-06 23:56:04 (running for 00:03:13.24)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.66  |      0.058 |                   26 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.122 |      0.119 |                   26 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.382 |      0.224 |                   26 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 16.833 |      0.17  |                   25 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.12313432835820895
[2m[36m(func pid=68418)[0m top5: 0.6002798507462687
[2m[36m(func pid=68418)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=68418)[0m f1_macro: 0.05842748532636316
[2m[36m(func pid=68418)[0m f1_weighted: 0.1321387609994325
[2m[36m(func pid=68418)[0m f1_per_class: [0.018, 0.055, 0.08, 0.0, 0.0, 0.0, 0.406, 0.0, 0.025, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.09934701492537314
[2m[36m(func pid=68792)[0m top5: 0.7709888059701493
[2m[36m(func pid=68792)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=68792)[0m f1_macro: 0.1389983370093437
[2m[36m(func pid=68792)[0m f1_weighted: 0.09582844589528175
[2m[36m(func pid=68792)[0m f1_per_class: [0.124, 0.079, 0.389, 0.167, 0.058, 0.0, 0.0, 0.488, 0.049, 0.036]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3087686567164179
[2m[36m(func pid=69189)[0m top5: 0.8936567164179104
[2m[36m(func pid=69189)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=69189)[0m f1_macro: 0.2272208397343615
[2m[36m(func pid=69189)[0m f1_weighted: 0.3110244193491915
[2m[36m(func pid=69189)[0m f1_per_class: [0.156, 0.426, 0.214, 0.163, 0.079, 0.314, 0.442, 0.305, 0.0, 0.174]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.07882462686567164
[2m[36m(func pid=69627)[0m top5: 0.7523320895522388
[2m[36m(func pid=69627)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=69627)[0m f1_macro: 0.10314374988032564
[2m[36m(func pid=69627)[0m f1_weighted: 0.06236643834515424
[2m[36m(func pid=69627)[0m f1_per_class: [0.262, 0.07, 0.0, 0.036, 0.0, 0.0, 0.0, 0.555, 0.076, 0.032]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6661 | Steps: 2 | Val loss: 2.3146 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0024 | Steps: 2 | Val loss: 2.1856 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0146 | Steps: 2 | Val loss: 2.7044 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 29.9786 | Steps: 2 | Val loss: 27.1352 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-06 23:56:09 (running for 00:03:18.50)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.666 |      0.063 |                   27 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.048 |      0.139 |                   27 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.401 |      0.227 |                   27 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 18.568 |      0.103 |                   26 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.13432835820895522
[2m[36m(func pid=68418)[0m top5: 0.6175373134328358
[2m[36m(func pid=68418)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=68418)[0m f1_macro: 0.0628611743883136
[2m[36m(func pid=68418)[0m f1_weighted: 0.1332160386539527
[2m[36m(func pid=68418)[0m f1_per_class: [0.028, 0.036, 0.119, 0.0, 0.0, 0.0, 0.419, 0.0, 0.026, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.1287313432835821
[2m[36m(func pid=68792)[0m top5: 0.8236940298507462
[2m[36m(func pid=68792)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=68792)[0m f1_macro: 0.151245260497953
[2m[36m(func pid=68792)[0m f1_weighted: 0.1579859893329442
[2m[36m(func pid=68792)[0m f1_per_class: [0.138, 0.155, 0.348, 0.17, 0.043, 0.0, 0.188, 0.329, 0.096, 0.046]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.2826492537313433
[2m[36m(func pid=69189)[0m top5: 0.8507462686567164
[2m[36m(func pid=69189)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=69189)[0m f1_macro: 0.24708198051483068
[2m[36m(func pid=69189)[0m f1_weighted: 0.32556407297238366
[2m[36m(func pid=69189)[0m f1_per_class: [0.107, 0.384, 0.212, 0.39, 0.062, 0.256, 0.275, 0.504, 0.151, 0.13]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.2150186567164179
[2m[36m(func pid=69627)[0m top5: 0.7555970149253731
[2m[36m(func pid=69627)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=69627)[0m f1_macro: 0.19133464902974548
[2m[36m(func pid=69627)[0m f1_weighted: 0.1813744318898112
[2m[36m(func pid=69627)[0m f1_per_class: [0.259, 0.485, 0.286, 0.0, 0.0, 0.0, 0.192, 0.519, 0.081, 0.092]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6713 | Steps: 2 | Val loss: 2.3032 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.9872 | Steps: 2 | Val loss: 2.1489 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.4905 | Steps: 2 | Val loss: 2.9416 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 18.6874 | Steps: 2 | Val loss: 28.0930 | Batch size: 32 | lr: 0.1 | Duration: 2.57s
== Status ==
Current time: 2024-01-06 23:56:14 (running for 00:03:23.76)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.671 |      0.068 |                   28 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.002 |      0.151 |                   28 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.015 |      0.247 |                   28 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 29.979 |      0.191 |                   27 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.14412313432835822
[2m[36m(func pid=68418)[0m top5: 0.6319962686567164
[2m[36m(func pid=68418)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=68418)[0m f1_macro: 0.06800677049005069
[2m[36m(func pid=68418)[0m f1_weighted: 0.13358018852127418
[2m[36m(func pid=68418)[0m f1_per_class: [0.036, 0.026, 0.167, 0.0, 0.0, 0.0, 0.424, 0.0, 0.027, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.16371268656716417
[2m[36m(func pid=68792)[0m top5: 0.8577425373134329
[2m[36m(func pid=68792)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=68792)[0m f1_macro: 0.1497840709041779
[2m[36m(func pid=68792)[0m f1_weighted: 0.1879015833727121
[2m[36m(func pid=68792)[0m f1_per_class: [0.166, 0.261, 0.355, 0.099, 0.038, 0.0, 0.347, 0.032, 0.087, 0.113]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.2224813432835821
[2m[36m(func pid=69189)[0m top5: 0.8152985074626866
[2m[36m(func pid=69189)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=69189)[0m f1_macro: 0.20708196874217027
[2m[36m(func pid=69189)[0m f1_weighted: 0.23015509900118714
[2m[36m(func pid=69189)[0m f1_per_class: [0.135, 0.238, 0.204, 0.411, 0.068, 0.259, 0.018, 0.522, 0.103, 0.113]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.31576492537313433
[2m[36m(func pid=69627)[0m top5: 0.8498134328358209
[2m[36m(func pid=69627)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=69627)[0m f1_macro: 0.18048558621414226
[2m[36m(func pid=69627)[0m f1_weighted: 0.24024578162500201
[2m[36m(func pid=69627)[0m f1_per_class: [0.168, 0.426, 0.367, 0.0, 0.0, 0.0, 0.488, 0.229, 0.05, 0.077]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6472 | Steps: 2 | Val loss: 2.2870 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0388 | Steps: 2 | Val loss: 2.1191 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9879 | Steps: 2 | Val loss: 2.9724 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 29.9721 | Steps: 2 | Val loss: 24.4580 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
== Status ==
Current time: 2024-01-06 23:56:20 (running for 00:03:29.03)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.647 |      0.074 |                   29 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.987 |      0.15  |                   29 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.491 |      0.207 |                   29 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 18.687 |      0.18  |                   28 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.1599813432835821
[2m[36m(func pid=68418)[0m top5: 0.6394589552238806
[2m[36m(func pid=68418)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=68418)[0m f1_macro: 0.07431854913441854
[2m[36m(func pid=68418)[0m f1_weighted: 0.1401159349473837
[2m[36m(func pid=68418)[0m f1_per_class: [0.04, 0.021, 0.206, 0.0, 0.0, 0.0, 0.448, 0.0, 0.028, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.19776119402985073
[2m[36m(func pid=68792)[0m top5: 0.867070895522388
[2m[36m(func pid=68792)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=68792)[0m f1_macro: 0.1685591871502143
[2m[36m(func pid=68792)[0m f1_weighted: 0.20698479855102256
[2m[36m(func pid=68792)[0m f1_per_class: [0.246, 0.318, 0.301, 0.054, 0.043, 0.0, 0.417, 0.0, 0.102, 0.204]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.19916044776119404
[2m[36m(func pid=69189)[0m top5: 0.7971082089552238
[2m[36m(func pid=69189)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=69189)[0m f1_macro: 0.21883871379802694
[2m[36m(func pid=69189)[0m f1_weighted: 0.20271656371649685
[2m[36m(func pid=69189)[0m f1_per_class: [0.189, 0.248, 0.237, 0.28, 0.083, 0.335, 0.0, 0.557, 0.111, 0.148]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.3460820895522388
[2m[36m(func pid=69627)[0m top5: 0.8969216417910447
[2m[36m(func pid=69627)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=69627)[0m f1_macro: 0.16044943532935885
[2m[36m(func pid=69627)[0m f1_weighted: 0.28061704278574334
[2m[36m(func pid=69627)[0m f1_per_class: [0.086, 0.497, 0.2, 0.0, 0.0, 0.148, 0.567, 0.107, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6505 | Steps: 2 | Val loss: 2.2726 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0266 | Steps: 2 | Val loss: 2.0832 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8380 | Steps: 2 | Val loss: 2.6733 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 23.8012 | Steps: 2 | Val loss: 25.2659 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
== Status ==
Current time: 2024-01-06 23:56:25 (running for 00:03:34.18)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.647 |      0.074 |                   29 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.027 |      0.183 |                   31 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.988 |      0.219 |                   30 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 29.972 |      0.16  |                   29 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.23367537313432835
[2m[36m(func pid=68792)[0m top5: 0.8913246268656716
[2m[36m(func pid=68792)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=68792)[0m f1_macro: 0.18280643124499898
[2m[36m(func pid=68792)[0m f1_weighted: 0.22597118257814358
[2m[36m(func pid=68792)[0m f1_per_class: [0.273, 0.309, 0.262, 0.045, 0.053, 0.0, 0.488, 0.0, 0.112, 0.286]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.24673507462686567
[2m[36m(func pid=69189)[0m top5: 0.8325559701492538
[2m[36m(func pid=69189)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=69189)[0m f1_macro: 0.27058162109676204
[2m[36m(func pid=69189)[0m f1_weighted: 0.23467064937843124
[2m[36m(func pid=69189)[0m f1_per_class: [0.373, 0.398, 0.31, 0.243, 0.112, 0.379, 0.025, 0.534, 0.128, 0.205]
[2m[36m(func pid=68418)[0m top1: 0.177705223880597
[2m[36m(func pid=68418)[0m top5: 0.6473880597014925
[2m[36m(func pid=68418)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=68418)[0m f1_macro: 0.08470924302364324
[2m[36m(func pid=68418)[0m f1_weighted: 0.14266177633470986
[2m[36m(func pid=68418)[0m f1_per_class: [0.056, 0.011, 0.282, 0.0, 0.0, 0.0, 0.458, 0.0, 0.04, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.37593283582089554
[2m[36m(func pid=69627)[0m top5: 0.8759328358208955
[2m[36m(func pid=69627)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=69627)[0m f1_macro: 0.22744291638319297
[2m[36m(func pid=69627)[0m f1_weighted: 0.35724874125057815
[2m[36m(func pid=69627)[0m f1_per_class: [0.109, 0.222, 0.135, 0.506, 0.0, 0.51, 0.295, 0.498, 0.0, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0019 | Steps: 2 | Val loss: 2.0536 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7394 | Steps: 2 | Val loss: 2.3139 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8139 | Steps: 2 | Val loss: 2.2624 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 26.7615 | Steps: 2 | Val loss: 38.1054 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=68792)[0m top1: 0.2537313432835821
[2m[36m(func pid=68792)[0m top5: 0.902518656716418
[2m[36m(func pid=68792)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=68792)[0m f1_macro: 0.16125604967783028
[2m[36m(func pid=68792)[0m f1_weighted: 0.23518281261440346
[2m[36m(func pid=68792)[0m f1_per_class: [0.102, 0.313, 0.237, 0.074, 0.062, 0.0, 0.505, 0.0, 0.121, 0.2]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-06 23:56:30 (running for 00:03:39.34)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.65  |      0.085 |                   30 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.002 |      0.161 |                   32 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.739 |      0.31  |                   32 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 23.801 |      0.227 |                   30 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.32975746268656714
[2m[36m(func pid=69189)[0m top5: 0.8768656716417911
[2m[36m(func pid=69189)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=69189)[0m f1_macro: 0.31023511611408655
[2m[36m(func pid=69189)[0m f1_weighted: 0.32360200067107964
[2m[36m(func pid=69189)[0m f1_per_class: [0.296, 0.43, 0.357, 0.279, 0.136, 0.406, 0.262, 0.515, 0.159, 0.262]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.19496268656716417
[2m[36m(func pid=68418)[0m top5: 0.6520522388059702
[2m[36m(func pid=68418)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=68418)[0m f1_macro: 0.09568390319135625
[2m[36m(func pid=68418)[0m f1_weighted: 0.15033405170202316
[2m[36m(func pid=68418)[0m f1_per_class: [0.067, 0.016, 0.355, 0.0, 0.0, 0.0, 0.479, 0.0, 0.04, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.32509328358208955
[2m[36m(func pid=69627)[0m top5: 0.8078358208955224
[2m[36m(func pid=69627)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=69627)[0m f1_macro: 0.17620345899586476
[2m[36m(func pid=69627)[0m f1_weighted: 0.2278309958481583
[2m[36m(func pid=69627)[0m f1_per_class: [0.118, 0.0, 0.103, 0.519, 0.0, 0.425, 0.0, 0.473, 0.125, 0.0]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3219 | Steps: 2 | Val loss: 2.0309 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7318 | Steps: 2 | Val loss: 2.1076 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6687 | Steps: 2 | Val loss: 2.2501 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 35.1535 | Steps: 2 | Val loss: 44.8011 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=68792)[0m top1: 0.28218283582089554
[2m[36m(func pid=68792)[0m top5: 0.9053171641791045
[2m[36m(func pid=68792)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=68792)[0m f1_macro: 0.16724415477155735
[2m[36m(func pid=68792)[0m f1_weighted: 0.2708035818626848
[2m[36m(func pid=68792)[0m f1_per_class: [0.098, 0.307, 0.232, 0.186, 0.068, 0.0, 0.526, 0.0, 0.112, 0.143]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-06 23:56:35 (running for 00:03:44.38)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.814 |      0.096 |                   31 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.322 |      0.167 |                   33 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.732 |      0.313 |                   33 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 26.762 |      0.176 |                   31 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.37826492537313433
[2m[36m(func pid=69189)[0m top5: 0.9211753731343284
[2m[36m(func pid=69189)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=69189)[0m f1_macro: 0.3132248596102466
[2m[36m(func pid=69189)[0m f1_weighted: 0.3734167079301161
[2m[36m(func pid=69189)[0m f1_per_class: [0.358, 0.423, 0.341, 0.249, 0.145, 0.425, 0.489, 0.348, 0.104, 0.25]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.2042910447761194
[2m[36m(func pid=68418)[0m top5: 0.664179104477612
[2m[36m(func pid=68418)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=68418)[0m f1_macro: 0.09694368296372229
[2m[36m(func pid=68418)[0m f1_weighted: 0.15280046415254464
[2m[36m(func pid=68418)[0m f1_per_class: [0.073, 0.021, 0.353, 0.0, 0.0, 0.0, 0.484, 0.0, 0.038, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.20708955223880596
[2m[36m(func pid=69627)[0m top5: 0.7430037313432836
[2m[36m(func pid=69627)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=69627)[0m f1_macro: 0.1576378532478399
[2m[36m(func pid=69627)[0m f1_weighted: 0.15915462588057347
[2m[36m(func pid=69627)[0m f1_per_class: [0.132, 0.0, 0.11, 0.286, 0.0, 0.377, 0.0, 0.497, 0.099, 0.077]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8728 | Steps: 2 | Val loss: 1.9994 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8935 | Steps: 2 | Val loss: 2.0060 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6083 | Steps: 2 | Val loss: 2.2332 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 43.9875 | Steps: 2 | Val loss: 42.8115 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=68792)[0m top1: 0.31203358208955223
[2m[36m(func pid=68792)[0m top5: 0.9071828358208955
[2m[36m(func pid=68792)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=68792)[0m f1_macro: 0.2016507396281811
[2m[36m(func pid=68792)[0m f1_weighted: 0.3081211187051076
[2m[36m(func pid=68792)[0m f1_per_class: [0.231, 0.272, 0.253, 0.313, 0.074, 0.0, 0.539, 0.0, 0.129, 0.207]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-06 23:56:40 (running for 00:03:49.40)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.669 |      0.097 |                   32 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.873 |      0.202 |                   34 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.894 |      0.316 |                   34 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 35.153 |      0.158 |                   32 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.41091417910447764
[2m[36m(func pid=69189)[0m top5: 0.929570895522388
[2m[36m(func pid=69189)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=69189)[0m f1_macro: 0.3155059963289287
[2m[36m(func pid=69189)[0m f1_weighted: 0.42202123182377255
[2m[36m(func pid=69189)[0m f1_per_class: [0.307, 0.432, 0.364, 0.392, 0.11, 0.388, 0.543, 0.312, 0.053, 0.255]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.21641791044776118
[2m[36m(func pid=68418)[0m top5: 0.6753731343283582
[2m[36m(func pid=68418)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=68418)[0m f1_macro: 0.10096712836976411
[2m[36m(func pid=68418)[0m f1_weighted: 0.15229727933617823
[2m[36m(func pid=68418)[0m f1_per_class: [0.083, 0.011, 0.389, 0.0, 0.0, 0.0, 0.487, 0.0, 0.041, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.1814365671641791
[2m[36m(func pid=69627)[0m top5: 0.699160447761194
[2m[36m(func pid=69627)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=69627)[0m f1_macro: 0.15072624462808318
[2m[36m(func pid=69627)[0m f1_weighted: 0.13184954940223256
[2m[36m(func pid=69627)[0m f1_per_class: [0.105, 0.0, 0.152, 0.174, 0.0, 0.413, 0.0, 0.51, 0.079, 0.074]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.0311 | Steps: 2 | Val loss: 1.9781 | Batch size: 32 | lr: 0.001 | Duration: 2.57s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7473 | Steps: 2 | Val loss: 2.0619 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5553 | Steps: 2 | Val loss: 2.2137 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 17.7419 | Steps: 2 | Val loss: 36.0726 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=68792)[0m top1: 0.34048507462686567
[2m[36m(func pid=68792)[0m top5: 0.8987873134328358
[2m[36m(func pid=68792)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=68792)[0m f1_macro: 0.2121561029221794
[2m[36m(func pid=68792)[0m f1_weighted: 0.33499957988801726
[2m[36m(func pid=68792)[0m f1_per_class: [0.23, 0.23, 0.286, 0.437, 0.074, 0.0, 0.537, 0.0, 0.121, 0.207]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-06 23:56:45 (running for 00:03:54.42)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.608 |      0.101 |                   33 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  2.031 |      0.212 |                   35 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.747 |      0.3   |                   35 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 43.987 |      0.151 |                   33 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.396455223880597
[2m[36m(func pid=69189)[0m top5: 0.9211753731343284
[2m[36m(func pid=69189)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=69189)[0m f1_macro: 0.29950371728678526
[2m[36m(func pid=69189)[0m f1_weighted: 0.41566336081276223
[2m[36m(func pid=69189)[0m f1_per_class: [0.255, 0.279, 0.368, 0.507, 0.082, 0.347, 0.514, 0.358, 0.053, 0.231]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.1865671641791045
[2m[36m(func pid=69627)[0m top5: 0.7238805970149254
[2m[36m(func pid=69627)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=69627)[0m f1_macro: 0.1764361602140625
[2m[36m(func pid=69627)[0m f1_weighted: 0.1658028453499892
[2m[36m(func pid=69627)[0m f1_per_class: [0.067, 0.096, 0.282, 0.209, 0.0, 0.523, 0.0, 0.435, 0.051, 0.101]
[2m[36m(func pid=68418)[0m top1: 0.2271455223880597
[2m[36m(func pid=68418)[0m top5: 0.6898320895522388
[2m[36m(func pid=68418)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=68418)[0m f1_macro: 0.0985794189410699
[2m[36m(func pid=68418)[0m f1_weighted: 0.15381140005723118
[2m[36m(func pid=68418)[0m f1_per_class: [0.096, 0.016, 0.323, 0.0, 0.0, 0.0, 0.486, 0.0, 0.065, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.9096 | Steps: 2 | Val loss: 1.9762 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0965 | Steps: 2 | Val loss: 2.2497 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 15.1714 | Steps: 2 | Val loss: 30.9324 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5768 | Steps: 2 | Val loss: 2.1986 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=68792)[0m top1: 0.355410447761194
[2m[36m(func pid=68792)[0m top5: 0.8955223880597015
[2m[36m(func pid=68792)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.2167039507081264
[2m[36m(func pid=68792)[0m f1_weighted: 0.34690276425717864
[2m[36m(func pid=68792)[0m f1_per_class: [0.209, 0.189, 0.324, 0.476, 0.075, 0.068, 0.543, 0.0, 0.083, 0.2]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-06 23:56:50 (running for 00:03:59.44)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.555 |      0.099 |                   34 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.91  |      0.217 |                   36 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.096 |      0.288 |                   36 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 17.742 |      0.176 |                   34 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.3628731343283582
[2m[36m(func pid=69189)[0m top5: 0.9034514925373134
[2m[36m(func pid=69189)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=69189)[0m f1_macro: 0.287670425771214
[2m[36m(func pid=69189)[0m f1_weighted: 0.3718522308902689
[2m[36m(func pid=69189)[0m f1_per_class: [0.24, 0.075, 0.364, 0.532, 0.066, 0.337, 0.438, 0.451, 0.151, 0.222]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.17164179104477612
[2m[36m(func pid=69627)[0m top5: 0.8236940298507462
[2m[36m(func pid=69627)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=69627)[0m f1_macro: 0.1757765987522867
[2m[36m(func pid=69627)[0m f1_weighted: 0.21928141776882737
[2m[36m(func pid=69627)[0m f1_per_class: [0.182, 0.174, 0.364, 0.203, 0.0, 0.0, 0.333, 0.471, 0.0, 0.032]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m top1: 0.23041044776119404
[2m[36m(func pid=68418)[0m top5: 0.7098880597014925
[2m[36m(func pid=68418)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=68418)[0m f1_macro: 0.09972804769993106
[2m[36m(func pid=68418)[0m f1_weighted: 0.15177181807449286
[2m[36m(func pid=68418)[0m f1_per_class: [0.103, 0.011, 0.333, 0.0, 0.0, 0.0, 0.482, 0.0, 0.069, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8458 | Steps: 2 | Val loss: 2.0067 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5048 | Steps: 2 | Val loss: 2.5669 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 23.3355 | Steps: 2 | Val loss: 25.0591 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6187 | Steps: 2 | Val loss: 2.1833 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=68792)[0m top1: 0.34794776119402987
[2m[36m(func pid=68792)[0m top5: 0.8857276119402985
[2m[36m(func pid=68792)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=68792)[0m f1_macro: 0.22277760827862664
[2m[36m(func pid=68792)[0m f1_weighted: 0.34067538803884617
[2m[36m(func pid=68792)[0m f1_per_class: [0.168, 0.083, 0.355, 0.489, 0.077, 0.181, 0.525, 0.016, 0.093, 0.242]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.2994402985074627
[2m[36m(func pid=69189)[0m top5: 0.871268656716418
[2m[36m(func pid=69189)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=69189)[0m f1_macro: 0.27118603431971816
[2m[36m(func pid=69189)[0m f1_weighted: 0.3053086375422826
[2m[36m(func pid=69189)[0m f1_per_class: [0.324, 0.042, 0.373, 0.486, 0.061, 0.343, 0.271, 0.458, 0.125, 0.23]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:56:56 (running for 00:04:05.07)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.577 |      0.1   |                   35 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.846 |      0.223 |                   37 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.505 |      0.271 |                   37 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 23.336 |      0.239 |                   36 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m top1: 0.37966417910447764
[2m[36m(func pid=69627)[0m top5: 0.8129664179104478
[2m[36m(func pid=69627)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=69627)[0m f1_macro: 0.2391036516352592
[2m[36m(func pid=69627)[0m f1_weighted: 0.3609899187427756
[2m[36m(func pid=69627)[0m f1_per_class: [0.223, 0.433, 0.4, 0.379, 0.167, 0.0, 0.551, 0.119, 0.0, 0.119]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m top1: 0.23880597014925373
[2m[36m(func pid=68418)[0m top5: 0.7276119402985075
[2m[36m(func pid=68418)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=68418)[0m f1_macro: 0.10264547121365372
[2m[36m(func pid=68418)[0m f1_weighted: 0.15268429702075667
[2m[36m(func pid=68418)[0m f1_per_class: [0.103, 0.011, 0.364, 0.0, 0.0, 0.0, 0.484, 0.0, 0.065, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7570 | Steps: 2 | Val loss: 2.0462 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6556 | Steps: 2 | Val loss: 2.9504 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 8.4606 | Steps: 2 | Val loss: 26.9855 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5378 | Steps: 2 | Val loss: 2.1693 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=68792)[0m top1: 0.3376865671641791
[2m[36m(func pid=68792)[0m top5: 0.8805970149253731
[2m[36m(func pid=68792)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=68792)[0m f1_macro: 0.24029716364426626
[2m[36m(func pid=68792)[0m f1_weighted: 0.3400148113483012
[2m[36m(func pid=68792)[0m f1_per_class: [0.145, 0.011, 0.415, 0.473, 0.078, 0.32, 0.504, 0.163, 0.024, 0.27]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.22294776119402984
[2m[36m(func pid=69189)[0m top5: 0.8372201492537313
[2m[36m(func pid=69189)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=69189)[0m f1_macro: 0.22872991187411493
[2m[36m(func pid=69189)[0m f1_weighted: 0.21277824738684412
[2m[36m(func pid=69189)[0m f1_per_class: [0.317, 0.016, 0.306, 0.366, 0.067, 0.353, 0.088, 0.45, 0.129, 0.195]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.386660447761194
[2m[36m(func pid=69627)[0m top5: 0.8092350746268657
[2m[36m(func pid=69627)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=69627)[0m f1_macro: 0.24598908449971307
[2m[36m(func pid=69627)[0m f1_weighted: 0.36720349742815755
[2m[36m(func pid=69627)[0m f1_per_class: [0.215, 0.41, 0.4, 0.431, 0.114, 0.0, 0.52, 0.208, 0.0, 0.162]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:57:01 (running for 00:04:10.85)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.538 |      0.102 |                   37 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.757 |      0.24  |                   38 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.656 |      0.229 |                   38 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  8.461 |      0.246 |                   37 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.2453358208955224
[2m[36m(func pid=68418)[0m top5: 0.7458022388059702
[2m[36m(func pid=68418)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=68418)[0m f1_macro: 0.10237788776688209
[2m[36m(func pid=68418)[0m f1_weighted: 0.15285491134513338
[2m[36m(func pid=68418)[0m f1_per_class: [0.115, 0.011, 0.353, 0.0, 0.0, 0.0, 0.485, 0.0, 0.061, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.7667 | Steps: 2 | Val loss: 2.0810 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8500 | Steps: 2 | Val loss: 3.1424 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 25.0703 | Steps: 2 | Val loss: 32.7554 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5488 | Steps: 2 | Val loss: 2.1555 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=68792)[0m top1: 0.32276119402985076
[2m[36m(func pid=68792)[0m top5: 0.855410447761194
[2m[36m(func pid=68792)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=68792)[0m f1_macro: 0.2552927278059684
[2m[36m(func pid=68792)[0m f1_weighted: 0.3319356575289415
[2m[36m(func pid=68792)[0m f1_per_class: [0.138, 0.005, 0.367, 0.468, 0.079, 0.36, 0.414, 0.461, 0.027, 0.233]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.20475746268656717
[2m[36m(func pid=69189)[0m top5: 0.8190298507462687
[2m[36m(func pid=69189)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=69189)[0m f1_macro: 0.20117410927560062
[2m[36m(func pid=69189)[0m f1_weighted: 0.1849282856258245
[2m[36m(func pid=69189)[0m f1_per_class: [0.154, 0.016, 0.272, 0.302, 0.082, 0.378, 0.054, 0.47, 0.126, 0.158]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.29384328358208955
[2m[36m(func pid=69627)[0m top5: 0.7887126865671642
[2m[36m(func pid=69627)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=69627)[0m f1_macro: 0.22400044195103877
[2m[36m(func pid=69627)[0m f1_weighted: 0.2619317624997579
[2m[36m(func pid=69627)[0m f1_per_class: [0.315, 0.158, 0.4, 0.504, 0.051, 0.0, 0.181, 0.476, 0.078, 0.077]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m top1: 0.2537313432835821
[2m[36m(func pid=68418)[0m top5: 0.7705223880597015
[2m[36m(func pid=68418)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=68418)[0m f1_macro: 0.11302202263854903
[2m[36m(func pid=68418)[0m f1_weighted: 0.15682447025459956
[2m[36m(func pid=68418)[0m f1_per_class: [0.125, 0.027, 0.421, 0.0, 0.0, 0.0, 0.486, 0.0, 0.072, 0.0]
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:57:07 (running for 00:04:16.14)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.549 |      0.113 |                   38 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.767 |      0.255 |                   39 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.85  |      0.201 |                   39 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 25.07  |      0.224 |                   38 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7390 | Steps: 2 | Val loss: 2.1302 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.0391 | Steps: 2 | Val loss: 2.9731 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 17.5482 | Steps: 2 | Val loss: 44.8276 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=68792)[0m top1: 0.2667910447761194
[2m[36m(func pid=68792)[0m top5: 0.8157649253731343
[2m[36m(func pid=68792)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.21791633121908954
[2m[36m(func pid=68792)[0m f1_weighted: 0.22904228254046788
[2m[36m(func pid=68792)[0m f1_per_class: [0.138, 0.011, 0.367, 0.462, 0.075, 0.385, 0.071, 0.425, 0.0, 0.245]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6223 | Steps: 2 | Val loss: 2.1460 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=69189)[0m top1: 0.26259328358208955
[2m[36m(func pid=69189)[0m top5: 0.8414179104477612
[2m[36m(func pid=69189)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=69189)[0m f1_macro: 0.24494341240626222
[2m[36m(func pid=69189)[0m f1_weighted: 0.2888040201758661
[2m[36m(func pid=69189)[0m f1_per_class: [0.194, 0.19, 0.22, 0.329, 0.106, 0.391, 0.268, 0.487, 0.119, 0.146]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.22014925373134328
[2m[36m(func pid=69627)[0m top5: 0.7691231343283582
[2m[36m(func pid=69627)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=69627)[0m f1_macro: 0.19547498171074532
[2m[36m(func pid=69627)[0m f1_weighted: 0.18411887471133212
[2m[36m(func pid=69627)[0m f1_per_class: [0.274, 0.052, 0.432, 0.481, 0.035, 0.0, 0.0, 0.471, 0.133, 0.077]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:57:12 (running for 00:04:21.25)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.622 |      0.113 |                   39 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.739 |      0.218 |                   40 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.039 |      0.245 |                   40 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 17.548 |      0.195 |                   39 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.25886194029850745
[2m[36m(func pid=68418)[0m top5: 0.7877798507462687
[2m[36m(func pid=68418)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=68418)[0m f1_macro: 0.11251157314552859
[2m[36m(func pid=68418)[0m f1_weighted: 0.15975156505951907
[2m[36m(func pid=68418)[0m f1_per_class: [0.134, 0.037, 0.375, 0.0, 0.0, 0.0, 0.488, 0.0, 0.092, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7733 | Steps: 2 | Val loss: 2.1890 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4419 | Steps: 2 | Val loss: 2.8788 | Batch size: 32 | lr: 0.01 | Duration: 2.57s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 21.9763 | Steps: 2 | Val loss: 52.4152 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5564 | Steps: 2 | Val loss: 2.1302 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=68792)[0m top1: 0.24113805970149255
[2m[36m(func pid=68792)[0m top5: 0.7887126865671642
[2m[36m(func pid=68792)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=68792)[0m f1_macro: 0.20182272526355954
[2m[36m(func pid=68792)[0m f1_weighted: 0.19799691167930789
[2m[36m(func pid=68792)[0m f1_per_class: [0.137, 0.021, 0.407, 0.438, 0.078, 0.382, 0.0, 0.351, 0.0, 0.203]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.310634328358209
[2m[36m(func pid=69189)[0m top5: 0.851679104477612
[2m[36m(func pid=69189)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=69189)[0m f1_macro: 0.25713266045485045
[2m[36m(func pid=69189)[0m f1_weighted: 0.33903063023595154
[2m[36m(func pid=69189)[0m f1_per_class: [0.102, 0.418, 0.212, 0.204, 0.149, 0.422, 0.439, 0.358, 0.124, 0.143]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.13712686567164178
[2m[36m(func pid=69627)[0m top5: 0.7527985074626866
[2m[36m(func pid=69627)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=69627)[0m f1_macro: 0.17153527601914667
[2m[36m(func pid=69627)[0m f1_weighted: 0.1215538803748184
[2m[36m(func pid=69627)[0m f1_per_class: [0.267, 0.016, 0.444, 0.277, 0.033, 0.0, 0.0, 0.496, 0.105, 0.077]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m top1: 0.26259328358208955
[2m[36m(func pid=68418)[0m top5: 0.8083022388059702
[2m[36m(func pid=68418)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=68418)[0m f1_macro: 0.10931209454386745
[2m[36m(func pid=68418)[0m f1_weighted: 0.1623455853967776
[2m[36m(func pid=68418)[0m f1_per_class: [0.141, 0.05, 0.34, 0.0, 0.0, 0.0, 0.491, 0.0, 0.071, 0.0]
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:57:17 (running for 00:04:26.38)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.556 |      0.109 |                   40 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.773 |      0.202 |                   41 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.442 |      0.257 |                   41 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 21.976 |      0.172 |                   40 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7207 | Steps: 2 | Val loss: 2.2357 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4638 | Steps: 2 | Val loss: 3.0313 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 20.8888 | Steps: 2 | Val loss: 47.2035 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=68792)[0m top1: 0.22761194029850745
[2m[36m(func pid=68792)[0m top5: 0.7709888059701493
[2m[36m(func pid=68792)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=68792)[0m f1_macro: 0.1970929345903885
[2m[36m(func pid=68792)[0m f1_weighted: 0.19647231405896468
[2m[36m(func pid=68792)[0m f1_per_class: [0.141, 0.074, 0.393, 0.416, 0.075, 0.362, 0.0, 0.312, 0.0, 0.197]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4804 | Steps: 2 | Val loss: 2.1187 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=69189)[0m top1: 0.324160447761194
[2m[36m(func pid=69189)[0m top5: 0.8521455223880597
[2m[36m(func pid=69189)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=69189)[0m f1_macro: 0.2344955332283484
[2m[36m(func pid=69189)[0m f1_weighted: 0.30553158552201093
[2m[36m(func pid=69189)[0m f1_per_class: [0.089, 0.484, 0.216, 0.038, 0.174, 0.439, 0.473, 0.227, 0.027, 0.179]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.15345149253731344
[2m[36m(func pid=69627)[0m top5: 0.753731343283582
[2m[36m(func pid=69627)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=69627)[0m f1_macro: 0.16400507697701885
[2m[36m(func pid=69627)[0m f1_weighted: 0.13293831472701745
[2m[36m(func pid=69627)[0m f1_per_class: [0.2, 0.0, 0.41, 0.338, 0.035, 0.0, 0.0, 0.476, 0.104, 0.077]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:57:22 (running for 00:04:31.57)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.48  |      0.115 |                   41 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.721 |      0.197 |                   42 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.464 |      0.234 |                   42 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 20.889 |      0.164 |                   41 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.2667910447761194
[2m[36m(func pid=68418)[0m top5: 0.8227611940298507
[2m[36m(func pid=68418)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=68418)[0m f1_macro: 0.1150210340054167
[2m[36m(func pid=68418)[0m f1_weighted: 0.1675893424805846
[2m[36m(func pid=68418)[0m f1_per_class: [0.152, 0.084, 0.361, 0.0, 0.0, 0.0, 0.489, 0.0, 0.065, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.7021 | Steps: 2 | Val loss: 2.2722 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5883 | Steps: 2 | Val loss: 3.2002 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 11.9743 | Steps: 2 | Val loss: 33.9347 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=68792)[0m top1: 0.21082089552238806
[2m[36m(func pid=68792)[0m top5: 0.7569962686567164
[2m[36m(func pid=68792)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=68792)[0m f1_macro: 0.18997101294310098
[2m[36m(func pid=68792)[0m f1_weighted: 0.19263945495147936
[2m[36m(func pid=68792)[0m f1_per_class: [0.146, 0.158, 0.349, 0.362, 0.073, 0.351, 0.0, 0.275, 0.027, 0.158]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4805 | Steps: 2 | Val loss: 2.1108 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=69189)[0m top1: 0.3414179104477612
[2m[36m(func pid=69189)[0m top5: 0.8577425373134329
[2m[36m(func pid=69189)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=69189)[0m f1_macro: 0.23011832220093836
[2m[36m(func pid=69189)[0m f1_weighted: 0.29721280862801375
[2m[36m(func pid=69189)[0m f1_per_class: [0.106, 0.496, 0.214, 0.01, 0.203, 0.447, 0.468, 0.201, 0.0, 0.156]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.2453358208955224
[2m[36m(func pid=69627)[0m top5: 0.8083022388059702
[2m[36m(func pid=69627)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=69627)[0m f1_macro: 0.2086730910084099
[2m[36m(func pid=69627)[0m f1_weighted: 0.2352298837833325
[2m[36m(func pid=69627)[0m f1_per_class: [0.138, 0.0, 0.419, 0.454, 0.055, 0.256, 0.153, 0.417, 0.074, 0.121]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m top1: 0.2737873134328358
[2m[36m(func pid=68418)[0m top5: 0.8353544776119403
[2m[36m(func pid=68418)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=68418)[0m f1_macro: 0.11618880928570127
[2m[36m(func pid=68418)[0m f1_weighted: 0.1792789343318337
[2m[36m(func pid=68418)[0m f1_per_class: [0.161, 0.148, 0.297, 0.0, 0.0, 0.0, 0.492, 0.0, 0.063, 0.0]
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:57:27 (running for 00:04:36.82)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   42 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.702 |      0.19  |                   43 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.588 |      0.23  |                   43 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 11.974 |      0.209 |                   42 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6850 | Steps: 2 | Val loss: 2.3095 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4984 | Steps: 2 | Val loss: 3.1726 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 8.3836 | Steps: 2 | Val loss: 27.2740 | Batch size: 32 | lr: 0.1 | Duration: 2.58s
[2m[36m(func pid=68792)[0m top1: 0.18936567164179105
[2m[36m(func pid=68792)[0m top5: 0.7658582089552238
[2m[36m(func pid=68792)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=68792)[0m f1_macro: 0.1827798483927096
[2m[36m(func pid=68792)[0m f1_weighted: 0.17056335951815316
[2m[36m(func pid=68792)[0m f1_per_class: [0.177, 0.241, 0.328, 0.237, 0.069, 0.339, 0.0, 0.258, 0.048, 0.13]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.341884328358209
[2m[36m(func pid=69189)[0m top5: 0.8722014925373134
[2m[36m(func pid=69189)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=69189)[0m f1_macro: 0.23051411368811853
[2m[36m(func pid=69189)[0m f1_weighted: 0.3040848193090387
[2m[36m(func pid=69189)[0m f1_per_class: [0.128, 0.476, 0.204, 0.052, 0.096, 0.392, 0.464, 0.286, 0.049, 0.158]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4663 | Steps: 2 | Val loss: 2.1054 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=69627)[0m top1: 0.35447761194029853
[2m[36m(func pid=69627)[0m top5: 0.8833955223880597
[2m[36m(func pid=69627)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=69627)[0m f1_macro: 0.23239776486847105
[2m[36m(func pid=69627)[0m f1_weighted: 0.33519321716934375
[2m[36m(func pid=69627)[0m f1_per_class: [0.161, 0.047, 0.36, 0.484, 0.174, 0.458, 0.435, 0.016, 0.0, 0.188]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5018 | Steps: 2 | Val loss: 3.0826 | Batch size: 32 | lr: 0.01 | Duration: 2.56s
== Status ==
Current time: 2024-01-06 23:57:33 (running for 00:04:42.00)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.466 |      0.119 |                   43 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.685 |      0.183 |                   44 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.498 |      0.231 |                   44 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  8.384 |      0.232 |                   43 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.28125
[2m[36m(func pid=68418)[0m top5: 0.8418843283582089
[2m[36m(func pid=68418)[0m f1_micro: 0.28125
[2m[36m(func pid=68418)[0m f1_macro: 0.11911670055887587
[2m[36m(func pid=68418)[0m f1_weighted: 0.19250643501247744
[2m[36m(func pid=68418)[0m f1_per_class: [0.16, 0.194, 0.256, 0.007, 0.0, 0.008, 0.501, 0.0, 0.065, 0.0]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.6649 | Steps: 2 | Val loss: 2.3236 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 10.7520 | Steps: 2 | Val loss: 27.4462 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=69189)[0m top1: 0.3670708955223881
[2m[36m(func pid=69189)[0m top5: 0.8656716417910447
[2m[36m(func pid=69189)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=69189)[0m f1_macro: 0.27939945798268045
[2m[36m(func pid=69189)[0m f1_weighted: 0.36148021259679547
[2m[36m(func pid=69189)[0m f1_per_class: [0.161, 0.531, 0.185, 0.311, 0.128, 0.366, 0.35, 0.424, 0.15, 0.188]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68792)[0m top1: 0.1730410447761194
[2m[36m(func pid=68792)[0m top5: 0.7691231343283582
[2m[36m(func pid=68792)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.17496287272119795
[2m[36m(func pid=68792)[0m f1_weighted: 0.14222706650011613
[2m[36m(func pid=68792)[0m f1_per_class: [0.229, 0.278, 0.319, 0.129, 0.062, 0.283, 0.0, 0.262, 0.08, 0.108]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4378 | Steps: 2 | Val loss: 2.1005 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=69627)[0m top1: 0.353544776119403
[2m[36m(func pid=69627)[0m top5: 0.9118470149253731
[2m[36m(func pid=69627)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=69627)[0m f1_macro: 0.23739678894233324
[2m[36m(func pid=69627)[0m f1_weighted: 0.3558715767167346
[2m[36m(func pid=69627)[0m f1_per_class: [0.245, 0.305, 0.373, 0.446, 0.1, 0.408, 0.414, 0.0, 0.0, 0.083]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6841 | Steps: 2 | Val loss: 2.3306 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4652 | Steps: 2 | Val loss: 3.2132 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=68418)[0m top1: 0.29244402985074625
[2m[36m(func pid=68418)[0m top5: 0.8549440298507462
[2m[36m(func pid=68418)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=68418)[0m f1_macro: 0.12320844410714213
[2m[36m(func pid=68418)[0m f1_weighted: 0.21160068159676598
[2m[36m(func pid=68418)[0m f1_per_class: [0.143, 0.242, 0.229, 0.032, 0.0, 0.008, 0.515, 0.0, 0.062, 0.0]
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:57:38 (running for 00:04:47.05)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.438 |      0.123 |                   44 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.665 |      0.175 |                   45 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.502 |      0.279 |                   45 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 10.752 |      0.237 |                   44 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 9.9337 | Steps: 2 | Val loss: 27.6820 | Batch size: 32 | lr: 0.1 | Duration: 2.53s
[2m[36m(func pid=68792)[0m top1: 0.15811567164179105
[2m[36m(func pid=68792)[0m top5: 0.7765858208955224
[2m[36m(func pid=68792)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=68792)[0m f1_macro: 0.16232870590306564
[2m[36m(func pid=68792)[0m f1_weighted: 0.11773635053711169
[2m[36m(func pid=68792)[0m f1_per_class: [0.247, 0.277, 0.289, 0.06, 0.058, 0.225, 0.0, 0.28, 0.096, 0.091]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.373134328358209
[2m[36m(func pid=69189)[0m top5: 0.8390858208955224
[2m[36m(func pid=69189)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=69189)[0m f1_macro: 0.28054261044813533
[2m[36m(func pid=69189)[0m f1_weighted: 0.3568919204582425
[2m[36m(func pid=69189)[0m f1_per_class: [0.18, 0.536, 0.158, 0.5, 0.118, 0.341, 0.149, 0.511, 0.135, 0.176]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.34328358208955223
[2m[36m(func pid=69627)[0m top5: 0.917910447761194
[2m[36m(func pid=69627)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=69627)[0m f1_macro: 0.2320290333121191
[2m[36m(func pid=69627)[0m f1_weighted: 0.3502550573342398
[2m[36m(func pid=69627)[0m f1_per_class: [0.286, 0.489, 0.344, 0.31, 0.0, 0.418, 0.414, 0.0, 0.0, 0.061]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4247 | Steps: 2 | Val loss: 2.0946 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.6693 | Steps: 2 | Val loss: 2.3357 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.9364 | Steps: 2 | Val loss: 3.4897 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-06 23:57:43 (running for 00:04:52.24)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.425 |      0.143 |                   45 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.684 |      0.162 |                   46 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.465 |      0.281 |                   46 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  9.934 |      0.232 |                   45 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.30177238805970147
[2m[36m(func pid=68418)[0m top5: 0.8689365671641791
[2m[36m(func pid=68418)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=68418)[0m f1_macro: 0.14349222061683467
[2m[36m(func pid=68418)[0m f1_weighted: 0.2344188994318444
[2m[36m(func pid=68418)[0m f1_per_class: [0.142, 0.248, 0.22, 0.093, 0.049, 0.008, 0.527, 0.0, 0.071, 0.077]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 15.3065 | Steps: 2 | Val loss: 26.0199 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=68792)[0m top1: 0.15065298507462688
[2m[36m(func pid=68792)[0m top5: 0.7784514925373134
[2m[36m(func pid=68792)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=68792)[0m f1_macro: 0.15532152914956493
[2m[36m(func pid=68792)[0m f1_weighted: 0.10490783323460294
[2m[36m(func pid=68792)[0m f1_per_class: [0.242, 0.271, 0.265, 0.032, 0.056, 0.166, 0.0, 0.314, 0.126, 0.081]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3316231343283582
[2m[36m(func pid=69189)[0m top5: 0.8348880597014925
[2m[36m(func pid=69189)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=69189)[0m f1_macro: 0.25515949174570507
[2m[36m(func pid=69189)[0m f1_weighted: 0.30096541157955814
[2m[36m(func pid=69189)[0m f1_per_class: [0.197, 0.374, 0.167, 0.529, 0.086, 0.296, 0.039, 0.514, 0.183, 0.167]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.3591417910447761
[2m[36m(func pid=69627)[0m top5: 0.9043843283582089
[2m[36m(func pid=69627)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=69627)[0m f1_macro: 0.2299257196122606
[2m[36m(func pid=69627)[0m f1_weighted: 0.33592983934073334
[2m[36m(func pid=69627)[0m f1_per_class: [0.295, 0.512, 0.319, 0.198, 0.0, 0.463, 0.439, 0.0, 0.0, 0.073]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4273 | Steps: 2 | Val loss: 2.0939 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.6710 | Steps: 2 | Val loss: 2.3183 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4199 | Steps: 2 | Val loss: 3.5162 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 12.1935 | Steps: 2 | Val loss: 25.9752 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
== Status ==
Current time: 2024-01-06 23:57:48 (running for 00:04:57.38)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.427 |      0.159 |                   46 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.669 |      0.155 |                   47 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  2.936 |      0.255 |                   47 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 15.306 |      0.23  |                   46 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.31949626865671643
[2m[36m(func pid=68418)[0m top5: 0.8815298507462687
[2m[36m(func pid=68418)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=68418)[0m f1_macro: 0.1589615356856185
[2m[36m(func pid=68418)[0m f1_weighted: 0.27963871197477225
[2m[36m(func pid=68418)[0m f1_per_class: [0.16, 0.259, 0.202, 0.215, 0.045, 0.016, 0.555, 0.0, 0.064, 0.074]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.14878731343283583
[2m[36m(func pid=68792)[0m top5: 0.7877798507462687
[2m[36m(func pid=68792)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=68792)[0m f1_macro: 0.15835668458474494
[2m[36m(func pid=68792)[0m f1_weighted: 0.10453615269420964
[2m[36m(func pid=68792)[0m f1_per_class: [0.259, 0.272, 0.265, 0.026, 0.055, 0.153, 0.0, 0.353, 0.132, 0.068]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.26632462686567165
[2m[36m(func pid=69189)[0m top5: 0.8404850746268657
[2m[36m(func pid=69189)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=69189)[0m f1_macro: 0.21744197786369415
[2m[36m(func pid=69189)[0m f1_weighted: 0.25276247220044507
[2m[36m(func pid=69189)[0m f1_per_class: [0.134, 0.181, 0.168, 0.486, 0.065, 0.292, 0.048, 0.476, 0.123, 0.2]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.3656716417910448
[2m[36m(func pid=69627)[0m top5: 0.8689365671641791
[2m[36m(func pid=69627)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=69627)[0m f1_macro: 0.24197910105122578
[2m[36m(func pid=69627)[0m f1_weighted: 0.33356471376728103
[2m[36m(func pid=69627)[0m f1_per_class: [0.269, 0.493, 0.297, 0.136, 0.0, 0.487, 0.45, 0.222, 0.0, 0.065]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4121 | Steps: 2 | Val loss: 2.0944 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5976 | Steps: 2 | Val loss: 2.2915 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4187 | Steps: 2 | Val loss: 3.6458 | Batch size: 32 | lr: 0.01 | Duration: 2.57s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 12.3772 | Steps: 2 | Val loss: 27.7579 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-06 23:57:53 (running for 00:05:02.75)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.412 |      0.169 |                   47 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.671 |      0.158 |                   48 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.42  |      0.217 |                   48 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 12.193 |      0.242 |                   47 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.3162313432835821
[2m[36m(func pid=68418)[0m top5: 0.8833955223880597
[2m[36m(func pid=68418)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=68418)[0m f1_macro: 0.1693498918633412
[2m[36m(func pid=68418)[0m f1_weighted: 0.3019032597407551
[2m[36m(func pid=68418)[0m f1_per_class: [0.136, 0.264, 0.183, 0.299, 0.056, 0.037, 0.542, 0.0, 0.032, 0.143]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m top1: 0.21548507462686567
[2m[36m(func pid=69189)[0m top5: 0.835820895522388
[2m[36m(func pid=69189)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=69189)[0m f1_macro: 0.20738662598894222
[2m[36m(func pid=69189)[0m f1_weighted: 0.23096874894624472
[2m[36m(func pid=69189)[0m f1_per_class: [0.08, 0.151, 0.179, 0.331, 0.061, 0.312, 0.134, 0.455, 0.154, 0.217]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68792)[0m top1: 0.14738805970149255
[2m[36m(func pid=68792)[0m top5: 0.8050373134328358
[2m[36m(func pid=68792)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=68792)[0m f1_macro: 0.15719509063628628
[2m[36m(func pid=68792)[0m f1_weighted: 0.11066593448355595
[2m[36m(func pid=68792)[0m f1_per_class: [0.208, 0.279, 0.256, 0.042, 0.053, 0.15, 0.0, 0.387, 0.132, 0.065]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69627)[0m top1: 0.2849813432835821
[2m[36m(func pid=69627)[0m top5: 0.8041044776119403
[2m[36m(func pid=69627)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=69627)[0m f1_macro: 0.22107688839334147
[2m[36m(func pid=69627)[0m f1_weighted: 0.2211026104522215
[2m[36m(func pid=69627)[0m f1_per_class: [0.258, 0.479, 0.272, 0.191, 0.0, 0.422, 0.015, 0.345, 0.14, 0.088]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3861 | Steps: 2 | Val loss: 2.0953 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.8752 | Steps: 2 | Val loss: 2.2727 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4677 | Steps: 2 | Val loss: 3.6399 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 18.8929 | Steps: 2 | Val loss: 30.7900 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-06 23:57:59 (running for 00:05:08.06)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.386 |      0.164 |                   48 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.598 |      0.157 |                   49 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.419 |      0.207 |                   49 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 12.377 |      0.221 |                   48 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.302705223880597
[2m[36m(func pid=68418)[0m top5: 0.882929104477612
[2m[36m(func pid=68418)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=68418)[0m f1_macro: 0.16409090558258538
[2m[36m(func pid=68418)[0m f1_weighted: 0.30710219529290833
[2m[36m(func pid=68418)[0m f1_per_class: [0.108, 0.241, 0.175, 0.365, 0.052, 0.063, 0.509, 0.0, 0.0, 0.129]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.15065298507462688
[2m[36m(func pid=68792)[0m top5: 0.8115671641791045
[2m[36m(func pid=68792)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=68792)[0m f1_macro: 0.1659591243985204
[2m[36m(func pid=68792)[0m f1_weighted: 0.13311300963647593
[2m[36m(func pid=68792)[0m f1_per_class: [0.182, 0.27, 0.244, 0.064, 0.049, 0.173, 0.045, 0.434, 0.122, 0.076]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.2196828358208955
[2m[36m(func pid=69189)[0m top5: 0.8330223880597015
[2m[36m(func pid=69189)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=69189)[0m f1_macro: 0.21342628748447218
[2m[36m(func pid=69189)[0m f1_weighted: 0.2494052230439848
[2m[36m(func pid=69189)[0m f1_per_class: [0.066, 0.113, 0.196, 0.186, 0.065, 0.35, 0.34, 0.474, 0.102, 0.241]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.2658582089552239
[2m[36m(func pid=69627)[0m top5: 0.784981343283582
[2m[36m(func pid=69627)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=69627)[0m f1_macro: 0.20854043652810636
[2m[36m(func pid=69627)[0m f1_weighted: 0.21554116451324948
[2m[36m(func pid=69627)[0m f1_per_class: [0.241, 0.507, 0.262, 0.251, 0.0, 0.222, 0.0, 0.349, 0.148, 0.105]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4879 | Steps: 2 | Val loss: 2.0989 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.5999 | Steps: 2 | Val loss: 2.2331 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4957 | Steps: 2 | Val loss: 3.5087 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 13.6308 | Steps: 2 | Val loss: 29.6789 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
== Status ==
Current time: 2024-01-06 23:58:04 (running for 00:05:13.18)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.488 |      0.16  |                   49 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.875 |      0.166 |                   50 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.468 |      0.213 |                   50 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 18.893 |      0.209 |                   49 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.2798507462686567
[2m[36m(func pid=68418)[0m top5: 0.878731343283582
[2m[36m(func pid=68418)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=68418)[0m f1_macro: 0.15960409493438554
[2m[36m(func pid=68418)[0m f1_weighted: 0.30029941635466234
[2m[36m(func pid=68418)[0m f1_per_class: [0.066, 0.242, 0.175, 0.398, 0.047, 0.096, 0.44, 0.031, 0.0, 0.103]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69189)[0m top1: 0.2523320895522388
[2m[36m(func pid=69189)[0m top5: 0.820429104477612
[2m[36m(func pid=69189)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=69189)[0m f1_macro: 0.21791595505753253
[2m[36m(func pid=69189)[0m f1_weighted: 0.2771210479746349
[2m[36m(func pid=69189)[0m f1_per_class: [0.069, 0.226, 0.218, 0.085, 0.079, 0.382, 0.466, 0.434, 0.047, 0.174]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68792)[0m top1: 0.20475746268656717
[2m[36m(func pid=68792)[0m top5: 0.8260261194029851
[2m[36m(func pid=68792)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=68792)[0m f1_macro: 0.219117368865494
[2m[36m(func pid=68792)[0m f1_weighted: 0.23895492506844138
[2m[36m(func pid=68792)[0m f1_per_class: [0.275, 0.248, 0.259, 0.119, 0.05, 0.172, 0.338, 0.513, 0.127, 0.089]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69627)[0m top1: 0.27052238805970147
[2m[36m(func pid=69627)[0m top5: 0.7947761194029851
[2m[36m(func pid=69627)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=69627)[0m f1_macro: 0.23325134290168018
[2m[36m(func pid=69627)[0m f1_weighted: 0.2477018349639897
[2m[36m(func pid=69627)[0m f1_per_class: [0.361, 0.47, 0.31, 0.397, 0.0, 0.129, 0.009, 0.414, 0.122, 0.12]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3517 | Steps: 2 | Val loss: 2.1034 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5720 | Steps: 2 | Val loss: 2.1772 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3925 | Steps: 2 | Val loss: 3.1862 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 8.1763 | Steps: 2 | Val loss: 26.5331 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-06 23:58:09 (running for 00:05:18.39)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.352 |      0.161 |                   50 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.6   |      0.219 |                   51 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.496 |      0.218 |                   51 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 13.631 |      0.233 |                   50 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.2630597014925373
[2m[36m(func pid=68418)[0m top5: 0.8638059701492538
[2m[36m(func pid=68418)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=68418)[0m f1_macro: 0.16054198794345037
[2m[36m(func pid=68418)[0m f1_weighted: 0.28465171983378035
[2m[36m(func pid=68418)[0m f1_per_class: [0.068, 0.223, 0.173, 0.44, 0.04, 0.108, 0.341, 0.088, 0.0, 0.122]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.23973880597014927
[2m[36m(func pid=68792)[0m top5: 0.8586753731343284
[2m[36m(func pid=68792)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=68792)[0m f1_macro: 0.22480404480902352
[2m[36m(func pid=68792)[0m f1_weighted: 0.2886555619399781
[2m[36m(func pid=68792)[0m f1_per_class: [0.245, 0.224, 0.282, 0.22, 0.051, 0.187, 0.451, 0.36, 0.109, 0.118]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.31529850746268656
[2m[36m(func pid=69189)[0m top5: 0.8134328358208955
[2m[36m(func pid=69189)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=69189)[0m f1_macro: 0.2347927320601463
[2m[36m(func pid=69189)[0m f1_weighted: 0.3165744763238263
[2m[36m(func pid=69189)[0m f1_per_class: [0.103, 0.417, 0.239, 0.069, 0.11, 0.399, 0.525, 0.281, 0.024, 0.181]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.3474813432835821
[2m[36m(func pid=69627)[0m top5: 0.8260261194029851
[2m[36m(func pid=69627)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=69627)[0m f1_macro: 0.27673133965517943
[2m[36m(func pid=69627)[0m f1_weighted: 0.35489371927713637
[2m[36m(func pid=69627)[0m f1_per_class: [0.282, 0.392, 0.367, 0.554, 0.215, 0.115, 0.295, 0.283, 0.135, 0.13]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3575 | Steps: 2 | Val loss: 2.1097 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4990 | Steps: 2 | Val loss: 2.1246 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4251 | Steps: 2 | Val loss: 2.9613 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 4.0532 | Steps: 2 | Val loss: 27.7797 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-06 23:58:14 (running for 00:05:23.54)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.357 |      0.167 |                   51 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.572 |      0.225 |                   52 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.393 |      0.235 |                   52 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  8.176 |      0.277 |                   51 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.24300373134328357
[2m[36m(func pid=68418)[0m top5: 0.8353544776119403
[2m[36m(func pid=68418)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=68418)[0m f1_macro: 0.16688190969017808
[2m[36m(func pid=68418)[0m f1_weighted: 0.25713040666990417
[2m[36m(func pid=68418)[0m f1_per_class: [0.073, 0.215, 0.179, 0.451, 0.038, 0.081, 0.217, 0.275, 0.0, 0.14]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.27472014925373134
[2m[36m(func pid=68792)[0m top5: 0.8708022388059702
[2m[36m(func pid=68792)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=68792)[0m f1_macro: 0.2169414618093497
[2m[36m(func pid=68792)[0m f1_weighted: 0.3128867661343579
[2m[36m(func pid=68792)[0m f1_per_class: [0.235, 0.184, 0.324, 0.318, 0.053, 0.199, 0.509, 0.107, 0.098, 0.144]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3619402985074627
[2m[36m(func pid=69189)[0m top5: 0.8176305970149254
[2m[36m(func pid=69189)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=69189)[0m f1_macro: 0.24972125695339056
[2m[36m(func pid=69189)[0m f1_weighted: 0.32807718348886705
[2m[36m(func pid=69189)[0m f1_per_class: [0.15, 0.501, 0.268, 0.029, 0.16, 0.427, 0.543, 0.265, 0.0, 0.154]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.40205223880597013
[2m[36m(func pid=69627)[0m top5: 0.8763992537313433
[2m[36m(func pid=69627)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=69627)[0m f1_macro: 0.25803665958558414
[2m[36m(func pid=69627)[0m f1_weighted: 0.3885177744661619
[2m[36m(func pid=69627)[0m f1_per_class: [0.226, 0.29, 0.391, 0.568, 0.142, 0.161, 0.491, 0.0, 0.174, 0.137]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3765 | Steps: 2 | Val loss: 2.1166 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.5236 | Steps: 2 | Val loss: 31.7985 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.6140 | Steps: 2 | Val loss: 2.0932 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3948 | Steps: 2 | Val loss: 2.7989 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-06 23:58:19 (running for 00:05:28.85)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.377 |      0.159 |                   52 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.499 |      0.217 |                   53 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.425 |      0.25  |                   53 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  4.053 |      0.258 |                   52 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.2294776119402985
[2m[36m(func pid=68418)[0m top5: 0.7835820895522388
[2m[36m(func pid=68418)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=68418)[0m f1_macro: 0.1586608495304991
[2m[36m(func pid=68418)[0m f1_weighted: 0.21999233712938496
[2m[36m(func pid=68418)[0m f1_per_class: [0.074, 0.202, 0.179, 0.465, 0.036, 0.049, 0.073, 0.42, 0.0, 0.088]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3101679104477612
[2m[36m(func pid=68792)[0m top5: 0.8773320895522388
[2m[36m(func pid=68792)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=68792)[0m f1_macro: 0.21613033422371206
[2m[36m(func pid=68792)[0m f1_weighted: 0.3330353759519476
[2m[36m(func pid=68792)[0m f1_per_class: [0.202, 0.119, 0.361, 0.415, 0.057, 0.205, 0.539, 0.016, 0.101, 0.146]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3712686567164179
[2m[36m(func pid=69189)[0m top5: 0.8488805970149254
[2m[36m(func pid=69189)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=69189)[0m f1_macro: 0.2713643228510037
[2m[36m(func pid=69189)[0m f1_weighted: 0.3308212688351855
[2m[36m(func pid=69189)[0m f1_per_class: [0.217, 0.505, 0.314, 0.045, 0.207, 0.43, 0.519, 0.274, 0.073, 0.129]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.3582089552238806
[2m[36m(func pid=69627)[0m top5: 0.8810634328358209
[2m[36m(func pid=69627)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=69627)[0m f1_macro: 0.22230102016772882
[2m[36m(func pid=69627)[0m f1_weighted: 0.35165392631810216
[2m[36m(func pid=69627)[0m f1_per_class: [0.184, 0.119, 0.4, 0.531, 0.076, 0.171, 0.515, 0.0, 0.025, 0.201]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3280 | Steps: 2 | Val loss: 2.1271 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.5226 | Steps: 2 | Val loss: 2.0715 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3083 | Steps: 2 | Val loss: 2.7195 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 15.3273 | Steps: 2 | Val loss: 35.2676 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-06 23:58:25 (running for 00:05:34.00)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.328 |      0.153 |                   53 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.614 |      0.216 |                   54 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.395 |      0.271 |                   54 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  2.524 |      0.222 |                   53 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.22807835820895522
[2m[36m(func pid=68418)[0m top5: 0.7168843283582089
[2m[36m(func pid=68418)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=68418)[0m f1_macro: 0.15259564565669068
[2m[36m(func pid=68418)[0m f1_weighted: 0.20141429977219527
[2m[36m(func pid=68418)[0m f1_per_class: [0.075, 0.187, 0.173, 0.477, 0.036, 0.044, 0.009, 0.421, 0.0, 0.103]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.33348880597014924
[2m[36m(func pid=68792)[0m top5: 0.8656716417910447
[2m[36m(func pid=68792)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=68792)[0m f1_macro: 0.22106210926989425
[2m[36m(func pid=68792)[0m f1_weighted: 0.338189073422992
[2m[36m(func pid=68792)[0m f1_per_class: [0.187, 0.046, 0.4, 0.471, 0.063, 0.21, 0.545, 0.0, 0.099, 0.188]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3656716417910448
[2m[36m(func pid=69189)[0m top5: 0.8675373134328358
[2m[36m(func pid=69189)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=69189)[0m f1_macro: 0.2783677833162815
[2m[36m(func pid=69189)[0m f1_weighted: 0.3432283064427301
[2m[36m(func pid=69189)[0m f1_per_class: [0.149, 0.498, 0.308, 0.128, 0.18, 0.438, 0.465, 0.368, 0.117, 0.132]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m top1: 0.30597014925373134
[2m[36m(func pid=69627)[0m top5: 0.8759328358208955
[2m[36m(func pid=69627)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=69627)[0m f1_macro: 0.2048192643126021
[2m[36m(func pid=69627)[0m f1_weighted: 0.3144830640929486
[2m[36m(func pid=69627)[0m f1_per_class: [0.169, 0.021, 0.412, 0.496, 0.051, 0.218, 0.466, 0.0, 0.0, 0.217]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3904 | Steps: 2 | Val loss: 2.1389 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4550 | Steps: 2 | Val loss: 2.0471 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.1595 | Steps: 2 | Val loss: 38.6590 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4453 | Steps: 2 | Val loss: 2.7261 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=68418)[0m top1: 0.22621268656716417
[2m[36m(func pid=68418)[0m top5: 0.652518656716418
[2m[36m(func pid=68418)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=68418)[0m f1_macro: 0.1418506830634373
[2m[36m(func pid=68418)[0m f1_weighted: 0.1951776176657621
[2m[36m(func pid=68418)[0m f1_per_class: [0.078, 0.171, 0.16, 0.49, 0.037, 0.03, 0.0, 0.381, 0.0, 0.071]
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:58:30 (running for 00:05:39.36)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.39  |      0.142 |                   54 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.523 |      0.221 |                   55 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.308 |      0.278 |                   55 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      | 15.327 |      0.205 |                   54 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.34654850746268656
[2m[36m(func pid=68792)[0m top5: 0.8694029850746269
[2m[36m(func pid=68792)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=68792)[0m f1_macro: 0.22154119713574638
[2m[36m(func pid=68792)[0m f1_weighted: 0.3429869426781635
[2m[36m(func pid=68792)[0m f1_per_class: [0.174, 0.037, 0.383, 0.487, 0.071, 0.23, 0.546, 0.0, 0.096, 0.193]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69627)[0m top1: 0.259794776119403
[2m[36m(func pid=69627)[0m top5: 0.8395522388059702
[2m[36m(func pid=69627)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=69627)[0m f1_macro: 0.22820788507103185
[2m[36m(func pid=69627)[0m f1_weighted: 0.28255536467732156
[2m[36m(func pid=69627)[0m f1_per_class: [0.158, 0.052, 0.41, 0.464, 0.045, 0.267, 0.289, 0.276, 0.07, 0.25]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3596082089552239
[2m[36m(func pid=69189)[0m top5: 0.8652052238805971
[2m[36m(func pid=69189)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=69189)[0m f1_macro: 0.28115333678520416
[2m[36m(func pid=69189)[0m f1_weighted: 0.3439833616464236
[2m[36m(func pid=69189)[0m f1_per_class: [0.078, 0.506, 0.321, 0.235, 0.158, 0.444, 0.351, 0.411, 0.189, 0.118]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3711 | Steps: 2 | Val loss: 2.1541 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.4925 | Steps: 2 | Val loss: 2.0320 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.9593 | Steps: 2 | Val loss: 42.7786 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2959 | Steps: 2 | Val loss: 2.7281 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-06 23:58:35 (running for 00:05:44.55)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.39  |      0.142 |                   54 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.492 |      0.23  |                   57 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.445 |      0.281 |                   56 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.16  |      0.228 |                   55 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.2196828358208955
[2m[36m(func pid=68418)[0m top5: 0.6222014925373134
[2m[36m(func pid=68418)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=68418)[0m f1_macro: 0.13709855204906995
[2m[36m(func pid=68418)[0m f1_weighted: 0.18926634003597725
[2m[36m(func pid=68418)[0m f1_per_class: [0.113, 0.164, 0.15, 0.487, 0.038, 0.016, 0.0, 0.327, 0.0, 0.075]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3572761194029851
[2m[36m(func pid=68792)[0m top5: 0.8717350746268657
[2m[36m(func pid=68792)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=68792)[0m f1_macro: 0.23004672427778622
[2m[36m(func pid=68792)[0m f1_weighted: 0.35304684163397554
[2m[36m(func pid=68792)[0m f1_per_class: [0.164, 0.021, 0.39, 0.505, 0.075, 0.304, 0.543, 0.0, 0.108, 0.19]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69627)[0m top1: 0.23880597014925373
[2m[36m(func pid=69627)[0m top5: 0.7933768656716418
[2m[36m(func pid=69627)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=69627)[0m f1_macro: 0.2232538602600528
[2m[36m(func pid=69627)[0m f1_weighted: 0.22479127841073188
[2m[36m(func pid=69627)[0m f1_per_class: [0.169, 0.161, 0.385, 0.443, 0.049, 0.3, 0.006, 0.443, 0.115, 0.162]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=69189)[0m top1: 0.36380597014925375
[2m[36m(func pid=69189)[0m top5: 0.855410447761194
[2m[36m(func pid=69189)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=69189)[0m f1_macro: 0.2925624445093493
[2m[36m(func pid=69189)[0m f1_weighted: 0.34199380201979257
[2m[36m(func pid=69189)[0m f1_per_class: [0.118, 0.532, 0.346, 0.389, 0.157, 0.417, 0.183, 0.446, 0.208, 0.13]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3325 | Steps: 2 | Val loss: 2.1697 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.9605 | Steps: 2 | Val loss: 1.9935 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 6.9648 | Steps: 2 | Val loss: 45.7809 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.5952 | Steps: 2 | Val loss: 2.6642 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=69627)[0m top1: 0.24347014925373134
[2m[36m(func pid=69627)[0m top5: 0.7583955223880597
[2m[36m(func pid=69627)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=69627)[0m f1_macro: 0.22907335426485437
[2m[36m(func pid=69627)[0m f1_weighted: 0.22703119720192916
[2m[36m(func pid=69627)[0m f1_per_class: [0.2, 0.31, 0.373, 0.373, 0.061, 0.31, 0.0, 0.39, 0.082, 0.194]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:58:40 (running for 00:05:49.79)
Memory usage on this node: 24.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.371 |      0.137 |                   55 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.492 |      0.23  |                   57 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.296 |      0.293 |                   57 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  6.965 |      0.229 |                   57 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.21175373134328357
[2m[36m(func pid=68418)[0m top5: 0.5909514925373134
[2m[36m(func pid=68418)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=68418)[0m f1_macro: 0.13127915304606882
[2m[36m(func pid=68418)[0m f1_weighted: 0.1812925545421047
[2m[36m(func pid=68418)[0m f1_per_class: [0.133, 0.142, 0.141, 0.484, 0.04, 0.0, 0.0, 0.294, 0.0, 0.078]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3605410447761194
[2m[36m(func pid=68792)[0m top5: 0.8847947761194029
[2m[36m(func pid=68792)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.23373678008959015
[2m[36m(func pid=68792)[0m f1_weighted: 0.35520079536231525
[2m[36m(func pid=68792)[0m f1_per_class: [0.181, 0.026, 0.39, 0.51, 0.075, 0.348, 0.528, 0.0, 0.072, 0.208]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3689365671641791
[2m[36m(func pid=69189)[0m top5: 0.8493470149253731
[2m[36m(func pid=69189)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=69189)[0m f1_macro: 0.3001470104891077
[2m[36m(func pid=69189)[0m f1_weighted: 0.344090615540526
[2m[36m(func pid=69189)[0m f1_per_class: [0.2, 0.511, 0.346, 0.478, 0.146, 0.402, 0.117, 0.45, 0.222, 0.13]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 8.0017 | Steps: 2 | Val loss: 45.8813 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4271 | Steps: 2 | Val loss: 1.9705 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1907 | Steps: 2 | Val loss: 2.6982 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3521 | Steps: 2 | Val loss: 2.1820 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-06 23:58:45 (running for 00:05:54.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.332 |      0.131 |                   56 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.96  |      0.234 |                   58 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  1.595 |      0.3   |                   58 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  6.965 |      0.229 |                   57 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m top1: 0.22807835820895522
[2m[36m(func pid=69627)[0m top5: 0.7350746268656716
[2m[36m(func pid=69627)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=69627)[0m f1_macro: 0.2185638981291343
[2m[36m(func pid=69627)[0m f1_weighted: 0.18441822321443807
[2m[36m(func pid=69627)[0m f1_per_class: [0.257, 0.377, 0.314, 0.173, 0.07, 0.322, 0.0, 0.355, 0.116, 0.2]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=69189)[0m top1: 0.37593283582089554
[2m[36m(func pid=69189)[0m top5: 0.8568097014925373
[2m[36m(func pid=69189)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=69189)[0m f1_macro: 0.30872172265076225
[2m[36m(func pid=69189)[0m f1_weighted: 0.36305774207847374
[2m[36m(func pid=69189)[0m f1_per_class: [0.325, 0.456, 0.333, 0.521, 0.114, 0.374, 0.176, 0.469, 0.173, 0.147]
[2m[36m(func pid=68418)[0m top1: 0.20102611940298507
[2m[36m(func pid=68418)[0m top5: 0.5755597014925373
[2m[36m(func pid=68418)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=68418)[0m f1_macro: 0.12599238121590775
[2m[36m(func pid=68418)[0m f1_weighted: 0.17096223348375472
[2m[36m(func pid=68418)[0m f1_per_class: [0.149, 0.106, 0.14, 0.475, 0.042, 0.0, 0.0, 0.261, 0.0, 0.087]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m top1: 0.36380597014925375
[2m[36m(func pid=68792)[0m top5: 0.8936567164179104
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68792)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=68792)[0m f1_macro: 0.23549792558815094
[2m[36m(func pid=68792)[0m f1_weighted: 0.35850251967052493
[2m[36m(func pid=68792)[0m f1_per_class: [0.2, 0.062, 0.348, 0.51, 0.081, 0.371, 0.505, 0.032, 0.049, 0.198]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 6.3303 | Steps: 2 | Val loss: 46.1048 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.4170 | Steps: 2 | Val loss: 1.9647 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2906 | Steps: 2 | Val loss: 2.8461 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3416 | Steps: 2 | Val loss: 2.1923 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=69627)[0m top1: 0.22901119402985073
[2m[36m(func pid=69627)[0m top5: 0.7210820895522388
[2m[36m(func pid=69627)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=69627)[0m f1_macro: 0.21011263596047797
[2m[36m(func pid=69627)[0m f1_weighted: 0.1517492504474285
[2m[36m(func pid=69627)[0m f1_per_class: [0.228, 0.428, 0.275, 0.01, 0.077, 0.343, 0.0, 0.365, 0.175, 0.2]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:58:51 (running for 00:05:59.93)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.352 |      0.126 |                   57 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.427 |      0.235 |                   59 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.191 |      0.309 |                   59 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  6.33  |      0.21  |                   59 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.37033582089552236
[2m[36m(func pid=68792)[0m top5: 0.9053171641791045
[2m[36m(func pid=68792)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=68792)[0m f1_macro: 0.25328313421638365
[2m[36m(func pid=68792)[0m f1_weighted: 0.3681552789382343
[2m[36m(func pid=68792)[0m f1_per_class: [0.205, 0.103, 0.36, 0.511, 0.089, 0.398, 0.482, 0.135, 0.047, 0.203]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.36473880597014924
[2m[36m(func pid=69189)[0m top5: 0.8675373134328358
[2m[36m(func pid=69189)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=69189)[0m f1_macro: 0.3034305104032502
[2m[36m(func pid=69189)[0m f1_weighted: 0.3710623508675238
[2m[36m(func pid=69189)[0m f1_per_class: [0.239, 0.383, 0.407, 0.522, 0.094, 0.347, 0.266, 0.447, 0.143, 0.186]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.18610074626865672
[2m[36m(func pid=68418)[0m top5: 0.5629664179104478
[2m[36m(func pid=68418)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=68418)[0m f1_macro: 0.1263873655255589
[2m[36m(func pid=68418)[0m f1_weighted: 0.1582062827310638
[2m[36m(func pid=68418)[0m f1_per_class: [0.152, 0.079, 0.144, 0.447, 0.045, 0.0, 0.0, 0.238, 0.0, 0.16]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 6.4029 | Steps: 2 | Val loss: 46.8656 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2366 | Steps: 2 | Val loss: 3.0643 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.3805 | Steps: 2 | Val loss: 1.9747 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3337 | Steps: 2 | Val loss: 2.2032 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=69627)[0m top1: 0.2416044776119403
[2m[36m(func pid=69627)[0m top5: 0.7486007462686567
[2m[36m(func pid=69627)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=69627)[0m f1_macro: 0.21159889389625913
[2m[36m(func pid=69627)[0m f1_weighted: 0.15411903862484516
[2m[36m(func pid=69627)[0m f1_per_class: [0.231, 0.475, 0.272, 0.0, 0.087, 0.313, 0.0, 0.371, 0.173, 0.194]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:58:56 (running for 00:06:05.28)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.342 |      0.126 |                   58 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.417 |      0.253 |                   60 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.237 |      0.283 |                   61 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  6.403 |      0.212 |                   60 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69189)[0m top1: 0.33488805970149255
[2m[36m(func pid=69189)[0m top5: 0.8726679104477612
[2m[36m(func pid=69189)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=69189)[0m f1_macro: 0.2834201241581953
[2m[36m(func pid=69189)[0m f1_weighted: 0.36874481595179315
[2m[36m(func pid=69189)[0m f1_per_class: [0.156, 0.311, 0.407, 0.465, 0.08, 0.342, 0.375, 0.358, 0.178, 0.162]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3843283582089552
[2m[36m(func pid=68792)[0m top5: 0.9020522388059702
[2m[36m(func pid=68792)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=68792)[0m f1_macro: 0.29223916042345116
[2m[36m(func pid=68792)[0m f1_weighted: 0.3909824602351253
[2m[36m(func pid=68792)[0m f1_per_class: [0.237, 0.155, 0.4, 0.519, 0.094, 0.425, 0.463, 0.342, 0.079, 0.208]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m top1: 0.16417910447761194
[2m[36m(func pid=68418)[0m top5: 0.5573694029850746
[2m[36m(func pid=68418)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=68418)[0m f1_macro: 0.11182618797215753
[2m[36m(func pid=68418)[0m f1_weighted: 0.13852185085744437
[2m[36m(func pid=68418)[0m f1_per_class: [0.137, 0.048, 0.149, 0.404, 0.048, 0.0, 0.0, 0.21, 0.0, 0.122]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 4.3078 | Steps: 2 | Val loss: 44.4696 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.3075 | Steps: 2 | Val loss: 2.0056 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2058 | Steps: 2 | Val loss: 3.3936 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3087 | Steps: 2 | Val loss: 2.2147 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=69627)[0m top1: 0.23740671641791045
[2m[36m(func pid=69627)[0m top5: 0.7947761194029851
[2m[36m(func pid=69627)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=69627)[0m f1_macro: 0.1867220691395358
[2m[36m(func pid=69627)[0m f1_weighted: 0.14730565333774906
[2m[36m(func pid=69627)[0m f1_per_class: [0.044, 0.487, 0.275, 0.0, 0.095, 0.24, 0.009, 0.394, 0.16, 0.162]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:59:01 (running for 00:06:10.29)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.334 |      0.112 |                   59 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.307 |      0.308 |                   62 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.237 |      0.283 |                   61 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  4.308 |      0.187 |                   61 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.375
[2m[36m(func pid=68792)[0m top5: 0.886660447761194
[2m[36m(func pid=68792)[0m f1_micro: 0.375
[2m[36m(func pid=68792)[0m f1_macro: 0.3083932456031065
[2m[36m(func pid=68792)[0m f1_weighted: 0.3909396067597196
[2m[36m(func pid=68792)[0m f1_per_class: [0.241, 0.296, 0.361, 0.507, 0.097, 0.414, 0.37, 0.461, 0.125, 0.213]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.283115671641791
[2m[36m(func pid=69189)[0m top5: 0.8857276119402985
[2m[36m(func pid=69189)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=69189)[0m f1_macro: 0.24221133701072564
[2m[36m(func pid=69189)[0m f1_weighted: 0.3303533529763756
[2m[36m(func pid=69189)[0m f1_per_class: [0.113, 0.274, 0.385, 0.368, 0.07, 0.327, 0.405, 0.2, 0.103, 0.176]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.14505597014925373
[2m[36m(func pid=68418)[0m top5: 0.550839552238806
[2m[36m(func pid=68418)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=68418)[0m f1_macro: 0.10542875687571791
[2m[36m(func pid=68418)[0m f1_weighted: 0.11925705023634475
[2m[36m(func pid=68418)[0m f1_per_class: [0.125, 0.035, 0.153, 0.346, 0.051, 0.0, 0.0, 0.194, 0.0, 0.151]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 5.9365 | Steps: 2 | Val loss: 36.2029 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.3411 | Steps: 2 | Val loss: 2.0428 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4114 | Steps: 2 | Val loss: 3.6702 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3063 | Steps: 2 | Val loss: 2.2271 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=69627)[0m top1: 0.28171641791044777
[2m[36m(func pid=69627)[0m top5: 0.8479477611940298
[2m[36m(func pid=69627)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=69627)[0m f1_macro: 0.19528865671842185
[2m[36m(func pid=69627)[0m f1_weighted: 0.24134833051640522
[2m[36m(func pid=69627)[0m f1_per_class: [0.044, 0.501, 0.272, 0.02, 0.101, 0.062, 0.392, 0.27, 0.164, 0.127]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:59:06 (running for 00:06:15.37)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.309 |      0.105 |                   60 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.341 |      0.291 |                   63 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.206 |      0.242 |                   62 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  5.936 |      0.195 |                   62 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.33348880597014924
[2m[36m(func pid=68792)[0m top5: 0.8777985074626866
[2m[36m(func pid=68792)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=68792)[0m f1_macro: 0.2906963724130426
[2m[36m(func pid=68792)[0m f1_weighted: 0.33214958982927634
[2m[36m(func pid=68792)[0m f1_per_class: [0.251, 0.352, 0.338, 0.44, 0.099, 0.409, 0.202, 0.467, 0.125, 0.222]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.2579291044776119
[2m[36m(func pid=69189)[0m top5: 0.8899253731343284
[2m[36m(func pid=69189)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=69189)[0m f1_macro: 0.22162910068435746
[2m[36m(func pid=69189)[0m f1_weighted: 0.30563820239090433
[2m[36m(func pid=69189)[0m f1_per_class: [0.095, 0.309, 0.385, 0.269, 0.073, 0.346, 0.413, 0.12, 0.025, 0.182]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.1259328358208955
[2m[36m(func pid=68418)[0m top5: 0.5485074626865671
[2m[36m(func pid=68418)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=68418)[0m f1_macro: 0.09214093284398595
[2m[36m(func pid=68418)[0m f1_weighted: 0.09778234422602881
[2m[36m(func pid=68418)[0m f1_per_class: [0.123, 0.016, 0.159, 0.286, 0.057, 0.0, 0.0, 0.176, 0.0, 0.105]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 6.7201 | Steps: 2 | Val loss: 30.1318 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.4804 | Steps: 2 | Val loss: 2.0783 | Batch size: 32 | lr: 0.001 | Duration: 2.58s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3642 | Steps: 2 | Val loss: 3.7204 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2819 | Steps: 2 | Val loss: 2.2401 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=69627)[0m top1: 0.38899253731343286
[2m[36m(func pid=69627)[0m top5: 0.8857276119402985
[2m[36m(func pid=69627)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=69627)[0m f1_macro: 0.22227462007566365
[2m[36m(func pid=69627)[0m f1_weighted: 0.3675882387920554
[2m[36m(func pid=69627)[0m f1_per_class: [0.128, 0.545, 0.232, 0.345, 0.121, 0.0, 0.554, 0.0, 0.19, 0.108]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m top1: 0.2887126865671642
[2m[36m(func pid=68792)[0m top5: 0.8647388059701493
[2m[36m(func pid=68792)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=68792)[0m f1_macro: 0.26662285173468825
[2m[36m(func pid=68792)[0m f1_weighted: 0.2571564107760944
[2m[36m(func pid=68792)[0m f1_per_class: [0.278, 0.37, 0.338, 0.323, 0.099, 0.401, 0.054, 0.444, 0.138, 0.219]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.26072761194029853
[2m[36m(func pid=69189)[0m top5: 0.8903917910447762
[2m[36m(func pid=69189)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=69189)[0m f1_macro: 0.22446404215604693
[2m[36m(func pid=69189)[0m f1_weighted: 0.30069347589221546
[2m[36m(func pid=69189)[0m f1_per_class: [0.093, 0.4, 0.407, 0.187, 0.081, 0.345, 0.423, 0.12, 0.0, 0.188]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:59:11 (running for 00:06:20.50)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.306 |      0.092 |                   61 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.48  |      0.267 |                   64 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.364 |      0.224 |                   64 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  6.72  |      0.222 |                   63 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.11473880597014925
[2m[36m(func pid=68418)[0m top5: 0.5489738805970149
[2m[36m(func pid=68418)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=68418)[0m f1_macro: 0.0902216699772483
[2m[36m(func pid=68418)[0m f1_weighted: 0.0836293500442405
[2m[36m(func pid=68418)[0m f1_per_class: [0.111, 0.005, 0.164, 0.242, 0.061, 0.0, 0.0, 0.168, 0.0, 0.151]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 5.5284 | Steps: 2 | Val loss: 29.7854 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7552 | Steps: 2 | Val loss: 2.1219 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5328 | Steps: 2 | Val loss: 3.3508 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3108 | Steps: 2 | Val loss: 2.2507 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=69627)[0m top1: 0.44216417910447764
[2m[36m(func pid=69627)[0m top5: 0.8913246268656716
[2m[36m(func pid=69627)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=69627)[0m f1_macro: 0.23640753996778385
[2m[36m(func pid=69627)[0m f1_weighted: 0.41321947183380703
[2m[36m(func pid=69627)[0m f1_per_class: [0.207, 0.502, 0.22, 0.535, 0.159, 0.0, 0.564, 0.0, 0.042, 0.137]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m top1: 0.25093283582089554
[2m[36m(func pid=68792)[0m top5: 0.8488805970149254
[2m[36m(func pid=68792)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=68792)[0m f1_macro: 0.24446790751730374
[2m[36m(func pid=68792)[0m f1_weighted: 0.19454578075430315
[2m[36m(func pid=68792)[0m f1_per_class: [0.29, 0.347, 0.338, 0.169, 0.1, 0.406, 0.003, 0.404, 0.173, 0.213]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3125
[2m[36m(func pid=69189)[0m top5: 0.8936567164179104
[2m[36m(func pid=69189)[0m f1_micro: 0.3125
[2m[36m(func pid=69189)[0m f1_macro: 0.24683343775877953
[2m[36m(func pid=69189)[0m f1_weighted: 0.34510282733226644
[2m[36m(func pid=69189)[0m f1_per_class: [0.109, 0.501, 0.367, 0.232, 0.099, 0.368, 0.459, 0.12, 0.025, 0.188]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:59:17 (running for 00:06:26.55)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.311 |      0.085 |                   63 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.755 |      0.244 |                   65 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.533 |      0.247 |                   65 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  5.528 |      0.236 |                   64 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.10354477611940298
[2m[36m(func pid=68418)[0m top5: 0.5489738805970149
[2m[36m(func pid=68418)[0m f1_micro: 0.10354477611940298
[2m[36m(func pid=68418)[0m f1_macro: 0.0849010334759098
[2m[36m(func pid=68418)[0m f1_weighted: 0.0675641041772439
[2m[36m(func pid=68418)[0m f1_per_class: [0.114, 0.005, 0.167, 0.185, 0.064, 0.0, 0.0, 0.163, 0.0, 0.151]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 6.3506 | Steps: 2 | Val loss: 32.1896 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.3259 | Steps: 2 | Val loss: 2.1866 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3103 | Steps: 2 | Val loss: 2.8462 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2904 | Steps: 2 | Val loss: 2.2593 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=69627)[0m top1: 0.44029850746268656
[2m[36m(func pid=69627)[0m top5: 0.8857276119402985
[2m[36m(func pid=69627)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=69627)[0m f1_macro: 0.23019490098242695
[2m[36m(func pid=69627)[0m f1_weighted: 0.4015585093716456
[2m[36m(func pid=69627)[0m f1_per_class: [0.242, 0.375, 0.234, 0.584, 0.182, 0.0, 0.553, 0.0, 0.0, 0.132]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m top1: 0.23694029850746268
[2m[36m(func pid=68792)[0m top5: 0.8367537313432836
[2m[36m(func pid=68792)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=68792)[0m f1_macro: 0.22919374805705234
[2m[36m(func pid=68792)[0m f1_weighted: 0.17376491708820613
[2m[36m(func pid=68792)[0m f1_per_class: [0.286, 0.359, 0.314, 0.1, 0.097, 0.399, 0.0, 0.394, 0.139, 0.203]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.3894589552238806
[2m[36m(func pid=69189)[0m top5: 0.9011194029850746
[2m[36m(func pid=69189)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=69189)[0m f1_macro: 0.2867900916517453
[2m[36m(func pid=69189)[0m f1_weighted: 0.4147252414443333
[2m[36m(func pid=69189)[0m f1_per_class: [0.176, 0.525, 0.314, 0.404, 0.109, 0.388, 0.486, 0.194, 0.084, 0.188]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.09748134328358209
[2m[36m(func pid=68418)[0m top5: 0.5471082089552238
[2m[36m(func pid=68418)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=68418)[0m f1_macro: 0.08265946932266867
[2m[36m(func pid=68418)[0m f1_weighted: 0.05862452666554222
[2m[36m(func pid=68418)[0m f1_per_class: [0.1, 0.0, 0.19, 0.154, 0.067, 0.008, 0.0, 0.159, 0.0, 0.148]
[2m[36m(func pid=68418)[0m 
== Status ==
Current time: 2024-01-06 23:59:22 (running for 00:06:31.73)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.29  |      0.083 |                   64 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.326 |      0.229 |                   66 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.31  |      0.287 |                   66 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  6.351 |      0.23  |                   65 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 9.1736 | Steps: 2 | Val loss: 33.8683 | Batch size: 32 | lr: 0.1 | Duration: 2.59s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.2971 | Steps: 2 | Val loss: 2.2438 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1393 | Steps: 2 | Val loss: 2.6382 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2837 | Steps: 2 | Val loss: 2.2686 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=69627)[0m top1: 0.3931902985074627
[2m[36m(func pid=69627)[0m top5: 0.8745335820895522
[2m[36m(func pid=69627)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=69627)[0m f1_macro: 0.23973863591552796
[2m[36m(func pid=69627)[0m f1_weighted: 0.36663896278621416
[2m[36m(func pid=69627)[0m f1_per_class: [0.247, 0.302, 0.25, 0.561, 0.211, 0.008, 0.444, 0.267, 0.0, 0.108]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m top1: 0.22621268656716417
[2m[36m(func pid=68792)[0m top5: 0.8194962686567164
[2m[36m(func pid=68792)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=68792)[0m f1_macro: 0.21904114378884051
[2m[36m(func pid=68792)[0m f1_weighted: 0.16249259109157363
[2m[36m(func pid=68792)[0m f1_per_class: [0.278, 0.36, 0.289, 0.065, 0.089, 0.393, 0.0, 0.389, 0.141, 0.186]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.43283582089552236
[2m[36m(func pid=69189)[0m top5: 0.9067164179104478
[2m[36m(func pid=69189)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=69189)[0m f1_macro: 0.3200792593644264
[2m[36m(func pid=69189)[0m f1_weighted: 0.4465199601358541
[2m[36m(func pid=69189)[0m f1_per_class: [0.252, 0.533, 0.289, 0.496, 0.133, 0.4, 0.461, 0.33, 0.131, 0.176]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:59:27 (running for 00:06:36.78)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.284 |      0.079 |                   65 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.297 |      0.219 |                   67 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.139 |      0.32  |                   67 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  9.174 |      0.24  |                   66 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.09001865671641791
[2m[36m(func pid=68418)[0m top5: 0.5471082089552238
[2m[36m(func pid=68418)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=68418)[0m f1_macro: 0.07868703037010971
[2m[36m(func pid=68418)[0m f1_weighted: 0.04622932878509876
[2m[36m(func pid=68418)[0m f1_per_class: [0.094, 0.0, 0.206, 0.11, 0.065, 0.008, 0.0, 0.158, 0.0, 0.145]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 3.9344 | Steps: 2 | Val loss: 39.0788 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.3742 | Steps: 2 | Val loss: 2.3055 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3310 | Steps: 2 | Val loss: 2.6473 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2818 | Steps: 2 | Val loss: 2.2795 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=69627)[0m top1: 0.302705223880597
[2m[36m(func pid=69627)[0m top5: 0.8334888059701493
[2m[36m(func pid=69627)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=69627)[0m f1_macro: 0.2225758585482466
[2m[36m(func pid=69627)[0m f1_weighted: 0.26877272799029733
[2m[36m(func pid=69627)[0m f1_per_class: [0.147, 0.353, 0.275, 0.527, 0.239, 0.207, 0.033, 0.342, 0.0, 0.102]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m top1: 0.2140858208955224
[2m[36m(func pid=68792)[0m top5: 0.8115671641791045
[2m[36m(func pid=68792)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=68792)[0m f1_macro: 0.20539014236392217
[2m[36m(func pid=68792)[0m f1_weighted: 0.15138362821512982
[2m[36m(func pid=68792)[0m f1_per_class: [0.239, 0.362, 0.268, 0.035, 0.086, 0.383, 0.0, 0.38, 0.13, 0.17]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.43097014925373134
[2m[36m(func pid=69189)[0m top5: 0.8997201492537313
[2m[36m(func pid=69189)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=69189)[0m f1_macro: 0.32709709473228676
[2m[36m(func pid=69189)[0m f1_weighted: 0.43513570760944226
[2m[36m(func pid=69189)[0m f1_per_class: [0.258, 0.528, 0.286, 0.508, 0.143, 0.404, 0.396, 0.403, 0.152, 0.195]
[2m[36m(func pid=69189)[0m 
== Status ==
Current time: 2024-01-06 23:59:33 (running for 00:06:41.98)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.282 |      0.076 |                   66 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.374 |      0.205 |                   68 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.331 |      0.327 |                   68 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.934 |      0.223 |                   67 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.08722014925373134
[2m[36m(func pid=68418)[0m top5: 0.5471082089552238
[2m[36m(func pid=68418)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=68418)[0m f1_macro: 0.07648491334065097
[2m[36m(func pid=68418)[0m f1_weighted: 0.03998877237864502
[2m[36m(func pid=68418)[0m f1_per_class: [0.092, 0.0, 0.218, 0.088, 0.068, 0.008, 0.0, 0.16, 0.0, 0.131]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 17.9579 | Steps: 2 | Val loss: 47.8247 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.2837 | Steps: 2 | Val loss: 2.3413 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1828 | Steps: 2 | Val loss: 2.7554 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2476 | Steps: 2 | Val loss: 2.2868 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=69627)[0m top1: 0.24953358208955223
[2m[36m(func pid=69627)[0m top5: 0.7971082089552238
[2m[36m(func pid=69627)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=69627)[0m f1_macro: 0.23193741169240467
[2m[36m(func pid=69627)[0m f1_weighted: 0.24250674402878158
[2m[36m(func pid=69627)[0m f1_per_class: [0.091, 0.377, 0.286, 0.362, 0.293, 0.428, 0.0, 0.353, 0.0, 0.129]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m top1: 0.20662313432835822
[2m[36m(func pid=68792)[0m top5: 0.8073694029850746
[2m[36m(func pid=68792)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=68792)[0m f1_macro: 0.19899835708713917
[2m[36m(func pid=68792)[0m f1_weighted: 0.14653326390800997
[2m[36m(func pid=68792)[0m f1_per_class: [0.23, 0.362, 0.265, 0.026, 0.083, 0.37, 0.0, 0.381, 0.123, 0.151]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.41884328358208955
[2m[36m(func pid=69189)[0m top5: 0.894589552238806
[2m[36m(func pid=69189)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=69189)[0m f1_macro: 0.30637088928968237
[2m[36m(func pid=69189)[0m f1_weighted: 0.40983574921337873
[2m[36m(func pid=69189)[0m f1_per_class: [0.085, 0.545, 0.278, 0.509, 0.156, 0.388, 0.314, 0.408, 0.157, 0.222]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.08582089552238806
[2m[36m(func pid=68418)[0m top5: 0.5485074626865671
[2m[36m(func pid=68418)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=68418)[0m f1_macro: 0.07621836494465264
[2m[36m(func pid=68418)[0m f1_weighted: 0.03458068821411257
[2m[36m(func pid=68418)[0m f1_per_class: [0.102, 0.005, 0.232, 0.064, 0.068, 0.008, 0.0, 0.164, 0.0, 0.119]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 3.3046 | Steps: 2 | Val loss: 54.2319 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2464 | Steps: 2 | Val loss: 2.3699 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1582 | Steps: 2 | Val loss: 2.8607 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.3052 | Steps: 2 | Val loss: 2.2935 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-06 23:59:41 (running for 00:06:50.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.248 |      0.076 |                   67 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.284 |      0.199 |                   69 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.183 |      0.306 |                   69 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.305 |      0.233 |                   69 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69627)[0m top1: 0.22434701492537312
[2m[36m(func pid=69627)[0m top5: 0.7639925373134329
[2m[36m(func pid=69627)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=69627)[0m f1_macro: 0.23282584195881503
[2m[36m(func pid=69627)[0m f1_weighted: 0.18996582337161216
[2m[36m(func pid=69627)[0m f1_per_class: [0.076, 0.449, 0.324, 0.094, 0.308, 0.474, 0.0, 0.421, 0.0, 0.183]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68792)[0m top1: 0.19542910447761194
[2m[36m(func pid=68792)[0m top5: 0.8092350746268657
[2m[36m(func pid=68792)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.19070256860389315
[2m[36m(func pid=68792)[0m f1_weighted: 0.1435733290636552
[2m[36m(func pid=68792)[0m f1_per_class: [0.194, 0.34, 0.256, 0.029, 0.08, 0.377, 0.0, 0.383, 0.129, 0.119]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.394589552238806
[2m[36m(func pid=69189)[0m top5: 0.8819962686567164
[2m[36m(func pid=69189)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=69189)[0m f1_macro: 0.28084872583000114
[2m[36m(func pid=69189)[0m f1_weighted: 0.3745004321820081
[2m[36m(func pid=69189)[0m f1_per_class: [0.044, 0.551, 0.275, 0.5, 0.14, 0.389, 0.214, 0.381, 0.138, 0.177]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.0853544776119403
[2m[36m(func pid=68418)[0m top5: 0.5503731343283582
[2m[36m(func pid=68418)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=68418)[0m f1_macro: 0.07657616576058093
[2m[36m(func pid=68418)[0m f1_weighted: 0.03139440368374808
[2m[36m(func pid=68418)[0m f1_per_class: [0.113, 0.005, 0.247, 0.052, 0.069, 0.008, 0.0, 0.165, 0.0, 0.107]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.5263 | Steps: 2 | Val loss: 60.6418 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1542 | Steps: 2 | Val loss: 2.3722 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7415 | Steps: 2 | Val loss: 2.8512 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=69627)[0m top1: 0.22014925373134328
[2m[36m(func pid=69627)[0m top5: 0.7234141791044776
[2m[36m(func pid=69627)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=69627)[0m f1_macro: 0.2396610339107391
[2m[36m(func pid=69627)[0m f1_weighted: 0.1737051432321136
[2m[36m(func pid=69627)[0m f1_per_class: [0.073, 0.485, 0.338, 0.003, 0.29, 0.427, 0.006, 0.498, 0.043, 0.231]
[2m[36m(func pid=69627)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2205 | Steps: 2 | Val loss: 2.2965 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-06 23:59:46 (running for 00:06:55.12)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.305 |      0.077 |                   68 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.154 |      0.19  |                   71 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.158 |      0.281 |                   70 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  2.526 |      0.24  |                   70 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.19076492537313433
[2m[36m(func pid=68792)[0m top5: 0.816231343283582
[2m[36m(func pid=68792)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=68792)[0m f1_macro: 0.18970013644936795
[2m[36m(func pid=68792)[0m f1_weighted: 0.15146484206125924
[2m[36m(func pid=68792)[0m f1_per_class: [0.179, 0.35, 0.256, 0.062, 0.072, 0.353, 0.0, 0.388, 0.115, 0.121]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=69189)[0m top1: 0.38619402985074625
[2m[36m(func pid=69189)[0m top5: 0.8824626865671642
[2m[36m(func pid=69189)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=69189)[0m f1_macro: 0.28440716746151196
[2m[36m(func pid=69189)[0m f1_weighted: 0.384377250127155
[2m[36m(func pid=69189)[0m f1_per_class: [0.044, 0.551, 0.272, 0.47, 0.158, 0.37, 0.282, 0.361, 0.175, 0.162]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.08442164179104478
[2m[36m(func pid=68418)[0m top5: 0.5541044776119403
[2m[36m(func pid=68418)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=68418)[0m f1_macro: 0.07853871164737283
[2m[36m(func pid=68418)[0m f1_weighted: 0.027546737322935617
[2m[36m(func pid=68418)[0m f1_per_class: [0.116, 0.0, 0.268, 0.039, 0.065, 0.008, 0.0, 0.171, 0.0, 0.118]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.9714 | Steps: 2 | Val loss: 61.9655 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.4408 | Steps: 2 | Val loss: 2.3483 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4316 | Steps: 2 | Val loss: 2.8708 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=69627)[0m top1: 0.23087686567164178
[2m[36m(func pid=69627)[0m top5: 0.7089552238805971
[2m[36m(func pid=69627)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=69627)[0m f1_macro: 0.2571927878647678
[2m[36m(func pid=69627)[0m f1_weighted: 0.1856098534437261
[2m[36m(func pid=69627)[0m f1_per_class: [0.083, 0.516, 0.324, 0.0, 0.338, 0.406, 0.031, 0.486, 0.112, 0.276]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:59:51 (running for 00:07:00.14)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.221 |      0.079 |                   69 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.441 |      0.193 |                   72 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.742 |      0.284 |                   71 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  2.971 |      0.257 |                   71 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.19263059701492538
[2m[36m(func pid=68792)[0m top5: 0.8255597014925373
[2m[36m(func pid=68792)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=68792)[0m f1_macro: 0.19275699488975748
[2m[36m(func pid=68792)[0m f1_weighted: 0.17119898472979866
[2m[36m(func pid=68792)[0m f1_per_class: [0.176, 0.34, 0.256, 0.131, 0.066, 0.313, 0.022, 0.402, 0.11, 0.111]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2428 | Steps: 2 | Val loss: 2.2990 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=69189)[0m top1: 0.3885261194029851
[2m[36m(func pid=69189)[0m top5: 0.8805970149253731
[2m[36m(func pid=69189)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=69189)[0m f1_macro: 0.2960733387015849
[2m[36m(func pid=69189)[0m f1_weighted: 0.41019625701408896
[2m[36m(func pid=69189)[0m f1_per_class: [0.125, 0.537, 0.247, 0.435, 0.16, 0.35, 0.407, 0.387, 0.18, 0.133]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=68418)[0m top1: 0.0853544776119403
[2m[36m(func pid=68418)[0m top5: 0.5592350746268657
[2m[36m(func pid=68418)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=68418)[0m f1_macro: 0.08029810710291993
[2m[36m(func pid=68418)[0m f1_weighted: 0.028708365347752018
[2m[36m(func pid=68418)[0m f1_per_class: [0.114, 0.0, 0.293, 0.042, 0.067, 0.008, 0.0, 0.177, 0.0, 0.102]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 4.2004 | Steps: 2 | Val loss: 60.4368 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.2165 | Steps: 2 | Val loss: 2.3206 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2332 | Steps: 2 | Val loss: 2.9766 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=69627)[0m top1: 0.22154850746268656
[2m[36m(func pid=69627)[0m top5: 0.7290111940298507
[2m[36m(func pid=69627)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=69627)[0m f1_macro: 0.22535814987160158
[2m[36m(func pid=69627)[0m f1_weighted: 0.1753628852261685
[2m[36m(func pid=69627)[0m f1_per_class: [0.111, 0.539, 0.31, 0.0, 0.282, 0.403, 0.041, 0.204, 0.095, 0.269]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-06 23:59:56 (running for 00:07:05.35)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.243 |      0.08  |                   70 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.216 |      0.21  |                   73 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.432 |      0.296 |                   72 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  4.2   |      0.225 |                   72 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.2140858208955224
[2m[36m(func pid=68792)[0m top5: 0.8367537313432836
[2m[36m(func pid=68792)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=68792)[0m f1_macro: 0.21019394679198394
[2m[36m(func pid=68792)[0m f1_weighted: 0.22606537674463337
[2m[36m(func pid=68792)[0m f1_per_class: [0.159, 0.31, 0.253, 0.22, 0.062, 0.295, 0.141, 0.441, 0.112, 0.109]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3243 | Steps: 2 | Val loss: 2.3002 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=69189)[0m top1: 0.37919776119402987
[2m[36m(func pid=69189)[0m top5: 0.8791977611940298
[2m[36m(func pid=69189)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=69189)[0m f1_macro: 0.30226984101447774
[2m[36m(func pid=69189)[0m f1_weighted: 0.408670157799167
[2m[36m(func pid=69189)[0m f1_per_class: [0.219, 0.526, 0.234, 0.322, 0.159, 0.345, 0.508, 0.413, 0.147, 0.15]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.8426 | Steps: 2 | Val loss: 55.8421 | Batch size: 32 | lr: 0.1 | Duration: 2.59s
[2m[36m(func pid=68418)[0m top1: 0.08582089552238806
[2m[36m(func pid=68418)[0m top5: 0.570429104477612
[2m[36m(func pid=68418)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=68418)[0m f1_macro: 0.08211192182116067
[2m[36m(func pid=68418)[0m f1_weighted: 0.028341741692833478
[2m[36m(func pid=68418)[0m f1_per_class: [0.112, 0.0, 0.301, 0.039, 0.065, 0.008, 0.0, 0.183, 0.0, 0.112]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2000 | Steps: 2 | Val loss: 2.3023 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5905 | Steps: 2 | Val loss: 3.1481 | Batch size: 32 | lr: 0.01 | Duration: 2.56s
[2m[36m(func pid=69627)[0m top1: 0.21735074626865672
[2m[36m(func pid=69627)[0m top5: 0.7826492537313433
[2m[36m(func pid=69627)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=69627)[0m f1_macro: 0.2064683193180285
[2m[36m(func pid=69627)[0m f1_weighted: 0.17781843325435737
[2m[36m(func pid=69627)[0m f1_per_class: [0.161, 0.54, 0.282, 0.0, 0.206, 0.412, 0.078, 0.032, 0.087, 0.267]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-07 00:00:01 (running for 00:07:10.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.324 |      0.082 |                   71 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.2   |      0.232 |                   74 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.233 |      0.302 |                   73 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  2.843 |      0.206 |                   73 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.24720149253731344
[2m[36m(func pid=68792)[0m top5: 0.847481343283582
[2m[36m(func pid=68792)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=68792)[0m f1_macro: 0.232362020515317
[2m[36m(func pid=68792)[0m f1_weighted: 0.28741101408107683
[2m[36m(func pid=68792)[0m f1_per_class: [0.147, 0.294, 0.253, 0.284, 0.062, 0.296, 0.289, 0.476, 0.114, 0.109]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2446 | Steps: 2 | Val loss: 2.3043 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=69189)[0m top1: 0.3572761194029851
[2m[36m(func pid=69189)[0m top5: 0.867070895522388
[2m[36m(func pid=69189)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=69189)[0m f1_macro: 0.2697190679255285
[2m[36m(func pid=69189)[0m f1_weighted: 0.36708356669798564
[2m[36m(func pid=69189)[0m f1_per_class: [0.216, 0.53, 0.247, 0.182, 0.155, 0.365, 0.533, 0.203, 0.116, 0.15]
[2m[36m(func pid=69189)[0m 
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.9161 | Steps: 2 | Val loss: 49.1386 | Batch size: 32 | lr: 0.1 | Duration: 2.54s
[2m[36m(func pid=68418)[0m top1: 0.08488805970149253
[2m[36m(func pid=68418)[0m top5: 0.5778917910447762
[2m[36m(func pid=68418)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=68418)[0m f1_macro: 0.08262161127921695
[2m[36m(func pid=68418)[0m f1_weighted: 0.028425933041764502
[2m[36m(func pid=68418)[0m f1_per_class: [0.103, 0.0, 0.314, 0.033, 0.065, 0.024, 0.0, 0.19, 0.0, 0.098]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.2985 | Steps: 2 | Val loss: 2.2562 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=69189)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4863 | Steps: 2 | Val loss: 3.4446 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=69627)[0m top1: 0.23647388059701493
[2m[36m(func pid=69627)[0m top5: 0.8250932835820896
[2m[36m(func pid=69627)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=69627)[0m f1_macro: 0.2050407036973551
[2m[36m(func pid=69627)[0m f1_weighted: 0.21431991267563225
[2m[36m(func pid=69627)[0m f1_per_class: [0.218, 0.524, 0.265, 0.003, 0.13, 0.387, 0.222, 0.0, 0.097, 0.203]
[2m[36m(func pid=69627)[0m 
== Status ==
Current time: 2024-01-07 00:00:06 (running for 00:07:15.50)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.253
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING  | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.245 |      0.083 |                   72 |
| train_51d3e_00001 | RUNNING  | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.298 |      0.253 |                   75 |
| train_51d3e_00002 | RUNNING  | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.591 |      0.27  |                   74 |
| train_51d3e_00003 | RUNNING  | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  2.916 |      0.205 |                   74 |
| train_51d3e_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.2896455223880597
[2m[36m(func pid=68792)[0m top5: 0.8661380597014925
[2m[36m(func pid=68792)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=68792)[0m f1_macro: 0.2529926070603167
[2m[36m(func pid=68792)[0m f1_weighted: 0.3426893182496634
[2m[36m(func pid=68792)[0m f1_per_class: [0.147, 0.257, 0.256, 0.385, 0.06, 0.267, 0.406, 0.494, 0.136, 0.122]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2008 | Steps: 2 | Val loss: 2.3029 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=69189)[0m top1: 0.3376865671641791
[2m[36m(func pid=69189)[0m top5: 0.8535447761194029
[2m[36m(func pid=69189)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=69189)[0m f1_macro: 0.25051788186156754
[2m[36m(func pid=69189)[0m f1_weighted: 0.33488479030312285
[2m[36m(func pid=69189)[0m f1_per_class: [0.13, 0.515, 0.275, 0.086, 0.157, 0.405, 0.528, 0.119, 0.126, 0.164]
[2m[36m(func pid=69627)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 3.0931 | Steps: 2 | Val loss: 43.7692 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.3253 | Steps: 2 | Val loss: 2.2145 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=68418)[0m top1: 0.08488805970149253
[2m[36m(func pid=68418)[0m top5: 0.5844216417910447
[2m[36m(func pid=68418)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=68418)[0m f1_macro: 0.08350802976766748
[2m[36m(func pid=68418)[0m f1_weighted: 0.028007331380956513
[2m[36m(func pid=68418)[0m f1_per_class: [0.106, 0.0, 0.333, 0.029, 0.062, 0.024, 0.0, 0.199, 0.0, 0.082]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=69627)[0m top1: 0.28218283582089554
[2m[36m(func pid=69627)[0m top5: 0.8577425373134329
[2m[36m(func pid=69627)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=69627)[0m f1_macro: 0.21557927237370614
[2m[36m(func pid=69627)[0m f1_weighted: 0.26682194185901764
[2m[36m(func pid=69627)[0m f1_per_class: [0.212, 0.519, 0.242, 0.026, 0.091, 0.32, 0.404, 0.0, 0.132, 0.211]
[2m[36m(func pid=68792)[0m top1: 0.3101679104477612
[2m[36m(func pid=68792)[0m top5: 0.8833955223880597
[2m[36m(func pid=68792)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=68792)[0m f1_macro: 0.2549641528315688
[2m[36m(func pid=68792)[0m f1_weighted: 0.3593084393636237
[2m[36m(func pid=68792)[0m f1_per_class: [0.146, 0.21, 0.259, 0.415, 0.06, 0.273, 0.469, 0.431, 0.143, 0.144]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2007 | Steps: 2 | Val loss: 2.3004 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.4414 | Steps: 2 | Val loss: 2.1633 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=68418)[0m top1: 0.08768656716417911
[2m[36m(func pid=68418)[0m top5: 0.5932835820895522
[2m[36m(func pid=68418)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=68418)[0m f1_macro: 0.089375516223778
[2m[36m(func pid=68418)[0m f1_weighted: 0.03414580378007596
[2m[36m(func pid=68418)[0m f1_per_class: [0.101, 0.0, 0.373, 0.048, 0.061, 0.024, 0.0, 0.214, 0.0, 0.073]
[2m[36m(func pid=68418)[0m 
[2m[36m(func pid=85916)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=85916)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=85916)[0m Configuration completed!
[2m[36m(func pid=85916)[0m New optimizer parameters:
[2m[36m(func pid=85916)[0m SGD (
[2m[36m(func pid=85916)[0m Parameter Group 0
[2m[36m(func pid=85916)[0m     dampening: 0
[2m[36m(func pid=85916)[0m     differentiable: False
[2m[36m(func pid=85916)[0m     foreach: None
[2m[36m(func pid=85916)[0m     lr: 0.0001
[2m[36m(func pid=85916)[0m     maximize: False
[2m[36m(func pid=85916)[0m     momentum: 0.9
[2m[36m(func pid=85916)[0m     nesterov: False
[2m[36m(func pid=85916)[0m     weight_decay: 0
[2m[36m(func pid=85916)[0m )
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.324160447761194
[2m[36m(func pid=68792)[0m top5: 0.8941231343283582
[2m[36m(func pid=68792)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.2541842152239744
[2m[36m(func pid=68792)[0m f1_weighted: 0.36770313440624736
[2m[36m(func pid=68792)[0m f1_per_class: [0.144, 0.194, 0.268, 0.442, 0.06, 0.278, 0.494, 0.349, 0.146, 0.165]
[2m[36m(func pid=68418)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2016 | Steps: 2 | Val loss: 2.3014 | Batch size: 32 | lr: 0.0001 | Duration: 2.62s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0512 | Steps: 2 | Val loss: 2.3712 | Batch size: 32 | lr: 0.0001 | Duration: 4.28s
== Status ==
Current time: 2024-01-07 00:00:14 (running for 00:07:23.49)
Memory usage on this node: 20.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | RUNNING    | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.201 |      0.089 |                   74 |
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.325 |      0.255 |                   76 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86021)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=86021)[0m Configuration completed!
[2m[36m(func pid=86021)[0m New optimizer parameters:
[2m[36m(func pid=86021)[0m SGD (
[2m[36m(func pid=86021)[0m Parameter Group 0
[2m[36m(func pid=86021)[0m     dampening: 0
[2m[36m(func pid=86021)[0m     differentiable: False
[2m[36m(func pid=86021)[0m     foreach: None
[2m[36m(func pid=86021)[0m     lr: 0.001
[2m[36m(func pid=86021)[0m     maximize: False
[2m[36m(func pid=86021)[0m     momentum: 0.9
[2m[36m(func pid=86021)[0m     nesterov: False
[2m[36m(func pid=86021)[0m     weight_decay: 0
[2m[36m(func pid=86021)[0m )
[2m[36m(func pid=86021)[0m 
== Status ==
Current time: 2024-01-07 00:00:19 (running for 00:07:28.58)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 3 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.441 |      0.254 |                   77 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |        |            |                      |
| train_51d3e_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68418)[0m top1: 0.08768656716417911
[2m[36m(func pid=68418)[0m top5: 0.6021455223880597
[2m[36m(func pid=68418)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=68418)[0m f1_macro: 0.09005093138218176
[2m[36m(func pid=68418)[0m f1_weighted: 0.033427772493645674
[2m[36m(func pid=68418)[0m f1_per_class: [0.099, 0.0, 0.373, 0.042, 0.059, 0.024, 0.0, 0.233, 0.0, 0.07]
[2m[36m(func pid=85916)[0m top1: 0.06623134328358209
[2m[36m(func pid=85916)[0m top5: 0.3903917910447761
[2m[36m(func pid=85916)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=85916)[0m f1_macro: 0.01659973875770735
[2m[36m(func pid=85916)[0m f1_weighted: 0.02032694992619746
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.0, 0.0, 0.049, 0.0, 0.0, 0.0, 0.117, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.2539 | Steps: 2 | Val loss: 2.1556 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0671 | Steps: 2 | Val loss: 2.3707 | Batch size: 32 | lr: 0.001 | Duration: 4.52s
[2m[36m(func pid=68792)[0m top1: 0.3460820895522388
[2m[36m(func pid=68792)[0m top5: 0.90625
[2m[36m(func pid=68792)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=68792)[0m f1_macro: 0.2557078602310834
[2m[36m(func pid=68792)[0m f1_weighted: 0.38591162534761814
[2m[36m(func pid=68792)[0m f1_per_class: [0.139, 0.22, 0.247, 0.47, 0.067, 0.306, 0.515, 0.297, 0.151, 0.145]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0938 | Steps: 2 | Val loss: 2.3277 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=86021)[0m top1: 0.07182835820895522
[2m[36m(func pid=86021)[0m top5: 0.32276119402985076
[2m[36m(func pid=86021)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=86021)[0m f1_macro: 0.019970741827208573
[2m[36m(func pid=86021)[0m f1_weighted: 0.028732788294813714
[2m[36m(func pid=86021)[0m f1_per_class: [0.0, 0.0, 0.0, 0.078, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=85916)[0m top1: 0.13805970149253732
[2m[36m(func pid=85916)[0m top5: 0.5223880597014925
[2m[36m(func pid=85916)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=85916)[0m f1_macro: 0.04112022757240804
[2m[36m(func pid=85916)[0m f1_weighted: 0.07767641338124663
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.0, 0.0, 0.244, 0.0, 0.0, 0.0, 0.167, 0.0, 0.0]
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.2804 | Steps: 2 | Val loss: 2.1454 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 00:00:26 (running for 00:07:35.05)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.254 |      0.256 |                   78 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  3.051 |      0.017 |                    1 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  3.067 |      0.02  |                    1 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86940)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=86940)[0m Configuration completed!
[2m[36m(func pid=86940)[0m New optimizer parameters:
[2m[36m(func pid=86940)[0m SGD (
[2m[36m(func pid=86940)[0m Parameter Group 0
[2m[36m(func pid=86940)[0m     dampening: 0
[2m[36m(func pid=86940)[0m     differentiable: False
[2m[36m(func pid=86940)[0m     foreach: None
[2m[36m(func pid=86940)[0m     lr: 0.01
[2m[36m(func pid=86940)[0m     maximize: False
[2m[36m(func pid=86940)[0m     momentum: 0.9
[2m[36m(func pid=86940)[0m     nesterov: False
[2m[36m(func pid=86940)[0m     weight_decay: 0
[2m[36m(func pid=86940)[0m )
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0563 | Steps: 2 | Val loss: 2.3253 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=68792)[0m top1: 0.3498134328358209
[2m[36m(func pid=68792)[0m top5: 0.9155783582089553
[2m[36m(func pid=68792)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=68792)[0m f1_macro: 0.24681466232484972
[2m[36m(func pid=68792)[0m f1_weighted: 0.384934999709919
[2m[36m(func pid=68792)[0m f1_per_class: [0.126, 0.232, 0.239, 0.477, 0.071, 0.32, 0.511, 0.229, 0.105, 0.158]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0683 | Steps: 2 | Val loss: 2.3169 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:00:31 (running for 00:07:40.26)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.28  |      0.247 |                   79 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  3.094 |      0.041 |                    2 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  3.056 |      0.056 |                    2 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |        |            |                      |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.14039179104477612
[2m[36m(func pid=86021)[0m top5: 0.5135261194029851
[2m[36m(func pid=86021)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=86021)[0m f1_macro: 0.05582085370246017
[2m[36m(func pid=86021)[0m f1_weighted: 0.0898160092238407
[2m[36m(func pid=86021)[0m f1_per_class: [0.0, 0.005, 0.022, 0.263, 0.0, 0.0, 0.0, 0.269, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.1072 | Steps: 2 | Val loss: 2.1249 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0299 | Steps: 2 | Val loss: 2.4089 | Batch size: 32 | lr: 0.01 | Duration: 4.33s
[2m[36m(func pid=85916)[0m top1: 0.177705223880597
[2m[36m(func pid=85916)[0m top5: 0.566231343283582
[2m[36m(func pid=85916)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=85916)[0m f1_macro: 0.045539490122895335
[2m[36m(func pid=85916)[0m f1_weighted: 0.09534763269059379
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.0, 0.0, 0.312, 0.0, 0.0, 0.0, 0.143, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8723 | Steps: 2 | Val loss: 2.3151 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=68792)[0m top1: 0.3591417910447761
[2m[36m(func pid=68792)[0m top5: 0.9174440298507462
[2m[36m(func pid=68792)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=68792)[0m f1_macro: 0.2542561131718747
[2m[36m(func pid=68792)[0m f1_weighted: 0.39334591228987925
[2m[36m(func pid=68792)[0m f1_per_class: [0.135, 0.279, 0.242, 0.475, 0.076, 0.327, 0.512, 0.216, 0.105, 0.176]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86940)[0m top1: 0.006063432835820896
[2m[36m(func pid=86940)[0m top5: 0.37966417910447764
[2m[36m(func pid=86940)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=86940)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=86940)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0470 | Steps: 2 | Val loss: 2.3146 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=86021)[0m top1: 0.04617537313432836
[2m[36m(func pid=86021)[0m top5: 0.5569029850746269
[2m[36m(func pid=86021)[0m f1_micro: 0.04617537313432836
[2m[36m(func pid=86021)[0m f1_macro: 0.021612890618743292
[2m[36m(func pid=86021)[0m f1_weighted: 0.049209905914662906
[2m[36m(func pid=86021)[0m f1_per_class: [0.0, 0.0, 0.015, 0.17, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.5984 | Steps: 2 | Val loss: 2.1232 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7635 | Steps: 2 | Val loss: 2.3765 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:00:37 (running for 00:07:46.80)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.107 |      0.254 |                   80 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  3.047 |      0.04  |                    4 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.872 |      0.022 |                    3 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  3.03  |      0.001 |                    1 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.19076492537313433
[2m[36m(func pid=85916)[0m top5: 0.5792910447761194
[2m[36m(func pid=85916)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=85916)[0m f1_macro: 0.040109956870918506
[2m[36m(func pid=85916)[0m f1_weighted: 0.10002247078003995
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.005, 0.0, 0.345, 0.0, 0.0, 0.0, 0.051, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8154 | Steps: 2 | Val loss: 2.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=68792)[0m top1: 0.3619402985074627
[2m[36m(func pid=68792)[0m top5: 0.9188432835820896
[2m[36m(func pid=68792)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=68792)[0m f1_macro: 0.2594795829059618
[2m[36m(func pid=68792)[0m f1_weighted: 0.3961869415403279
[2m[36m(func pid=68792)[0m f1_per_class: [0.145, 0.321, 0.232, 0.467, 0.08, 0.329, 0.502, 0.216, 0.124, 0.18]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86940)[0m top1: 0.010261194029850746
[2m[36m(func pid=86940)[0m top5: 0.3978544776119403
[2m[36m(func pid=86940)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=86940)[0m f1_macro: 0.004516129032258065
[2m[36m(func pid=86940)[0m f1_weighted: 0.00046039961482908045
[2m[36m(func pid=86940)[0m f1_per_class: [0.013, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0248 | Steps: 2 | Val loss: 2.3165 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=86021)[0m top1: 0.009794776119402986
[2m[36m(func pid=86021)[0m top5: 0.5550373134328358
[2m[36m(func pid=86021)[0m f1_micro: 0.009794776119402986
[2m[36m(func pid=86021)[0m f1_macro: 0.0037338082949499793
[2m[36m(func pid=86021)[0m f1_weighted: 0.007058433956290654
[2m[36m(func pid=86021)[0m f1_per_class: [0.0, 0.0, 0.012, 0.025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.1379 | Steps: 2 | Val loss: 2.0942 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6987 | Steps: 2 | Val loss: 2.3036 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:00:43 (running for 00:07:51.94)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.598 |      0.259 |                   81 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  3.025 |      0.039 |                    5 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.815 |      0.004 |                    4 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.764 |      0.005 |                    2 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.20102611940298507
[2m[36m(func pid=85916)[0m top5: 0.5802238805970149
[2m[36m(func pid=85916)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=85916)[0m f1_macro: 0.03913127537539473
[2m[36m(func pid=85916)[0m f1_weighted: 0.10164982748819543
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.0, 0.0, 0.357, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7619 | Steps: 2 | Val loss: 2.3157 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=68792)[0m top1: 0.3736007462686567
[2m[36m(func pid=68792)[0m top5: 0.9193097014925373
[2m[36m(func pid=68792)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=68792)[0m f1_macro: 0.2758945712297576
[2m[36m(func pid=68792)[0m f1_weighted: 0.4084158640718808
[2m[36m(func pid=68792)[0m f1_per_class: [0.152, 0.387, 0.244, 0.458, 0.088, 0.358, 0.494, 0.229, 0.155, 0.193]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86940)[0m top1: 0.028917910447761194
[2m[36m(func pid=86940)[0m top5: 0.5601679104477612
[2m[36m(func pid=86940)[0m f1_micro: 0.028917910447761194
[2m[36m(func pid=86940)[0m f1_macro: 0.0515331470364845
[2m[36m(func pid=86940)[0m f1_weighted: 0.005135599262836216
[2m[36m(func pid=86940)[0m f1_per_class: [0.05, 0.0, 0.387, 0.0, 0.065, 0.0, 0.003, 0.0, 0.011, 0.0]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0175 | Steps: 2 | Val loss: 2.3210 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=86021)[0m top1: 0.0065298507462686565
[2m[36m(func pid=86021)[0m top5: 0.5564365671641791
[2m[36m(func pid=86021)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=86021)[0m f1_macro: 0.0015411213022956938
[2m[36m(func pid=86021)[0m f1_weighted: 0.0009880496997125403
[2m[36m(func pid=86021)[0m f1_per_class: [0.0, 0.0, 0.012, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.1013 | Steps: 2 | Val loss: 2.0602 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6364 | Steps: 2 | Val loss: 2.2218 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=85916)[0m top1: 0.208955223880597
[2m[36m(func pid=85916)[0m top5: 0.5732276119402985
[2m[36m(func pid=85916)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=85916)[0m f1_macro: 0.03903634380432022
[2m[36m(func pid=85916)[0m f1_weighted: 0.10355207518789504
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.0, 0.0, 0.366, 0.0, 0.0, 0.0, 0.024, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 00:00:48 (running for 00:07:57.00)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.138 |      0.276 |                   82 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  3.018 |      0.039 |                    6 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.762 |      0.002 |                    5 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.699 |      0.052 |                    3 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7722 | Steps: 2 | Val loss: 2.3103 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=68792)[0m top1: 0.3880597014925373
[2m[36m(func pid=68792)[0m top5: 0.9160447761194029
[2m[36m(func pid=68792)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=68792)[0m f1_macro: 0.29555054765250965
[2m[36m(func pid=68792)[0m f1_weighted: 0.41969116007903834
[2m[36m(func pid=68792)[0m f1_per_class: [0.193, 0.426, 0.259, 0.46, 0.099, 0.376, 0.482, 0.301, 0.154, 0.205]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86940)[0m top1: 0.10914179104477612
[2m[36m(func pid=86940)[0m top5: 0.6096082089552238
[2m[36m(func pid=86940)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=86940)[0m f1_macro: 0.08482259522641342
[2m[36m(func pid=86940)[0m f1_weighted: 0.11884231393294487
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.196, 0.293, 0.0, 0.027, 0.0, 0.272, 0.0, 0.06, 0.0]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9815 | Steps: 2 | Val loss: 2.3276 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=86021)[0m top1: 0.007462686567164179
[2m[36m(func pid=86021)[0m top5: 0.5652985074626866
[2m[36m(func pid=86021)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=86021)[0m f1_macro: 0.0021910453194200086
[2m[36m(func pid=86021)[0m f1_weighted: 0.0027648893228754004
[2m[36m(func pid=86021)[0m f1_per_class: [0.0, 0.0, 0.012, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.1111 | Steps: 2 | Val loss: 2.0609 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6296 | Steps: 2 | Val loss: 2.2020 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=85916)[0m top1: 0.21222014925373134
== Status ==
Current time: 2024-01-07 00:00:53 (running for 00:08:02.08)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.101 |      0.296 |                   83 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.981 |      0.045 |                    7 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.772 |      0.002 |                    6 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.636 |      0.085 |                    4 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=85916)[0m top5: 0.5643656716417911

[2m[36m(func pid=85916)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=85916)[0m f1_macro: 0.04545202538010987
[2m[36m(func pid=85916)[0m f1_weighted: 0.10557093446440936
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.0, 0.057, 0.372, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6989 | Steps: 2 | Val loss: 2.2972 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=68792)[0m top1: 0.37966417910447764
[2m[36m(func pid=68792)[0m top5: 0.90625
[2m[36m(func pid=68792)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=68792)[0m f1_macro: 0.3001519093944748
[2m[36m(func pid=68792)[0m f1_weighted: 0.4079732367704457
[2m[36m(func pid=68792)[0m f1_per_class: [0.208, 0.448, 0.268, 0.421, 0.111, 0.395, 0.45, 0.354, 0.132, 0.213]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86940)[0m top1: 0.02751865671641791
[2m[36m(func pid=86940)[0m top5: 0.6114738805970149
[2m[36m(func pid=86940)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=86940)[0m f1_macro: 0.03503458750825441
[2m[36m(func pid=86940)[0m f1_weighted: 0.018150646886595845
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.095, 0.16, 0.0, 0.069, 0.0, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9582 | Steps: 2 | Val loss: 2.3303 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=86021)[0m top1: 0.03264925373134328
[2m[36m(func pid=86021)[0m top5: 0.5895522388059702
[2m[36m(func pid=86021)[0m f1_micro: 0.03264925373134328
[2m[36m(func pid=86021)[0m f1_macro: 0.020125161148036235
[2m[36m(func pid=86021)[0m f1_weighted: 0.03356601389274815
[2m[36m(func pid=86021)[0m f1_per_class: [0.071, 0.0, 0.015, 0.115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.0655 | Steps: 2 | Val loss: 2.0702 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.3931 | Steps: 2 | Val loss: 2.2412 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:00:58 (running for 00:08:07.16)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.111 |      0.3   |                   84 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.958 |      0.05  |                    8 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.699 |      0.02  |                    7 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.63  |      0.035 |                    5 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.22061567164179105
[2m[36m(func pid=85916)[0m top5: 0.5643656716417911
[2m[36m(func pid=85916)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=85916)[0m f1_macro: 0.0499315492163704
[2m[36m(func pid=85916)[0m f1_weighted: 0.11089440653055008
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.005, 0.046, 0.386, 0.0, 0.0, 0.0, 0.029, 0.0, 0.033]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7029 | Steps: 2 | Val loss: 2.2752 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=68792)[0m top1: 0.3712686567164179
[2m[36m(func pid=68792)[0m top5: 0.9020522388059702
[2m[36m(func pid=68792)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=68792)[0m f1_macro: 0.30460250355353874
[2m[36m(func pid=68792)[0m f1_weighted: 0.3917038075826612
[2m[36m(func pid=68792)[0m f1_per_class: [0.226, 0.468, 0.282, 0.373, 0.129, 0.411, 0.413, 0.388, 0.142, 0.213]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86940)[0m top1: 0.012126865671641791
[2m[36m(func pid=86940)[0m top5: 0.4916044776119403
[2m[36m(func pid=86940)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=86940)[0m f1_macro: 0.0028510078443054316
[2m[36m(func pid=86940)[0m f1_weighted: 0.0011927499918489176
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9319 | Steps: 2 | Val loss: 2.3339 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86021)[0m top1: 0.07136194029850747
[2m[36m(func pid=86021)[0m top5: 0.6422574626865671
[2m[36m(func pid=86021)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=86021)[0m f1_macro: 0.027963574938845187
[2m[36m(func pid=86021)[0m f1_weighted: 0.0560733159774413
[2m[36m(func pid=86021)[0m f1_per_class: [0.055, 0.0, 0.029, 0.196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.1837 | Steps: 2 | Val loss: 2.0623 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4604 | Steps: 2 | Val loss: 2.2188 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 00:01:03 (running for 00:08:12.23)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.065 |      0.305 |                   85 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.932 |      0.047 |                    9 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.703 |      0.028 |                    8 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.393 |      0.003 |                    6 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.21688432835820895
[2m[36m(func pid=85916)[0m top5: 0.5517723880597015
[2m[36m(func pid=85916)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=85916)[0m f1_macro: 0.04701038033352943
[2m[36m(func pid=85916)[0m f1_weighted: 0.11367631572009469
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.016, 0.032, 0.391, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3572761194029851
[2m[36m(func pid=68792)[0m top5: 0.8950559701492538
[2m[36m(func pid=68792)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=68792)[0m f1_macro: 0.3069338316421608
[2m[36m(func pid=68792)[0m f1_weighted: 0.3684940216471369
[2m[36m(func pid=68792)[0m f1_per_class: [0.248, 0.474, 0.324, 0.342, 0.139, 0.413, 0.353, 0.417, 0.132, 0.229]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6393 | Steps: 2 | Val loss: 2.2539 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=86940)[0m top1: 0.018190298507462687
[2m[36m(func pid=86940)[0m top5: 0.5009328358208955
[2m[36m(func pid=86940)[0m f1_micro: 0.018190298507462687
[2m[36m(func pid=86940)[0m f1_macro: 0.01763770349296665
[2m[36m(func pid=86940)[0m f1_weighted: 0.008789835659803255
[2m[36m(func pid=86940)[0m f1_per_class: [0.079, 0.0, 0.0, 0.01, 0.0, 0.008, 0.0, 0.056, 0.0, 0.024]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8823 | Steps: 2 | Val loss: 2.3390 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=86021)[0m top1: 0.09281716417910447
[2m[36m(func pid=86021)[0m top5: 0.7201492537313433
[2m[36m(func pid=86021)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=86021)[0m f1_macro: 0.04770525156068593
[2m[36m(func pid=86021)[0m f1_weighted: 0.05980653941178046
[2m[36m(func pid=86021)[0m f1_per_class: [0.07, 0.009, 0.2, 0.183, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.0132 | Steps: 2 | Val loss: 2.0719 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2713 | Steps: 2 | Val loss: 2.1570 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 00:01:08 (running for 00:08:17.27)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.184 |      0.307 |                   86 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.882 |      0.045 |                   10 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.639 |      0.048 |                    9 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.46  |      0.018 |                    7 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.20149253731343283
[2m[36m(func pid=85916)[0m top5: 0.5410447761194029
[2m[36m(func pid=85916)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=85916)[0m f1_macro: 0.04544265099260506
[2m[36m(func pid=85916)[0m f1_weighted: 0.10975131269049691
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.0, 0.032, 0.385, 0.0, 0.0, 0.0, 0.038, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.35261194029850745
[2m[36m(func pid=68792)[0m top5: 0.8861940298507462
[2m[36m(func pid=68792)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=68792)[0m f1_macro: 0.3166028334559988
[2m[36m(func pid=68792)[0m f1_weighted: 0.35691775229480743
[2m[36m(func pid=68792)[0m f1_per_class: [0.245, 0.474, 0.344, 0.324, 0.149, 0.429, 0.308, 0.489, 0.149, 0.257]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6329 | Steps: 2 | Val loss: 2.2289 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=86940)[0m top1: 0.13246268656716417
[2m[36m(func pid=86940)[0m top5: 0.5713619402985075
[2m[36m(func pid=86940)[0m f1_micro: 0.13246268656716417
[2m[36m(func pid=86940)[0m f1_macro: 0.08247521257598159
[2m[36m(func pid=86940)[0m f1_weighted: 0.06601203882829555
[2m[36m(func pid=86940)[0m f1_per_class: [0.078, 0.146, 0.0, 0.029, 0.067, 0.237, 0.0, 0.011, 0.0, 0.256]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9258 | Steps: 2 | Val loss: 2.3454 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=86021)[0m top1: 0.1044776119402985
[2m[36m(func pid=86021)[0m top5: 0.8362873134328358
[2m[36m(func pid=86021)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=86021)[0m f1_macro: 0.06824403219055471
[2m[36m(func pid=86021)[0m f1_weighted: 0.0797615024279272
[2m[36m(func pid=86021)[0m f1_per_class: [0.083, 0.02, 0.324, 0.183, 0.0, 0.0, 0.072, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.9755 | Steps: 2 | Val loss: 2.0779 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.1541 | Steps: 2 | Val loss: 2.1237 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:01:13 (running for 00:08:22.39)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.013 |      0.317 |                   87 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.926 |      0.047 |                   11 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.633 |      0.068 |                   10 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.271 |      0.082 |                    8 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.17397388059701493
[2m[36m(func pid=85916)[0m top5: 0.5345149253731343
[2m[36m(func pid=85916)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=85916)[0m f1_macro: 0.047376985559829866
[2m[36m(func pid=85916)[0m f1_weighted: 0.10867292234194466
[2m[36m(func pid=85916)[0m f1_per_class: [0.035, 0.016, 0.031, 0.373, 0.0, 0.0, 0.0, 0.019, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.34048507462686567
[2m[36m(func pid=68792)[0m top5: 0.875
[2m[36m(func pid=68792)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=68792)[0m f1_macro: 0.31274195906967
[2m[36m(func pid=68792)[0m f1_weighted: 0.33880179218567374
[2m[36m(func pid=68792)[0m f1_per_class: [0.251, 0.468, 0.367, 0.324, 0.148, 0.425, 0.249, 0.504, 0.134, 0.257]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5785 | Steps: 2 | Val loss: 2.2106 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=86940)[0m top1: 0.10494402985074627
[2m[36m(func pid=86940)[0m top5: 0.6851679104477612
[2m[36m(func pid=86940)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=86940)[0m f1_macro: 0.048321156329221085
[2m[36m(func pid=86940)[0m f1_weighted: 0.06418795610427094
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.211, 0.0, 0.003, 0.036, 0.233, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8801 | Steps: 2 | Val loss: 2.3491 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=86021)[0m top1: 0.1333955223880597
[2m[36m(func pid=86021)[0m top5: 0.8498134328358209
[2m[36m(func pid=86021)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=86021)[0m f1_macro: 0.09099440915504045
[2m[36m(func pid=86021)[0m f1_weighted: 0.1282271673387514
[2m[36m(func pid=86021)[0m f1_per_class: [0.087, 0.095, 0.343, 0.193, 0.0, 0.016, 0.176, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.9794 | Steps: 2 | Val loss: 2.0888 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1172 | Steps: 2 | Val loss: 2.1428 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:01:18 (running for 00:08:27.59)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.976 |      0.313 |                   88 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.88  |      0.044 |                   12 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.579 |      0.091 |                   11 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.154 |      0.048 |                    9 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.14272388059701493
[2m[36m(func pid=85916)[0m top5: 0.5298507462686567
[2m[36m(func pid=85916)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=85916)[0m f1_macro: 0.04385983189101169
[2m[36m(func pid=85916)[0m f1_weighted: 0.09990276954422234
[2m[36m(func pid=85916)[0m f1_per_class: [0.033, 0.015, 0.025, 0.341, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3255597014925373
[2m[36m(func pid=68792)[0m top5: 0.863339552238806
[2m[36m(func pid=68792)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=68792)[0m f1_macro: 0.30650974312372525
[2m[36m(func pid=68792)[0m f1_weighted: 0.31555090447449363
[2m[36m(func pid=68792)[0m f1_per_class: [0.262, 0.466, 0.393, 0.308, 0.145, 0.424, 0.187, 0.507, 0.123, 0.25]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5736 | Steps: 2 | Val loss: 2.1986 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=86940)[0m top1: 0.054104477611940295
[2m[36m(func pid=86940)[0m top5: 0.7164179104477612
[2m[36m(func pid=86940)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=86940)[0m f1_macro: 0.04051931656187644
[2m[36m(func pid=86940)[0m f1_weighted: 0.05038104647487053
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.171, 0.0, 0.013, 0.023, 0.129, 0.0, 0.0, 0.069, 0.0]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8131 | Steps: 2 | Val loss: 2.3504 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=86021)[0m top1: 0.1646455223880597
[2m[36m(func pid=86021)[0m top5: 0.8446828358208955
[2m[36m(func pid=86021)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=86021)[0m f1_macro: 0.10845208549327365
[2m[36m(func pid=86021)[0m f1_weighted: 0.15125697792539455
[2m[36m(func pid=86021)[0m f1_per_class: [0.107, 0.23, 0.343, 0.165, 0.0, 0.055, 0.185, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.0722 | Steps: 2 | Val loss: 2.1100 | Batch size: 32 | lr: 0.001 | Duration: 2.58s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0398 | Steps: 2 | Val loss: 2.1635 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:01:23 (running for 00:08:32.85)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.979 |      0.307 |                   89 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.813 |      0.034 |                   13 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.574 |      0.108 |                   12 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.117 |      0.041 |                   10 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10914179104477612
[2m[36m(func pid=85916)[0m top5: 0.527518656716418
[2m[36m(func pid=85916)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=85916)[0m f1_macro: 0.03418693991262953
[2m[36m(func pid=85916)[0m f1_weighted: 0.08458232237919347
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.015, 0.022, 0.29, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.314365671641791
[2m[36m(func pid=68792)[0m top5: 0.8582089552238806
[2m[36m(func pid=68792)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=68792)[0m f1_macro: 0.29812608051875167
[2m[36m(func pid=68792)[0m f1_weighted: 0.3009022020399774
[2m[36m(func pid=68792)[0m f1_per_class: [0.242, 0.457, 0.4, 0.311, 0.137, 0.419, 0.146, 0.495, 0.131, 0.243]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5362 | Steps: 2 | Val loss: 2.1945 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=86940)[0m top1: 0.08395522388059702
[2m[36m(func pid=86940)[0m top5: 0.7388059701492538
[2m[36m(func pid=86940)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=86940)[0m f1_macro: 0.06757566063060065
[2m[36m(func pid=86940)[0m f1_weighted: 0.09006130511664957
[2m[36m(func pid=86940)[0m f1_per_class: [0.144, 0.0, 0.0, 0.25, 0.039, 0.096, 0.009, 0.0, 0.083, 0.054]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8225 | Steps: 2 | Val loss: 2.3529 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.2432 | Steps: 2 | Val loss: 2.1034 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=86021)[0m top1: 0.16977611940298507
[2m[36m(func pid=86021)[0m top5: 0.8316231343283582
[2m[36m(func pid=86021)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=86021)[0m f1_macro: 0.10721527488773275
[2m[36m(func pid=86021)[0m f1_weighted: 0.12360883211272462
[2m[36m(func pid=86021)[0m f1_per_class: [0.139, 0.27, 0.368, 0.072, 0.0, 0.078, 0.144, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1271 | Steps: 2 | Val loss: 2.1844 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 00:01:29 (running for 00:08:38.01)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.072 |      0.298 |                   90 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.823 |      0.034 |                   14 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.536 |      0.107 |                   13 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.04  |      0.068 |                   11 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.08488805970149253
[2m[36m(func pid=85916)[0m top5: 0.5247201492537313
[2m[36m(func pid=85916)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=85916)[0m f1_macro: 0.03406315482315768
[2m[36m(func pid=85916)[0m f1_weighted: 0.07414740872525603
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.015, 0.019, 0.248, 0.0, 0.0, 0.0, 0.031, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3031716417910448
[2m[36m(func pid=68792)[0m top5: 0.8577425373134329
[2m[36m(func pid=68792)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=68792)[0m f1_macro: 0.287952934156815
[2m[36m(func pid=68792)[0m f1_weighted: 0.28700477772283045
[2m[36m(func pid=68792)[0m f1_per_class: [0.243, 0.437, 0.407, 0.323, 0.123, 0.41, 0.105, 0.491, 0.127, 0.212]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5066 | Steps: 2 | Val loss: 2.1921 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=86940)[0m top1: 0.07509328358208955
[2m[36m(func pid=86940)[0m top5: 0.7742537313432836
[2m[36m(func pid=86940)[0m f1_micro: 0.07509328358208955
[2m[36m(func pid=86940)[0m f1_macro: 0.0595089775682092
[2m[36m(func pid=86940)[0m f1_weighted: 0.08464905918308263
[2m[36m(func pid=86940)[0m f1_per_class: [0.187, 0.011, 0.0, 0.262, 0.046, 0.0, 0.012, 0.0, 0.043, 0.034]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7891 | Steps: 2 | Val loss: 2.3534 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.0838 | Steps: 2 | Val loss: 2.1339 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=86021)[0m top1: 0.1707089552238806
[2m[36m(func pid=86021)[0m top5: 0.8031716417910447
[2m[36m(func pid=86021)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=86021)[0m f1_macro: 0.10948529835973239
[2m[36m(func pid=86021)[0m f1_weighted: 0.11807700010439996
[2m[36m(func pid=86021)[0m f1_per_class: [0.164, 0.274, 0.333, 0.065, 0.018, 0.133, 0.108, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9942 | Steps: 2 | Val loss: 2.1835 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 00:01:34 (running for 00:08:43.22)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.243 |      0.288 |                   91 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.789 |      0.027 |                   15 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.507 |      0.109 |                   14 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.127 |      0.06  |                   12 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.06763059701492537
[2m[36m(func pid=85916)[0m top5: 0.5293843283582089
[2m[36m(func pid=85916)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=85916)[0m f1_macro: 0.026962517859982355
[2m[36m(func pid=85916)[0m f1_weighted: 0.06490314854563328
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.02, 0.017, 0.217, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m top1: 0.2957089552238806
[2m[36m(func pid=68792)[0m top5: 0.8549440298507462
[2m[36m(func pid=68792)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=68792)[0m f1_macro: 0.280734020683846
[2m[36m(func pid=68792)[0m f1_weighted: 0.2811925085344824
[2m[36m(func pid=68792)[0m f1_per_class: [0.237, 0.431, 0.4, 0.346, 0.112, 0.393, 0.077, 0.485, 0.128, 0.2]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5348 | Steps: 2 | Val loss: 2.1924 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=86940)[0m top1: 0.05970149253731343
[2m[36m(func pid=86940)[0m top5: 0.8344216417910447
[2m[36m(func pid=86940)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=86940)[0m f1_macro: 0.061792245088424194
[2m[36m(func pid=86940)[0m f1_weighted: 0.07138388846022231
[2m[36m(func pid=86940)[0m f1_per_class: [0.187, 0.09, 0.0, 0.162, 0.083, 0.0, 0.012, 0.016, 0.042, 0.027]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.2049 | Steps: 2 | Val loss: 2.1328 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8198 | Steps: 2 | Val loss: 2.3553 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=86021)[0m top1: 0.16977611940298507
[2m[36m(func pid=86021)[0m top5: 0.7719216417910447
[2m[36m(func pid=86021)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=86021)[0m f1_macro: 0.11469330915037361
[2m[36m(func pid=86021)[0m f1_weighted: 0.1122727709507085
[2m[36m(func pid=86021)[0m f1_per_class: [0.185, 0.283, 0.348, 0.067, 0.022, 0.182, 0.061, 0.0, 0.0, 0.0]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9044 | Steps: 2 | Val loss: 2.1334 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 00:01:39 (running for 00:08:48.41)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.084 |      0.281 |                   92 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.82  |      0.023 |                   16 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.535 |      0.115 |                   15 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.994 |      0.062 |                   13 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68792)[0m top1: 0.2957089552238806
[2m[36m(func pid=68792)[0m top5: 0.8563432835820896
[2m[36m(func pid=68792)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=68792)[0m f1_macro: 0.2775391741989713
[2m[36m(func pid=68792)[0m f1_weighted: 0.28750963934469886
[2m[36m(func pid=68792)[0m f1_per_class: [0.229, 0.408, 0.4, 0.368, 0.105, 0.401, 0.093, 0.458, 0.131, 0.182]
[2m[36m(func pid=85916)[0m top1: 0.05177238805970149
[2m[36m(func pid=85916)[0m top5: 0.5265858208955224
[2m[36m(func pid=85916)[0m f1_micro: 0.05177238805970149
[2m[36m(func pid=85916)[0m f1_macro: 0.023085500343028153
[2m[36m(func pid=85916)[0m f1_weighted: 0.05382932533795599
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.025, 0.016, 0.174, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4696 | Steps: 2 | Val loss: 2.1967 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=86940)[0m top1: 0.06949626865671642
[2m[36m(func pid=86940)[0m top5: 0.8069029850746269
[2m[36m(func pid=86940)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=86940)[0m f1_macro: 0.08331203612004603
[2m[36m(func pid=86940)[0m f1_weighted: 0.08639902358030788
[2m[36m(func pid=86940)[0m f1_per_class: [0.176, 0.147, 0.0, 0.077, 0.109, 0.0, 0.077, 0.192, 0.027, 0.027]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.9738 | Steps: 2 | Val loss: 2.1529 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8433 | Steps: 2 | Val loss: 2.3579 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=86021)[0m top1: 0.16744402985074627
[2m[36m(func pid=86021)[0m top5: 0.7098880597014925
[2m[36m(func pid=86021)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=86021)[0m f1_macro: 0.14048297076518032
[2m[36m(func pid=86021)[0m f1_weighted: 0.12703675052637375
[2m[36m(func pid=86021)[0m f1_per_class: [0.18, 0.278, 0.364, 0.117, 0.051, 0.227, 0.021, 0.13, 0.0, 0.036]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0799 | Steps: 2 | Val loss: 2.1026 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=68792)[0m top1: 0.2957089552238806
[2m[36m(func pid=68792)[0m top5: 0.8582089552238806
[2m[36m(func pid=68792)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=68792)[0m f1_macro: 0.2781071436745486
[2m[36m(func pid=68792)[0m f1_weighted: 0.2935253354941576
[2m[36m(func pid=68792)[0m f1_per_class: [0.222, 0.392, 0.4, 0.389, 0.098, 0.401, 0.106, 0.447, 0.124, 0.202]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-07 00:01:44 (running for 00:08:53.64)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.974 |      0.278 |                   94 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.843 |      0.018 |                   17 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.47  |      0.14  |                   16 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.904 |      0.083 |                   14 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.04057835820895522
[2m[36m(func pid=85916)[0m top5: 0.5303171641791045
[2m[36m(func pid=85916)[0m f1_micro: 0.04057835820895522
[2m[36m(func pid=85916)[0m f1_macro: 0.01834586380165146
[2m[36m(func pid=85916)[0m f1_weighted: 0.043767308435510445
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.031, 0.015, 0.138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4921 | Steps: 2 | Val loss: 2.2026 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=86940)[0m top1: 0.1044776119402985
[2m[36m(func pid=86940)[0m top5: 0.7182835820895522
[2m[36m(func pid=86940)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=86940)[0m f1_macro: 0.11212123559110801
[2m[36m(func pid=86940)[0m f1_weighted: 0.11481806941284986
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.104, 0.273, 0.232, 0.105, 0.008, 0.033, 0.31, 0.027, 0.031]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.0420 | Steps: 2 | Val loss: 2.1409 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7651 | Steps: 2 | Val loss: 2.3569 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=86021)[0m top1: 0.15625
[2m[36m(func pid=86021)[0m top5: 0.6716417910447762
[2m[36m(func pid=86021)[0m f1_micro: 0.15625
[2m[36m(func pid=86021)[0m f1_macro: 0.1568087352583018
[2m[36m(func pid=86021)[0m f1_weighted: 0.13411859598375644
[2m[36m(func pid=86021)[0m f1_per_class: [0.166, 0.247, 0.37, 0.134, 0.04, 0.246, 0.003, 0.326, 0.0, 0.036]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.8318 | Steps: 2 | Val loss: 2.1116 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=68792)[0m top1: 0.291044776119403
[2m[36m(func pid=68792)[0m top5: 0.8596082089552238
[2m[36m(func pid=68792)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=68792)[0m f1_macro: 0.2725760669538041
[2m[36m(func pid=68792)[0m f1_weighted: 0.29755050831413915
[2m[36m(func pid=68792)[0m f1_per_class: [0.222, 0.342, 0.4, 0.402, 0.088, 0.391, 0.142, 0.45, 0.119, 0.169]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-07 00:01:49 (running for 00:08:58.68)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.042 |      0.273 |                   95 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.765 |      0.017 |                   18 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.492 |      0.157 |                   17 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.08  |      0.112 |                   15 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.03591417910447761
[2m[36m(func pid=85916)[0m top5: 0.5261194029850746
[2m[36m(func pid=85916)[0m f1_micro: 0.03591417910447761
[2m[36m(func pid=85916)[0m f1_macro: 0.016877537597458615
[2m[36m(func pid=85916)[0m f1_weighted: 0.03973631695133296
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.031, 0.015, 0.123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4493 | Steps: 2 | Val loss: 2.2120 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=86940)[0m top1: 0.13805970149253732
[2m[36m(func pid=86940)[0m top5: 0.7588619402985075
[2m[36m(func pid=86940)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=86940)[0m f1_macro: 0.14410839033030043
[2m[36m(func pid=86940)[0m f1_weighted: 0.1308844261537219
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.011, 0.333, 0.275, 0.08, 0.167, 0.015, 0.373, 0.114, 0.072]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.0263 | Steps: 2 | Val loss: 2.1334 | Batch size: 32 | lr: 0.001 | Duration: 2.56s
[2m[36m(func pid=86021)[0m top1: 0.1259328358208955
[2m[36m(func pid=86021)[0m top5: 0.6548507462686567
[2m[36m(func pid=86021)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=86021)[0m f1_macro: 0.13761718377176396
[2m[36m(func pid=86021)[0m f1_weighted: 0.10832587074443352
[2m[36m(func pid=86021)[0m f1_per_class: [0.138, 0.143, 0.333, 0.108, 0.04, 0.243, 0.003, 0.336, 0.0, 0.033]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7614 | Steps: 2 | Val loss: 2.3583 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9254 | Steps: 2 | Val loss: 2.1440 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=68792)[0m top1: 0.29197761194029853
[2m[36m(func pid=68792)[0m top5: 0.8610074626865671
[2m[36m(func pid=68792)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=68792)[0m f1_macro: 0.27166460839330664
[2m[36m(func pid=68792)[0m f1_weighted: 0.3074304746358381
[2m[36m(func pid=68792)[0m f1_per_class: [0.216, 0.305, 0.393, 0.396, 0.082, 0.393, 0.201, 0.458, 0.125, 0.148]
[2m[36m(func pid=68792)[0m 
[2m[36m(func pid=85916)[0m top1: 0.03031716417910448
[2m[36m(func pid=85916)[0m top5: 0.5289179104477612
[2m[36m(func pid=85916)[0m f1_micro: 0.03031716417910448
[2m[36m(func pid=85916)[0m f1_macro: 0.015476347261927762
[2m[36m(func pid=85916)[0m f1_weighted: 0.03483030007359944
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.041, 0.014, 0.099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
== Status ==
Current time: 2024-01-07 00:01:55 (running for 00:09:03.94)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.026 |      0.272 |                   96 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.761 |      0.015 |                   19 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.449 |      0.138 |                   18 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.832 |      0.144 |                   16 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4463 | Steps: 2 | Val loss: 2.2170 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=86940)[0m top1: 0.1259328358208955
[2m[36m(func pid=86940)[0m top5: 0.7737873134328358
[2m[36m(func pid=86940)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=86940)[0m f1_macro: 0.13747727098138735
[2m[36m(func pid=86940)[0m f1_weighted: 0.08787984638373625
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.0, 0.328, 0.129, 0.054, 0.241, 0.0, 0.269, 0.112, 0.242]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.9676 | Steps: 2 | Val loss: 2.1119 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7633 | Steps: 2 | Val loss: 2.3591 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=86021)[0m top1: 0.11520522388059702
[2m[36m(func pid=86021)[0m top5: 0.6520522388059702
[2m[36m(func pid=86021)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=86021)[0m f1_macro: 0.12321555237288243
[2m[36m(func pid=86021)[0m f1_weighted: 0.09614710515969062
[2m[36m(func pid=86021)[0m f1_per_class: [0.111, 0.096, 0.286, 0.107, 0.038, 0.228, 0.0, 0.325, 0.0, 0.041]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7955 | Steps: 2 | Val loss: 2.1226 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=68792)[0m top1: 0.3069029850746269
[2m[36m(func pid=68792)[0m top5: 0.8642723880597015
[2m[36m(func pid=68792)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=68792)[0m f1_macro: 0.2769012841448099
[2m[36m(func pid=68792)[0m f1_weighted: 0.33273695263618913
[2m[36m(func pid=68792)[0m f1_per_class: [0.218, 0.271, 0.393, 0.437, 0.077, 0.387, 0.27, 0.453, 0.129, 0.134]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-07 00:02:00 (running for 00:09:09.08)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.968 |      0.277 |                   97 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.763 |      0.015 |                   20 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.446 |      0.123 |                   19 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.925 |      0.137 |                   17 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.027052238805970148
[2m[36m(func pid=85916)[0m top5: 0.527518656716418
[2m[36m(func pid=85916)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=85916)[0m f1_macro: 0.014693966536789
[2m[36m(func pid=85916)[0m f1_weighted: 0.03163705066318267
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.051, 0.014, 0.081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4560 | Steps: 2 | Val loss: 2.2141 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86940)[0m top1: 0.15811567164179105
[2m[36m(func pid=86940)[0m top5: 0.8390858208955224
[2m[36m(func pid=86940)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=86940)[0m f1_macro: 0.19262724136652742
[2m[36m(func pid=86940)[0m f1_weighted: 0.15881841582323028
[2m[36m(func pid=86940)[0m f1_per_class: [0.213, 0.185, 0.41, 0.246, 0.043, 0.216, 0.006, 0.312, 0.148, 0.147]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.9423 | Steps: 2 | Val loss: 2.0938 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7628 | Steps: 2 | Val loss: 2.3599 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=86021)[0m top1: 0.11567164179104478
[2m[36m(func pid=86021)[0m top5: 0.6646455223880597
[2m[36m(func pid=86021)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=86021)[0m f1_macro: 0.12014602240229252
[2m[36m(func pid=86021)[0m f1_weighted: 0.10176571185203766
[2m[36m(func pid=86021)[0m f1_per_class: [0.093, 0.098, 0.242, 0.131, 0.038, 0.213, 0.0, 0.343, 0.0, 0.044]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1690 | Steps: 2 | Val loss: 2.0974 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=68792)[0m top1: 0.31763059701492535
[2m[36m(func pid=68792)[0m top5: 0.8726679104477612
[2m[36m(func pid=68792)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=68792)[0m f1_macro: 0.27768259309303434
[2m[36m(func pid=68792)[0m f1_weighted: 0.35083555454478227
[2m[36m(func pid=68792)[0m f1_per_class: [0.217, 0.237, 0.386, 0.449, 0.073, 0.367, 0.349, 0.453, 0.113, 0.133]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-07 00:02:05 (running for 00:09:14.29)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.942 |      0.278 |                   98 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.763 |      0.013 |                   21 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.456 |      0.12  |                   20 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.796 |      0.193 |                   18 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.025186567164179104
[2m[36m(func pid=85916)[0m top5: 0.5270522388059702
[2m[36m(func pid=85916)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=85916)[0m f1_macro: 0.013497882379841026
[2m[36m(func pid=85916)[0m f1_weighted: 0.028862774845829164
[2m[36m(func pid=85916)[0m f1_per_class: [0.0, 0.046, 0.014, 0.075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5777 | Steps: 2 | Val loss: 2.2118 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=86940)[0m top1: 0.1310634328358209
[2m[36m(func pid=86940)[0m top5: 0.7910447761194029
[2m[36m(func pid=86940)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=86940)[0m f1_macro: 0.16220049720942803
[2m[36m(func pid=86940)[0m f1_weighted: 0.10708123853454504
[2m[36m(func pid=86940)[0m f1_per_class: [0.276, 0.27, 0.32, 0.04, 0.038, 0.111, 0.006, 0.444, 0.0, 0.117]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.1084 | Steps: 2 | Val loss: 2.0994 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7427 | Steps: 2 | Val loss: 2.3603 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=86021)[0m top1: 0.12313432835820895
[2m[36m(func pid=86021)[0m top5: 0.7019589552238806
[2m[36m(func pid=86021)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=86021)[0m f1_macro: 0.13247326393186507
[2m[36m(func pid=86021)[0m f1_weighted: 0.12324197788797091
[2m[36m(func pid=86021)[0m f1_per_class: [0.09, 0.14, 0.229, 0.178, 0.038, 0.197, 0.003, 0.366, 0.025, 0.06]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7236 | Steps: 2 | Val loss: 2.0831 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=68792)[0m top1: 0.3292910447761194
[2m[36m(func pid=68792)[0m top5: 0.8745335820895522
[2m[36m(func pid=68792)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=68792)[0m f1_macro: 0.28209622471761725
[2m[36m(func pid=68792)[0m f1_weighted: 0.3677979571300733
[2m[36m(func pid=68792)[0m f1_per_class: [0.206, 0.24, 0.367, 0.449, 0.071, 0.352, 0.405, 0.478, 0.126, 0.127]
[2m[36m(func pid=68792)[0m 
== Status ==
Current time: 2024-01-07 00:02:10 (running for 00:09:19.67)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00001 | RUNNING    | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  1.108 |      0.282 |                   99 |
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.743 |      0.015 |                   22 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.578 |      0.132 |                   21 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  2.169 |      0.162 |                   19 |
| train_51d3e_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.0228544776119403
[2m[36m(func pid=85916)[0m top5: 0.5219216417910447
[2m[36m(func pid=85916)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=85916)[0m f1_macro: 0.014951864786314759
[2m[36m(func pid=85916)[0m f1_weighted: 0.02631283476065751
[2m[36m(func pid=85916)[0m f1_per_class: [0.022, 0.055, 0.014, 0.058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4213 | Steps: 2 | Val loss: 2.2019 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=86940)[0m top1: 0.13059701492537312
[2m[36m(func pid=86940)[0m top5: 0.7518656716417911
[2m[36m(func pid=86940)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=86940)[0m f1_macro: 0.10633409319325877
[2m[36m(func pid=86940)[0m f1_weighted: 0.0971053432118011
[2m[36m(func pid=86940)[0m f1_per_class: [0.044, 0.265, 0.0, 0.063, 0.029, 0.008, 0.0, 0.523, 0.0, 0.13]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=68792)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.9988 | Steps: 2 | Val loss: 2.0895 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7507 | Steps: 2 | Val loss: 2.3610 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=86021)[0m top1: 0.12406716417910447
[2m[36m(func pid=86021)[0m top5: 0.7178171641791045
[2m[36m(func pid=86021)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=86021)[0m f1_macro: 0.134167467142692
[2m[36m(func pid=86021)[0m f1_weighted: 0.12657838095934099
[2m[36m(func pid=86021)[0m f1_per_class: [0.092, 0.131, 0.247, 0.201, 0.037, 0.173, 0.003, 0.388, 0.024, 0.047]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=68792)[0m top1: 0.3353544776119403
[2m[36m(func pid=68792)[0m top5: 0.8833955223880597
[2m[36m(func pid=68792)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=68792)[0m f1_macro: 0.27784875764369915
[2m[36m(func pid=68792)[0m f1_weighted: 0.37492434198045377
[2m[36m(func pid=68792)[0m f1_per_class: [0.201, 0.21, 0.355, 0.461, 0.069, 0.349, 0.444, 0.452, 0.103, 0.134]
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6761 | Steps: 2 | Val loss: 2.0031 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=85916)[0m top1: 0.021455223880597014
[2m[36m(func pid=85916)[0m top5: 0.5209888059701493
[2m[36m(func pid=85916)[0m f1_micro: 0.021455223880597014
[2m[36m(func pid=85916)[0m f1_macro: 0.015618031760572046
[2m[36m(func pid=85916)[0m f1_weighted: 0.02391992211123496
[2m[36m(func pid=85916)[0m f1_per_class: [0.041, 0.049, 0.014, 0.052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.3804 | Steps: 2 | Val loss: 2.1886 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=86940)[0m top1: 0.24440298507462688
[2m[36m(func pid=86940)[0m top5: 0.8101679104477612
[2m[36m(func pid=86940)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=86940)[0m f1_macro: 0.1768944813600243
[2m[36m(func pid=86940)[0m f1_weighted: 0.19236251654619363
[2m[36m(func pid=86940)[0m f1_per_class: [0.274, 0.04, 0.32, 0.531, 0.033, 0.008, 0.0, 0.476, 0.0, 0.086]
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7425 | Steps: 2 | Val loss: 2.3620 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=86021)[0m top1: 0.14458955223880596
[2m[36m(func pid=86021)[0m top5: 0.7318097014925373
[2m[36m(func pid=86021)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=86021)[0m f1_macro: 0.14985197707475145
[2m[36m(func pid=86021)[0m f1_weighted: 0.1477573179422124
[2m[36m(func pid=86021)[0m f1_per_class: [0.118, 0.135, 0.306, 0.257, 0.038, 0.181, 0.012, 0.404, 0.0, 0.048]
[2m[36m(func pid=85916)[0m top1: 0.022388059701492536
[2m[36m(func pid=85916)[0m top5: 0.5223880597014925
[2m[36m(func pid=85916)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=85916)[0m f1_macro: 0.015986479589069052
[2m[36m(func pid=85916)[0m f1_weighted: 0.025166764626333805
[2m[36m(func pid=85916)[0m f1_per_class: [0.04, 0.048, 0.014, 0.057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 00:02:15 (running for 00:09:24.72)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.751 |      0.016 |                   23 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.421 |      0.134 |                   22 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.724 |      0.106 |                   20 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=91972)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=91972)[0m Configuration completed!
[2m[36m(func pid=91972)[0m New optimizer parameters:
[2m[36m(func pid=91972)[0m SGD (
[2m[36m(func pid=91972)[0m Parameter Group 0
[2m[36m(func pid=91972)[0m     dampening: 0
[2m[36m(func pid=91972)[0m     differentiable: False
[2m[36m(func pid=91972)[0m     foreach: None
[2m[36m(func pid=91972)[0m     lr: 0.1
[2m[36m(func pid=91972)[0m     maximize: False
[2m[36m(func pid=91972)[0m     momentum: 0.9
[2m[36m(func pid=91972)[0m     nesterov: False
[2m[36m(func pid=91972)[0m     weight_decay: 0
[2m[36m(func pid=91972)[0m )
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:02:22 (running for 00:09:31.48)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.751 |      0.016 |                   23 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.421 |      0.134 |                   22 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.676 |      0.177 |                   21 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8060 | Steps: 2 | Val loss: 2.3646 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.3693 | Steps: 2 | Val loss: 2.1791 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8544 | Steps: 2 | Val loss: 2.0621 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 5.8352 | Steps: 2 | Val loss: 2.8614 | Batch size: 32 | lr: 0.1 | Duration: 4.41s
== Status ==
Current time: 2024-01-07 00:02:27 (running for 00:09:36.49)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.742 |      0.016 |                   24 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.38  |      0.15  |                   23 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.676 |      0.177 |                   21 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |        |            |                      |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m top1: 0.24347014925373134
[2m[36m(func pid=86940)[0m top5: 0.7719216417910447
[2m[36m(func pid=86940)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=86940)[0m f1_macro: 0.18688642583199158
[2m[36m(func pid=86940)[0m f1_weighted: 0.2035227908040838
[2m[36m(func pid=86940)[0m f1_per_class: [0.32, 0.0, 0.357, 0.544, 0.045, 0.089, 0.031, 0.369, 0.041, 0.073]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.022388059701492536
[2m[36m(func pid=85916)[0m top5: 0.5205223880597015
[2m[36m(func pid=85916)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=85916)[0m f1_macro: 0.016641292789223407
[2m[36m(func pid=85916)[0m f1_weighted: 0.02559100664373204
[2m[36m(func pid=85916)[0m f1_per_class: [0.039, 0.065, 0.014, 0.048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.16138059701492538
[2m[36m(func pid=86021)[0m top5: 0.7448694029850746
[2m[36m(func pid=86021)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=86021)[0m f1_macro: 0.1681452528417699
[2m[36m(func pid=86021)[0m f1_weighted: 0.16291357385646335
[2m[36m(func pid=86021)[0m f1_per_class: [0.128, 0.131, 0.355, 0.289, 0.04, 0.195, 0.019, 0.435, 0.024, 0.065]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m top1: 0.020522388059701493
[2m[36m(func pid=91972)[0m top5: 0.6380597014925373
[2m[36m(func pid=91972)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=91972)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=91972)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=91972)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.7644 | Steps: 2 | Val loss: 2.1207 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7203 | Steps: 2 | Val loss: 2.3628 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3801 | Steps: 2 | Val loss: 2.1690 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 11.3233 | Steps: 2 | Val loss: 2.7483 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=86021)[0m top1: 0.15485074626865672
[2m[36m(func pid=86021)[0m top5: 0.7644589552238806
[2m[36m(func pid=86021)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=86021)[0m f1_macro: 0.17117463881609446
[2m[36m(func pid=86021)[0m f1_weighted: 0.16654415880742343
[2m[36m(func pid=86021)[0m f1_per_class: [0.123, 0.147, 0.4, 0.266, 0.04, 0.178, 0.051, 0.426, 0.025, 0.056]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m top1: 0.1828358208955224
[2m[36m(func pid=86940)[0m top5: 0.753731343283582
[2m[36m(func pid=86940)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=86940)[0m f1_macro: 0.20467531511861528
[2m[36m(func pid=86940)[0m f1_weighted: 0.20177893846595657
[2m[36m(func pid=86940)[0m f1_per_class: [0.345, 0.0, 0.393, 0.235, 0.073, 0.182, 0.265, 0.374, 0.126, 0.054]
== Status ==
Current time: 2024-01-07 00:02:32 (running for 00:09:41.80)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.806 |      0.017 |                   25 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.38  |      0.171 |                   25 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.854 |      0.187 |                   22 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  5.835 |      0.004 |                    1 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.026119402985074626
[2m[36m(func pid=85916)[0m top5: 0.519589552238806
[2m[36m(func pid=85916)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=85916)[0m f1_macro: 0.020235615196663472
[2m[36m(func pid=85916)[0m f1_weighted: 0.030024061844695128
[2m[36m(func pid=85916)[0m f1_per_class: [0.051, 0.087, 0.015, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=91972)[0m top1: 0.021921641791044777
[2m[36m(func pid=91972)[0m top5: 0.715018656716418
[2m[36m(func pid=91972)[0m f1_micro: 0.021921641791044777
[2m[36m(func pid=91972)[0m f1_macro: 0.014179989049437337
[2m[36m(func pid=91972)[0m f1_weighted: 0.022399447792183627
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.0, 0.0, 0.013, 0.017, 0.0, 0.056, 0.0, 0.056, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4036 | Steps: 2 | Val loss: 2.1593 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.7390 | Steps: 2 | Val loss: 2.0455 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7528 | Steps: 2 | Val loss: 2.3638 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 15.5522 | Steps: 2 | Val loss: 3.9577 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 00:02:37 (running for 00:09:46.89)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.72  |      0.02  |                   26 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.404 |      0.175 |                   26 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.764 |      0.205 |                   23 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 11.323 |      0.014 |                    2 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.1707089552238806
[2m[36m(func pid=86021)[0m top5: 0.7831156716417911
[2m[36m(func pid=86021)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=86021)[0m f1_macro: 0.17547065569807183
[2m[36m(func pid=86021)[0m f1_weighted: 0.19021153674078248
[2m[36m(func pid=86021)[0m f1_per_class: [0.121, 0.152, 0.364, 0.326, 0.041, 0.169, 0.076, 0.422, 0.025, 0.058]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m top1: 0.23414179104477612
[2m[36m(func pid=86940)[0m top5: 0.8353544776119403
[2m[36m(func pid=86940)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=86940)[0m f1_macro: 0.22389781636292683
[2m[36m(func pid=86940)[0m f1_weighted: 0.27482867310317105
[2m[36m(func pid=86940)[0m f1_per_class: [0.205, 0.032, 0.381, 0.325, 0.081, 0.268, 0.38, 0.411, 0.1, 0.056]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.027985074626865673
[2m[36m(func pid=85916)[0m top5: 0.5153917910447762
[2m[36m(func pid=85916)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=85916)[0m f1_macro: 0.01949733691587611
[2m[36m(func pid=85916)[0m f1_weighted: 0.03188531655982316
[2m[36m(func pid=85916)[0m f1_per_class: [0.033, 0.094, 0.015, 0.054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.05783582089552239
[2m[36m(func pid=91972)[0m top5: 0.7518656716417911
[2m[36m(func pid=91972)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=91972)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=91972)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4821 | Steps: 2 | Val loss: 2.1458 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.5867 | Steps: 2 | Val loss: 2.0170 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7307 | Steps: 2 | Val loss: 2.3624 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 9.1583 | Steps: 2 | Val loss: 4.5727 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=86021)[0m top1: 0.19309701492537312
[2m[36m(func pid=86021)[0m top5: 0.804570895522388
[2m[36m(func pid=86021)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=86021)[0m f1_macro: 0.1764104650840796
[2m[36m(func pid=86021)[0m f1_weighted: 0.22075315398362552
[2m[36m(func pid=86021)[0m f1_per_class: [0.125, 0.166, 0.282, 0.353, 0.043, 0.13, 0.166, 0.398, 0.025, 0.075]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=85916)[0m top1: 0.03311567164179104
[2m[36m(func pid=85916)[0m top5: 0.5186567164179104
[2m[36m(func pid=85916)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=85916)[0m f1_macro: 0.021493071025232176
[2m[36m(func pid=85916)[0m f1_weighted: 0.03639494693007279
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.113, 0.016, 0.059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
== Status ==
Current time: 2024-01-07 00:02:43 (running for 00:09:52.14)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.731 |      0.021 |                   28 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.482 |      0.176 |                   27 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.739 |      0.224 |                   24 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 15.552 |      0.011 |                    3 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m top1: 0.22434701492537312
[2m[36m(func pid=86940)[0m top5: 0.8390858208955224
[2m[36m(func pid=86940)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=86940)[0m f1_macro: 0.21673254665300595
[2m[36m(func pid=86940)[0m f1_weighted: 0.22767096740410064
[2m[36m(func pid=86940)[0m f1_per_class: [0.271, 0.286, 0.367, 0.168, 0.068, 0.305, 0.22, 0.376, 0.0, 0.106]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=91972)[0m top1: 0.02751865671641791
[2m[36m(func pid=91972)[0m top5: 0.44169776119402987
[2m[36m(func pid=91972)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=91972)[0m f1_macro: 0.03951139018058566
[2m[36m(func pid=91972)[0m f1_weighted: 0.0035282423227194523
[2m[36m(func pid=91972)[0m f1_per_class: [0.076, 0.0, 0.293, 0.0, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3265 | Steps: 2 | Val loss: 2.1293 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.5441 | Steps: 2 | Val loss: 2.0575 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6997 | Steps: 2 | Val loss: 2.3603 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 14.3148 | Steps: 2 | Val loss: 4.4382 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=86021)[0m top1: 0.22294776119402984
[2m[36m(func pid=86021)[0m top5: 0.8213619402985075
[2m[36m(func pid=86021)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=86021)[0m f1_macro: 0.18066428197901527
[2m[36m(func pid=86021)[0m f1_weighted: 0.25237795452467865
[2m[36m(func pid=86021)[0m f1_per_class: [0.131, 0.239, 0.242, 0.374, 0.046, 0.12, 0.233, 0.294, 0.025, 0.102]
[2m[36m(func pid=86021)[0m 
== Status ==
Current time: 2024-01-07 00:02:48 (running for 00:09:57.26)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.731 |      0.021 |                   28 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.326 |      0.181 |                   28 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.544 |      0.176 |                   26 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  9.158 |      0.04  |                    4 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m top1: 0.18936567164179105
[2m[36m(func pid=86940)[0m top5: 0.8376865671641791
[2m[36m(func pid=86940)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=86940)[0m f1_macro: 0.1755704385658122
[2m[36m(func pid=86940)[0m f1_weighted: 0.16151868209707587
[2m[36m(func pid=86940)[0m f1_per_class: [0.149, 0.286, 0.265, 0.013, 0.054, 0.251, 0.162, 0.431, 0.0, 0.144]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.03871268656716418
[2m[36m(func pid=85916)[0m top5: 0.5167910447761194
[2m[36m(func pid=85916)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=85916)[0m f1_macro: 0.02418326799154133
[2m[36m(func pid=85916)[0m f1_weighted: 0.04013801539904502
[2m[36m(func pid=85916)[0m f1_per_class: [0.033, 0.13, 0.018, 0.061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.26119402985074625
[2m[36m(func pid=91972)[0m top5: 0.7220149253731343
[2m[36m(func pid=91972)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=91972)[0m f1_macro: 0.0679078014184397
[2m[36m(func pid=91972)[0m f1_weighted: 0.11794513375372377
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.0, 0.262, 0.417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3400 | Steps: 2 | Val loss: 2.1209 | Batch size: 32 | lr: 0.001 | Duration: 2.58s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.4707 | Steps: 2 | Val loss: 2.0554 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8182 | Steps: 2 | Val loss: 2.3615 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 13.3469 | Steps: 2 | Val loss: 6.7400 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=86021)[0m top1: 0.228544776119403
[2m[36m(func pid=86021)[0m top5: 0.84375
[2m[36m(func pid=86021)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=86021)[0m f1_macro: 0.18683377644555627
[2m[36m(func pid=86021)[0m f1_weighted: 0.25973983471324097
[2m[36m(func pid=86021)[0m f1_per_class: [0.15, 0.265, 0.229, 0.311, 0.049, 0.164, 0.29, 0.265, 0.022, 0.123]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m top1: 0.2080223880597015
[2m[36m(func pid=86940)[0m top5: 0.8418843283582089
[2m[36m(func pid=86940)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=86940)[0m f1_macro: 0.1991776367757379
[2m[36m(func pid=86940)[0m f1_weighted: 0.22184104894184958
[2m[36m(func pid=86940)[0m f1_per_class: [0.182, 0.309, 0.224, 0.195, 0.04, 0.182, 0.191, 0.478, 0.067, 0.123]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:02:53 (running for 00:10:02.39)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.7   |      0.024 |                   29 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.34  |      0.187 |                   29 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.471 |      0.199 |                   27 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 14.315 |      0.068 |                    5 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.0457089552238806
[2m[36m(func pid=85916)[0m top5: 0.5121268656716418
[2m[36m(func pid=85916)[0m f1_micro: 0.0457089552238806
[2m[36m(func pid=85916)[0m f1_macro: 0.02679480757193179
[2m[36m(func pid=85916)[0m f1_weighted: 0.04598645945904079
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.152, 0.019, 0.068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.17164179104477612
[2m[36m(func pid=91972)[0m top5: 0.6184701492537313
[2m[36m(func pid=91972)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=91972)[0m f1_macro: 0.02934609250398724
[2m[36m(func pid=91972)[0m f1_weighted: 0.05050703420695565
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3103 | Steps: 2 | Val loss: 2.1191 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.5373 | Steps: 2 | Val loss: 2.0446 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7469 | Steps: 2 | Val loss: 2.3594 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 12.7983 | Steps: 2 | Val loss: 8.2901 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=86021)[0m top1: 0.21921641791044777
[2m[36m(func pid=86021)[0m top5: 0.8582089552238806
[2m[36m(func pid=86021)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=86021)[0m f1_macro: 0.17727079388153003
[2m[36m(func pid=86021)[0m f1_weighted: 0.24737024279280054
[2m[36m(func pid=86021)[0m f1_per_class: [0.156, 0.279, 0.227, 0.228, 0.049, 0.177, 0.321, 0.228, 0.018, 0.089]
[2m[36m(func pid=86021)[0m 
== Status ==
Current time: 2024-01-07 00:02:58 (running for 00:10:07.54)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.818 |      0.027 |                   30 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.31  |      0.177 |                   30 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.537 |      0.236 |                   28 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 13.347 |      0.029 |                    6 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m top1: 0.25046641791044777
[2m[36m(func pid=86940)[0m top5: 0.8512126865671642
[2m[36m(func pid=86940)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=86940)[0m f1_macro: 0.23569908372304363
[2m[36m(func pid=86940)[0m f1_weighted: 0.27714024853687486
[2m[36m(func pid=86940)[0m f1_per_class: [0.214, 0.242, 0.272, 0.454, 0.037, 0.16, 0.166, 0.488, 0.136, 0.189]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.05177238805970149
[2m[36m(func pid=85916)[0m top5: 0.5102611940298507
[2m[36m(func pid=85916)[0m f1_micro: 0.05177238805970149
[2m[36m(func pid=85916)[0m f1_macro: 0.02975038919877725
[2m[36m(func pid=85916)[0m f1_weighted: 0.05026480666811943
[2m[36m(func pid=85916)[0m f1_per_class: [0.025, 0.161, 0.021, 0.075, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.012126865671641791
[2m[36m(func pid=91972)[0m top5: 0.46921641791044777
[2m[36m(func pid=91972)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=91972)[0m f1_macro: 0.003851540616246499
[2m[36m(func pid=91972)[0m f1_weighted: 0.0007646248798026674
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.024]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3000 | Steps: 2 | Val loss: 2.1221 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.4641 | Steps: 2 | Val loss: 2.0176 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7242 | Steps: 2 | Val loss: 2.3551 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 10.2414 | Steps: 2 | Val loss: 6.8892 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=86021)[0m top1: 0.21828358208955223
[2m[36m(func pid=86021)[0m top5: 0.8605410447761194
[2m[36m(func pid=86021)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=86021)[0m f1_macro: 0.1819769118323194
[2m[36m(func pid=86021)[0m f1_weighted: 0.24922433065972913
[2m[36m(func pid=86021)[0m f1_per_class: [0.143, 0.285, 0.222, 0.205, 0.048, 0.177, 0.337, 0.256, 0.054, 0.093]
[2m[36m(func pid=86021)[0m 
== Status ==
Current time: 2024-01-07 00:03:03 (running for 00:10:12.69)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.747 |      0.03  |                   31 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.3   |      0.182 |                   31 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.464 |      0.249 |                   29 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 12.798 |      0.004 |                    7 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m top1: 0.302705223880597
[2m[36m(func pid=86940)[0m top5: 0.8605410447761194
[2m[36m(func pid=86940)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=86940)[0m f1_macro: 0.24890386011289975
[2m[36m(func pid=86940)[0m f1_weighted: 0.29728546042033105
[2m[36m(func pid=86940)[0m f1_per_class: [0.227, 0.0, 0.268, 0.528, 0.054, 0.313, 0.241, 0.498, 0.128, 0.231]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.058768656716417914
[2m[36m(func pid=85916)[0m top5: 0.511660447761194
[2m[36m(func pid=85916)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=85916)[0m f1_macro: 0.03204786568748578
[2m[36m(func pid=85916)[0m f1_weighted: 0.05438544281510051
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.168, 0.023, 0.086, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.18236940298507462
[2m[36m(func pid=91972)[0m top5: 0.7117537313432836
[2m[36m(func pid=91972)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=91972)[0m f1_macro: 0.05486082544906075
[2m[36m(func pid=91972)[0m f1_weighted: 0.14646571236781947
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.0, 0.0, 0.524, 0.024, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3170 | Steps: 2 | Val loss: 2.1253 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4556 | Steps: 2 | Val loss: 2.0288 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.7003 | Steps: 2 | Val loss: 2.3507 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=86021)[0m top1: 0.22294776119402984
[2m[36m(func pid=86021)[0m top5: 0.8549440298507462
[2m[36m(func pid=86021)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=86021)[0m f1_macro: 0.19004256204649508
[2m[36m(func pid=86021)[0m f1_weighted: 0.2611778951881416
[2m[36m(func pid=86021)[0m f1_per_class: [0.133, 0.282, 0.21, 0.19, 0.048, 0.252, 0.365, 0.242, 0.073, 0.105]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 11.0288 | Steps: 2 | Val loss: 7.9596 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 00:03:08 (running for 00:10:17.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.724 |      0.032 |                   32 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.317 |      0.19  |                   32 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.456 |      0.249 |                   30 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 10.241 |      0.055 |                    8 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m top1: 0.2933768656716418
[2m[36m(func pid=86940)[0m top5: 0.8596082089552238
[2m[36m(func pid=86940)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=86940)[0m f1_macro: 0.24941100041265366
[2m[36m(func pid=86940)[0m f1_weighted: 0.3078382160138264
[2m[36m(func pid=86940)[0m f1_per_class: [0.206, 0.011, 0.265, 0.449, 0.077, 0.388, 0.32, 0.492, 0.14, 0.147]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.06389925373134328
[2m[36m(func pid=85916)[0m top5: 0.5097947761194029
[2m[36m(func pid=85916)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=85916)[0m f1_macro: 0.03624099247130741
[2m[36m(func pid=85916)[0m f1_weighted: 0.05791234992093243
[2m[36m(func pid=85916)[0m f1_per_class: [0.029, 0.168, 0.026, 0.092, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.00792910447761194
[2m[36m(func pid=91972)[0m top5: 0.7602611940298507
[2m[36m(func pid=91972)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=91972)[0m f1_macro: 0.0038748832866479926
[2m[36m(func pid=91972)[0m f1_weighted: 0.000899955753445657
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.024, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3005 | Steps: 2 | Val loss: 2.1379 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.4348 | Steps: 2 | Val loss: 2.0052 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7556 | Steps: 2 | Val loss: 2.3487 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=86021)[0m top1: 0.20988805970149255
[2m[36m(func pid=86021)[0m top5: 0.84375
[2m[36m(func pid=86021)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=86021)[0m f1_macro: 0.18544560690495615
[2m[36m(func pid=86021)[0m f1_weighted: 0.24460473428059393
[2m[36m(func pid=86021)[0m f1_per_class: [0.122, 0.268, 0.183, 0.149, 0.05, 0.266, 0.345, 0.261, 0.098, 0.111]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 10.2300 | Steps: 2 | Val loss: 7.4657 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=86940)[0m top1: 0.3031716417910448
[2m[36m(func pid=86940)[0m top5: 0.8703358208955224
[2m[36m(func pid=86940)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=86940)[0m f1_macro: 0.2579863959622559
[2m[36m(func pid=86940)[0m f1_weighted: 0.33461691693896545
[2m[36m(func pid=86940)[0m f1_per_class: [0.179, 0.112, 0.344, 0.394, 0.098, 0.415, 0.417, 0.389, 0.099, 0.133]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:14 (running for 00:10:23.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.7   |      0.036 |                   33 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.301 |      0.185 |                   33 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.435 |      0.258 |                   31 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 11.029 |      0.004 |                    9 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.06763059701492537
[2m[36m(func pid=85916)[0m top5: 0.5055970149253731
[2m[36m(func pid=85916)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=85916)[0m f1_macro: 0.03691384695236975
[2m[36m(func pid=85916)[0m f1_weighted: 0.06019925052033619
[2m[36m(func pid=85916)[0m f1_per_class: [0.027, 0.164, 0.029, 0.102, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.06436567164179105
[2m[36m(func pid=91972)[0m top5: 0.808768656716418
[2m[36m(func pid=91972)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=91972)[0m f1_macro: 0.01810972201709129
[2m[36m(func pid=91972)[0m f1_weighted: 0.020860318160646867
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.048, 0.0, 0.0, 0.0, 0.0, 0.02, 0.113, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2696 | Steps: 2 | Val loss: 2.1514 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.3815 | Steps: 2 | Val loss: 1.9989 | Batch size: 32 | lr: 0.01 | Duration: 2.60s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6745 | Steps: 2 | Val loss: 2.3430 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=86021)[0m top1: 0.1935634328358209
[2m[36m(func pid=86021)[0m top5: 0.8260261194029851
[2m[36m(func pid=86021)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=86021)[0m f1_macro: 0.18308732404408684
[2m[36m(func pid=86021)[0m f1_weighted: 0.21749811588733053
[2m[36m(func pid=86021)[0m f1_per_class: [0.099, 0.258, 0.191, 0.08, 0.053, 0.269, 0.306, 0.352, 0.1, 0.121]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 9.4043 | Steps: 2 | Val loss: 6.0152 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=86940)[0m top1: 0.3111007462686567
[2m[36m(func pid=86940)[0m top5: 0.8768656716417911
[2m[36m(func pid=86940)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=86940)[0m f1_macro: 0.26984126713088247
[2m[36m(func pid=86940)[0m f1_weighted: 0.3476196167257711
[2m[36m(func pid=86940)[0m f1_per_class: [0.16, 0.243, 0.361, 0.37, 0.082, 0.406, 0.407, 0.392, 0.134, 0.143]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:19 (running for 00:10:28.79)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.675 |      0.039 |                   35 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.27  |      0.183 |                   34 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.381 |      0.27  |                   32 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      | 10.23  |      0.018 |                   10 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.07182835820895522
[2m[36m(func pid=85916)[0m top5: 0.5088619402985075
[2m[36m(func pid=85916)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=85916)[0m f1_macro: 0.03942812963659621
[2m[36m(func pid=85916)[0m f1_weighted: 0.06250709363352157
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.174, 0.033, 0.101, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.2555970149253731
[2m[36m(func pid=91972)[0m top5: 0.7630597014925373
[2m[36m(func pid=91972)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=91972)[0m f1_macro: 0.0812844651032806
[2m[36m(func pid=91972)[0m f1_weighted: 0.1467689258989022
[2m[36m(func pid=91972)[0m f1_per_class: [0.224, 0.107, 0.0, 0.442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3430 | Steps: 2 | Val loss: 2.1722 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.4437 | Steps: 2 | Val loss: 1.9898 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6710 | Steps: 2 | Val loss: 2.3385 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=86021)[0m top1: 0.1730410447761194
[2m[36m(func pid=86021)[0m top5: 0.7994402985074627
[2m[36m(func pid=86021)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=86021)[0m f1_macro: 0.17197493546131012
[2m[36m(func pid=86021)[0m f1_weighted: 0.1820164573623548
[2m[36m(func pid=86021)[0m f1_per_class: [0.093, 0.232, 0.18, 0.039, 0.055, 0.283, 0.231, 0.386, 0.093, 0.129]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 9.4798 | Steps: 2 | Val loss: 12.1946 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=86940)[0m top1: 0.279384328358209
[2m[36m(func pid=86940)[0m top5: 0.8833955223880597
[2m[36m(func pid=86940)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=86940)[0m f1_macro: 0.2568866003262686
[2m[36m(func pid=86940)[0m f1_weighted: 0.278349340873165
[2m[36m(func pid=86940)[0m f1_per_class: [0.207, 0.33, 0.319, 0.155, 0.083, 0.375, 0.325, 0.437, 0.115, 0.222]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:25 (running for 00:10:33.96)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.671 |      0.042 |                   36 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.343 |      0.172 |                   35 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.444 |      0.257 |                   33 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  9.404 |      0.081 |                   11 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.07462686567164178
[2m[36m(func pid=85916)[0m top5: 0.5135261194029851
[2m[36m(func pid=85916)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=85916)[0m f1_macro: 0.04190037070373579
[2m[36m(func pid=85916)[0m f1_weighted: 0.0643343408231182
[2m[36m(func pid=85916)[0m f1_per_class: [0.025, 0.177, 0.037, 0.103, 0.0, 0.0, 0.0, 0.078, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.042444029850746266
[2m[36m(func pid=91972)[0m top5: 0.6497201492537313
[2m[36m(func pid=91972)[0m f1_micro: 0.042444029850746266
[2m[36m(func pid=91972)[0m f1_macro: 0.020436889843850094
[2m[36m(func pid=91972)[0m f1_weighted: 0.031175999112338284
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.3001 | Steps: 2 | Val loss: 2.1833 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.4900 | Steps: 2 | Val loss: 1.9395 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8153 | Steps: 2 | Val loss: 2.3397 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=86021)[0m top1: 0.1571828358208955
[2m[36m(func pid=86021)[0m top5: 0.7854477611940298
[2m[36m(func pid=86021)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=86021)[0m f1_macro: 0.1601434869005564
[2m[36m(func pid=86021)[0m f1_weighted: 0.146727825593549
[2m[36m(func pid=86021)[0m f1_per_class: [0.087, 0.233, 0.18, 0.016, 0.056, 0.28, 0.134, 0.381, 0.101, 0.133]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 9.8230 | Steps: 2 | Val loss: 6.6587 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=86940)[0m top1: 0.28777985074626866
[2m[36m(func pid=86940)[0m top5: 0.8936567164179104
[2m[36m(func pid=86940)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=86940)[0m f1_macro: 0.25207457298161284
[2m[36m(func pid=86940)[0m f1_weighted: 0.3000607177133911
[2m[36m(func pid=86940)[0m f1_per_class: [0.248, 0.337, 0.256, 0.324, 0.064, 0.348, 0.253, 0.44, 0.051, 0.2]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:30 (running for 00:10:39.07)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.815 |      0.041 |                   37 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.3   |      0.16  |                   36 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.49  |      0.252 |                   34 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  9.48  |      0.02  |                   12 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.07602611940298508
[2m[36m(func pid=85916)[0m top5: 0.5158582089552238
[2m[36m(func pid=85916)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=85916)[0m f1_macro: 0.0412706325797117
[2m[36m(func pid=85916)[0m f1_weighted: 0.06539207298287744
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.183, 0.037, 0.106, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2674 | Steps: 2 | Val loss: 2.1840 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=91972)[0m top1: 0.20102611940298507
[2m[36m(func pid=91972)[0m top5: 0.683768656716418
[2m[36m(func pid=91972)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=91972)[0m f1_macro: 0.08482729292626731
[2m[36m(func pid=91972)[0m f1_weighted: 0.1605496530813208
[2m[36m(func pid=91972)[0m f1_per_class: [0.222, 0.0, 0.0, 0.553, 0.0, 0.0, 0.0, 0.0, 0.041, 0.032]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2962 | Steps: 2 | Val loss: 1.9417 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6811 | Steps: 2 | Val loss: 2.3319 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=86021)[0m top1: 0.15438432835820895
[2m[36m(func pid=86021)[0m top5: 0.7756529850746269
[2m[36m(func pid=86021)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=86021)[0m f1_macro: 0.15700041750301597
[2m[36m(func pid=86021)[0m f1_weighted: 0.13864660214214725
[2m[36m(func pid=86021)[0m f1_per_class: [0.085, 0.229, 0.193, 0.01, 0.054, 0.282, 0.114, 0.382, 0.104, 0.118]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 6.6724 | Steps: 2 | Val loss: 6.1489 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=86940)[0m top1: 0.3185634328358209
[2m[36m(func pid=86940)[0m top5: 0.8885261194029851
[2m[36m(func pid=86940)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=86940)[0m f1_macro: 0.26781743043411044
[2m[36m(func pid=86940)[0m f1_weighted: 0.3431106214728297
[2m[36m(func pid=86940)[0m f1_per_class: [0.172, 0.277, 0.2, 0.475, 0.056, 0.339, 0.281, 0.481, 0.13, 0.267]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:35 (running for 00:10:44.14)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.681 |      0.041 |                   38 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.267 |      0.157 |                   37 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.296 |      0.268 |                   35 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  9.823 |      0.085 |                   13 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.07555970149253731
[2m[36m(func pid=85916)[0m top5: 0.5242537313432836
[2m[36m(func pid=85916)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=85916)[0m f1_macro: 0.041078219063950525
[2m[36m(func pid=85916)[0m f1_weighted: 0.06329979375854268
[2m[36m(func pid=85916)[0m f1_per_class: [0.025, 0.182, 0.042, 0.099, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.051305970149253734
[2m[36m(func pid=91972)[0m top5: 0.8549440298507462
[2m[36m(func pid=91972)[0m f1_micro: 0.051305970149253734
[2m[36m(func pid=91972)[0m f1_macro: 0.06810883605391553
[2m[36m(func pid=91972)[0m f1_weighted: 0.07084539025607889
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.085, 0.324, 0.139, 0.017, 0.0, 0.044, 0.0, 0.072, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.2463 | Steps: 2 | Val loss: 2.1730 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.4029 | Steps: 2 | Val loss: 1.9878 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7410 | Steps: 2 | Val loss: 2.3312 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=86021)[0m top1: 0.15764925373134328
[2m[36m(func pid=86021)[0m top5: 0.7896455223880597
[2m[36m(func pid=86021)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=86021)[0m f1_macro: 0.16207581515405886
[2m[36m(func pid=86021)[0m f1_weighted: 0.14440973990996217
[2m[36m(func pid=86021)[0m f1_per_class: [0.095, 0.244, 0.218, 0.032, 0.05, 0.277, 0.106, 0.369, 0.112, 0.118]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 6.5690 | Steps: 2 | Val loss: 6.6147 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=86940)[0m top1: 0.3045708955223881
[2m[36m(func pid=86940)[0m top5: 0.8773320895522388
[2m[36m(func pid=86940)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=86940)[0m f1_macro: 0.25246435370361586
[2m[36m(func pid=86940)[0m f1_weighted: 0.3321618257468549
[2m[36m(func pid=86940)[0m f1_per_class: [0.23, 0.142, 0.224, 0.502, 0.049, 0.191, 0.346, 0.5, 0.161, 0.179]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:40 (running for 00:10:49.31)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.741 |      0.042 |                   39 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.246 |      0.162 |                   38 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.403 |      0.252 |                   36 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  6.672 |      0.068 |                   14 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.07929104477611941
[2m[36m(func pid=85916)[0m top5: 0.5293843283582089
[2m[36m(func pid=85916)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=85916)[0m f1_macro: 0.042429390388674416
[2m[36m(func pid=85916)[0m f1_weighted: 0.06513620526717293
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.198, 0.044, 0.096, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.16930970149253732
[2m[36m(func pid=91972)[0m top5: 0.7201492537313433
[2m[36m(func pid=91972)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=91972)[0m f1_macro: 0.1232073796314396
[2m[36m(func pid=91972)[0m f1_weighted: 0.10038788399817614
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.143, 0.276, 0.065, 0.046, 0.268, 0.0, 0.435, 0.0, 0.0]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2229 | Steps: 2 | Val loss: 2.1706 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.2936 | Steps: 2 | Val loss: 2.0131 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6722 | Steps: 2 | Val loss: 2.3246 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=86021)[0m top1: 0.15764925373134328
[2m[36m(func pid=86021)[0m top5: 0.7887126865671642
[2m[36m(func pid=86021)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=86021)[0m f1_macro: 0.1581878293565201
[2m[36m(func pid=86021)[0m f1_weighted: 0.15235819749919877
[2m[36m(func pid=86021)[0m f1_per_class: [0.085, 0.247, 0.208, 0.078, 0.048, 0.276, 0.096, 0.355, 0.098, 0.093]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.9754 | Steps: 2 | Val loss: 6.3011 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=86940)[0m top1: 0.28824626865671643
[2m[36m(func pid=86940)[0m top5: 0.8805970149253731
[2m[36m(func pid=86940)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=86940)[0m f1_macro: 0.26264806161863363
[2m[36m(func pid=86940)[0m f1_weighted: 0.33875425821831606
[2m[36m(func pid=86940)[0m f1_per_class: [0.181, 0.267, 0.4, 0.367, 0.057, 0.186, 0.436, 0.474, 0.107, 0.151]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:45 (running for 00:10:54.45)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.672 |      0.047 |                   40 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.223 |      0.158 |                   39 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.294 |      0.263 |                   37 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  6.569 |      0.123 |                   15 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.08395522388059702
[2m[36m(func pid=85916)[0m top5: 0.5340485074626866
[2m[36m(func pid=85916)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=85916)[0m f1_macro: 0.04703886783188364
[2m[36m(func pid=85916)[0m f1_weighted: 0.07010411337662768
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.204, 0.046, 0.104, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.09375
[2m[36m(func pid=91972)[0m top5: 0.816231343283582
[2m[36m(func pid=91972)[0m f1_micro: 0.09375
[2m[36m(func pid=91972)[0m f1_macro: 0.12239829149009648
[2m[36m(func pid=91972)[0m f1_weighted: 0.10845397004485083
[2m[36m(func pid=91972)[0m f1_per_class: [0.179, 0.016, 0.105, 0.253, 0.133, 0.0, 0.0, 0.512, 0.0, 0.026]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2284 | Steps: 2 | Val loss: 2.1642 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.2971 | Steps: 2 | Val loss: 2.0043 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6747 | Steps: 2 | Val loss: 2.3189 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=86021)[0m top1: 0.16371268656716417
[2m[36m(func pid=86021)[0m top5: 0.7919776119402985
[2m[36m(func pid=86021)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=86021)[0m f1_macro: 0.1672822005573547
[2m[36m(func pid=86021)[0m f1_weighted: 0.16884480942486835
[2m[36m(func pid=86021)[0m f1_per_class: [0.084, 0.247, 0.198, 0.129, 0.047, 0.258, 0.106, 0.358, 0.104, 0.14]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3297 | Steps: 2 | Val loss: 4.2789 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=86940)[0m top1: 0.302705223880597
[2m[36m(func pid=86940)[0m top5: 0.8908582089552238
[2m[36m(func pid=86940)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=86940)[0m f1_macro: 0.26739915195300507
[2m[36m(func pid=86940)[0m f1_weighted: 0.35182170441899485
[2m[36m(func pid=86940)[0m f1_per_class: [0.139, 0.336, 0.421, 0.325, 0.062, 0.194, 0.48, 0.467, 0.109, 0.141]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:03:50 (running for 00:10:59.46)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.675 |      0.048 |                   41 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.228 |      0.167 |                   40 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.297 |      0.267 |                   38 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  3.975 |      0.122 |                   16 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.08582089552238806
[2m[36m(func pid=85916)[0m top5: 0.5433768656716418
[2m[36m(func pid=85916)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=85916)[0m f1_macro: 0.047626197938906205
[2m[36m(func pid=85916)[0m f1_weighted: 0.07128849513928244
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.202, 0.049, 0.109, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.2206 | Steps: 2 | Val loss: 2.1570 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=91972)[0m top1: 0.19542910447761194
[2m[36m(func pid=91972)[0m top5: 0.8041044776119403
[2m[36m(func pid=91972)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=91972)[0m f1_macro: 0.10336124706209684
[2m[36m(func pid=91972)[0m f1_weighted: 0.19370784326897406
[2m[36m(func pid=91972)[0m f1_per_class: [0.0, 0.229, 0.0, 0.0, 0.127, 0.016, 0.493, 0.0, 0.115, 0.053]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2544 | Steps: 2 | Val loss: 1.9298 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.6637 | Steps: 2 | Val loss: 2.3143 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=86021)[0m top1: 0.18097014925373134
[2m[36m(func pid=86021)[0m top5: 0.8017723880597015
[2m[36m(func pid=86021)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=86021)[0m f1_macro: 0.17839350799289927
[2m[36m(func pid=86021)[0m f1_weighted: 0.199161593591366
[2m[36m(func pid=86021)[0m f1_per_class: [0.084, 0.263, 0.206, 0.155, 0.047, 0.262, 0.173, 0.361, 0.105, 0.129]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4840 | Steps: 2 | Val loss: 5.3129 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=86940)[0m top1: 0.34095149253731344
[2m[36m(func pid=86940)[0m top5: 0.8810634328358209
[2m[36m(func pid=86940)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=86940)[0m f1_macro: 0.2965297043157731
[2m[36m(func pid=86940)[0m f1_weighted: 0.3642388746193942
[2m[36m(func pid=86940)[0m f1_per_class: [0.214, 0.422, 0.379, 0.301, 0.093, 0.375, 0.415, 0.482, 0.13, 0.155]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.08861940298507463
[2m[36m(func pid=85916)[0m top5: 0.550839552238806
[2m[36m(func pid=85916)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=85916)[0m f1_macro: 0.04842567934603782
[2m[36m(func pid=85916)[0m f1_weighted: 0.07208761868825102
[2m[36m(func pid=85916)[0m f1_per_class: [0.025, 0.211, 0.05, 0.106, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
== Status ==
Current time: 2024-01-07 00:03:55 (running for 00:11:04.87)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.664 |      0.048 |                   42 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.221 |      0.178 |                   41 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.254 |      0.297 |                   39 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  2.484 |      0.17  |                   18 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.2583955223880597
[2m[36m(func pid=91972)[0m top5: 0.7817164179104478
[2m[36m(func pid=91972)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=91972)[0m f1_macro: 0.1700475386495583
[2m[36m(func pid=91972)[0m f1_weighted: 0.15309199481847557
[2m[36m(func pid=91972)[0m f1_per_class: [0.207, 0.434, 0.0, 0.066, 0.353, 0.347, 0.0, 0.216, 0.0, 0.077]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.2592 | Steps: 2 | Val loss: 2.1450 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.3553 | Steps: 2 | Val loss: 1.9642 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6605 | Steps: 2 | Val loss: 2.3101 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=86021)[0m top1: 0.20335820895522388
[2m[36m(func pid=86021)[0m top5: 0.8148320895522388
[2m[36m(func pid=86021)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=86021)[0m f1_macro: 0.19451888046767224
[2m[36m(func pid=86021)[0m f1_weighted: 0.2276238143504698
[2m[36m(func pid=86021)[0m f1_per_class: [0.088, 0.271, 0.206, 0.161, 0.053, 0.285, 0.247, 0.367, 0.098, 0.169]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.0253 | Steps: 2 | Val loss: 6.3789 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=86940)[0m top1: 0.30130597014925375
[2m[36m(func pid=86940)[0m top5: 0.8773320895522388
[2m[36m(func pid=86940)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=86940)[0m f1_macro: 0.2787828366242808
[2m[36m(func pid=86940)[0m f1_weighted: 0.2905032919268108
[2m[36m(func pid=86940)[0m f1_per_class: [0.24, 0.409, 0.319, 0.139, 0.119, 0.408, 0.307, 0.491, 0.14, 0.217]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.08955223880597014
[2m[36m(func pid=85916)[0m top5: 0.5527052238805971
[2m[36m(func pid=85916)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=85916)[0m f1_macro: 0.05011688516200528
[2m[36m(func pid=85916)[0m f1_weighted: 0.07325326444317043
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.212, 0.052, 0.107, 0.0, 0.0, 0.0, 0.105, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
== Status ==
Current time: 2024-01-07 00:04:01 (running for 00:11:10.13)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.66  |      0.05  |                   43 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.259 |      0.195 |                   42 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.355 |      0.279 |                   40 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  3.025 |      0.147 |                   19 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.21361940298507462
[2m[36m(func pid=91972)[0m top5: 0.6683768656716418
[2m[36m(func pid=91972)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=91972)[0m f1_macro: 0.14682024826989365
[2m[36m(func pid=91972)[0m f1_weighted: 0.1891350893600312
[2m[36m(func pid=91972)[0m f1_per_class: [0.087, 0.042, 0.105, 0.529, 0.024, 0.0, 0.0, 0.513, 0.0, 0.167]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2100 | Steps: 2 | Val loss: 2.1411 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.4354 | Steps: 2 | Val loss: 1.9605 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.6917 | Steps: 2 | Val loss: 2.3043 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=86021)[0m top1: 0.22061567164179105
[2m[36m(func pid=86021)[0m top5: 0.8180970149253731
[2m[36m(func pid=86021)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=86021)[0m f1_macro: 0.20407104093752748
[2m[36m(func pid=86021)[0m f1_weighted: 0.25508300679732
[2m[36m(func pid=86021)[0m f1_per_class: [0.094, 0.243, 0.227, 0.223, 0.06, 0.304, 0.286, 0.4, 0.078, 0.125]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 5.2568 | Steps: 2 | Val loss: 6.7935 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=86940)[0m top1: 0.29524253731343286
[2m[36m(func pid=86940)[0m top5: 0.8782649253731343
[2m[36m(func pid=86940)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=86940)[0m f1_macro: 0.2641064630317228
[2m[36m(func pid=86940)[0m f1_weighted: 0.29089999014193624
[2m[36m(func pid=86940)[0m f1_per_class: [0.218, 0.32, 0.275, 0.402, 0.096, 0.375, 0.132, 0.487, 0.13, 0.207]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.08955223880597014
[2m[36m(func pid=85916)[0m top5: 0.5666977611940298
[2m[36m(func pid=85916)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=85916)[0m f1_macro: 0.05155386054042619
[2m[36m(func pid=85916)[0m f1_weighted: 0.07322497422156543
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.209, 0.055, 0.106, 0.0, 0.0, 0.0, 0.119, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
== Status ==
Current time: 2024-01-07 00:04:06 (running for 00:11:15.19)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.692 |      0.052 |                   44 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.21  |      0.204 |                   43 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.435 |      0.264 |                   41 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  5.257 |      0.165 |                   20 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.10774253731343283
[2m[36m(func pid=91972)[0m top5: 0.6870335820895522
[2m[36m(func pid=91972)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=91972)[0m f1_macro: 0.16532620426146724
[2m[36m(func pid=91972)[0m f1_weighted: 0.08599176810598712
[2m[36m(func pid=91972)[0m f1_per_class: [0.245, 0.272, 0.286, 0.0, 0.308, 0.0, 0.0, 0.515, 0.0, 0.028]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.1840 | Steps: 2 | Val loss: 2.1323 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2367 | Steps: 2 | Val loss: 1.9937 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6532 | Steps: 2 | Val loss: 2.3020 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=86021)[0m top1: 0.23460820895522388
[2m[36m(func pid=86021)[0m top5: 0.8250932835820896
[2m[36m(func pid=86021)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=86021)[0m f1_macro: 0.20521249748955378
[2m[36m(func pid=86021)[0m f1_weighted: 0.27255268991840786
[2m[36m(func pid=86021)[0m f1_per_class: [0.103, 0.221, 0.25, 0.308, 0.07, 0.305, 0.282, 0.41, 0.03, 0.073]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.9112 | Steps: 2 | Val loss: 3.3625 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=86940)[0m top1: 0.292910447761194
[2m[36m(func pid=86940)[0m top5: 0.8745335820895522
[2m[36m(func pid=86940)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=86940)[0m f1_macro: 0.25239722770206635
[2m[36m(func pid=86940)[0m f1_weighted: 0.2874702299810128
[2m[36m(func pid=86940)[0m f1_per_class: [0.197, 0.187, 0.185, 0.484, 0.077, 0.353, 0.122, 0.515, 0.158, 0.246]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:04:11 (running for 00:11:20.25)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.692 |      0.052 |                   44 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.184 |      0.205 |                   44 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.237 |      0.252 |                   42 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  2.911 |      0.281 |                   21 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.09001865671641791
[2m[36m(func pid=85916)[0m top5: 0.570429104477612
[2m[36m(func pid=85916)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=85916)[0m f1_macro: 0.05191832369298928
[2m[36m(func pid=85916)[0m f1_weighted: 0.07408436985983709
[2m[36m(func pid=85916)[0m f1_per_class: [0.027, 0.205, 0.056, 0.111, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m top1: 0.4230410447761194
[2m[36m(func pid=91972)[0m top5: 0.8875932835820896
[2m[36m(func pid=91972)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=91972)[0m f1_macro: 0.280990389120012
[2m[36m(func pid=91972)[0m f1_weighted: 0.3502263645373986
[2m[36m(func pid=91972)[0m f1_per_class: [0.286, 0.508, 0.345, 0.045, 0.0, 0.506, 0.547, 0.316, 0.0, 0.256]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.1894 | Steps: 2 | Val loss: 2.1264 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.1931 | Steps: 2 | Val loss: 1.9861 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.6971 | Steps: 2 | Val loss: 2.3041 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=86021)[0m top1: 0.23973880597014927
[2m[36m(func pid=86021)[0m top5: 0.8325559701492538
[2m[36m(func pid=86021)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=86021)[0m f1_macro: 0.20894197431705824
[2m[36m(func pid=86021)[0m f1_weighted: 0.28111010974226164
[2m[36m(func pid=86021)[0m f1_per_class: [0.105, 0.196, 0.265, 0.336, 0.072, 0.299, 0.299, 0.429, 0.021, 0.068]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2153 | Steps: 2 | Val loss: 3.2610 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=86940)[0m top1: 0.31902985074626866
[2m[36m(func pid=86940)[0m top5: 0.882929104477612
[2m[36m(func pid=86940)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=86940)[0m f1_macro: 0.25989569802180934
[2m[36m(func pid=86940)[0m f1_weighted: 0.36023452733937644
[2m[36m(func pid=86940)[0m f1_per_class: [0.171, 0.235, 0.176, 0.48, 0.06, 0.33, 0.369, 0.464, 0.133, 0.182]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m top1: 0.08955223880597014
[2m[36m(func pid=85916)[0m top5: 0.5732276119402985
[2m[36m(func pid=85916)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=85916)[0m f1_macro: 0.050398517867163505
[2m[36m(func pid=85916)[0m f1_weighted: 0.07335252688903036
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.207, 0.054, 0.11, 0.0, 0.0, 0.0, 0.105, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 00:04:16 (running for 00:11:25.39)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.697 |      0.05  |                   46 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.189 |      0.209 |                   45 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.193 |      0.26  |                   43 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  2.911 |      0.281 |                   21 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.1843 | Steps: 2 | Val loss: 2.1173 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=91972)[0m top1: 0.3885261194029851
[2m[36m(func pid=91972)[0m top5: 0.9095149253731343
[2m[36m(func pid=91972)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=91972)[0m f1_macro: 0.2890145938477596
[2m[36m(func pid=91972)[0m f1_weighted: 0.3766182235844171
[2m[36m(func pid=91972)[0m f1_per_class: [0.128, 0.404, 0.324, 0.54, 0.113, 0.375, 0.246, 0.519, 0.109, 0.133]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.2172 | Steps: 2 | Val loss: 2.0074 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=86021)[0m top1: 0.24813432835820895
[2m[36m(func pid=86021)[0m top5: 0.8376865671641791
[2m[36m(func pid=86021)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=86021)[0m f1_macro: 0.21117524124160597
[2m[36m(func pid=86021)[0m f1_weighted: 0.2811446736203289
[2m[36m(func pid=86021)[0m f1_per_class: [0.106, 0.148, 0.289, 0.376, 0.078, 0.303, 0.285, 0.43, 0.026, 0.07]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.0242 | Steps: 2 | Val loss: 5.2976 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7706 | Steps: 2 | Val loss: 2.3066 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=86940)[0m top1: 0.2980410447761194
[2m[36m(func pid=86940)[0m top5: 0.8726679104477612
[2m[36m(func pid=86940)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=86940)[0m f1_macro: 0.24086382295288802
[2m[36m(func pid=86940)[0m f1_weighted: 0.3343884929143136
[2m[36m(func pid=86940)[0m f1_per_class: [0.189, 0.371, 0.268, 0.343, 0.057, 0.063, 0.435, 0.477, 0.095, 0.111]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1766 | Steps: 2 | Val loss: 2.1084 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 00:04:21 (running for 00:11:30.62)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.697 |      0.05  |                   46 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.184 |      0.211 |                   46 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.217 |      0.241 |                   44 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  3.024 |      0.21  |                   23 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.1912313432835821
[2m[36m(func pid=91972)[0m top5: 0.8227611940298507
[2m[36m(func pid=91972)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=91972)[0m f1_macro: 0.21002674463741472
[2m[36m(func pid=91972)[0m f1_weighted: 0.19395076643426007
[2m[36m(func pid=91972)[0m f1_per_class: [0.315, 0.314, 0.389, 0.323, 0.029, 0.0, 0.025, 0.545, 0.0, 0.159]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=85916)[0m top1: 0.08908582089552239
[2m[36m(func pid=85916)[0m top5: 0.5764925373134329
[2m[36m(func pid=85916)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=85916)[0m f1_macro: 0.050912479053018524
[2m[36m(func pid=85916)[0m f1_weighted: 0.07327130556296384
[2m[36m(func pid=85916)[0m f1_per_class: [0.022, 0.213, 0.051, 0.104, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1691 | Steps: 2 | Val loss: 2.0488 | Batch size: 32 | lr: 0.01 | Duration: 2.60s
[2m[36m(func pid=86021)[0m top1: 0.26119402985074625
[2m[36m(func pid=86021)[0m top5: 0.8376865671641791
[2m[36m(func pid=86021)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=86021)[0m f1_macro: 0.21156591268904879
[2m[36m(func pid=86021)[0m f1_weighted: 0.2890550845465841
[2m[36m(func pid=86021)[0m f1_per_class: [0.112, 0.121, 0.275, 0.419, 0.075, 0.297, 0.291, 0.424, 0.027, 0.074]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4570 | Steps: 2 | Val loss: 6.3776 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.6906 | Steps: 2 | Val loss: 2.3070 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=86940)[0m top1: 0.26259328358208955
[2m[36m(func pid=86940)[0m top5: 0.867070895522388
[2m[36m(func pid=86940)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=86940)[0m f1_macro: 0.21755795637687828
[2m[36m(func pid=86940)[0m f1_weighted: 0.2817310902094782
[2m[36m(func pid=86940)[0m f1_per_class: [0.157, 0.359, 0.293, 0.168, 0.063, 0.078, 0.431, 0.462, 0.068, 0.096]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3601 | Steps: 2 | Val loss: 2.1103 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=91972)[0m top1: 0.24580223880597016
[2m[36m(func pid=91972)[0m top5: 0.8652052238805971
[2m[36m(func pid=91972)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=91972)[0m f1_macro: 0.2281030702968768
[2m[36m(func pid=91972)[0m f1_weighted: 0.19053007057805796
[2m[36m(func pid=91972)[0m f1_per_class: [0.103, 0.486, 0.345, 0.003, 0.212, 0.396, 0.079, 0.512, 0.0, 0.144]
== Status ==
Current time: 2024-01-07 00:04:26 (running for 00:11:35.76)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.771 |      0.051 |                   47 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.177 |      0.212 |                   47 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.169 |      0.218 |                   45 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.457 |      0.228 |                   24 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=85916)[0m top1: 0.0914179104477612
[2m[36m(func pid=85916)[0m top5: 0.582089552238806
[2m[36m(func pid=85916)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=85916)[0m f1_macro: 0.05223203899506461
[2m[36m(func pid=85916)[0m f1_weighted: 0.07559220365347445
[2m[36m(func pid=85916)[0m f1_per_class: [0.025, 0.218, 0.051, 0.105, 0.0, 0.0, 0.003, 0.12, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1481 | Steps: 2 | Val loss: 2.0396 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=86021)[0m top1: 0.25699626865671643
[2m[36m(func pid=86021)[0m top5: 0.8381529850746269
[2m[36m(func pid=86021)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=86021)[0m f1_macro: 0.19584062952207287
[2m[36m(func pid=86021)[0m f1_weighted: 0.2818808957027245
[2m[36m(func pid=86021)[0m f1_per_class: [0.104, 0.073, 0.242, 0.426, 0.07, 0.3, 0.306, 0.325, 0.027, 0.085]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.0403 | Steps: 2 | Val loss: 4.0845 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6746 | Steps: 2 | Val loss: 2.3067 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=86940)[0m top1: 0.25886194029850745
[2m[36m(func pid=86940)[0m top5: 0.8610074626865671
[2m[36m(func pid=86940)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=86940)[0m f1_macro: 0.23803592676448151
[2m[36m(func pid=86940)[0m f1_weighted: 0.2799645481789279
[2m[36m(func pid=86940)[0m f1_per_class: [0.183, 0.373, 0.297, 0.183, 0.061, 0.252, 0.329, 0.449, 0.143, 0.11]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.2085 | Steps: 2 | Val loss: 2.0984 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 00:04:31 (running for 00:11:40.91)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.691 |      0.052 |                   48 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.36  |      0.196 |                   48 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.148 |      0.238 |                   46 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  3.04  |      0.3   |                   25 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.42630597014925375
[2m[36m(func pid=91972)[0m top5: 0.929570895522388
[2m[36m(func pid=91972)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=91972)[0m f1_macro: 0.3003481200009641
[2m[36m(func pid=91972)[0m f1_weighted: 0.4145636828034463
[2m[36m(func pid=91972)[0m f1_per_class: [0.224, 0.547, 0.348, 0.436, 0.0, 0.458, 0.4, 0.299, 0.0, 0.293]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=85916)[0m top1: 0.09328358208955224
[2m[36m(func pid=85916)[0m top5: 0.5872201492537313
[2m[36m(func pid=85916)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=85916)[0m f1_macro: 0.0541830027858443
[2m[36m(func pid=85916)[0m f1_weighted: 0.0776116828229303
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.222, 0.052, 0.108, 0.0, 0.0, 0.003, 0.133, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.4353 | Steps: 2 | Val loss: 1.9601 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=86021)[0m top1: 0.26865671641791045
[2m[36m(func pid=86021)[0m top5: 0.8367537313432836
[2m[36m(func pid=86021)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=86021)[0m f1_macro: 0.19681087601379083
[2m[36m(func pid=86021)[0m f1_weighted: 0.29005102551038825
[2m[36m(func pid=86021)[0m f1_per_class: [0.114, 0.061, 0.244, 0.444, 0.069, 0.304, 0.327, 0.316, 0.0, 0.089]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5417 | Steps: 2 | Val loss: 3.5754 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.6560 | Steps: 2 | Val loss: 2.3053 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=86940)[0m top1: 0.2994402985074627
[2m[36m(func pid=86940)[0m top5: 0.8745335820895522
[2m[36m(func pid=86940)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=86940)[0m f1_macro: 0.267439664774772
[2m[36m(func pid=86940)[0m f1_weighted: 0.31868545353623495
[2m[36m(func pid=86940)[0m f1_per_class: [0.216, 0.308, 0.224, 0.405, 0.084, 0.378, 0.231, 0.472, 0.14, 0.217]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3425 | Steps: 2 | Val loss: 2.1041 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 00:04:37 (running for 00:11:46.14)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.675 |      0.054 |                   49 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.209 |      0.197 |                   49 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.435 |      0.267 |                   47 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  2.542 |      0.318 |                   26 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.44402985074626866
[2m[36m(func pid=91972)[0m top5: 0.9272388059701493
[2m[36m(func pid=91972)[0m f1_micro: 0.44402985074626866
[2m[36m(func pid=91972)[0m f1_macro: 0.31756012319616345
[2m[36m(func pid=91972)[0m f1_weighted: 0.43416870196082685
[2m[36m(func pid=91972)[0m f1_per_class: [0.324, 0.495, 0.314, 0.396, 0.0, 0.468, 0.483, 0.529, 0.0, 0.167]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=85916)[0m top1: 0.09421641791044776
[2m[36m(func pid=85916)[0m top5: 0.5862873134328358
[2m[36m(func pid=85916)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=85916)[0m f1_macro: 0.054520107760931594
[2m[36m(func pid=85916)[0m f1_weighted: 0.0786019296631679
[2m[36m(func pid=85916)[0m f1_per_class: [0.024, 0.223, 0.053, 0.111, 0.0, 0.0, 0.003, 0.131, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1488 | Steps: 2 | Val loss: 2.0066 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=86021)[0m top1: 0.25886194029850745
[2m[36m(func pid=86021)[0m top5: 0.8316231343283582
[2m[36m(func pid=86021)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=86021)[0m f1_macro: 0.19151956138924772
[2m[36m(func pid=86021)[0m f1_weighted: 0.2864478430104878
[2m[36m(func pid=86021)[0m f1_per_class: [0.101, 0.046, 0.229, 0.431, 0.056, 0.283, 0.342, 0.328, 0.0, 0.099]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3026 | Steps: 2 | Val loss: 7.8566 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6357 | Steps: 2 | Val loss: 2.3028 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=86940)[0m top1: 0.2756529850746269
[2m[36m(func pid=86940)[0m top5: 0.875
[2m[36m(func pid=86940)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=86940)[0m f1_macro: 0.25341728113882145
[2m[36m(func pid=86940)[0m f1_weighted: 0.28291306531638033
[2m[36m(func pid=86940)[0m f1_per_class: [0.188, 0.345, 0.161, 0.337, 0.094, 0.382, 0.147, 0.508, 0.141, 0.233]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1977 | Steps: 2 | Val loss: 2.1097 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 00:04:42 (running for 00:11:51.24)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.656 |      0.055 |                   50 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.342 |      0.192 |                   50 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.149 |      0.253 |                   48 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.303 |      0.144 |                   27 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.07602611940298508
[2m[36m(func pid=91972)[0m top5: 0.7966417910447762
[2m[36m(func pid=91972)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=91972)[0m f1_macro: 0.14449717361166464
[2m[36m(func pid=91972)[0m f1_weighted: 0.04903556606728743
[2m[36m(func pid=91972)[0m f1_per_class: [0.277, 0.046, 0.286, 0.0, 0.098, 0.0, 0.0, 0.495, 0.074, 0.17]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=85916)[0m top1: 0.0960820895522388
[2m[36m(func pid=85916)[0m top5: 0.5904850746268657
[2m[36m(func pid=85916)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=85916)[0m f1_macro: 0.05543515883620426
[2m[36m(func pid=85916)[0m f1_weighted: 0.07982521713809419
[2m[36m(func pid=85916)[0m f1_per_class: [0.027, 0.222, 0.055, 0.115, 0.0, 0.0, 0.003, 0.131, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1134 | Steps: 2 | Val loss: 2.0229 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=86021)[0m top1: 0.24440298507462688
[2m[36m(func pid=86021)[0m top5: 0.8208955223880597
[2m[36m(func pid=86021)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=86021)[0m f1_macro: 0.1911991452443063
[2m[36m(func pid=86021)[0m f1_weighted: 0.27826512597761716
[2m[36m(func pid=86021)[0m f1_per_class: [0.108, 0.047, 0.244, 0.41, 0.05, 0.245, 0.345, 0.322, 0.027, 0.113]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.5474 | Steps: 2 | Val loss: 5.9973 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=86940)[0m top1: 0.2691231343283582
[2m[36m(func pid=86940)[0m top5: 0.8773320895522388
[2m[36m(func pid=86940)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=86940)[0m f1_macro: 0.24959983404264818
[2m[36m(func pid=86940)[0m f1_weighted: 0.2793241734690264
[2m[36m(func pid=86940)[0m f1_per_class: [0.197, 0.365, 0.162, 0.234, 0.098, 0.386, 0.222, 0.486, 0.134, 0.211]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6967 | Steps: 2 | Val loss: 2.3033 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1698 | Steps: 2 | Val loss: 2.1092 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 00:04:47 (running for 00:11:56.43)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.636 |      0.055 |                   51 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.198 |      0.191 |                   51 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.113 |      0.25  |                   49 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.547 |      0.182 |                   28 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.21595149253731344
[2m[36m(func pid=91972)[0m top5: 0.8773320895522388
[2m[36m(func pid=91972)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=91972)[0m f1_macro: 0.1817309085242474
[2m[36m(func pid=91972)[0m f1_weighted: 0.24180700330171653
[2m[36m(func pid=91972)[0m f1_per_class: [0.132, 0.005, 0.367, 0.206, 0.035, 0.008, 0.502, 0.454, 0.0, 0.107]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=85916)[0m top1: 0.09654850746268656
[2m[36m(func pid=85916)[0m top5: 0.5965485074626866
[2m[36m(func pid=85916)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=85916)[0m f1_macro: 0.05395878106647041
[2m[36m(func pid=85916)[0m f1_weighted: 0.0795039461527521
[2m[36m(func pid=85916)[0m f1_per_class: [0.025, 0.226, 0.053, 0.115, 0.0, 0.0, 0.003, 0.118, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2455 | Steps: 2 | Val loss: 1.9642 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=86021)[0m top1: 0.24580223880597016
[2m[36m(func pid=86021)[0m top5: 0.820429104477612
[2m[36m(func pid=86021)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=86021)[0m f1_macro: 0.20025410029643315
[2m[36m(func pid=86021)[0m f1_weighted: 0.2829353215021054
[2m[36m(func pid=86021)[0m f1_per_class: [0.112, 0.051, 0.259, 0.384, 0.049, 0.242, 0.382, 0.32, 0.026, 0.178]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.2866 | Steps: 2 | Val loss: 4.2940 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=86940)[0m top1: 0.3381529850746269
[2m[36m(func pid=86940)[0m top5: 0.8969216417910447
[2m[36m(func pid=86940)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=86940)[0m f1_macro: 0.2804347509357294
[2m[36m(func pid=86940)[0m f1_weighted: 0.3828832755787668
[2m[36m(func pid=86940)[0m f1_per_class: [0.142, 0.361, 0.196, 0.423, 0.076, 0.376, 0.407, 0.457, 0.147, 0.219]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6665 | Steps: 2 | Val loss: 2.3030 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1563 | Steps: 2 | Val loss: 2.1142 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 00:04:52 (running for 00:12:01.48)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.697 |      0.054 |                   52 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.17  |      0.2   |                   52 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.245 |      0.28  |                   50 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.287 |      0.266 |                   29 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.363339552238806
[2m[36m(func pid=91972)[0m top5: 0.9202425373134329
[2m[36m(func pid=91972)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=91972)[0m f1_macro: 0.2658643879802459
[2m[36m(func pid=91972)[0m f1_weighted: 0.36589086514758645
[2m[36m(func pid=91972)[0m f1_per_class: [0.189, 0.219, 0.348, 0.475, 0.158, 0.394, 0.43, 0.216, 0.0, 0.229]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=85916)[0m top1: 0.09934701492537314
[2m[36m(func pid=85916)[0m top5: 0.6021455223880597
[2m[36m(func pid=85916)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=85916)[0m f1_macro: 0.057450087644185174
[2m[36m(func pid=85916)[0m f1_weighted: 0.08293671147737247
[2m[36m(func pid=85916)[0m f1_per_class: [0.025, 0.227, 0.053, 0.121, 0.0, 0.0, 0.003, 0.145, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1005 | Steps: 2 | Val loss: 1.9408 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=86021)[0m top1: 0.22807835820895522
[2m[36m(func pid=86021)[0m top5: 0.8138992537313433
[2m[36m(func pid=86021)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=86021)[0m f1_macro: 0.19851304595390584
[2m[36m(func pid=86021)[0m f1_weighted: 0.2673153323478616
[2m[36m(func pid=86021)[0m f1_per_class: [0.107, 0.079, 0.275, 0.312, 0.051, 0.274, 0.365, 0.34, 0.017, 0.165]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.0457 | Steps: 2 | Val loss: 4.1863 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=86940)[0m top1: 0.33255597014925375
[2m[36m(func pid=86940)[0m top5: 0.9020522388059702
[2m[36m(func pid=86940)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=86940)[0m f1_macro: 0.27163158373540197
[2m[36m(func pid=86940)[0m f1_weighted: 0.37253274330472064
[2m[36m(func pid=86940)[0m f1_per_class: [0.15, 0.433, 0.282, 0.317, 0.071, 0.3, 0.472, 0.423, 0.073, 0.194]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.6520 | Steps: 2 | Val loss: 2.3005 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1397 | Steps: 2 | Val loss: 2.1158 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=91972)[0m top1: 0.3493470149253731
[2m[36m(func pid=91972)[0m top5: 0.8959888059701493
[2m[36m(func pid=91972)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=91972)[0m f1_macro: 0.2925250136302511
[2m[36m(func pid=91972)[0m f1_weighted: 0.3031586436647165
[2m[36m(func pid=91972)[0m f1_per_class: [0.163, 0.442, 0.4, 0.251, 0.222, 0.426, 0.219, 0.534, 0.08, 0.188]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:04:57 (running for 00:12:06.64)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.667 |      0.057 |                   53 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.156 |      0.199 |                   53 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.101 |      0.272 |                   51 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.046 |      0.293 |                   30 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10121268656716417
[2m[36m(func pid=85916)[0m top5: 0.6072761194029851
[2m[36m(func pid=85916)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=85916)[0m f1_macro: 0.05774489357691105
[2m[36m(func pid=85916)[0m f1_weighted: 0.08394177906965368
[2m[36m(func pid=85916)[0m f1_per_class: [0.026, 0.232, 0.054, 0.119, 0.0, 0.0, 0.006, 0.141, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.4462 | Steps: 2 | Val loss: 1.9304 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=86021)[0m top1: 0.22667910447761194
[2m[36m(func pid=86021)[0m top5: 0.7938432835820896
[2m[36m(func pid=86021)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=86021)[0m f1_macro: 0.20898528319951853
[2m[36m(func pid=86021)[0m f1_weighted: 0.2574658108397844
[2m[36m(func pid=86021)[0m f1_per_class: [0.118, 0.117, 0.278, 0.235, 0.056, 0.314, 0.355, 0.357, 0.093, 0.167]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.0431 | Steps: 2 | Val loss: 4.6758 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=86940)[0m top1: 0.3278917910447761
[2m[36m(func pid=86940)[0m top5: 0.8987873134328358
[2m[36m(func pid=86940)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=86940)[0m f1_macro: 0.24965759574271468
[2m[36m(func pid=86940)[0m f1_weighted: 0.3605101120545376
[2m[36m(func pid=86940)[0m f1_per_class: [0.178, 0.425, 0.31, 0.325, 0.065, 0.168, 0.5, 0.333, 0.053, 0.14]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6366 | Steps: 2 | Val loss: 2.2969 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.1373 | Steps: 2 | Val loss: 2.1201 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=91972)[0m top1: 0.23787313432835822
[2m[36m(func pid=91972)[0m top5: 0.8292910447761194
[2m[36m(func pid=91972)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=91972)[0m f1_macro: 0.26655961741075684
[2m[36m(func pid=91972)[0m f1_weighted: 0.23324501445689919
[2m[36m(func pid=91972)[0m f1_per_class: [0.354, 0.411, 0.356, 0.345, 0.125, 0.151, 0.003, 0.539, 0.108, 0.274]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:05:03 (running for 00:12:12.71)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.637 |      0.059 |                   55 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.14  |      0.209 |                   54 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.446 |      0.25  |                   52 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.043 |      0.267 |                   31 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10167910447761194
[2m[36m(func pid=85916)[0m top5: 0.613339552238806
[2m[36m(func pid=85916)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=85916)[0m f1_macro: 0.05925251971286087
[2m[36m(func pid=85916)[0m f1_weighted: 0.08500478260361113
[2m[36m(func pid=85916)[0m f1_per_class: [0.026, 0.231, 0.055, 0.121, 0.0, 0.0, 0.006, 0.154, 0.0, 0.0]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0951 | Steps: 2 | Val loss: 1.9827 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=86021)[0m top1: 0.20335820895522388
[2m[36m(func pid=86021)[0m top5: 0.7947761194029851
[2m[36m(func pid=86021)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=86021)[0m f1_macro: 0.19835693597656978
[2m[36m(func pid=86021)[0m f1_weighted: 0.22062798930264033
[2m[36m(func pid=86021)[0m f1_per_class: [0.13, 0.14, 0.278, 0.122, 0.055, 0.315, 0.323, 0.347, 0.105, 0.169]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6170 | Steps: 2 | Val loss: 4.6552 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=86940)[0m top1: 0.310634328358209
[2m[36m(func pid=86940)[0m top5: 0.8805970149253731
[2m[36m(func pid=86940)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=86940)[0m f1_macro: 0.25154301515583527
[2m[36m(func pid=86940)[0m f1_weighted: 0.34105991275683384
[2m[36m(func pid=86940)[0m f1_per_class: [0.168, 0.412, 0.275, 0.256, 0.072, 0.174, 0.479, 0.437, 0.112, 0.131]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6194 | Steps: 2 | Val loss: 2.2933 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1280 | Steps: 2 | Val loss: 2.1268 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=91972)[0m top1: 0.34048507462686567
[2m[36m(func pid=91972)[0m top5: 0.8666044776119403
[2m[36m(func pid=91972)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=91972)[0m f1_macro: 0.2565721341136987
[2m[36m(func pid=91972)[0m f1_weighted: 0.34540383698075183
[2m[36m(func pid=91972)[0m f1_per_class: [0.171, 0.0, 0.424, 0.457, 0.067, 0.132, 0.549, 0.479, 0.071, 0.215]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.1321 | Steps: 2 | Val loss: 2.0683 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=85916)[0m top1: 0.1021455223880597
[2m[36m(func pid=85916)[0m top5: 0.6152052238805971
[2m[36m(func pid=85916)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=85916)[0m f1_macro: 0.0623505025441673
[2m[36m(func pid=85916)[0m f1_weighted: 0.08512972167741849
[2m[36m(func pid=85916)[0m f1_per_class: [0.029, 0.23, 0.058, 0.122, 0.0, 0.0, 0.009, 0.126, 0.0, 0.05]
== Status ==
Current time: 2024-01-07 00:05:09 (running for 00:12:17.98)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.619 |      0.062 |                   56 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.137 |      0.198 |                   55 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.095 |      0.252 |                   53 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.617 |      0.257 |                   32 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.1828358208955224
[2m[36m(func pid=86021)[0m top5: 0.7905783582089553
[2m[36m(func pid=86021)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=86021)[0m f1_macro: 0.18802256228048334
[2m[36m(func pid=86021)[0m f1_weighted: 0.19616705307822624
[2m[36m(func pid=86021)[0m f1_per_class: [0.138, 0.124, 0.259, 0.092, 0.05, 0.295, 0.281, 0.377, 0.088, 0.176]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8584 | Steps: 2 | Val loss: 4.1706 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=86940)[0m top1: 0.25326492537313433
[2m[36m(func pid=86940)[0m top5: 0.8442164179104478
[2m[36m(func pid=86940)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=86940)[0m f1_macro: 0.2379189380396706
[2m[36m(func pid=86940)[0m f1_weighted: 0.26220630809735473
[2m[36m(func pid=86940)[0m f1_per_class: [0.188, 0.41, 0.272, 0.174, 0.084, 0.301, 0.234, 0.477, 0.112, 0.127]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6245 | Steps: 2 | Val loss: 2.2906 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1141 | Steps: 2 | Val loss: 2.1322 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=91972)[0m top1: 0.3614738805970149
[2m[36m(func pid=91972)[0m top5: 0.8987873134328358
[2m[36m(func pid=91972)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=91972)[0m f1_macro: 0.2727096417407049
[2m[36m(func pid=91972)[0m f1_weighted: 0.36199813720925267
[2m[36m(func pid=91972)[0m f1_per_class: [0.165, 0.135, 0.372, 0.377, 0.112, 0.424, 0.518, 0.367, 0.0, 0.257]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:05:14 (running for 00:12:23.13)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.624 |      0.064 |                   57 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.128 |      0.188 |                   56 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.132 |      0.238 |                   54 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.858 |      0.273 |                   33 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10587686567164178
[2m[36m(func pid=85916)[0m top5: 0.6175373134328358
[2m[36m(func pid=85916)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=85916)[0m f1_macro: 0.06385491890731436
[2m[36m(func pid=85916)[0m f1_weighted: 0.0896803703338588
[2m[36m(func pid=85916)[0m f1_per_class: [0.029, 0.241, 0.059, 0.119, 0.0, 0.0, 0.021, 0.124, 0.0, 0.045]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.0282 | Steps: 2 | Val loss: 2.0450 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=86021)[0m top1: 0.166044776119403
[2m[36m(func pid=86021)[0m top5: 0.7915111940298507
[2m[36m(func pid=86021)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=86021)[0m f1_macro: 0.1803160930228521
[2m[36m(func pid=86021)[0m f1_weighted: 0.18041548103873076
[2m[36m(func pid=86021)[0m f1_per_class: [0.142, 0.125, 0.268, 0.069, 0.046, 0.233, 0.272, 0.38, 0.085, 0.182]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4480 | Steps: 2 | Val loss: 5.1783 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=86940)[0m top1: 0.2537313432835821
[2m[36m(func pid=86940)[0m top5: 0.8367537313432836
[2m[36m(func pid=86940)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=86940)[0m f1_macro: 0.24660726261386584
[2m[36m(func pid=86940)[0m f1_weighted: 0.22236173754812597
[2m[36m(func pid=86940)[0m f1_per_class: [0.255, 0.416, 0.289, 0.156, 0.094, 0.379, 0.072, 0.496, 0.116, 0.194]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6497 | Steps: 2 | Val loss: 2.2926 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.1067 | Steps: 2 | Val loss: 2.1349 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=91972)[0m top1: 0.2971082089552239
[2m[36m(func pid=91972)[0m top5: 0.8964552238805971
[2m[36m(func pid=91972)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=91972)[0m f1_macro: 0.23264999564646588
[2m[36m(func pid=91972)[0m f1_weighted: 0.20618168514443808
[2m[36m(func pid=91972)[0m f1_per_class: [0.044, 0.418, 0.409, 0.025, 0.195, 0.379, 0.16, 0.502, 0.0, 0.194]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.1402 | Steps: 2 | Val loss: 2.0167 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:05:19 (running for 00:12:28.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.65  |      0.066 |                   58 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.114 |      0.18  |                   57 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.028 |      0.247 |                   55 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.448 |      0.233 |                   34 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10914179104477612
[2m[36m(func pid=85916)[0m top5: 0.613339552238806
[2m[36m(func pid=85916)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=85916)[0m f1_macro: 0.06609020203717242
[2m[36m(func pid=85916)[0m f1_weighted: 0.09232220686630685
[2m[36m(func pid=85916)[0m f1_per_class: [0.029, 0.249, 0.059, 0.115, 0.0, 0.0, 0.027, 0.136, 0.0, 0.047]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.1571828358208955
[2m[36m(func pid=86021)[0m top5: 0.7905783582089553
[2m[36m(func pid=86021)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=86021)[0m f1_macro: 0.17751646400232726
[2m[36m(func pid=86021)[0m f1_weighted: 0.17049529196176993
[2m[36m(func pid=86021)[0m f1_per_class: [0.138, 0.109, 0.275, 0.069, 0.044, 0.217, 0.253, 0.381, 0.089, 0.2]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.7660 | Steps: 2 | Val loss: 4.9794 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=86940)[0m top1: 0.2635261194029851
[2m[36m(func pid=86940)[0m top5: 0.847481343283582
[2m[36m(func pid=86940)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=86940)[0m f1_macro: 0.2567152103121434
[2m[36m(func pid=86940)[0m f1_weighted: 0.23860719327110194
[2m[36m(func pid=86940)[0m f1_per_class: [0.259, 0.387, 0.282, 0.283, 0.098, 0.361, 0.028, 0.502, 0.114, 0.253]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6175 | Steps: 2 | Val loss: 2.2917 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.1514 | Steps: 2 | Val loss: 2.1349 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=91972)[0m top1: 0.302705223880597
[2m[36m(func pid=91972)[0m top5: 0.8596082089552238
[2m[36m(func pid=91972)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=91972)[0m f1_macro: 0.21525734201540464
[2m[36m(func pid=91972)[0m f1_weighted: 0.2307021751088036
[2m[36m(func pid=91972)[0m f1_per_class: [0.266, 0.016, 0.31, 0.522, 0.068, 0.132, 0.085, 0.492, 0.12, 0.141]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.1262 | Steps: 2 | Val loss: 1.9164 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 00:05:24 (running for 00:12:33.76)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.618 |      0.073 |                   59 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.107 |      0.178 |                   58 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.14  |      0.257 |                   56 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.766 |      0.215 |                   35 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10914179104477612
[2m[36m(func pid=85916)[0m top5: 0.6175373134328358
[2m[36m(func pid=85916)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=85916)[0m f1_macro: 0.07306309575102157
[2m[36m(func pid=85916)[0m f1_weighted: 0.09557482995775005
[2m[36m(func pid=85916)[0m f1_per_class: [0.031, 0.242, 0.061, 0.114, 0.0, 0.0, 0.036, 0.161, 0.0, 0.085]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.15438432835820895
[2m[36m(func pid=86021)[0m top5: 0.7985074626865671
[2m[36m(func pid=86021)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=86021)[0m f1_macro: 0.17502203525762028
[2m[36m(func pid=86021)[0m f1_weighted: 0.16940672135711832
[2m[36m(func pid=86021)[0m f1_per_class: [0.145, 0.128, 0.259, 0.069, 0.043, 0.182, 0.249, 0.401, 0.085, 0.189]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.0253 | Steps: 2 | Val loss: 8.3697 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=86940)[0m top1: 0.32276119402985076
[2m[36m(func pid=86940)[0m top5: 0.8731343283582089
[2m[36m(func pid=86940)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=86940)[0m f1_macro: 0.27716308844540716
[2m[36m(func pid=86940)[0m f1_weighted: 0.31770241795093035
[2m[36m(func pid=86940)[0m f1_per_class: [0.252, 0.239, 0.256, 0.495, 0.096, 0.381, 0.173, 0.484, 0.164, 0.233]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6094 | Steps: 2 | Val loss: 2.2907 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.1046 | Steps: 2 | Val loss: 2.1364 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=91972)[0m top1: 0.2560634328358209
[2m[36m(func pid=91972)[0m top5: 0.7630597014925373
[2m[36m(func pid=91972)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=91972)[0m f1_macro: 0.226447054229083
[2m[36m(func pid=91972)[0m f1_weighted: 0.28724834247410097
[2m[36m(func pid=91972)[0m f1_per_class: [0.111, 0.0, 0.355, 0.35, 0.09, 0.278, 0.426, 0.341, 0.118, 0.196]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0700 | Steps: 2 | Val loss: 1.8473 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:05:30 (running for 00:12:39.06)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.609 |      0.073 |                   60 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.151 |      0.175 |                   59 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.126 |      0.277 |                   57 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  2.025 |      0.226 |                   36 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10774253731343283
[2m[36m(func pid=85916)[0m top5: 0.6231343283582089
[2m[36m(func pid=85916)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=85916)[0m f1_macro: 0.07330080533448882
[2m[36m(func pid=85916)[0m f1_weighted: 0.09644389179682643
[2m[36m(func pid=85916)[0m f1_per_class: [0.03, 0.239, 0.063, 0.113, 0.0, 0.008, 0.039, 0.161, 0.0, 0.08]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.15811567164179105
[2m[36m(func pid=86021)[0m top5: 0.7989738805970149
[2m[36m(func pid=86021)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=86021)[0m f1_macro: 0.17951920635838825
[2m[36m(func pid=86021)[0m f1_weighted: 0.1724310083438831
[2m[36m(func pid=86021)[0m f1_per_class: [0.136, 0.125, 0.278, 0.069, 0.044, 0.219, 0.246, 0.401, 0.09, 0.185]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.3776 | Steps: 2 | Val loss: 9.0332 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=86940)[0m top1: 0.3596082089552239
[2m[36m(func pid=86940)[0m top5: 0.8992537313432836
[2m[36m(func pid=86940)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=86940)[0m f1_macro: 0.27501907530522923
[2m[36m(func pid=86940)[0m f1_weighted: 0.3531712073913841
[2m[36m(func pid=86940)[0m f1_per_class: [0.295, 0.071, 0.293, 0.552, 0.076, 0.377, 0.359, 0.356, 0.165, 0.206]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6157 | Steps: 2 | Val loss: 2.2910 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.0925 | Steps: 2 | Val loss: 2.1357 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=91972)[0m top1: 0.22061567164179105
[2m[36m(func pid=91972)[0m top5: 0.7565298507462687
[2m[36m(func pid=91972)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=91972)[0m f1_macro: 0.20488424482253992
[2m[36m(func pid=91972)[0m f1_weighted: 0.2086528411492214
[2m[36m(func pid=91972)[0m f1_per_class: [0.089, 0.0, 0.256, 0.177, 0.233, 0.411, 0.281, 0.306, 0.109, 0.188]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0633 | Steps: 2 | Val loss: 1.8291 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:05:35 (running for 00:12:44.27)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.616 |      0.073 |                   61 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.105 |      0.18  |                   60 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.07  |      0.275 |                   58 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  3.378 |      0.205 |                   37 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.10634328358208955
[2m[36m(func pid=85916)[0m top5: 0.6240671641791045
[2m[36m(func pid=85916)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=85916)[0m f1_macro: 0.0729114653257497
[2m[36m(func pid=85916)[0m f1_weighted: 0.0970678592970889
[2m[36m(func pid=85916)[0m f1_per_class: [0.026, 0.234, 0.062, 0.112, 0.0, 0.008, 0.045, 0.161, 0.0, 0.08]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.166044776119403
[2m[36m(func pid=86021)[0m top5: 0.7975746268656716
[2m[36m(func pid=86021)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=86021)[0m f1_macro: 0.1838287347093778
[2m[36m(func pid=86021)[0m f1_weighted: 0.18579054495533787
[2m[36m(func pid=86021)[0m f1_per_class: [0.122, 0.114, 0.282, 0.089, 0.046, 0.232, 0.273, 0.41, 0.088, 0.182]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.4423 | Steps: 2 | Val loss: 5.4349 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=86940)[0m top1: 0.36800373134328357
[2m[36m(func pid=86940)[0m top5: 0.9090485074626866
[2m[36m(func pid=86940)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=86940)[0m f1_macro: 0.2845248307961569
[2m[36m(func pid=86940)[0m f1_weighted: 0.3829665282815428
[2m[36m(func pid=86940)[0m f1_per_class: [0.242, 0.119, 0.393, 0.532, 0.062, 0.35, 0.469, 0.356, 0.105, 0.219]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6210 | Steps: 2 | Val loss: 2.2875 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2191 | Steps: 2 | Val loss: 2.1328 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=91972)[0m top1: 0.3362873134328358
[2m[36m(func pid=91972)[0m top5: 0.863339552238806
[2m[36m(func pid=91972)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=91972)[0m f1_macro: 0.2535433664095844
[2m[36m(func pid=91972)[0m f1_weighted: 0.32845858875236433
[2m[36m(func pid=91972)[0m f1_per_class: [0.08, 0.0, 0.188, 0.482, 0.239, 0.423, 0.355, 0.503, 0.155, 0.111]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.0121 | Steps: 2 | Val loss: 1.8424 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:05:40 (running for 00:12:49.45)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.616 |      0.073 |                   61 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.219 |      0.184 |                   62 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.063 |      0.285 |                   59 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  3.442 |      0.254 |                   38 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.17490671641791045
[2m[36m(func pid=86021)[0m top5: 0.8036380597014925
[2m[36m(func pid=86021)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=86021)[0m f1_macro: 0.1840872945291536
[2m[36m(func pid=86021)[0m f1_weighted: 0.2059107129187708
[2m[36m(func pid=86021)[0m f1_per_class: [0.119, 0.097, 0.278, 0.135, 0.044, 0.201, 0.322, 0.407, 0.089, 0.148]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=85916)[0m top1: 0.11054104477611941
[2m[36m(func pid=85916)[0m top5: 0.636660447761194
[2m[36m(func pid=85916)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=85916)[0m f1_macro: 0.07892552157385434
[2m[36m(func pid=85916)[0m f1_weighted: 0.10443449967960053
[2m[36m(func pid=85916)[0m f1_per_class: [0.027, 0.234, 0.062, 0.116, 0.0, 0.008, 0.057, 0.211, 0.0, 0.074]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.9936 | Steps: 2 | Val loss: 6.9521 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=86940)[0m top1: 0.3596082089552239
[2m[36m(func pid=86940)[0m top5: 0.9113805970149254
[2m[36m(func pid=86940)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=86940)[0m f1_macro: 0.2935836136667661
[2m[36m(func pid=86940)[0m f1_weighted: 0.3957944964953371
[2m[36m(func pid=86940)[0m f1_per_class: [0.201, 0.256, 0.431, 0.502, 0.055, 0.245, 0.489, 0.424, 0.115, 0.218]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6004 | Steps: 2 | Val loss: 2.2871 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.0847 | Steps: 2 | Val loss: 2.1275 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=91972)[0m top1: 0.25326492537313433
[2m[36m(func pid=91972)[0m top5: 0.851679104477612
[2m[36m(func pid=91972)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=91972)[0m f1_macro: 0.14744913599701834
[2m[36m(func pid=91972)[0m f1_weighted: 0.15144716084805213
[2m[36m(func pid=91972)[0m f1_per_class: [0.044, 0.363, 0.239, 0.0, 0.0, 0.0, 0.189, 0.456, 0.077, 0.105]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:05:45 (running for 00:12:54.60)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.621 |      0.079 |                   62 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.085 |      0.189 |                   63 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.012 |      0.294 |                   60 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.994 |      0.147 |                   39 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.18097014925373134
[2m[36m(func pid=86021)[0m top5: 0.8078358208955224
[2m[36m(func pid=86021)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=86021)[0m f1_macro: 0.18913065775977947
[2m[36m(func pid=86021)[0m f1_weighted: 0.21735710590366222
[2m[36m(func pid=86021)[0m f1_per_class: [0.121, 0.071, 0.278, 0.212, 0.043, 0.165, 0.312, 0.425, 0.094, 0.17]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.1655 | Steps: 2 | Val loss: 1.8716 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=85916)[0m top1: 0.11054104477611941
[2m[36m(func pid=85916)[0m top5: 0.6361940298507462
[2m[36m(func pid=85916)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=85916)[0m f1_macro: 0.07973312115879244
[2m[36m(func pid=85916)[0m f1_weighted: 0.10684221707428246
[2m[36m(func pid=85916)[0m f1_per_class: [0.027, 0.227, 0.062, 0.12, 0.0, 0.016, 0.062, 0.209, 0.0, 0.074]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6401 | Steps: 2 | Val loss: 4.8315 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=86940)[0m top1: 0.35867537313432835
[2m[36m(func pid=86940)[0m top5: 0.9020522388059702
[2m[36m(func pid=86940)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=86940)[0m f1_macro: 0.3100898192334428
[2m[36m(func pid=86940)[0m f1_weighted: 0.4040301020571737
[2m[36m(func pid=86940)[0m f1_per_class: [0.238, 0.412, 0.349, 0.436, 0.057, 0.249, 0.465, 0.504, 0.156, 0.235]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6437 | Steps: 2 | Val loss: 2.2876 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2089 | Steps: 2 | Val loss: 2.1249 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=91972)[0m top1: 0.3148320895522388
[2m[36m(func pid=91972)[0m top5: 0.8973880597014925
[2m[36m(func pid=91972)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=91972)[0m f1_macro: 0.2407869835086342
[2m[36m(func pid=91972)[0m f1_weighted: 0.3605761036773518
[2m[36m(func pid=91972)[0m f1_per_class: [0.244, 0.279, 0.218, 0.458, 0.037, 0.0, 0.493, 0.432, 0.15, 0.096]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:05:50 (running for 00:12:59.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.644 |      0.081 |                   64 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.085 |      0.189 |                   63 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.166 |      0.31  |                   61 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.64  |      0.241 |                   40 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.11333955223880597
[2m[36m(func pid=85916)[0m top5: 0.6399253731343284
[2m[36m(func pid=85916)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=85916)[0m f1_macro: 0.0811644190442135
[2m[36m(func pid=85916)[0m f1_weighted: 0.11031045838618568
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.229, 0.06, 0.127, 0.0, 0.016, 0.065, 0.215, 0.0, 0.071]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1429 | Steps: 2 | Val loss: 1.9664 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=86021)[0m top1: 0.1767723880597015
[2m[36m(func pid=86021)[0m top5: 0.8083022388059702
[2m[36m(func pid=86021)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=86021)[0m f1_macro: 0.18370630091820828
[2m[36m(func pid=86021)[0m f1_weighted: 0.20586408234777917
[2m[36m(func pid=86021)[0m f1_per_class: [0.121, 0.026, 0.278, 0.261, 0.043, 0.146, 0.258, 0.438, 0.087, 0.178]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5820 | Steps: 2 | Val loss: 6.1218 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=86940)[0m top1: 0.2994402985074627
[2m[36m(func pid=86940)[0m top5: 0.8736007462686567
[2m[36m(func pid=86940)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=86940)[0m f1_macro: 0.26287715214078816
[2m[36m(func pid=86940)[0m f1_weighted: 0.3244156967813876
[2m[36m(func pid=86940)[0m f1_per_class: [0.224, 0.452, 0.186, 0.374, 0.065, 0.295, 0.231, 0.459, 0.148, 0.194]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6029 | Steps: 2 | Val loss: 2.2850 | Batch size: 32 | lr: 0.0001 | Duration: 2.62s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0770 | Steps: 2 | Val loss: 2.1109 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=91972)[0m top1: 0.2849813432835821
[2m[36m(func pid=91972)[0m top5: 0.8395522388059702
[2m[36m(func pid=91972)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=91972)[0m f1_macro: 0.19821662672566856
[2m[36m(func pid=91972)[0m f1_weighted: 0.2640770435866474
[2m[36m(func pid=91972)[0m f1_per_class: [0.197, 0.0, 0.259, 0.524, 0.062, 0.289, 0.208, 0.234, 0.047, 0.162]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:05:55 (running for 00:13:04.84)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.603 |      0.083 |                   65 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.209 |      0.184 |                   64 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.143 |      0.263 |                   62 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.582 |      0.198 |                   41 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.1166044776119403
[2m[36m(func pid=85916)[0m top5: 0.644589552238806
[2m[36m(func pid=85916)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=85916)[0m f1_macro: 0.08278234215730321
[2m[36m(func pid=85916)[0m f1_weighted: 0.11369429486477874
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.233, 0.061, 0.137, 0.0, 0.016, 0.065, 0.214, 0.0, 0.074]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.1921641791044776
[2m[36m(func pid=86021)[0m top5: 0.8069029850746269
[2m[36m(func pid=86021)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=86021)[0m f1_macro: 0.19219918853664458
[2m[36m(func pid=86021)[0m f1_weighted: 0.21776065923790924
[2m[36m(func pid=86021)[0m f1_per_class: [0.137, 0.031, 0.278, 0.342, 0.042, 0.134, 0.223, 0.434, 0.092, 0.208]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0533 | Steps: 2 | Val loss: 2.1026 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2738 | Steps: 2 | Val loss: 5.7505 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=86940)[0m top1: 0.2653917910447761
[2m[36m(func pid=86940)[0m top5: 0.8460820895522388
[2m[36m(func pid=86940)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=86940)[0m f1_macro: 0.2390332233525732
[2m[36m(func pid=86940)[0m f1_weighted: 0.27469745448592875
[2m[36m(func pid=86940)[0m f1_per_class: [0.201, 0.417, 0.16, 0.306, 0.078, 0.353, 0.128, 0.478, 0.118, 0.15]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6191 | Steps: 2 | Val loss: 2.2842 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.1293 | Steps: 2 | Val loss: 2.1140 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=91972)[0m top1: 0.2630597014925373
[2m[36m(func pid=91972)[0m top5: 0.8055037313432836
[2m[36m(func pid=91972)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=91972)[0m f1_macro: 0.22785759825806812
[2m[36m(func pid=91972)[0m f1_weighted: 0.2856588975377505
[2m[36m(func pid=91972)[0m f1_per_class: [0.159, 0.0, 0.338, 0.324, 0.074, 0.378, 0.422, 0.245, 0.112, 0.226]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:01 (running for 00:13:10.17)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.619 |      0.082 |                   66 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.077 |      0.192 |                   65 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.053 |      0.239 |                   63 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  1.274 |      0.228 |                   42 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.11473880597014925
[2m[36m(func pid=85916)[0m top5: 0.6487873134328358
[2m[36m(func pid=85916)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=85916)[0m f1_macro: 0.08169551761665841
[2m[36m(func pid=85916)[0m f1_weighted: 0.11234121792495284
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.227, 0.062, 0.14, 0.0, 0.016, 0.062, 0.206, 0.0, 0.075]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9703 | Steps: 2 | Val loss: 2.1835 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=86021)[0m top1: 0.18423507462686567
[2m[36m(func pid=86021)[0m top5: 0.8078358208955224
[2m[36m(func pid=86021)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=86021)[0m f1_macro: 0.18323643457467592
[2m[36m(func pid=86021)[0m f1_weighted: 0.2080800611865261
[2m[36m(func pid=86021)[0m f1_per_class: [0.139, 0.041, 0.278, 0.341, 0.039, 0.076, 0.209, 0.438, 0.083, 0.189]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.9572 | Steps: 2 | Val loss: 4.0265 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=86940)[0m top1: 0.259794776119403
[2m[36m(func pid=86940)[0m top5: 0.8348880597014925
[2m[36m(func pid=86940)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=86940)[0m f1_macro: 0.2361735716420675
[2m[36m(func pid=86940)[0m f1_weighted: 0.2592305114990494
[2m[36m(func pid=86940)[0m f1_per_class: [0.137, 0.421, 0.15, 0.306, 0.093, 0.373, 0.066, 0.497, 0.111, 0.208]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.5926 | Steps: 2 | Val loss: 2.2816 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.0806 | Steps: 2 | Val loss: 2.1090 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=91972)[0m top1: 0.39505597014925375
[2m[36m(func pid=91972)[0m top5: 0.8810634328358209
[2m[36m(func pid=91972)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=91972)[0m f1_macro: 0.3011103363305391
[2m[36m(func pid=91972)[0m f1_weighted: 0.4090308498575081
[2m[36m(func pid=91972)[0m f1_per_class: [0.177, 0.186, 0.349, 0.498, 0.158, 0.296, 0.548, 0.468, 0.142, 0.188]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:06 (running for 00:13:15.32)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.593 |      0.083 |                   67 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.129 |      0.183 |                   66 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  0.97  |      0.236 |                   64 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.957 |      0.301 |                   43 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.11567164179104478
[2m[36m(func pid=85916)[0m top5: 0.6576492537313433
[2m[36m(func pid=85916)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=85916)[0m f1_macro: 0.0827501900790241
[2m[36m(func pid=85916)[0m f1_weighted: 0.1147649021095595
[2m[36m(func pid=85916)[0m f1_per_class: [0.026, 0.224, 0.067, 0.142, 0.0, 0.016, 0.071, 0.206, 0.0, 0.075]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.9454 | Steps: 2 | Val loss: 2.0917 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=86021)[0m top1: 0.19029850746268656
[2m[36m(func pid=86021)[0m top5: 0.8120335820895522
[2m[36m(func pid=86021)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=86021)[0m f1_macro: 0.18193035067681196
[2m[36m(func pid=86021)[0m f1_weighted: 0.21765448709623642
[2m[36m(func pid=86021)[0m f1_per_class: [0.142, 0.078, 0.278, 0.353, 0.037, 0.03, 0.226, 0.443, 0.09, 0.143]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4259 | Steps: 2 | Val loss: 4.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=86940)[0m top1: 0.31716417910447764
[2m[36m(func pid=86940)[0m top5: 0.8610074626865671
[2m[36m(func pid=86940)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=86940)[0m f1_macro: 0.2661674480251707
[2m[36m(func pid=86940)[0m f1_weighted: 0.3307650332188779
[2m[36m(func pid=86940)[0m f1_per_class: [0.117, 0.415, 0.153, 0.442, 0.112, 0.377, 0.175, 0.496, 0.157, 0.218]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0760 | Steps: 2 | Val loss: 2.1091 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6205 | Steps: 2 | Val loss: 2.2814 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=91972)[0m top1: 0.35401119402985076
[2m[36m(func pid=91972)[0m top5: 0.9230410447761194
[2m[36m(func pid=91972)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=91972)[0m f1_macro: 0.26068346139760956
[2m[36m(func pid=91972)[0m f1_weighted: 0.3161776742316085
[2m[36m(func pid=91972)[0m f1_per_class: [0.299, 0.45, 0.36, 0.228, 0.0, 0.358, 0.336, 0.417, 0.0, 0.16]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:11 (running for 00:13:20.41)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.593 |      0.083 |                   67 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.076 |      0.193 |                   68 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  0.945 |      0.266 |                   65 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.426 |      0.261 |                   44 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.20708955223880596
[2m[36m(func pid=86021)[0m top5: 0.8194962686567164
[2m[36m(func pid=86021)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=86021)[0m f1_macro: 0.1932463934224248
[2m[36m(func pid=86021)[0m f1_weighted: 0.24544588692183397
[2m[36m(func pid=86021)[0m f1_per_class: [0.14, 0.165, 0.256, 0.358, 0.037, 0.037, 0.262, 0.438, 0.091, 0.148]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.0743 | Steps: 2 | Val loss: 1.9654 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=85916)[0m top1: 0.1166044776119403
[2m[36m(func pid=85916)[0m top5: 0.6613805970149254
[2m[36m(func pid=85916)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=85916)[0m f1_macro: 0.08457057524406594
[2m[36m(func pid=85916)[0m f1_weighted: 0.1163241366385655
[2m[36m(func pid=85916)[0m f1_per_class: [0.032, 0.22, 0.069, 0.147, 0.0, 0.008, 0.074, 0.218, 0.0, 0.078]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4553 | Steps: 2 | Val loss: 4.0182 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=86940)[0m top1: 0.3694029850746269
[2m[36m(func pid=86940)[0m top5: 0.8927238805970149
[2m[36m(func pid=86940)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=86940)[0m f1_macro: 0.2887341382813565
[2m[36m(func pid=86940)[0m f1_weighted: 0.39882585104402757
[2m[36m(func pid=86940)[0m f1_per_class: [0.136, 0.431, 0.176, 0.484, 0.119, 0.398, 0.362, 0.413, 0.148, 0.22]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0680 | Steps: 2 | Val loss: 2.1063 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6193 | Steps: 2 | Val loss: 2.2801 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=91972)[0m top1: 0.36613805970149255
[2m[36m(func pid=91972)[0m top5: 0.9319029850746269
[2m[36m(func pid=91972)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=91972)[0m f1_macro: 0.30194471152860175
[2m[36m(func pid=91972)[0m f1_weighted: 0.3313383103147176
[2m[36m(func pid=91972)[0m f1_per_class: [0.265, 0.497, 0.333, 0.239, 0.192, 0.423, 0.31, 0.448, 0.0, 0.312]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:16 (running for 00:13:25.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.62  |      0.085 |                   68 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.068 |      0.196 |                   69 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.074 |      0.289 |                   66 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.455 |      0.302 |                   45 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.11613805970149253
[2m[36m(func pid=85916)[0m top5: 0.6660447761194029
[2m[36m(func pid=85916)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=85916)[0m f1_macro: 0.08513162223596105
[2m[36m(func pid=85916)[0m f1_weighted: 0.11834430412224223
[2m[36m(func pid=85916)[0m f1_per_class: [0.03, 0.214, 0.071, 0.149, 0.0, 0.008, 0.082, 0.217, 0.0, 0.08]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.21315298507462688
[2m[36m(func pid=86021)[0m top5: 0.8236940298507462
[2m[36m(func pid=86021)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=86021)[0m f1_macro: 0.19559024529149788
[2m[36m(func pid=86021)[0m f1_weighted: 0.25224201882471636
[2m[36m(func pid=86021)[0m f1_per_class: [0.135, 0.222, 0.244, 0.352, 0.037, 0.023, 0.262, 0.444, 0.105, 0.132]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.9196 | Steps: 2 | Val loss: 1.8580 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1500 | Steps: 2 | Val loss: 4.2219 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=86940)[0m top1: 0.40111940298507465
[2m[36m(func pid=86940)[0m top5: 0.9109141791044776
[2m[36m(func pid=86940)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=86940)[0m f1_macro: 0.2993078013729578
[2m[36m(func pid=86940)[0m f1_weighted: 0.4181890998338213
[2m[36m(func pid=86940)[0m f1_per_class: [0.188, 0.538, 0.247, 0.433, 0.143, 0.406, 0.432, 0.3, 0.117, 0.19]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.0429 | Steps: 2 | Val loss: 2.1101 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6185 | Steps: 2 | Val loss: 2.2792 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=91972)[0m top1: 0.3306902985074627
[2m[36m(func pid=91972)[0m top5: 0.9263059701492538
[2m[36m(func pid=91972)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=91972)[0m f1_macro: 0.2806282209163419
[2m[36m(func pid=91972)[0m f1_weighted: 0.36810585017658143
[2m[36m(func pid=91972)[0m f1_per_class: [0.23, 0.265, 0.31, 0.409, 0.057, 0.326, 0.443, 0.491, 0.026, 0.25]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:21 (running for 00:13:30.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.619 |      0.085 |                   69 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.043 |      0.19  |                   70 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  0.92  |      0.299 |                   67 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.15  |      0.281 |                   46 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.20382462686567165
[2m[36m(func pid=86021)[0m top5: 0.8269589552238806
[2m[36m(func pid=86021)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=86021)[0m f1_macro: 0.19020798058387162
[2m[36m(func pid=86021)[0m f1_weighted: 0.24265175437461475
[2m[36m(func pid=86021)[0m f1_per_class: [0.137, 0.232, 0.247, 0.308, 0.039, 0.051, 0.259, 0.423, 0.107, 0.099]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=85916)[0m top1: 0.11287313432835822
[2m[36m(func pid=85916)[0m top5: 0.6711753731343284
[2m[36m(func pid=85916)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=85916)[0m f1_macro: 0.08024015693229244
[2m[36m(func pid=85916)[0m f1_weighted: 0.11616154669750689
[2m[36m(func pid=85916)[0m f1_per_class: [0.029, 0.206, 0.068, 0.147, 0.0, 0.016, 0.082, 0.205, 0.0, 0.049]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.2797 | Steps: 2 | Val loss: 1.8438 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2851 | Steps: 2 | Val loss: 4.2813 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=86940)[0m top1: 0.396455223880597
[2m[36m(func pid=86940)[0m top5: 0.9155783582089553
[2m[36m(func pid=86940)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=86940)[0m f1_macro: 0.30287963018634845
[2m[36m(func pid=86940)[0m f1_weighted: 0.41282760800360013
[2m[36m(func pid=86940)[0m f1_per_class: [0.194, 0.529, 0.256, 0.35, 0.131, 0.393, 0.496, 0.3, 0.136, 0.242]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6062 | Steps: 2 | Val loss: 2.2754 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0477 | Steps: 2 | Val loss: 2.1183 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=91972)[0m top1: 0.33722014925373134
[2m[36m(func pid=91972)[0m top5: 0.8955223880597015
[2m[36m(func pid=91972)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=91972)[0m f1_macro: 0.2714775808626074
[2m[36m(func pid=91972)[0m f1_weighted: 0.37632863314564546
[2m[36m(func pid=91972)[0m f1_per_class: [0.223, 0.179, 0.289, 0.466, 0.054, 0.297, 0.493, 0.371, 0.116, 0.226]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:26 (running for 00:13:35.82)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.618 |      0.08  |                   70 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.048 |      0.177 |                   71 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  1.28  |      0.303 |                   68 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.285 |      0.271 |                   47 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.11707089552238806
[2m[36m(func pid=85916)[0m top5: 0.679570895522388
[2m[36m(func pid=85916)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=85916)[0m f1_macro: 0.08510677763156624
[2m[36m(func pid=85916)[0m f1_weighted: 0.12262321146318945
[2m[36m(func pid=85916)[0m f1_per_class: [0.029, 0.2, 0.071, 0.157, 0.0, 0.016, 0.093, 0.232, 0.0, 0.054]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.1865671641791045
[2m[36m(func pid=86021)[0m top5: 0.8236940298507462
[2m[36m(func pid=86021)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=86021)[0m f1_macro: 0.176707152064786
[2m[36m(func pid=86021)[0m f1_weighted: 0.21891549355462364
[2m[36m(func pid=86021)[0m f1_per_class: [0.13, 0.222, 0.256, 0.287, 0.039, 0.044, 0.218, 0.385, 0.09, 0.098]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.8907 | Steps: 2 | Val loss: 1.8647 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1492 | Steps: 2 | Val loss: 3.9452 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=86940)[0m top1: 0.35074626865671643
[2m[36m(func pid=86940)[0m top5: 0.9053171641791045
[2m[36m(func pid=86940)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=86940)[0m f1_macro: 0.28435432041408515
[2m[36m(func pid=86940)[0m f1_weighted: 0.38706920911836856
[2m[36m(func pid=86940)[0m f1_per_class: [0.226, 0.448, 0.247, 0.326, 0.077, 0.339, 0.485, 0.404, 0.099, 0.192]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.5905 | Steps: 2 | Val loss: 2.2712 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0250 | Steps: 2 | Val loss: 2.1168 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=91972)[0m top1: 0.3474813432835821
[2m[36m(func pid=91972)[0m top5: 0.8959888059701493
[2m[36m(func pid=91972)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=91972)[0m f1_macro: 0.3000509002873127
[2m[36m(func pid=91972)[0m f1_weighted: 0.3838894661650357
[2m[36m(func pid=91972)[0m f1_per_class: [0.262, 0.294, 0.297, 0.448, 0.098, 0.385, 0.431, 0.349, 0.131, 0.306]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:31 (running for 00:13:40.82)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.591 |      0.088 |                   72 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.048 |      0.177 |                   71 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  0.891 |      0.284 |                   69 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.149 |      0.3   |                   48 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.12126865671641791
[2m[36m(func pid=85916)[0m top5: 0.6847014925373134
[2m[36m(func pid=85916)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=85916)[0m f1_macro: 0.08752404346519638
[2m[36m(func pid=85916)[0m f1_weighted: 0.12684448582078223
[2m[36m(func pid=85916)[0m f1_per_class: [0.029, 0.205, 0.075, 0.165, 0.0, 0.016, 0.095, 0.235, 0.0, 0.056]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.19076492537313433
[2m[36m(func pid=86021)[0m top5: 0.8255597014925373
[2m[36m(func pid=86021)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=86021)[0m f1_macro: 0.18148926340298674
[2m[36m(func pid=86021)[0m f1_weighted: 0.2265360687488524
[2m[36m(func pid=86021)[0m f1_per_class: [0.123, 0.214, 0.268, 0.279, 0.041, 0.098, 0.234, 0.393, 0.082, 0.082]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.9941 | Steps: 2 | Val loss: 1.9091 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.1459 | Steps: 2 | Val loss: 3.7453 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5718 | Steps: 2 | Val loss: 2.2700 | Batch size: 32 | lr: 0.0001 | Duration: 2.61s
[2m[36m(func pid=86940)[0m top1: 0.322294776119403
[2m[36m(func pid=86940)[0m top5: 0.8889925373134329
[2m[36m(func pid=86940)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=86940)[0m f1_macro: 0.28254724976694867
[2m[36m(func pid=86940)[0m f1_weighted: 0.36872722552763076
[2m[36m(func pid=86940)[0m f1_per_class: [0.254, 0.34, 0.256, 0.356, 0.063, 0.342, 0.441, 0.474, 0.104, 0.195]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2323 | Steps: 2 | Val loss: 2.1192 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=91972)[0m top1: 0.41044776119402987
[2m[36m(func pid=91972)[0m top5: 0.9118470149253731
[2m[36m(func pid=91972)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=91972)[0m f1_macro: 0.3484255976273315
[2m[36m(func pid=91972)[0m f1_weighted: 0.4250768219793777
[2m[36m(func pid=91972)[0m f1_per_class: [0.255, 0.455, 0.324, 0.451, 0.244, 0.462, 0.426, 0.411, 0.153, 0.304]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:36 (running for 00:13:45.85)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.572 |      0.088 |                   73 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.025 |      0.181 |                   72 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  0.994 |      0.283 |                   70 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.146 |      0.348 |                   49 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=85916)[0m top1: 0.12033582089552239
[2m[36m(func pid=85916)[0m top5: 0.6865671641791045
[2m[36m(func pid=85916)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=85916)[0m f1_macro: 0.08807412898131947
[2m[36m(func pid=85916)[0m f1_weighted: 0.12708753847866727
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.202, 0.077, 0.162, 0.0, 0.016, 0.1, 0.238, 0.0, 0.057]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86021)[0m top1: 0.19776119402985073
[2m[36m(func pid=86021)[0m top5: 0.8278917910447762
[2m[36m(func pid=86021)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=86021)[0m f1_macro: 0.18366585886985692
[2m[36m(func pid=86021)[0m f1_weighted: 0.23604890896722716
[2m[36m(func pid=86021)[0m f1_per_class: [0.117, 0.204, 0.259, 0.273, 0.042, 0.159, 0.259, 0.409, 0.019, 0.097]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.9848 | Steps: 2 | Val loss: 1.9636 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1145 | Steps: 2 | Val loss: 3.7539 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=86940)[0m top1: 0.3064365671641791
[2m[36m(func pid=86940)[0m top5: 0.8754664179104478
[2m[36m(func pid=86940)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=86940)[0m f1_macro: 0.2649129594999945
[2m[36m(func pid=86940)[0m f1_weighted: 0.34112153347783225
[2m[36m(func pid=86940)[0m f1_per_class: [0.185, 0.324, 0.253, 0.357, 0.071, 0.383, 0.355, 0.426, 0.099, 0.196]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.5681 | Steps: 2 | Val loss: 2.2675 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.0324 | Steps: 2 | Val loss: 2.1077 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=91972)[0m top1: 0.4388992537313433
[2m[36m(func pid=91972)[0m top5: 0.9235074626865671
[2m[36m(func pid=91972)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=91972)[0m f1_macro: 0.3412936777528448
[2m[36m(func pid=91972)[0m f1_weighted: 0.44153980406545207
[2m[36m(func pid=91972)[0m f1_per_class: [0.211, 0.481, 0.338, 0.425, 0.213, 0.47, 0.486, 0.474, 0.122, 0.194]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:42 (running for 00:13:51.09)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.572 |      0.088 |                   73 |
| train_51d3e_00005 | RUNNING    | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.032 |      0.189 |                   74 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  0.985 |      0.265 |                   71 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.114 |      0.341 |                   50 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.20662313432835822
[2m[36m(func pid=86021)[0m top5: 0.8260261194029851
[2m[36m(func pid=86021)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=86021)[0m f1_macro: 0.18850881797209804
[2m[36m(func pid=86021)[0m f1_weighted: 0.23250055617979937
[2m[36m(func pid=86021)[0m f1_per_class: [0.121, 0.169, 0.278, 0.348, 0.049, 0.234, 0.17, 0.382, 0.044, 0.091]
[2m[36m(func pid=86021)[0m 
[2m[36m(func pid=85916)[0m top1: 0.11986940298507463
[2m[36m(func pid=85916)[0m top5: 0.6893656716417911
[2m[36m(func pid=85916)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=85916)[0m f1_macro: 0.08664385366817151
[2m[36m(func pid=85916)[0m f1_weighted: 0.12572188477240098
[2m[36m(func pid=85916)[0m f1_per_class: [0.028, 0.204, 0.081, 0.161, 0.0, 0.008, 0.1, 0.229, 0.0, 0.056]
[2m[36m(func pid=85916)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.8981 | Steps: 2 | Val loss: 1.9583 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2044 | Steps: 2 | Val loss: 3.7415 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=86021)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0579 | Steps: 2 | Val loss: 2.0922 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=86940)[0m top1: 0.30736940298507465
[2m[36m(func pid=86940)[0m top5: 0.875
[2m[36m(func pid=86940)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=86940)[0m f1_macro: 0.2665779774439426
[2m[36m(func pid=86940)[0m f1_weighted: 0.32837247453744933
[2m[36m(func pid=86940)[0m f1_per_class: [0.182, 0.276, 0.278, 0.393, 0.089, 0.39, 0.3, 0.436, 0.104, 0.217]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=85916)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6016 | Steps: 2 | Val loss: 2.2671 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=91972)[0m top1: 0.42630597014925375
[2m[36m(func pid=91972)[0m top5: 0.9286380597014925
[2m[36m(func pid=91972)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=91972)[0m f1_macro: 0.32423774268246885
[2m[36m(func pid=91972)[0m f1_weighted: 0.43258746729800707
[2m[36m(func pid=91972)[0m f1_per_class: [0.245, 0.477, 0.314, 0.389, 0.186, 0.402, 0.53, 0.472, 0.0, 0.228]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:06:47 (running for 00:13:56.16)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00004 | RUNNING    | 192.168.7.53:85916 | 0.0001 |       0.9  |         0      |  2.568 |      0.087 |                   74 |
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940 | 0.01   |       0.9  |         0      |  0.898 |      0.267 |                   72 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972 | 0.1    |       0.9  |         0      |  0.204 |      0.324 |                   51 |
| train_51d3e_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418 | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792 | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189 | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627 | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021 | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86021)[0m top1: 0.23087686567164178
[2m[36m(func pid=86021)[0m top5: 0.8264925373134329
[2m[36m(func pid=86021)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=86021)[0m f1_macro: 0.19650866103024536
[2m[36m(func pid=86021)[0m f1_weighted: 0.24074759064029533
[2m[36m(func pid=86021)[0m f1_per_class: [0.135, 0.184, 0.282, 0.398, 0.056, 0.285, 0.12, 0.409, 0.0, 0.095]
[2m[36m(func pid=85916)[0m top1: 0.11707089552238806
[2m[36m(func pid=85916)[0m top5: 0.6893656716417911
[2m[36m(func pid=85916)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=85916)[0m f1_macro: 0.08593128037453308
[2m[36m(func pid=85916)[0m f1_weighted: 0.1240090416370544
[2m[36m(func pid=85916)[0m f1_per_class: [0.027, 0.204, 0.083, 0.148, 0.0, 0.016, 0.105, 0.221, 0.0, 0.056]
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.0082 | Steps: 2 | Val loss: 1.9254 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2556 | Steps: 2 | Val loss: 4.1315 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=86940)[0m top1: 0.32649253731343286
[2m[36m(func pid=86940)[0m top5: 0.8754664179104478
[2m[36m(func pid=86940)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=86940)[0m f1_macro: 0.28864444874636735
[2m[36m(func pid=86940)[0m f1_weighted: 0.34742969412152347
[2m[36m(func pid=86940)[0m f1_per_class: [0.24, 0.344, 0.268, 0.41, 0.106, 0.393, 0.292, 0.478, 0.13, 0.224]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=91972)[0m top1: 0.35447761194029853
[2m[36m(func pid=91972)[0m top5: 0.9137126865671642
[2m[36m(func pid=91972)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=91972)[0m f1_macro: 0.2577987198471397
[2m[36m(func pid=91972)[0m f1_weighted: 0.38321757980879095
[2m[36m(func pid=91972)[0m f1_per_class: [0.169, 0.267, 0.297, 0.419, 0.063, 0.213, 0.558, 0.372, 0.0, 0.22]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9678 | Steps: 2 | Val loss: 1.8791 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1853 | Steps: 2 | Val loss: 4.5582 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=86940)[0m top1: 0.36427238805970147
[2m[36m(func pid=86940)[0m top5: 0.8885261194029851
[2m[36m(func pid=86940)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=86940)[0m f1_macro: 0.308124004534209
[2m[36m(func pid=86940)[0m f1_weighted: 0.3902399910112889
[2m[36m(func pid=86940)[0m f1_per_class: [0.212, 0.46, 0.214, 0.446, 0.141, 0.403, 0.329, 0.482, 0.161, 0.235]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=91972)[0m top1: 0.30363805970149255
[2m[36m(func pid=91972)[0m top5: 0.9020522388059702
[2m[36m(func pid=91972)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=91972)[0m f1_macro: 0.229561079551734
[2m[36m(func pid=91972)[0m f1_weighted: 0.3392234658595811
[2m[36m(func pid=91972)[0m f1_per_class: [0.14, 0.215, 0.297, 0.35, 0.049, 0.205, 0.54, 0.175, 0.061, 0.262]
[2m[36m(func pid=103380)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103380)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=103380)[0m Configuration completed!
[2m[36m(func pid=103380)[0m New optimizer parameters:
[2m[36m(func pid=103380)[0m SGD (
[2m[36m(func pid=103380)[0m Parameter Group 0
[2m[36m(func pid=103380)[0m     dampening: 0
[2m[36m(func pid=103380)[0m     differentiable: False
[2m[36m(func pid=103380)[0m     foreach: None
[2m[36m(func pid=103380)[0m     lr: 0.0001
[2m[36m(func pid=103380)[0m     maximize: False
[2m[36m(func pid=103380)[0m     momentum: 0.99
[2m[36m(func pid=103380)[0m     nesterov: False
[2m[36m(func pid=103380)[0m     weight_decay: 0.0001
[2m[36m(func pid=103380)[0m )
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:06:55 (running for 00:14:04.41)
Memory usage on this node: 21.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.24225
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.968 |      0.308 |                   74 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.256 |      0.258 |                   52 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103390)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103390)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=103390)[0m Configuration completed!
[2m[36m(func pid=103390)[0m New optimizer parameters:
[2m[36m(func pid=103390)[0m SGD (
[2m[36m(func pid=103390)[0m Parameter Group 0
[2m[36m(func pid=103390)[0m     dampening: 0
[2m[36m(func pid=103390)[0m     differentiable: False
[2m[36m(func pid=103390)[0m     foreach: None
[2m[36m(func pid=103390)[0m     lr: 0.001
[2m[36m(func pid=103390)[0m     maximize: False
[2m[36m(func pid=103390)[0m     momentum: 0.99
[2m[36m(func pid=103390)[0m     nesterov: False
[2m[36m(func pid=103390)[0m     weight_decay: 0.0001
[2m[36m(func pid=103390)[0m )
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.0324 | Steps: 2 | Val loss: 1.8847 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2099 | Steps: 2 | Val loss: 4.1710 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0746 | Steps: 2 | Val loss: 2.3723 | Batch size: 32 | lr: 0.0001 | Duration: 4.38s
== Status ==
Current time: 2024-01-07 00:07:00 (running for 00:14:09.64)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.032 |      0.313 |                   75 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.185 |      0.23  |                   53 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m top1: 0.3917910447761194
[2m[36m(func pid=86940)[0m top5: 0.894589552238806
[2m[36m(func pid=86940)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=86940)[0m f1_macro: 0.31277702048003975
[2m[36m(func pid=86940)[0m f1_weighted: 0.4130044579623512
[2m[36m(func pid=86940)[0m f1_per_class: [0.195, 0.531, 0.195, 0.469, 0.158, 0.406, 0.353, 0.43, 0.161, 0.231]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0892 | Steps: 2 | Val loss: 2.3709 | Batch size: 32 | lr: 0.001 | Duration: 4.61s
[2m[36m(func pid=91972)[0m top1: 0.3451492537313433
[2m[36m(func pid=91972)[0m top5: 0.9197761194029851
[2m[36m(func pid=91972)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=91972)[0m f1_macro: 0.2781785708779772
[2m[36m(func pid=91972)[0m f1_weighted: 0.36987664885787075
[2m[36m(func pid=91972)[0m f1_per_class: [0.147, 0.463, 0.31, 0.334, 0.118, 0.378, 0.438, 0.175, 0.114, 0.304]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.06669776119402986
[2m[36m(func pid=103380)[0m top5: 0.38619402985074625
[2m[36m(func pid=103380)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=103380)[0m f1_macro: 0.01688733267680636
[2m[36m(func pid=103380)[0m f1_weighted: 0.02110447053494972
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.0, 0.051, 0.0, 0.0, 0.0, 0.118, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.9270 | Steps: 2 | Val loss: 1.9705 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=103390)[0m top1: 0.0732276119402985
[2m[36m(func pid=103390)[0m top5: 0.32649253731343286
[2m[36m(func pid=103390)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=103390)[0m f1_macro: 0.02116603222859429
[2m[36m(func pid=103390)[0m f1_weighted: 0.031234891271074552
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.005, 0.0, 0.083, 0.0, 0.0, 0.0, 0.123, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0892 | Steps: 2 | Val loss: 4.0992 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1087 | Steps: 2 | Val loss: 2.3279 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=86940)[0m top1: 0.3530783582089552
[2m[36m(func pid=86940)[0m top5: 0.8875932835820896
[2m[36m(func pid=86940)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=86940)[0m f1_macro: 0.3014999670104453
[2m[36m(func pid=86940)[0m f1_weighted: 0.37750801067978124
[2m[36m(func pid=86940)[0m f1_per_class: [0.223, 0.541, 0.222, 0.373, 0.141, 0.39, 0.317, 0.486, 0.119, 0.203]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0104 | Steps: 2 | Val loss: 2.3242 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:07:06 (running for 00:14:15.78)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.927 |      0.301 |                   76 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.089 |      0.32  |                   55 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  3.075 |      0.017 |                    1 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  3.089 |      0.021 |                    1 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.38526119402985076
[2m[36m(func pid=91972)[0m top5: 0.9221082089552238
[2m[36m(func pid=91972)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=91972)[0m f1_macro: 0.31964950783849855
[2m[36m(func pid=91972)[0m f1_weighted: 0.3821522295648068
[2m[36m(func pid=91972)[0m f1_per_class: [0.171, 0.525, 0.314, 0.336, 0.267, 0.424, 0.389, 0.312, 0.135, 0.323]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.14039179104477612
[2m[36m(func pid=103380)[0m top5: 0.527518656716418
[2m[36m(func pid=103380)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=103380)[0m f1_macro: 0.04165906501135014
[2m[36m(func pid=103380)[0m f1_weighted: 0.07914932421637627
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.0, 0.249, 0.0, 0.0, 0.0, 0.168, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.9318 | Steps: 2 | Val loss: 2.0103 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=103390)[0m top1: 0.1333955223880597
[2m[36m(func pid=103390)[0m top5: 0.5130597014925373
[2m[36m(func pid=103390)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=103390)[0m f1_macro: 0.06069681470904399
[2m[36m(func pid=103390)[0m f1_weighted: 0.08970262931776503
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.005, 0.031, 0.252, 0.0, 0.0, 0.0, 0.319, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2323 | Steps: 2 | Val loss: 4.2416 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0654 | Steps: 2 | Val loss: 2.3172 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=86940)[0m top1: 0.32975746268656714
[2m[36m(func pid=86940)[0m top5: 0.8852611940298507
[2m[36m(func pid=86940)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=86940)[0m f1_macro: 0.29434516703617913
[2m[36m(func pid=86940)[0m f1_weighted: 0.355585299044483
[2m[36m(func pid=86940)[0m f1_per_class: [0.161, 0.52, 0.306, 0.353, 0.103, 0.375, 0.281, 0.49, 0.124, 0.231]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8814 | Steps: 2 | Val loss: 2.3167 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:07:12 (running for 00:14:20.99)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.932 |      0.294 |                   77 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.232 |      0.289 |                   56 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  3.109 |      0.042 |                    2 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  3.01  |      0.061 |                    2 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.37033582089552236
[2m[36m(func pid=91972)[0m top5: 0.8927238805970149
[2m[36m(func pid=91972)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=91972)[0m f1_macro: 0.2894479510506362
[2m[36m(func pid=91972)[0m f1_weighted: 0.332261206120938
[2m[36m(func pid=91972)[0m f1_per_class: [0.179, 0.523, 0.301, 0.382, 0.057, 0.449, 0.147, 0.476, 0.131, 0.25]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.18003731343283583
[2m[36m(func pid=103380)[0m top5: 0.5685634328358209
[2m[36m(func pid=103380)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=103380)[0m f1_macro: 0.046699214383387144
[2m[36m(func pid=103380)[0m f1_weighted: 0.09642570515332123
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.0, 0.314, 0.0, 0.0, 0.0, 0.153, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.0615 | Steps: 2 | Val loss: 2.0867 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=103390)[0m top1: 0.012126865671641791
[2m[36m(func pid=103390)[0m top5: 0.5652985074626866
[2m[36m(func pid=103390)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=103390)[0m f1_macro: 0.0077955476129513215
[2m[36m(func pid=103390)[0m f1_weighted: 0.011351320295472572
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.0, 0.012, 0.034, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1361 | Steps: 2 | Val loss: 4.2297 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=86940)[0m top1: 0.28125
[2m[36m(func pid=86940)[0m top5: 0.8838619402985075
[2m[36m(func pid=86940)[0m f1_micro: 0.28125
[2m[36m(func pid=86940)[0m f1_macro: 0.27682572783991766
[2m[36m(func pid=86940)[0m f1_weighted: 0.31195569249348615
[2m[36m(func pid=86940)[0m f1_per_class: [0.119, 0.428, 0.349, 0.288, 0.078, 0.355, 0.256, 0.502, 0.12, 0.275]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0398 | Steps: 2 | Val loss: 2.3175 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=91972)[0m top1: 0.34095149253731344
[2m[36m(func pid=91972)[0m top5: 0.8889925373134329
[2m[36m(func pid=91972)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=91972)[0m f1_macro: 0.2557113534438535
[2m[36m(func pid=91972)[0m f1_weighted: 0.3120497745355206
[2m[36m(func pid=91972)[0m f1_per_class: [0.164, 0.512, 0.272, 0.446, 0.17, 0.412, 0.081, 0.353, 0.0, 0.148]
== Status ==
Current time: 2024-01-07 00:07:17 (running for 00:14:26.07)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.061 |      0.277 |                   78 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.136 |      0.256 |                   57 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  3.065 |      0.047 |                    3 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.881 |      0.008 |                    3 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7759 | Steps: 2 | Val loss: 2.3200 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.19263059701492538
[2m[36m(func pid=103380)[0m top5: 0.5746268656716418
[2m[36m(func pid=103380)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=103380)[0m f1_macro: 0.040021214893244325
[2m[36m(func pid=103380)[0m f1_weighted: 0.10012020526128192
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.0, 0.348, 0.0, 0.0, 0.0, 0.052, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.8388 | Steps: 2 | Val loss: 2.0736 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=103390)[0m top1: 0.006063432835820896
[2m[36m(func pid=103390)[0m top5: 0.5489738805970149
[2m[36m(func pid=103390)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103390)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=103390)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0982 | Steps: 2 | Val loss: 3.7292 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=86940)[0m top1: 0.27238805970149255
[2m[36m(func pid=86940)[0m top5: 0.8782649253731343
[2m[36m(func pid=86940)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=86940)[0m f1_macro: 0.26227048026636646
[2m[36m(func pid=86940)[0m f1_weighted: 0.3013607416533033
[2m[36m(func pid=86940)[0m f1_per_class: [0.13, 0.312, 0.319, 0.319, 0.073, 0.357, 0.258, 0.493, 0.143, 0.219]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9829 | Steps: 2 | Val loss: 2.3210 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=91972)[0m top1: 0.38619402985074625
[2m[36m(func pid=91972)[0m top5: 0.9039179104477612
[2m[36m(func pid=91972)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=91972)[0m f1_macro: 0.2957525050466405
[2m[36m(func pid=91972)[0m f1_weighted: 0.39418618276181705
[2m[36m(func pid=91972)[0m f1_per_class: [0.25, 0.451, 0.293, 0.295, 0.111, 0.261, 0.557, 0.424, 0.118, 0.197]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:07:22 (running for 00:14:31.36)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.839 |      0.262 |                   79 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  1.098 |      0.296 |                   58 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  3.04  |      0.04  |                    4 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.776 |      0.001 |                    4 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7555 | Steps: 2 | Val loss: 2.3248 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=103380)[0m top1: 0.20242537313432835
[2m[36m(func pid=103380)[0m top5: 0.5690298507462687
[2m[36m(func pid=103380)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=103380)[0m f1_macro: 0.03868119958681651
[2m[36m(func pid=103380)[0m f1_weighted: 0.10195370397126755
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.0, 0.36, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.5817 | Steps: 2 | Val loss: 1.9653 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=103390)[0m top1: 0.006063432835820896
[2m[36m(func pid=103390)[0m top5: 0.5513059701492538
[2m[36m(func pid=103390)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103390)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=103390)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6318 | Steps: 2 | Val loss: 4.1636 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=86940)[0m top1: 0.3283582089552239
[2m[36m(func pid=86940)[0m top5: 0.8917910447761194
[2m[36m(func pid=86940)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=86940)[0m f1_macro: 0.2819258486557919
[2m[36m(func pid=86940)[0m f1_weighted: 0.3577022942023818
[2m[36m(func pid=86940)[0m f1_per_class: [0.178, 0.233, 0.324, 0.471, 0.076, 0.362, 0.351, 0.467, 0.144, 0.215]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0004 | Steps: 2 | Val loss: 2.3259 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 00:07:27 (running for 00:14:36.56)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.582 |      0.282 |                   80 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.632 |      0.263 |                   59 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.983 |      0.039 |                    5 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.756 |      0.001 |                    5 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.29384328358208955
[2m[36m(func pid=91972)[0m top5: 0.8852611940298507
[2m[36m(func pid=91972)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=91972)[0m f1_macro: 0.26312181905856563
[2m[36m(func pid=91972)[0m f1_weighted: 0.32174675718214396
[2m[36m(func pid=91972)[0m f1_per_class: [0.254, 0.345, 0.282, 0.165, 0.093, 0.361, 0.493, 0.218, 0.141, 0.28]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9435 | Steps: 2 | Val loss: 2.3175 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=103380)[0m top1: 0.21595149253731344
[2m[36m(func pid=103380)[0m top5: 0.5690298507462687
[2m[36m(func pid=103380)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=103380)[0m f1_macro: 0.05448251250933724
[2m[36m(func pid=103380)[0m f1_weighted: 0.10697460888610048
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.143, 0.375, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.8111 | Steps: 2 | Val loss: 1.8568 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.1917 | Steps: 2 | Val loss: 4.4095 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=103390)[0m top1: 0.006063432835820896
[2m[36m(func pid=103390)[0m top5: 0.5909514925373134
[2m[36m(func pid=103390)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103390)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=103390)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=86940)[0m top1: 0.36847014925373134
[2m[36m(func pid=86940)[0m top5: 0.8899253731343284
[2m[36m(func pid=86940)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=86940)[0m f1_macro: 0.2934754429455676
[2m[36m(func pid=86940)[0m f1_weighted: 0.3892005765835789
[2m[36m(func pid=86940)[0m f1_per_class: [0.217, 0.178, 0.455, 0.525, 0.088, 0.391, 0.453, 0.316, 0.119, 0.192]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9285 | Steps: 2 | Val loss: 2.3302 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=91972)[0m top1: 0.2994402985074627
[2m[36m(func pid=91972)[0m top5: 0.8913246268656716
[2m[36m(func pid=91972)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=91972)[0m f1_macro: 0.2705589505518147
[2m[36m(func pid=91972)[0m f1_weighted: 0.2929367603925389
[2m[36m(func pid=91972)[0m f1_per_class: [0.043, 0.446, 0.289, 0.362, 0.176, 0.359, 0.134, 0.357, 0.147, 0.391]
== Status ==
Current time: 2024-01-07 00:07:32 (running for 00:14:41.63)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.811 |      0.293 |                   81 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.192 |      0.271 |                   60 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  3     |      0.054 |                    6 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.943 |      0.001 |                    6 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7606 | Steps: 2 | Val loss: 2.2895 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=103380)[0m top1: 0.2126865671641791
[2m[36m(func pid=103380)[0m top5: 0.5625
[2m[36m(func pid=103380)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=103380)[0m f1_macro: 0.049688527344403884
[2m[36m(func pid=103380)[0m f1_weighted: 0.11253673211852407
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.011, 0.033, 0.388, 0.0, 0.0, 0.0, 0.034, 0.0, 0.032]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.8948 | Steps: 2 | Val loss: 1.8278 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2147 | Steps: 2 | Val loss: 4.1069 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=103390)[0m top1: 0.020522388059701493
[2m[36m(func pid=103390)[0m top5: 0.5606343283582089
[2m[36m(func pid=103390)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=103390)[0m f1_macro: 0.015654200753148943
[2m[36m(func pid=103390)[0m f1_weighted: 0.017715046456459424
[2m[36m(func pid=103390)[0m f1_per_class: [0.044, 0.097, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=86940)[0m top1: 0.3736007462686567
[2m[36m(func pid=86940)[0m top5: 0.886660447761194
[2m[36m(func pid=86940)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=86940)[0m f1_macro: 0.30616096144805
[2m[36m(func pid=86940)[0m f1_weighted: 0.3973626751402929
[2m[36m(func pid=86940)[0m f1_per_class: [0.272, 0.208, 0.444, 0.526, 0.095, 0.396, 0.445, 0.375, 0.128, 0.172]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8816 | Steps: 2 | Val loss: 2.3346 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 00:07:37 (running for 00:14:46.74)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.895 |      0.306 |                   82 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.215 |      0.293 |                   61 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.928 |      0.05  |                    7 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.761 |      0.016 |                    7 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.37826492537313433
[2m[36m(func pid=91972)[0m top5: 0.9029850746268657
[2m[36m(func pid=91972)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=91972)[0m f1_macro: 0.292610575893517
[2m[36m(func pid=91972)[0m f1_weighted: 0.35375240300909927
[2m[36m(func pid=91972)[0m f1_per_class: [0.083, 0.486, 0.31, 0.467, 0.211, 0.378, 0.212, 0.357, 0.119, 0.303]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.18423507462686567
[2m[36m(func pid=103380)[0m top5: 0.5447761194029851
[2m[36m(func pid=103380)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=103380)[0m f1_macro: 0.044451327546644506
[2m[36m(func pid=103380)[0m f1_weighted: 0.11209152855936683
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.011, 0.031, 0.392, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.8632 | Steps: 2 | Val loss: 1.8095 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8039 | Steps: 2 | Val loss: 2.2616 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.4899 | Steps: 2 | Val loss: 3.9159 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=86940)[0m top1: 0.35074626865671643
[2m[36m(func pid=86940)[0m top5: 0.8871268656716418
[2m[36m(func pid=86940)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=86940)[0m f1_macro: 0.3085896753030337
[2m[36m(func pid=86940)[0m f1_weighted: 0.37809508269877623
[2m[36m(func pid=86940)[0m f1_per_class: [0.275, 0.288, 0.444, 0.48, 0.089, 0.376, 0.377, 0.378, 0.179, 0.198]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8474 | Steps: 2 | Val loss: 2.3375 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=103390)[0m top1: 0.14225746268656717
[2m[36m(func pid=103390)[0m top5: 0.5979477611940298
[2m[36m(func pid=103390)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=103390)[0m f1_macro: 0.09657300849046561
[2m[36m(func pid=103390)[0m f1_weighted: 0.141428860538319
[2m[36m(func pid=103390)[0m f1_per_class: [0.072, 0.14, 0.298, 0.0, 0.0, 0.0, 0.374, 0.0, 0.082, 0.0]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:07:43 (running for 00:14:52.01)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.863 |      0.309 |                   83 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  1.49  |      0.287 |                   62 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.882 |      0.044 |                    8 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.804 |      0.097 |                    8 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.37546641791044777
[2m[36m(func pid=91972)[0m top5: 0.9029850746268657
[2m[36m(func pid=91972)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=91972)[0m f1_macro: 0.2872977976070475
[2m[36m(func pid=91972)[0m f1_weighted: 0.41815535166516965
[2m[36m(func pid=91972)[0m f1_per_class: [0.151, 0.473, 0.297, 0.437, 0.083, 0.296, 0.513, 0.304, 0.066, 0.253]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.12173507462686567
[2m[36m(func pid=103380)[0m top5: 0.5401119402985075
[2m[36m(func pid=103380)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=103380)[0m f1_macro: 0.03722756354897633
[2m[36m(func pid=103380)[0m f1_weighted: 0.09481631399754628
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.02, 0.336, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.8186 | Steps: 2 | Val loss: 1.8582 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6474 | Steps: 2 | Val loss: 2.2278 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3267 | Steps: 2 | Val loss: 6.3252 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8325 | Steps: 2 | Val loss: 2.3422 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=86940)[0m top1: 0.3208955223880597
[2m[36m(func pid=86940)[0m top5: 0.8810634328358209
[2m[36m(func pid=86940)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=86940)[0m f1_macro: 0.3179368925631252
[2m[36m(func pid=86940)[0m f1_weighted: 0.35257974293723493
[2m[36m(func pid=86940)[0m f1_per_class: [0.299, 0.381, 0.462, 0.383, 0.089, 0.368, 0.312, 0.475, 0.155, 0.256]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m top1: 0.21455223880597016
[2m[36m(func pid=103390)[0m top5: 0.5946828358208955
[2m[36m(func pid=103390)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=103390)[0m f1_macro: 0.0813881721088784
[2m[36m(func pid=103390)[0m f1_weighted: 0.16594165997275365
[2m[36m(func pid=103390)[0m f1_per_class: [0.081, 0.079, 0.111, 0.0, 0.0, 0.0, 0.498, 0.0, 0.044, 0.0]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:07:48 (running for 00:14:57.05)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.819 |      0.318 |                   84 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.327 |      0.183 |                   63 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.847 |      0.037 |                    9 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.647 |      0.081 |                    9 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.20708955223880596
[2m[36m(func pid=91972)[0m top5: 0.840018656716418
[2m[36m(func pid=91972)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=91972)[0m f1_macro: 0.18330833735522442
[2m[36m(func pid=91972)[0m f1_weighted: 0.2273454379323752
[2m[36m(func pid=91972)[0m f1_per_class: [0.061, 0.12, 0.259, 0.019, 0.073, 0.186, 0.504, 0.419, 0.0, 0.191]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.05643656716417911
[2m[36m(func pid=103380)[0m top5: 0.5410447761194029
[2m[36m(func pid=103380)[0m f1_micro: 0.05643656716417911
[2m[36m(func pid=103380)[0m f1_macro: 0.022581291381359288
[2m[36m(func pid=103380)[0m f1_weighted: 0.05886829135264203
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.015, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.2973 | Steps: 2 | Val loss: 1.9093 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7129 | Steps: 2 | Val loss: 2.2008 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4843 | Steps: 2 | Val loss: 4.2920 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8085 | Steps: 2 | Val loss: 2.3463 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=86940)[0m top1: 0.29990671641791045
[2m[36m(func pid=86940)[0m top5: 0.8805970149253731
[2m[36m(func pid=86940)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=86940)[0m f1_macro: 0.2852186657773985
[2m[36m(func pid=86940)[0m f1_weighted: 0.3171467363671646
[2m[36m(func pid=86940)[0m f1_per_class: [0.304, 0.351, 0.297, 0.411, 0.07, 0.349, 0.194, 0.473, 0.168, 0.233]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m top1: 0.2537313432835821
[2m[36m(func pid=103390)[0m top5: 0.6072761194029851
[2m[36m(func pid=103390)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=103390)[0m f1_macro: 0.07217437649036046
[2m[36m(func pid=103390)[0m f1_weighted: 0.15589454611632825
[2m[36m(func pid=103390)[0m f1_per_class: [0.099, 0.015, 0.0, 0.0, 0.074, 0.0, 0.502, 0.0, 0.032, 0.0]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:07:53 (running for 00:15:02.24)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.297 |      0.285 |                   85 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.484 |      0.288 |                   64 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.832 |      0.023 |                   10 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.713 |      0.072 |                   10 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.35261194029850745
[2m[36m(func pid=91972)[0m top5: 0.8810634328358209
[2m[36m(func pid=91972)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=91972)[0m f1_macro: 0.28837552172771436
[2m[36m(func pid=91972)[0m f1_weighted: 0.38864890447856737
[2m[36m(func pid=91972)[0m f1_per_class: [0.14, 0.498, 0.183, 0.308, 0.152, 0.397, 0.451, 0.464, 0.095, 0.197]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.02332089552238806
[2m[36m(func pid=103380)[0m top5: 0.5471082089552238
[2m[36m(func pid=103380)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=103380)[0m f1_macro: 0.010934031596266575
[2m[36m(func pid=103380)[0m f1_weighted: 0.026919455351662586
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.013, 0.096, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.7435 | Steps: 2 | Val loss: 2.0579 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7672 | Steps: 2 | Val loss: 2.1778 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4430 | Steps: 2 | Val loss: 4.4082 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7691 | Steps: 2 | Val loss: 2.3521 | Batch size: 32 | lr: 0.0001 | Duration: 2.60s
[2m[36m(func pid=86940)[0m top1: 0.3125
[2m[36m(func pid=86940)[0m top5: 0.8852611940298507
[2m[36m(func pid=86940)[0m f1_micro: 0.3125
[2m[36m(func pid=86940)[0m f1_macro: 0.2675764295298248
[2m[36m(func pid=86940)[0m f1_weighted: 0.35099587469026355
[2m[36m(func pid=86940)[0m f1_per_class: [0.256, 0.344, 0.119, 0.45, 0.058, 0.323, 0.296, 0.466, 0.153, 0.209]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m top1: 0.27238805970149255
[2m[36m(func pid=103390)[0m top5: 0.6259328358208955
[2m[36m(func pid=103390)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=103390)[0m f1_macro: 0.07498032101056908
[2m[36m(func pid=103390)[0m f1_weighted: 0.1599911801047177
[2m[36m(func pid=103390)[0m f1_per_class: [0.123, 0.021, 0.0, 0.0, 0.036, 0.0, 0.509, 0.0, 0.061, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m top1: 0.3628731343283582
[2m[36m(func pid=91972)[0m top5: 0.9109141791044776
[2m[36m(func pid=91972)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=91972)[0m f1_macro: 0.2544118498822582
[2m[36m(func pid=91972)[0m f1_weighted: 0.31750368305973425
[2m[36m(func pid=91972)[0m f1_per_class: [0.083, 0.482, 0.237, 0.068, 0.0, 0.393, 0.444, 0.466, 0.18, 0.19]
== Status ==
Current time: 2024-01-07 00:07:58 (running for 00:15:07.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.744 |      0.268 |                   86 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.443 |      0.254 |                   65 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.808 |      0.011 |                   11 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.767 |      0.075 |                   11 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.010727611940298507
[2m[36m(func pid=103380)[0m top5: 0.5489738805970149
[2m[36m(func pid=103380)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=103380)[0m f1_macro: 0.004316231999847819
[2m[36m(func pid=103380)[0m f1_weighted: 0.008670170306374876
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.031, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.9993 | Steps: 2 | Val loss: 2.0857 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6964 | Steps: 2 | Val loss: 2.1503 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3131 | Steps: 2 | Val loss: 3.9698 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7890 | Steps: 2 | Val loss: 2.3580 | Batch size: 32 | lr: 0.0001 | Duration: 2.58s
[2m[36m(func pid=86940)[0m top1: 0.3591417910447761
[2m[36m(func pid=86940)[0m top5: 0.9039179104477612
[2m[36m(func pid=86940)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=86940)[0m f1_macro: 0.2796786805572459
[2m[36m(func pid=86940)[0m f1_weighted: 0.4087413578879137
[2m[36m(func pid=86940)[0m f1_per_class: [0.192, 0.467, 0.105, 0.449, 0.067, 0.304, 0.439, 0.458, 0.107, 0.209]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m top1: 0.27425373134328357
[2m[36m(func pid=103390)[0m top5: 0.6492537313432836
[2m[36m(func pid=103390)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=103390)[0m f1_macro: 0.09683265181021669
[2m[36m(func pid=103390)[0m f1_weighted: 0.19227334678862118
[2m[36m(func pid=103390)[0m f1_per_class: [0.147, 0.141, 0.0, 0.0, 0.054, 0.016, 0.538, 0.0, 0.072, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m top1: 0.386660447761194
[2m[36m(func pid=91972)[0m top5: 0.9244402985074627
[2m[36m(func pid=91972)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=91972)[0m f1_macro: 0.2750246138312046
[2m[36m(func pid=91972)[0m f1_weighted: 0.3662012125571059
[2m[36m(func pid=91972)[0m f1_per_class: [0.044, 0.491, 0.323, 0.196, 0.079, 0.239, 0.545, 0.432, 0.184, 0.216]
[2m[36m(func pid=91972)[0m 
== Status ==
Current time: 2024-01-07 00:08:03 (running for 00:15:12.49)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.999 |      0.28  |                   87 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.313 |      0.275 |                   66 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.769 |      0.004 |                   12 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.696 |      0.097 |                   12 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.006063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5438432835820896
[2m[36m(func pid=103380)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.0012115563839701772
[2m[36m(func pid=103380)[0m f1_weighted: 7.346190761013201e-05
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.3040 | Steps: 2 | Val loss: 2.0399 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4829 | Steps: 2 | Val loss: 2.1193 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4862 | Steps: 2 | Val loss: 3.3340 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7432 | Steps: 2 | Val loss: 2.3637 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86940)[0m top1: 0.3591417910447761
[2m[36m(func pid=86940)[0m top5: 0.9020522388059702
[2m[36m(func pid=86940)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=86940)[0m f1_macro: 0.2684965166479629
[2m[36m(func pid=86940)[0m f1_weighted: 0.38002190664823804
[2m[36m(func pid=86940)[0m f1_per_class: [0.105, 0.553, 0.161, 0.298, 0.141, 0.367, 0.43, 0.414, 0.047, 0.169]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m top1: 0.21175373134328357
[2m[36m(func pid=103390)[0m top5: 0.7210820895522388
[2m[36m(func pid=103390)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=103390)[0m f1_macro: 0.10223519992540957
[2m[36m(func pid=103390)[0m f1_weighted: 0.19828705531255855
[2m[36m(func pid=103390)[0m f1_per_class: [0.184, 0.249, 0.0, 0.0, 0.033, 0.008, 0.5, 0.0, 0.049, 0.0]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:08:08 (running for 00:15:17.61)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.304 |      0.268 |                   88 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.313 |      0.275 |                   66 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.743 |      0.001 |                   14 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.483 |      0.102 |                   13 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.39972014925373134
[2m[36m(func pid=91972)[0m top5: 0.9225746268656716
[2m[36m(func pid=91972)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=91972)[0m f1_macro: 0.2832174819819279
[2m[36m(func pid=91972)[0m f1_weighted: 0.4089543159822148
[2m[36m(func pid=91972)[0m f1_per_class: [0.345, 0.279, 0.357, 0.528, 0.069, 0.219, 0.54, 0.218, 0.097, 0.18]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103380)[0m top1: 0.006063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5373134328358209
[2m[36m(func pid=103380)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=103380)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.9395 | Steps: 2 | Val loss: 2.0919 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4671 | Steps: 2 | Val loss: 2.0992 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7387 | Steps: 2 | Val loss: 2.3684 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1134 | Steps: 2 | Val loss: 3.6097 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=86940)[0m top1: 0.32975746268656714
[2m[36m(func pid=86940)[0m top5: 0.8889925373134329
[2m[36m(func pid=86940)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=86940)[0m f1_macro: 0.27640850445447146
[2m[36m(func pid=86940)[0m f1_weighted: 0.32765953028400235
[2m[36m(func pid=86940)[0m f1_per_class: [0.123, 0.551, 0.444, 0.182, 0.151, 0.414, 0.346, 0.396, 0.024, 0.133]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m top1: 0.10494402985074627
[2m[36m(func pid=103390)[0m top5: 0.8334888059701493
[2m[36m(func pid=103390)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=103390)[0m f1_macro: 0.04830321960586191
[2m[36m(func pid=103390)[0m f1_weighted: 0.10836099432654776
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.214, 0.0, 0.0, 0.025, 0.008, 0.236, 0.0, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m top1: 0.006063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5237873134328358
[2m[36m(func pid=103380)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=103380)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 00:08:13 (running for 00:15:22.69)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.939 |      0.276 |                   89 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.486 |      0.283 |                   67 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.739 |      0.001 |                   15 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.467 |      0.048 |                   14 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=91972)[0m top1: 0.394589552238806
[2m[36m(func pid=91972)[0m top5: 0.8731343283582089
[2m[36m(func pid=91972)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=91972)[0m f1_macro: 0.2571791759064717
[2m[36m(func pid=91972)[0m f1_weighted: 0.37702865895526577
[2m[36m(func pid=91972)[0m f1_per_class: [0.352, 0.032, 0.356, 0.514, 0.118, 0.378, 0.553, 0.134, 0.024, 0.111]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.8887 | Steps: 2 | Val loss: 2.0172 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4674 | Steps: 2 | Val loss: 2.0919 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8035 | Steps: 2 | Val loss: 2.3757 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4115 | Steps: 2 | Val loss: 3.5701 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=86940)[0m top1: 0.34281716417910446
[2m[36m(func pid=86940)[0m top5: 0.8656716417910447
[2m[36m(func pid=86940)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=86940)[0m f1_macro: 0.2958209328059041
[2m[36m(func pid=86940)[0m f1_weighted: 0.3279213633977233
[2m[36m(func pid=86940)[0m f1_per_class: [0.163, 0.552, 0.4, 0.303, 0.19, 0.423, 0.201, 0.48, 0.107, 0.14]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m top1: 0.06902985074626866
[2m[36m(func pid=103390)[0m top5: 0.871268656716418
[2m[36m(func pid=103390)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=103390)[0m f1_macro: 0.03263054077849421
[2m[36m(func pid=103390)[0m f1_weighted: 0.060919773173500884
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.204, 0.0, 0.007, 0.022, 0.022, 0.071, 0.0, 0.0, 0.0]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:08:18 (running for 00:15:27.83)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.889 |      0.296 |                   90 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.113 |      0.257 |                   68 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.804 |      0.001 |                   16 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.467 |      0.033 |                   15 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.006063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5158582089552238
[2m[36m(func pid=103380)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=103380)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=91972)[0m top1: 0.3591417910447761
[2m[36m(func pid=91972)[0m top5: 0.8600746268656716
[2m[36m(func pid=91972)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=91972)[0m f1_macro: 0.30287060402160404
[2m[36m(func pid=91972)[0m f1_weighted: 0.3704195955906697
[2m[36m(func pid=91972)[0m f1_per_class: [0.253, 0.09, 0.438, 0.436, 0.184, 0.452, 0.469, 0.481, 0.098, 0.127]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.0690 | Steps: 2 | Val loss: 1.9030 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3594 | Steps: 2 | Val loss: 2.1047 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7425 | Steps: 2 | Val loss: 2.3793 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2117 | Steps: 2 | Val loss: 3.9859 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=86940)[0m top1: 0.3530783582089552
[2m[36m(func pid=86940)[0m top5: 0.8619402985074627
[2m[36m(func pid=86940)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=86940)[0m f1_macro: 0.33058374939046253
[2m[36m(func pid=86940)[0m f1_weighted: 0.3368356728147129
[2m[36m(func pid=86940)[0m f1_per_class: [0.36, 0.483, 0.367, 0.495, 0.186, 0.426, 0.06, 0.52, 0.148, 0.262]
[2m[36m(func pid=86940)[0m 
== Status ==
Current time: 2024-01-07 00:08:24 (running for 00:15:32.98)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.069 |      0.331 |                   91 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.411 |      0.303 |                   69 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.742 |      0.001 |                   17 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.467 |      0.033 |                   15 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.006063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5130597014925373
[2m[36m(func pid=103380)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=103380)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.08395522388059702
[2m[36m(func pid=103390)[0m top5: 0.7527985074626866
[2m[36m(func pid=103390)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=103390)[0m f1_macro: 0.10892419150270134
[2m[36m(func pid=103390)[0m f1_weighted: 0.09029755573071219
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.201, 0.308, 0.1, 0.022, 0.026, 0.003, 0.362, 0.0, 0.067]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m top1: 0.28591417910447764
[2m[36m(func pid=91972)[0m top5: 0.8656716417910447
[2m[36m(func pid=91972)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=91972)[0m f1_macro: 0.29283404023966636
[2m[36m(func pid=91972)[0m f1_weighted: 0.28468250334364104
[2m[36m(func pid=91972)[0m f1_per_class: [0.23, 0.473, 0.414, 0.154, 0.128, 0.388, 0.258, 0.391, 0.115, 0.377]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.8734 | Steps: 2 | Val loss: 2.0055 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7543 | Steps: 2 | Val loss: 2.3824 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3113 | Steps: 2 | Val loss: 2.1376 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.8105 | Steps: 2 | Val loss: 3.8153 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=86940)[0m top1: 0.2957089552238806
[2m[36m(func pid=86940)[0m top5: 0.8777985074626866
[2m[36m(func pid=86940)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=86940)[0m f1_macro: 0.24032831190919976
[2m[36m(func pid=86940)[0m f1_weighted: 0.2861673891728622
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.207, 0.182, 0.493, 0.104, 0.388, 0.098, 0.511, 0.147, 0.273]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m top1: 0.006063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5144589552238806
[2m[36m(func pid=103380)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=103380)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:08:29 (running for 00:15:38.26)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.873 |      0.24  |                   92 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  1.212 |      0.293 |                   70 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.754 |      0.001 |                   18 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.311 |      0.12  |                   17 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m top1: 0.1287313432835821
[2m[36m(func pid=103390)[0m top5: 0.5755597014925373
[2m[36m(func pid=103390)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=103390)[0m f1_macro: 0.1200283854760138
[2m[36m(func pid=103390)[0m f1_weighted: 0.131786702477652
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.164, 0.316, 0.284, 0.027, 0.029, 0.0, 0.312, 0.0, 0.068]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=91972)[0m top1: 0.322294776119403
[2m[36m(func pid=91972)[0m top5: 0.8978544776119403
[2m[36m(func pid=91972)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=91972)[0m f1_macro: 0.30554843436669493
[2m[36m(func pid=91972)[0m f1_weighted: 0.36271576123967153
[2m[36m(func pid=91972)[0m f1_per_class: [0.317, 0.375, 0.361, 0.256, 0.076, 0.306, 0.498, 0.457, 0.134, 0.275]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.5113 | Steps: 2 | Val loss: 1.9898 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7565 | Steps: 2 | Val loss: 2.3844 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1000 | Steps: 2 | Val loss: 4.0410 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2693 | Steps: 2 | Val loss: 2.1800 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=86940)[0m top1: 0.33348880597014924
[2m[36m(func pid=86940)[0m top5: 0.8819962686567164
[2m[36m(func pid=86940)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=86940)[0m f1_macro: 0.24868511525872986
[2m[36m(func pid=86940)[0m f1_weighted: 0.3483128126610646
[2m[36m(func pid=86940)[0m f1_per_class: [0.0, 0.203, 0.145, 0.506, 0.08, 0.37, 0.311, 0.457, 0.207, 0.208]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m top1: 0.006063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5153917910447762
[2m[36m(func pid=103380)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=103380)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:08:34 (running for 00:15:43.53)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.511 |      0.249 |                   93 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.1   |      0.241 |                   72 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.756 |      0.001 |                   19 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.311 |      0.12  |                   17 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.386660447761194
[2m[36m(func pid=91972)[0m top5: 0.9095149253731343
[2m[36m(func pid=91972)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=91972)[0m f1_macro: 0.24111286627778136
[2m[36m(func pid=91972)[0m f1_weighted: 0.3764850377117484
[2m[36m(func pid=91972)[0m f1_per_class: [0.2, 0.131, 0.237, 0.56, 0.075, 0.335, 0.48, 0.078, 0.108, 0.208]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103390)[0m top1: 0.12686567164179105
[2m[36m(func pid=103390)[0m top5: 0.534981343283582
[2m[36m(func pid=103390)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=103390)[0m f1_macro: 0.09824095512416517
[2m[36m(func pid=103390)[0m f1_weighted: 0.10679103061046694
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.058, 0.321, 0.294, 0.064, 0.0, 0.0, 0.205, 0.0, 0.04]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.5451 | Steps: 2 | Val loss: 1.8621 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8128 | Steps: 2 | Val loss: 2.3899 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6706 | Steps: 2 | Val loss: 4.1317 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3943 | Steps: 2 | Val loss: 2.2264 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=86940)[0m top1: 0.35447761194029853
[2m[36m(func pid=86940)[0m top5: 0.8917910447761194
[2m[36m(func pid=86940)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=86940)[0m f1_macro: 0.27955287843454973
[2m[36m(func pid=86940)[0m f1_weighted: 0.39275749087106243
[2m[36m(func pid=86940)[0m f1_per_class: [0.247, 0.311, 0.224, 0.445, 0.065, 0.342, 0.476, 0.351, 0.14, 0.194]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m top1: 0.0065298507462686565
[2m[36m(func pid=103380)[0m top5: 0.5163246268656716
[2m[36m(func pid=103380)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=103380)[0m f1_macro: 0.0017464774607631752
[2m[36m(func pid=103380)[0m f1_weighted: 0.0010034358182012767
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:08:39 (running for 00:15:48.84)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  1.545 |      0.28  |                   94 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.671 |      0.252 |                   73 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.813 |      0.002 |                   20 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.269 |      0.098 |                   18 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.3572761194029851
[2m[36m(func pid=91972)[0m top5: 0.9053171641791045
[2m[36m(func pid=91972)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=91972)[0m f1_macro: 0.2517228745792804
[2m[36m(func pid=91972)[0m f1_weighted: 0.3766209779372636
[2m[36m(func pid=91972)[0m f1_per_class: [0.216, 0.286, 0.206, 0.419, 0.074, 0.343, 0.522, 0.032, 0.14, 0.281]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103390)[0m top1: 0.11986940298507463
[2m[36m(func pid=103390)[0m top5: 0.5214552238805971
[2m[36m(func pid=103390)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=103390)[0m f1_macro: 0.10059548687834648
[2m[36m(func pid=103390)[0m f1_weighted: 0.0918858660311889
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.011, 0.344, 0.272, 0.164, 0.0, 0.0, 0.18, 0.0, 0.035]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.7629 | Steps: 2 | Val loss: 1.8594 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7779 | Steps: 2 | Val loss: 2.3905 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1005 | Steps: 2 | Val loss: 6.7268 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3371 | Steps: 2 | Val loss: 2.2772 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=86940)[0m top1: 0.3362873134328358
[2m[36m(func pid=86940)[0m top5: 0.9006529850746269
[2m[36m(func pid=86940)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=86940)[0m f1_macro: 0.2853705916979911
[2m[36m(func pid=86940)[0m f1_weighted: 0.3700015353346586
[2m[36m(func pid=86940)[0m f1_per_class: [0.251, 0.295, 0.483, 0.346, 0.055, 0.357, 0.521, 0.211, 0.103, 0.233]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m top1: 0.0065298507462686565
[2m[36m(func pid=103380)[0m top5: 0.5303171641791045
[2m[36m(func pid=103380)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=103380)[0m f1_macro: 0.0017549263415680978
[2m[36m(func pid=103380)[0m f1_weighted: 0.0010039481104142617
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:08:45 (running for 00:15:53.92)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.252
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.763 |      0.285 |                   95 |
| train_51d3e_00007 | RUNNING    | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.101 |      0.255 |                   74 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.778 |      0.002 |                   21 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.394 |      0.101 |                   19 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=91972)[0m top1: 0.30783582089552236
[2m[36m(func pid=91972)[0m top5: 0.7042910447761194
[2m[36m(func pid=91972)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=91972)[0m f1_macro: 0.2545191818398765
[2m[36m(func pid=91972)[0m f1_weighted: 0.3203738859628022
[2m[36m(func pid=91972)[0m f1_per_class: [0.064, 0.537, 0.208, 0.0, 0.1, 0.353, 0.53, 0.347, 0.067, 0.339]
[2m[36m(func pid=91972)[0m 
[2m[36m(func pid=103390)[0m top1: 0.10634328358208955
[2m[36m(func pid=103390)[0m top5: 0.5191231343283582
[2m[36m(func pid=103390)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=103390)[0m f1_macro: 0.08454933111948779
[2m[36m(func pid=103390)[0m f1_weighted: 0.07578178980538151
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.0, 0.25, 0.226, 0.161, 0.0, 0.0, 0.166, 0.0, 0.042]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.8366 | Steps: 2 | Val loss: 1.9178 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7635 | Steps: 2 | Val loss: 2.3889 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=91972)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8175 | Steps: 2 | Val loss: 6.7750 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4150 | Steps: 2 | Val loss: 2.3140 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=86940)[0m top1: 0.3064365671641791
[2m[36m(func pid=86940)[0m top5: 0.9081156716417911
[2m[36m(func pid=86940)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=86940)[0m f1_macro: 0.2704621634233802
[2m[36m(func pid=86940)[0m f1_weighted: 0.3468403315503961
[2m[36m(func pid=86940)[0m f1_per_class: [0.194, 0.262, 0.476, 0.355, 0.058, 0.337, 0.464, 0.199, 0.142, 0.219]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103380)[0m top1: 0.00792910447761194
[2m[36m(func pid=103380)[0m top5: 0.5527052238805971
[2m[36m(func pid=103380)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=103380)[0m f1_macro: 0.003375859286421998
[2m[36m(func pid=103380)[0m f1_weighted: 0.003737560537472595
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.021, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=91972)[0m top1: 0.25466417910447764
[2m[36m(func pid=91972)[0m top5: 0.7000932835820896
[2m[36m(func pid=91972)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=91972)[0m f1_macro: 0.19964843824262418
[2m[36m(func pid=91972)[0m f1_weighted: 0.206349533555926
[2m[36m(func pid=91972)[0m f1_per_class: [0.077, 0.53, 0.2, 0.0, 0.074, 0.346, 0.159, 0.363, 0.048, 0.2]
== Status ==
Current time: 2024-01-07 00:08:50 (running for 00:15:59.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2515
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.837 |      0.27  |                   96 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.764 |      0.003 |                   22 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.337 |      0.085 |                   20 |
| train_51d3e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m top1: 0.10494402985074627
[2m[36m(func pid=103390)[0m top5: 0.5205223880597015
[2m[36m(func pid=103390)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=103390)[0m f1_macro: 0.08273883211970648
[2m[36m(func pid=103390)[0m f1_weighted: 0.07515020763756906
[2m[36m(func pid=103390)[0m f1_per_class: [0.0, 0.0, 0.224, 0.223, 0.17, 0.0, 0.0, 0.17, 0.0, 0.04]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.8663 | Steps: 2 | Val loss: 1.8584 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7873 | Steps: 2 | Val loss: 2.3858 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=86940)[0m top1: 0.32136194029850745
[2m[36m(func pid=86940)[0m top5: 0.9067164179104478
[2m[36m(func pid=86940)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=86940)[0m f1_macro: 0.2806954875859319
[2m[36m(func pid=86940)[0m f1_weighted: 0.3506470315932225
[2m[36m(func pid=86940)[0m f1_per_class: [0.238, 0.23, 0.444, 0.459, 0.071, 0.357, 0.37, 0.309, 0.11, 0.219]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4258 | Steps: 2 | Val loss: 2.3257 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=103380)[0m top1: 0.012126865671641791
[2m[36m(func pid=103380)[0m top5: 0.5671641791044776
[2m[36m(func pid=103380)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=103380)[0m f1_macro: 0.008514456649904686
[2m[36m(func pid=103380)[0m f1_weighted: 0.009951895220950833
[2m[36m(func pid=103380)[0m f1_per_class: [0.016, 0.055, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.9667 | Steps: 2 | Val loss: 1.8741 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=103390)[0m top1: 0.10027985074626866
[2m[36m(func pid=103390)[0m top5: 0.5228544776119403
[2m[36m(func pid=103390)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=103390)[0m f1_macro: 0.09871433668090704
[2m[36m(func pid=103390)[0m f1_weighted: 0.06778806874357957
[2m[36m(func pid=103390)[0m f1_per_class: [0.14, 0.0, 0.247, 0.183, 0.2, 0.0, 0.0, 0.179, 0.0, 0.037]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7698 | Steps: 2 | Val loss: 2.3787 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=86940)[0m top1: 0.3302238805970149
[2m[36m(func pid=86940)[0m top5: 0.8964552238805971
[2m[36m(func pid=86940)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=86940)[0m f1_macro: 0.2922956243256999
[2m[36m(func pid=86940)[0m f1_weighted: 0.33598209739089824
[2m[36m(func pid=86940)[0m f1_per_class: [0.275, 0.244, 0.373, 0.497, 0.086, 0.361, 0.238, 0.481, 0.123, 0.246]
== Status ==
Current time: 2024-01-07 00:08:56 (running for 00:16:05.35)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.866 |      0.281 |                   97 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.787 |      0.009 |                   23 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.426 |      0.099 |                   22 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=109192)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=109192)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=109192)[0m Configuration completed!
[2m[36m(func pid=109192)[0m New optimizer parameters:
[2m[36m(func pid=109192)[0m SGD (
[2m[36m(func pid=109192)[0m Parameter Group 0
[2m[36m(func pid=109192)[0m     dampening: 0
[2m[36m(func pid=109192)[0m     differentiable: False
[2m[36m(func pid=109192)[0m     foreach: None
[2m[36m(func pid=109192)[0m     lr: 0.01
[2m[36m(func pid=109192)[0m     maximize: False
[2m[36m(func pid=109192)[0m     momentum: 0.99
[2m[36m(func pid=109192)[0m     nesterov: False
[2m[36m(func pid=109192)[0m     weight_decay: 0.0001
[2m[36m(func pid=109192)[0m )
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2654 | Steps: 2 | Val loss: 2.3255 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=103380)[0m top1: 0.025186567164179104
[2m[36m(func pid=103380)[0m top5: 0.5629664179104478
[2m[36m(func pid=103380)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=103380)[0m f1_macro: 0.01848888152401667
[2m[36m(func pid=103380)[0m f1_weighted: 0.029346137898262698
[2m[36m(func pid=103380)[0m f1_per_class: [0.012, 0.097, 0.019, 0.0, 0.0, 0.0, 0.039, 0.0, 0.017, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.8000 | Steps: 2 | Val loss: 1.9363 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:09:01 (running for 00:16:10.88)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.967 |      0.292 |                   98 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.77  |      0.018 |                   24 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.265 |      0.108 |                   23 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m top1: 0.08908582089552239
[2m[36m(func pid=103390)[0m top5: 0.5457089552238806
[2m[36m(func pid=103390)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=103390)[0m f1_macro: 0.10802634778353773
[2m[36m(func pid=103390)[0m f1_weighted: 0.043815483691143554
[2m[36m(func pid=103390)[0m f1_per_class: [0.163, 0.0, 0.37, 0.087, 0.219, 0.0, 0.0, 0.206, 0.0, 0.035]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7469 | Steps: 2 | Val loss: 2.3677 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0308 | Steps: 2 | Val loss: 2.4052 | Batch size: 32 | lr: 0.01 | Duration: 4.27s
[2m[36m(func pid=86940)[0m top1: 0.33908582089552236
[2m[36m(func pid=86940)[0m top5: 0.878731343283582
[2m[36m(func pid=86940)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=86940)[0m f1_macro: 0.27410832393651957
[2m[36m(func pid=86940)[0m f1_weighted: 0.32094685278903207
[2m[36m(func pid=86940)[0m f1_per_class: [0.208, 0.334, 0.193, 0.525, 0.099, 0.371, 0.109, 0.478, 0.168, 0.257]
[2m[36m(func pid=86940)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1630 | Steps: 2 | Val loss: 2.3211 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=103380)[0m top1: 0.04757462686567164
[2m[36m(func pid=103380)[0m top5: 0.5564365671641791
[2m[36m(func pid=103380)[0m f1_micro: 0.04757462686567164
[2m[36m(func pid=103380)[0m f1_macro: 0.033034708446115314
[2m[36m(func pid=103380)[0m f1_weighted: 0.05836153982261166
[2m[36m(func pid=103380)[0m f1_per_class: [0.013, 0.17, 0.035, 0.0, 0.0, 0.0, 0.094, 0.0, 0.018, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.006063432835820896
[2m[36m(func pid=109192)[0m top5: 0.3931902985074627
[2m[36m(func pid=109192)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=109192)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=109192)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=86940)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.7064 | Steps: 2 | Val loss: 1.9878 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 00:09:07 (running for 00:16:16.27)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00006 | RUNNING    | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.8   |      0.274 |                   99 |
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.747 |      0.033 |                   25 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.163 |      0.105 |                   24 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  3.031 |      0.001 |                    1 |
| train_51d3e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m top1: 0.08208955223880597
[2m[36m(func pid=103390)[0m top5: 0.5984141791044776
[2m[36m(func pid=103390)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=103390)[0m f1_macro: 0.10463135296481829
[2m[36m(func pid=103390)[0m f1_weighted: 0.03067679199673808
[2m[36m(func pid=103390)[0m f1_per_class: [0.136, 0.0, 0.378, 0.03, 0.202, 0.0, 0.0, 0.267, 0.0, 0.033]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6609 | Steps: 2 | Val loss: 2.3503 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7655 | Steps: 2 | Val loss: 2.3161 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=86940)[0m top1: 0.34794776119402987
[2m[36m(func pid=86940)[0m top5: 0.8857276119402985
[2m[36m(func pid=86940)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=86940)[0m f1_macro: 0.27374426930154677
[2m[36m(func pid=86940)[0m f1_weighted: 0.3552495522700841
[2m[36m(func pid=86940)[0m f1_per_class: [0.162, 0.399, 0.148, 0.506, 0.097, 0.357, 0.219, 0.467, 0.143, 0.239]
[2m[36m(func pid=103380)[0m top1: 0.06063432835820896
[2m[36m(func pid=103380)[0m top5: 0.5760261194029851
[2m[36m(func pid=103380)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=103380)[0m f1_macro: 0.04201174439902939
[2m[36m(func pid=103380)[0m f1_weighted: 0.07789856826123827
[2m[36m(func pid=103380)[0m f1_per_class: [0.014, 0.165, 0.058, 0.0, 0.0, 0.0, 0.161, 0.0, 0.021, 0.0]
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3692 | Steps: 2 | Val loss: 2.3239 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.013059701492537313
[2m[36m(func pid=109192)[0m top5: 0.5685634328358209
[2m[36m(func pid=109192)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=109192)[0m f1_macro: 0.008673100226881779
[2m[36m(func pid=109192)[0m f1_weighted: 0.012705211038781458
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.073, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.07789179104477612
[2m[36m(func pid=103390)[0m top5: 0.648320895522388
[2m[36m(func pid=103390)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=103390)[0m f1_macro: 0.10614026536027213
[2m[36m(func pid=103390)[0m f1_weighted: 0.028036205332470414
[2m[36m(func pid=103390)[0m f1_per_class: [0.111, 0.0, 0.4, 0.003, 0.154, 0.0, 0.0, 0.361, 0.0, 0.032]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8751 | Steps: 2 | Val loss: 2.2431 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6616 | Steps: 2 | Val loss: 2.3360 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=109192)[0m top1: 0.08908582089552239
[2m[36m(func pid=109192)[0m top5: 0.715018656716418
[2m[36m(func pid=109192)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=109192)[0m f1_macro: 0.04274150257011909
[2m[36m(func pid=109192)[0m f1_weighted: 0.09685091281020725
[2m[36m(func pid=109192)[0m f1_per_class: [0.05, 0.132, 0.0, 0.0, 0.0, 0.0, 0.245, 0.0, 0.0, 0.0]
[2m[36m(func pid=103380)[0m top1: 0.06809701492537314
[2m[36m(func pid=103380)[0m top5: 0.6152052238805971
[2m[36m(func pid=103380)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=103380)[0m f1_macro: 0.047446149422491986
[2m[36m(func pid=103380)[0m f1_weighted: 0.08809028524285174
[2m[36m(func pid=103380)[0m f1_per_class: [0.017, 0.174, 0.075, 0.0, 0.0, 0.0, 0.19, 0.0, 0.019, 0.0]
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1820 | Steps: 2 | Val loss: 2.3118 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 00:09:12 (running for 00:16:21.72)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.661 |      0.042 |                   26 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.369 |      0.106 |                   25 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.765 |      0.009 |                    2 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=110192)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=110192)[0m Configuration completed!
[2m[36m(func pid=110192)[0m New optimizer parameters:
[2m[36m(func pid=110192)[0m SGD (
[2m[36m(func pid=110192)[0m Parameter Group 0
[2m[36m(func pid=110192)[0m     dampening: 0
[2m[36m(func pid=110192)[0m     differentiable: False
[2m[36m(func pid=110192)[0m     foreach: None
[2m[36m(func pid=110192)[0m     lr: 0.1
[2m[36m(func pid=110192)[0m     maximize: False
[2m[36m(func pid=110192)[0m     momentum: 0.99
[2m[36m(func pid=110192)[0m     nesterov: False
[2m[36m(func pid=110192)[0m     weight_decay: 0.0001
[2m[36m(func pid=110192)[0m )
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:09:18 (running for 00:16:27.10)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.661 |      0.042 |                   26 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.369 |      0.106 |                   25 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.875 |      0.043 |                    3 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.08442164179104478
[2m[36m(func pid=103390)[0m top5: 0.6352611940298507
[2m[36m(func pid=103390)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=103390)[0m f1_macro: 0.11744168683120089
[2m[36m(func pid=103390)[0m f1_weighted: 0.050313011888682826
[2m[36m(func pid=103390)[0m f1_per_class: [0.096, 0.021, 0.357, 0.013, 0.082, 0.109, 0.0, 0.41, 0.048, 0.038]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6491 | Steps: 2 | Val loss: 2.3185 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.5688 | Steps: 2 | Val loss: 2.1775 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.1091 | Steps: 2 | Val loss: 2.2778 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.6821 | Steps: 2 | Val loss: 2.5980 | Batch size: 32 | lr: 0.1 | Duration: 4.62s
== Status ==
Current time: 2024-01-07 00:09:23 (running for 00:16:32.23)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.662 |      0.047 |                   27 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.182 |      0.117 |                   26 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.875 |      0.043 |                    3 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.17957089552238806
[2m[36m(func pid=109192)[0m top5: 0.7518656716417911
[2m[36m(func pid=109192)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=109192)[0m f1_macro: 0.07695929064488691
[2m[36m(func pid=109192)[0m f1_weighted: 0.1526584719440709
[2m[36m(func pid=109192)[0m f1_per_class: [0.135, 0.0, 0.0, 0.02, 0.037, 0.024, 0.469, 0.0, 0.035, 0.05]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.07742537313432836
[2m[36m(func pid=103380)[0m top5: 0.6389925373134329
[2m[36m(func pid=103380)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=103380)[0m f1_macro: 0.05526656352591612
[2m[36m(func pid=103380)[0m f1_weighted: 0.09753217810898367
[2m[36m(func pid=103380)[0m f1_per_class: [0.023, 0.181, 0.114, 0.0, 0.0, 0.0, 0.217, 0.0, 0.018, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.11333955223880597
[2m[36m(func pid=103390)[0m top5: 0.6996268656716418
[2m[36m(func pid=103390)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=103390)[0m f1_macro: 0.1537407713062245
[2m[36m(func pid=103390)[0m f1_weighted: 0.08704661012251849
[2m[36m(func pid=103390)[0m f1_per_class: [0.105, 0.123, 0.357, 0.0, 0.057, 0.254, 0.0, 0.498, 0.078, 0.066]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.02332089552238806
[2m[36m(func pid=110192)[0m top5: 0.601679104477612
[2m[36m(func pid=110192)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=110192)[0m f1_macro: 0.005836351589826227
[2m[36m(func pid=110192)[0m f1_weighted: 0.005545309479752246
[2m[36m(func pid=110192)[0m f1_per_class: [0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.4373 | Steps: 2 | Val loss: 2.1790 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6297 | Steps: 2 | Val loss: 2.2980 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0230 | Steps: 2 | Val loss: 2.2215 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 4.9981 | Steps: 2 | Val loss: 2.2875 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:09:28 (running for 00:16:37.54)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.649 |      0.055 |                   28 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.109 |      0.154 |                   27 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.437 |      0.05  |                    5 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  3.682 |      0.006 |                    1 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.042444029850746266
[2m[36m(func pid=109192)[0m top5: 0.7220149253731343
[2m[36m(func pid=109192)[0m f1_micro: 0.042444029850746266
[2m[36m(func pid=109192)[0m f1_macro: 0.05042308358609278
[2m[36m(func pid=109192)[0m f1_weighted: 0.04848247390607339
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.016, 0.24, 0.148, 0.032, 0.0, 0.0, 0.041, 0.0, 0.027]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.09561567164179105
[2m[36m(func pid=103380)[0m top5: 0.6623134328358209
[2m[36m(func pid=103380)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=103380)[0m f1_macro: 0.06836410692672716
[2m[36m(func pid=103380)[0m f1_weighted: 0.11653704041706206
[2m[36m(func pid=103380)[0m f1_per_class: [0.03, 0.199, 0.164, 0.0, 0.0, 0.0, 0.268, 0.0, 0.022, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.13152985074626866
[2m[36m(func pid=103390)[0m top5: 0.8292910447761194
[2m[36m(func pid=103390)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=103390)[0m f1_macro: 0.17850899778608997
[2m[36m(func pid=103390)[0m f1_weighted: 0.13742566989258262
[2m[36m(func pid=103390)[0m f1_per_class: [0.13, 0.199, 0.375, 0.0, 0.056, 0.27, 0.162, 0.224, 0.076, 0.294]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.279384328358209
[2m[36m(func pid=110192)[0m top5: 0.6021455223880597
[2m[36m(func pid=110192)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=110192)[0m f1_macro: 0.0508703762362299
[2m[36m(func pid=110192)[0m f1_weighted: 0.12723420302142543
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.026, 0.0, 0.438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5098 | Steps: 2 | Val loss: 2.2598 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6517 | Steps: 2 | Val loss: 2.2830 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.9671 | Steps: 2 | Val loss: 2.1695 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 10.8028 | Steps: 2 | Val loss: 4.7432 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 00:09:33 (running for 00:16:42.68)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.63  |      0.068 |                   29 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.023 |      0.179 |                   28 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.51  |      0.047 |                    6 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  4.998 |      0.051 |                    2 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.11100746268656717
[2m[36m(func pid=103380)[0m top5: 0.6814365671641791
[2m[36m(func pid=103380)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=103380)[0m f1_macro: 0.07826969903483115
[2m[36m(func pid=103380)[0m f1_weighted: 0.1317604684460779
[2m[36m(func pid=103380)[0m f1_per_class: [0.037, 0.203, 0.206, 0.0, 0.0, 0.0, 0.316, 0.0, 0.022, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.029850746268656716
[2m[36m(func pid=109192)[0m top5: 0.5876865671641791
[2m[36m(func pid=109192)[0m f1_micro: 0.029850746268656716
[2m[36m(func pid=109192)[0m f1_macro: 0.04702902875060848
[2m[36m(func pid=109192)[0m f1_weighted: 0.009534336321106755
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.005, 0.296, 0.003, 0.033, 0.0, 0.0, 0.089, 0.0, 0.043]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.15764925373134328
[2m[36m(func pid=103390)[0m top5: 0.8493470149253731
[2m[36m(func pid=103390)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=103390)[0m f1_macro: 0.1694644165226868
[2m[36m(func pid=103390)[0m f1_weighted: 0.1688932848280448
[2m[36m(func pid=103390)[0m f1_per_class: [0.191, 0.272, 0.4, 0.0, 0.064, 0.257, 0.271, 0.016, 0.075, 0.148]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.06623134328358209
[2m[36m(func pid=110192)[0m top5: 0.4944029850746269
[2m[36m(func pid=110192)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=110192)[0m f1_macro: 0.04402067956412643
[2m[36m(func pid=110192)[0m f1_weighted: 0.020626935812511626
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.0, 0.279, 0.0, 0.0, 0.008, 0.038, 0.115, 0.0, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3094 | Steps: 2 | Val loss: 2.3048 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5858 | Steps: 2 | Val loss: 2.2659 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.2504 | Steps: 2 | Val loss: 2.1340 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 14.1361 | Steps: 2 | Val loss: 8.7651 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 00:09:38 (running for 00:16:47.77)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.652 |      0.078 |                   30 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.967 |      0.169 |                   29 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.309 |      0.026 |                    7 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 10.803 |      0.044 |                    3 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.12779850746268656
[2m[36m(func pid=103380)[0m top5: 0.7024253731343284
[2m[36m(func pid=103380)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=103380)[0m f1_macro: 0.09235437819943373
[2m[36m(func pid=103380)[0m f1_weighted: 0.14506536607361004
[2m[36m(func pid=103380)[0m f1_per_class: [0.048, 0.218, 0.278, 0.0, 0.0, 0.0, 0.348, 0.0, 0.03, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.02658582089552239
[2m[36m(func pid=109192)[0m top5: 0.36240671641791045
[2m[36m(func pid=109192)[0m f1_micro: 0.02658582089552239
[2m[36m(func pid=109192)[0m f1_macro: 0.025661189655884202
[2m[36m(func pid=109192)[0m f1_weighted: 0.021176917021974943
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.087, 0.0, 0.0, 0.023, 0.008, 0.0, 0.054, 0.046, 0.038]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.18703358208955223
[2m[36m(func pid=103390)[0m top5: 0.8544776119402985
[2m[36m(func pid=103390)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=103390)[0m f1_macro: 0.18236995639305711
[2m[36m(func pid=103390)[0m f1_weighted: 0.19564701752974942
[2m[36m(func pid=103390)[0m f1_per_class: [0.326, 0.291, 0.386, 0.0, 0.068, 0.233, 0.354, 0.0, 0.088, 0.077]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.01166044776119403
[2m[36m(func pid=110192)[0m top5: 0.46828358208955223
[2m[36m(func pid=110192)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=110192)[0m f1_macro: 0.029210334257772586
[2m[36m(func pid=110192)[0m f1_weighted: 0.0017923900421988725
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.0, 0.277, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6483 | Steps: 2 | Val loss: 2.2538 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.1744 | Steps: 2 | Val loss: 2.2582 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9848 | Steps: 2 | Val loss: 2.0897 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 13.5230 | Steps: 2 | Val loss: 10.7505 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=103380)[0m top1: 0.14365671641791045
[2m[36m(func pid=103380)[0m top5: 0.7252798507462687
[2m[36m(func pid=103380)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=103380)[0m f1_macro: 0.10667893863685882
[2m[36m(func pid=103380)[0m f1_weighted: 0.15835145255327396
[2m[36m(func pid=103380)[0m f1_per_class: [0.06, 0.236, 0.367, 0.0, 0.0, 0.0, 0.381, 0.0, 0.023, 0.0]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:09:43 (running for 00:16:52.88)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.648 |      0.107 |                   32 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.25  |      0.182 |                   30 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.174 |      0.067 |                    8 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 14.136 |      0.029 |                    4 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.08395522388059702
[2m[36m(func pid=109192)[0m top5: 0.4048507462686567
[2m[36m(func pid=109192)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=109192)[0m f1_macro: 0.06726935408254722
[2m[36m(func pid=109192)[0m f1_weighted: 0.06080263548744354
[2m[36m(func pid=109192)[0m f1_per_class: [0.065, 0.193, 0.0, 0.0, 0.028, 0.205, 0.0, 0.0, 0.024, 0.158]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.20382462686567165
[2m[36m(func pid=103390)[0m top5: 0.8628731343283582
[2m[36m(func pid=103390)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=103390)[0m f1_macro: 0.1642889224099005
[2m[36m(func pid=103390)[0m f1_weighted: 0.20707269640794213
[2m[36m(func pid=103390)[0m f1_per_class: [0.265, 0.269, 0.338, 0.0, 0.072, 0.162, 0.44, 0.0, 0.097, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.022388059701492536
[2m[36m(func pid=110192)[0m top5: 0.240205223880597
[2m[36m(func pid=110192)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=110192)[0m f1_macro: 0.018143409488063207
[2m[36m(func pid=110192)[0m f1_weighted: 0.003516274200416798
[2m[36m(func pid=110192)[0m f1_per_class: [0.158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6034 | Steps: 2 | Val loss: 2.2350 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.1706 | Steps: 2 | Val loss: 2.1753 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2765 | Steps: 2 | Val loss: 2.0662 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 17.7722 | Steps: 2 | Val loss: 10.3683 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 00:09:49 (running for 00:16:57.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.603 |      0.111 |                   33 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.985 |      0.164 |                   31 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.174 |      0.067 |                    8 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 13.523 |      0.018 |                    5 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.1553171641791045
[2m[36m(func pid=103380)[0m top5: 0.7453358208955224
[2m[36m(func pid=103380)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=103380)[0m f1_macro: 0.11063039736361388
[2m[36m(func pid=103380)[0m f1_weighted: 0.16676145111956717
[2m[36m(func pid=103380)[0m f1_per_class: [0.065, 0.252, 0.356, 0.0, 0.0, 0.0, 0.398, 0.0, 0.035, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.09654850746268656
[2m[36m(func pid=109192)[0m top5: 0.6021455223880597
[2m[36m(func pid=109192)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=109192)[0m f1_macro: 0.05028872276599006
[2m[36m(func pid=109192)[0m f1_weighted: 0.060291701268341345
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.246, 0.0, 0.0, 0.033, 0.147, 0.0, 0.0, 0.0, 0.077]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.04057835820895522
[2m[36m(func pid=110192)[0m top5: 0.23833955223880596
[2m[36m(func pid=110192)[0m f1_micro: 0.04057835820895522
[2m[36m(func pid=110192)[0m f1_macro: 0.024694349540078847
[2m[36m(func pid=110192)[0m f1_weighted: 0.010876085243951073
[2m[36m(func pid=110192)[0m f1_per_class: [0.053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.163, 0.0, 0.031]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.2234141791044776
[2m[36m(func pid=103390)[0m top5: 0.871268656716418
[2m[36m(func pid=103390)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=103390)[0m f1_macro: 0.1378237321316787
[2m[36m(func pid=103390)[0m f1_weighted: 0.21099934806713463
[2m[36m(func pid=103390)[0m f1_per_class: [0.08, 0.266, 0.272, 0.0, 0.071, 0.086, 0.497, 0.0, 0.107, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5747 | Steps: 2 | Val loss: 2.2186 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0002 | Steps: 2 | Val loss: 2.1415 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 14.1491 | Steps: 2 | Val loss: 4.5025 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.9989 | Steps: 2 | Val loss: 2.0313 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 00:09:54 (running for 00:17:03.09)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.575 |      0.119 |                   34 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.276 |      0.138 |                   32 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.171 |      0.05  |                    9 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 17.772 |      0.025 |                    6 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.1707089552238806
[2m[36m(func pid=103380)[0m top5: 0.7597947761194029
[2m[36m(func pid=103380)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=103380)[0m f1_macro: 0.11935036237976451
[2m[36m(func pid=103380)[0m f1_weighted: 0.17798998962910287
[2m[36m(func pid=103380)[0m f1_per_class: [0.074, 0.277, 0.389, 0.0, 0.0, 0.0, 0.42, 0.0, 0.032, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.06389925373134328
[2m[36m(func pid=109192)[0m top5: 0.7318097014925373
[2m[36m(func pid=109192)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=109192)[0m f1_macro: 0.04977205487760968
[2m[36m(func pid=109192)[0m f1_weighted: 0.07205525703597676
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.199, 0.0, 0.121, 0.02, 0.011, 0.003, 0.0, 0.0, 0.143]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2234141791044776
[2m[36m(func pid=110192)[0m top5: 0.6347947761194029
[2m[36m(func pid=110192)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=110192)[0m f1_macro: 0.06584817840644634
[2m[36m(func pid=110192)[0m f1_weighted: 0.09828922588675892
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.398, 0.0, 0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.23880597014925373
[2m[36m(func pid=103390)[0m top5: 0.8768656716417911
[2m[36m(func pid=103390)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=103390)[0m f1_macro: 0.13397843157719821
[2m[36m(func pid=103390)[0m f1_weighted: 0.21506704546813546
[2m[36m(func pid=103390)[0m f1_per_class: [0.041, 0.264, 0.259, 0.003, 0.07, 0.085, 0.512, 0.0, 0.106, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5889 | Steps: 2 | Val loss: 2.1991 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.9147 | Steps: 2 | Val loss: 2.1983 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 13.4711 | Steps: 2 | Val loss: 5.9412 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 00:09:59 (running for 00:17:08.23)
Memory usage on this node: 24.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.575 |      0.119 |                   34 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.999 |      0.134 |                   33 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.915 |      0.052 |                   11 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 14.149 |      0.066 |                    7 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.1865671641791045
[2m[36m(func pid=103380)[0m top5: 0.7779850746268657
[2m[36m(func pid=103380)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=103380)[0m f1_macro: 0.12226506582640755
[2m[36m(func pid=103380)[0m f1_weighted: 0.18679738179335512
[2m[36m(func pid=103380)[0m f1_per_class: [0.085, 0.293, 0.364, 0.0, 0.0, 0.0, 0.439, 0.0, 0.041, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.0477 | Steps: 2 | Val loss: 2.0091 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=109192)[0m top1: 0.10680970149253731
[2m[36m(func pid=109192)[0m top5: 0.7723880597014925
[2m[36m(func pid=109192)[0m f1_micro: 0.10680970149253732
[2m[36m(func pid=109192)[0m f1_macro: 0.05220169344149066
[2m[36m(func pid=109192)[0m f1_weighted: 0.11539284167631705
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.0, 0.0, 0.393, 0.02, 0.0, 0.015, 0.0, 0.0, 0.094]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.29011194029850745
[2m[36m(func pid=110192)[0m top5: 0.8036380597014925
[2m[36m(func pid=110192)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=110192)[0m f1_macro: 0.08887653502379186
[2m[36m(func pid=110192)[0m f1_weighted: 0.1931760132726942
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.31, 0.0, 0.491, 0.0, 0.0, 0.0, 0.0, 0.088, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.2681902985074627
[2m[36m(func pid=103390)[0m top5: 0.8838619402985075
[2m[36m(func pid=103390)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=103390)[0m f1_macro: 0.1503707794477955
[2m[36m(func pid=103390)[0m f1_weighted: 0.24543181031067687
[2m[36m(func pid=103390)[0m f1_per_class: [0.102, 0.273, 0.242, 0.075, 0.068, 0.099, 0.532, 0.0, 0.114, 0.0]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5868 | Steps: 2 | Val loss: 2.1827 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.8911 | Steps: 2 | Val loss: 2.3192 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:10:04 (running for 00:17:13.41)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.587 |      0.133 |                   36 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.048 |      0.15  |                   34 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.915 |      0.052 |                   11 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 13.471 |      0.089 |                    8 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.20382462686567165
[2m[36m(func pid=103380)[0m top5: 0.7924440298507462
[2m[36m(func pid=103380)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=103380)[0m f1_macro: 0.13283911936637566
[2m[36m(func pid=103380)[0m f1_weighted: 0.19601947122032973
[2m[36m(func pid=103380)[0m f1_per_class: [0.095, 0.323, 0.412, 0.0, 0.0, 0.0, 0.451, 0.0, 0.048, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.0960820895522388
[2m[36m(func pid=109192)[0m top5: 0.7047574626865671
[2m[36m(func pid=109192)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=109192)[0m f1_macro: 0.09956591896436219
[2m[36m(func pid=109192)[0m f1_weighted: 0.1085773062425987
[2m[36m(func pid=109192)[0m f1_per_class: [0.098, 0.0, 0.32, 0.32, 0.043, 0.0, 0.018, 0.158, 0.0, 0.038]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 16.9239 | Steps: 2 | Val loss: 8.5587 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.9630 | Steps: 2 | Val loss: 1.9937 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=110192)[0m top1: 0.28638059701492535
[2m[36m(func pid=110192)[0m top5: 0.7910447761194029
[2m[36m(func pid=110192)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=110192)[0m f1_macro: 0.08210898735815679
[2m[36m(func pid=110192)[0m f1_weighted: 0.2008481935578182
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.348, 0.0, 0.0, 0.0, 0.0, 0.473, 0.0, 0.0, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5722 | Steps: 2 | Val loss: 2.1694 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.6851 | Steps: 2 | Val loss: 2.4161 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=103390)[0m top1: 0.31156716417910446
[2m[36m(func pid=103390)[0m top5: 0.8852611940298507
[2m[36m(func pid=103390)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=103390)[0m f1_macro: 0.18929598601192935
[2m[36m(func pid=103390)[0m f1_weighted: 0.32049415082889793
[2m[36m(func pid=103390)[0m f1_per_class: [0.135, 0.262, 0.234, 0.297, 0.064, 0.204, 0.54, 0.0, 0.079, 0.077]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:10:09 (running for 00:17:18.69)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.587 |      0.133 |                   36 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.963 |      0.189 |                   35 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.685 |      0.093 |                   13 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 16.924 |      0.082 |                    9 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.21082089552238806
[2m[36m(func pid=103380)[0m top5: 0.8092350746268657
[2m[36m(func pid=103380)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=103380)[0m f1_macro: 0.13266595161944
[2m[36m(func pid=103380)[0m f1_weighted: 0.19906076456373661
[2m[36m(func pid=103380)[0m f1_per_class: [0.104, 0.343, 0.389, 0.0, 0.0, 0.0, 0.45, 0.0, 0.04, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.05643656716417911
[2m[36m(func pid=109192)[0m top5: 0.6152052238805971
[2m[36m(func pid=109192)[0m f1_micro: 0.05643656716417911
[2m[36m(func pid=109192)[0m f1_macro: 0.09338231515047746
[2m[36m(func pid=109192)[0m f1_weighted: 0.058353637181377774
[2m[36m(func pid=109192)[0m f1_per_class: [0.196, 0.026, 0.111, 0.097, 0.077, 0.0, 0.003, 0.295, 0.101, 0.028]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 15.5009 | Steps: 2 | Val loss: 17.8590 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8828 | Steps: 2 | Val loss: 1.9956 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5544 | Steps: 2 | Val loss: 2.1557 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9087 | Steps: 2 | Val loss: 2.4757 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=110192)[0m top1: 0.008861940298507462
[2m[36m(func pid=110192)[0m top5: 0.7075559701492538
[2m[36m(func pid=110192)[0m f1_micro: 0.008861940298507462
[2m[36m(func pid=110192)[0m f1_macro: 0.007803013983503288
[2m[36m(func pid=110192)[0m f1_weighted: 0.0022668488938457567
[2m[36m(func pid=110192)[0m f1_per_class: [0.06, 0.0, 0.0, 0.003, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.333955223880597
[2m[36m(func pid=103390)[0m top5: 0.8773320895522388
[2m[36m(func pid=103390)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=103390)[0m f1_macro: 0.21521030085475293
[2m[36m(func pid=103390)[0m f1_weighted: 0.35333174508273896
[2m[36m(func pid=103390)[0m f1_per_class: [0.231, 0.236, 0.239, 0.419, 0.057, 0.257, 0.525, 0.016, 0.024, 0.148]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:10:14 (running for 00:17:23.79)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.572 |      0.133 |                   37 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.883 |      0.215 |                   36 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.909 |      0.083 |                   14 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 15.501 |      0.008 |                   10 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.049440298507462684
[2m[36m(func pid=109192)[0m top5: 0.6683768656716418
[2m[36m(func pid=109192)[0m f1_micro: 0.049440298507462684
[2m[36m(func pid=109192)[0m f1_macro: 0.08328741932593942
[2m[36m(func pid=109192)[0m f1_weighted: 0.04575555752637243
[2m[36m(func pid=109192)[0m f1_per_class: [0.189, 0.096, 0.0, 0.003, 0.086, 0.0, 0.006, 0.303, 0.12, 0.028]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.21875
[2m[36m(func pid=103380)[0m top5: 0.8213619402985075
[2m[36m(func pid=103380)[0m f1_micro: 0.21875
[2m[36m(func pid=103380)[0m f1_macro: 0.13499628382069195
[2m[36m(func pid=103380)[0m f1_weighted: 0.20108850864268402
[2m[36m(func pid=103380)[0m f1_per_class: [0.112, 0.34, 0.39, 0.0, 0.0, 0.0, 0.457, 0.0, 0.051, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 11.1406 | Steps: 2 | Val loss: 15.2989 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8649 | Steps: 2 | Val loss: 2.0140 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.7433 | Steps: 2 | Val loss: 2.4868 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5809 | Steps: 2 | Val loss: 2.1453 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=110192)[0m top1: 0.01166044776119403
[2m[36m(func pid=110192)[0m top5: 0.4920708955223881
[2m[36m(func pid=110192)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=110192)[0m f1_macro: 0.002314814814814815
[2m[36m(func pid=110192)[0m f1_weighted: 0.0002699177722498618
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.332089552238806
[2m[36m(func pid=103390)[0m top5: 0.8577425373134329
[2m[36m(func pid=103390)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=103390)[0m f1_macro: 0.22330645795543744
[2m[36m(func pid=103390)[0m f1_weighted: 0.3525542395203796
[2m[36m(func pid=103390)[0m f1_per_class: [0.22, 0.188, 0.259, 0.474, 0.055, 0.277, 0.469, 0.149, 0.0, 0.143]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:10:20 (running for 00:17:28.91)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.554 |      0.135 |                   38 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.865 |      0.223 |                   37 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.743 |      0.061 |                   15 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 11.141 |      0.002 |                   11 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.05317164179104478
[2m[36m(func pid=109192)[0m top5: 0.7234141791044776
[2m[36m(func pid=109192)[0m f1_micro: 0.05317164179104478
[2m[36m(func pid=109192)[0m f1_macro: 0.061489321505712735
[2m[36m(func pid=109192)[0m f1_weighted: 0.04512812996436949
[2m[36m(func pid=109192)[0m f1_per_class: [0.0, 0.112, 0.0, 0.0, 0.052, 0.007, 0.015, 0.276, 0.111, 0.041]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.23460820895522388
[2m[36m(func pid=103380)[0m top5: 0.8306902985074627
[2m[36m(func pid=103380)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=103380)[0m f1_macro: 0.13884430348726381
[2m[36m(func pid=103380)[0m f1_weighted: 0.21045954925971444
[2m[36m(func pid=103380)[0m f1_per_class: [0.113, 0.366, 0.373, 0.0, 0.0, 0.0, 0.472, 0.0, 0.064, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 11.7737 | Steps: 2 | Val loss: 13.6787 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7436 | Steps: 2 | Val loss: 2.0499 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.8406 | Steps: 2 | Val loss: 2.4434 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.4887 | Steps: 2 | Val loss: 2.1287 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=110192)[0m top1: 0.1884328358208955
[2m[36m(func pid=110192)[0m top5: 0.6427238805970149
[2m[36m(func pid=110192)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=110192)[0m f1_macro: 0.09580530240680925
[2m[36m(func pid=110192)[0m f1_weighted: 0.1586643477836666
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.0, 0.286, 0.551, 0.0, 0.0, 0.0, 0.016, 0.056, 0.05]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.3111007462686567
[2m[36m(func pid=103390)[0m top5: 0.832089552238806
[2m[36m(func pid=103390)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=103390)[0m f1_macro: 0.24693538084749508
[2m[36m(func pid=103390)[0m f1_weighted: 0.32671404840241963
[2m[36m(func pid=103390)[0m f1_per_class: [0.175, 0.088, 0.272, 0.484, 0.054, 0.282, 0.363, 0.474, 0.0, 0.278]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:10:25 (running for 00:17:33.92)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.581 |      0.139 |                   39 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.744 |      0.247 |                   38 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.841 |      0.117 |                   16 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 11.774 |      0.096 |                   12 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.1287313432835821
[2m[36m(func pid=109192)[0m top5: 0.7262126865671642
[2m[36m(func pid=109192)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=109192)[0m f1_macro: 0.11700702287938532
[2m[36m(func pid=109192)[0m f1_weighted: 0.14392584110427464
[2m[36m(func pid=109192)[0m f1_per_class: [0.082, 0.055, 0.105, 0.378, 0.037, 0.076, 0.0, 0.2, 0.156, 0.081]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.24766791044776118
[2m[36m(func pid=103380)[0m top5: 0.8432835820895522
[2m[36m(func pid=103380)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=103380)[0m f1_macro: 0.13947384991667144
[2m[36m(func pid=103380)[0m f1_weighted: 0.21347442047179685
[2m[36m(func pid=103380)[0m f1_per_class: [0.132, 0.363, 0.355, 0.0, 0.0, 0.0, 0.484, 0.0, 0.061, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 12.5124 | Steps: 2 | Val loss: 15.1146 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8532 | Steps: 2 | Val loss: 2.1091 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.4943 | Steps: 2 | Val loss: 2.3025 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4719 | Steps: 2 | Val loss: 2.1175 | Batch size: 32 | lr: 0.0001 | Duration: 2.60s
[2m[36m(func pid=110192)[0m top1: 0.2439365671641791
[2m[36m(func pid=110192)[0m top5: 0.6627798507462687
[2m[36m(func pid=110192)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=110192)[0m f1_macro: 0.1401887321537464
[2m[36m(func pid=110192)[0m f1_weighted: 0.11739698089723276
[2m[36m(func pid=110192)[0m f1_per_class: [0.226, 0.432, 0.357, 0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.077]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.24813432835820895
[2m[36m(func pid=103390)[0m top5: 0.7835820895522388
[2m[36m(func pid=103390)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=103390)[0m f1_macro: 0.199666331424738
[2m[36m(func pid=103390)[0m f1_weighted: 0.21408114668302872
[2m[36m(func pid=103390)[0m f1_per_class: [0.172, 0.032, 0.306, 0.476, 0.053, 0.296, 0.037, 0.391, 0.0, 0.235]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:10:30 (running for 00:17:39.16)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.489 |      0.139 |                   40 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.853 |      0.2   |                   39 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.494 |      0.156 |                   17 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.512 |      0.14  |                   13 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.2658582089552239
[2m[36m(func pid=103380)[0m top5: 0.8512126865671642
[2m[36m(func pid=103380)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=103380)[0m f1_macro: 0.14171322877819753
[2m[36m(func pid=103380)[0m f1_weighted: 0.22169820682469069
[2m[36m(func pid=103380)[0m f1_per_class: [0.157, 0.381, 0.31, 0.0, 0.0, 0.0, 0.499, 0.0, 0.07, 0.0]
[2m[36m(func pid=109192)[0m top1: 0.21315298507462688
[2m[36m(func pid=109192)[0m top5: 0.8190298507462687
[2m[36m(func pid=109192)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=109192)[0m f1_macro: 0.15580492298816367
[2m[36m(func pid=109192)[0m f1_weighted: 0.18540656836607455
[2m[36m(func pid=109192)[0m f1_per_class: [0.167, 0.026, 0.32, 0.514, 0.041, 0.152, 0.003, 0.213, 0.0, 0.123]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 16.9585 | Steps: 2 | Val loss: 21.6129 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.8101 | Steps: 2 | Val loss: 2.1871 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4676 | Steps: 2 | Val loss: 2.1060 | Batch size: 32 | lr: 0.0001 | Duration: 2.64s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4580 | Steps: 2 | Val loss: 2.1999 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=110192)[0m top1: 0.22061567164179105
[2m[36m(func pid=110192)[0m top5: 0.6385261194029851
[2m[36m(func pid=110192)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=110192)[0m f1_macro: 0.1141074438243989
[2m[36m(func pid=110192)[0m f1_weighted: 0.0995262032240946
[2m[36m(func pid=110192)[0m f1_per_class: [0.156, 0.37, 0.348, 0.0, 0.0, 0.268, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.22154850746268656
[2m[36m(func pid=103390)[0m top5: 0.7546641791044776
[2m[36m(func pid=103390)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=103390)[0m f1_macro: 0.17399058186748134
[2m[36m(func pid=103390)[0m f1_weighted: 0.18768940047384705
[2m[36m(func pid=103390)[0m f1_per_class: [0.146, 0.0, 0.324, 0.464, 0.052, 0.272, 0.0, 0.35, 0.0, 0.132]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:10:35 (running for 00:17:44.20)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.472 |      0.142 |                   41 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.81  |      0.174 |                   40 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.458 |      0.177 |                   18 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 16.958 |      0.114 |                   14 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.21921641791044777
[2m[36m(func pid=109192)[0m top5: 0.8558768656716418
[2m[36m(func pid=109192)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=109192)[0m f1_macro: 0.17745104130135644
[2m[36m(func pid=109192)[0m f1_weighted: 0.20875771599438753
[2m[36m(func pid=109192)[0m f1_per_class: [0.312, 0.031, 0.32, 0.511, 0.04, 0.065, 0.086, 0.316, 0.0, 0.094]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.28078358208955223
[2m[36m(func pid=103380)[0m top5: 0.8600746268656716
[2m[36m(func pid=103380)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=103380)[0m f1_macro: 0.144108055755252
[2m[36m(func pid=103380)[0m f1_weighted: 0.22664215271518634
[2m[36m(func pid=103380)[0m f1_per_class: [0.186, 0.389, 0.282, 0.0, 0.0, 0.0, 0.509, 0.0, 0.075, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 20.0905 | Steps: 2 | Val loss: 13.9639 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7481 | Steps: 2 | Val loss: 2.2593 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.3204 | Steps: 2 | Val loss: 2.1407 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4864 | Steps: 2 | Val loss: 2.0986 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=110192)[0m top1: 0.09328358208955224
[2m[36m(func pid=110192)[0m top5: 0.820429104477612
[2m[36m(func pid=110192)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=110192)[0m f1_macro: 0.09501594382410203
[2m[36m(func pid=110192)[0m f1_weighted: 0.06298144742733529
[2m[36m(func pid=110192)[0m f1_per_class: [0.226, 0.3, 0.316, 0.0, 0.0, 0.0, 0.0, 0.077, 0.0, 0.032]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.20102611940298507
[2m[36m(func pid=103390)[0m top5: 0.7378731343283582
[2m[36m(func pid=103390)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=103390)[0m f1_macro: 0.16478282044118497
[2m[36m(func pid=103390)[0m f1_weighted: 0.1729066962846866
[2m[36m(func pid=103390)[0m f1_per_class: [0.139, 0.0, 0.349, 0.432, 0.055, 0.239, 0.0, 0.317, 0.0, 0.117]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.2574626865671642
[2m[36m(func pid=109192)[0m top5: 0.8908582089552238
[2m[36m(func pid=109192)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=109192)[0m f1_macro: 0.22110158101374325
[2m[36m(func pid=109192)[0m f1_weighted: 0.30880010299070426
[2m[36m(func pid=109192)[0m f1_per_class: [0.265, 0.254, 0.345, 0.442, 0.051, 0.0, 0.374, 0.318, 0.102, 0.06]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:10:40 (running for 00:17:49.40)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.486 |      0.142 |                   43 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.748 |      0.165 |                   41 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.32  |      0.221 |                   19 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 20.091 |      0.095 |                   15 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.29384328358208955
[2m[36m(func pid=103380)[0m top5: 0.871268656716418
[2m[36m(func pid=103380)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=103380)[0m f1_macro: 0.14221610492500114
[2m[36m(func pid=103380)[0m f1_weighted: 0.23188817625683275
[2m[36m(func pid=103380)[0m f1_per_class: [0.171, 0.385, 0.237, 0.003, 0.0, 0.016, 0.52, 0.0, 0.09, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 14.8320 | Steps: 2 | Val loss: 7.9937 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7428 | Steps: 2 | Val loss: 2.3131 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.3169 | Steps: 2 | Val loss: 2.1515 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4377 | Steps: 2 | Val loss: 2.0895 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=110192)[0m top1: 0.2789179104477612
[2m[36m(func pid=110192)[0m top5: 0.8661380597014925
[2m[36m(func pid=110192)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=110192)[0m f1_macro: 0.14585749354976382
[2m[36m(func pid=110192)[0m f1_weighted: 0.23823448986356516
[2m[36m(func pid=110192)[0m f1_per_class: [0.27, 0.0, 0.333, 0.319, 0.0, 0.0, 0.47, 0.016, 0.0, 0.05]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.18190298507462688
[2m[36m(func pid=103390)[0m top5: 0.7299440298507462
[2m[36m(func pid=103390)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=103390)[0m f1_macro: 0.1534065084754827
[2m[36m(func pid=103390)[0m f1_weighted: 0.15859833008332308
[2m[36m(func pid=103390)[0m f1_per_class: [0.133, 0.0, 0.349, 0.395, 0.059, 0.224, 0.0, 0.283, 0.0, 0.09]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.2234141791044776
[2m[36m(func pid=109192)[0m top5: 0.8745335820895522
[2m[36m(func pid=109192)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=109192)[0m f1_macro: 0.20914748206140338
[2m[36m(func pid=109192)[0m f1_weighted: 0.22111981575152972
[2m[36m(func pid=109192)[0m f1_per_class: [0.316, 0.291, 0.333, 0.007, 0.064, 0.0, 0.434, 0.439, 0.139, 0.068]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:10:45 (running for 00:17:54.51)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.438 |      0.144 |                   44 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.743 |      0.153 |                   42 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.317 |      0.209 |                   20 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 14.832 |      0.146 |                   16 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.3050373134328358
[2m[36m(func pid=103380)[0m top5: 0.8791977611940298
[2m[36m(func pid=103380)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=103380)[0m f1_macro: 0.14425482875385884
[2m[36m(func pid=103380)[0m f1_weighted: 0.23943221029802056
[2m[36m(func pid=103380)[0m f1_per_class: [0.164, 0.384, 0.227, 0.022, 0.0, 0.024, 0.525, 0.0, 0.096, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 11.2955 | Steps: 2 | Val loss: 13.9408 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.8025 | Steps: 2 | Val loss: 2.3593 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.4049 | Steps: 2 | Val loss: 2.1881 | Batch size: 32 | lr: 0.01 | Duration: 2.61s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4153 | Steps: 2 | Val loss: 2.0850 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=110192)[0m top1: 0.14878731343283583
[2m[36m(func pid=110192)[0m top5: 0.699160447761194
[2m[36m(func pid=110192)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=110192)[0m f1_macro: 0.13590966147083222
[2m[36m(func pid=110192)[0m f1_weighted: 0.14556226354182886
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.0, 0.296, 0.407, 0.0, 0.0, 0.0, 0.46, 0.057, 0.138]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.16884328358208955
[2m[36m(func pid=103390)[0m top5: 0.7234141791044776
[2m[36m(func pid=103390)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=103390)[0m f1_macro: 0.14103247822096301
[2m[36m(func pid=103390)[0m f1_weighted: 0.14585926764389154
[2m[36m(func pid=103390)[0m f1_per_class: [0.131, 0.0, 0.333, 0.377, 0.066, 0.166, 0.0, 0.272, 0.0, 0.065]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.21875
[2m[36m(func pid=109192)[0m top5: 0.8544776119402985
[2m[36m(func pid=109192)[0m f1_micro: 0.21875
[2m[36m(func pid=109192)[0m f1_macro: 0.22245895561302603
[2m[36m(func pid=109192)[0m f1_weighted: 0.20649629673993136
[2m[36m(func pid=109192)[0m f1_per_class: [0.33, 0.338, 0.364, 0.01, 0.094, 0.092, 0.31, 0.47, 0.151, 0.065]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:10:50 (running for 00:17:59.73)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.415 |      0.139 |                   45 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.803 |      0.141 |                   43 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.405 |      0.222 |                   21 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 11.296 |      0.136 |                   17 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.3045708955223881
[2m[36m(func pid=103380)[0m top5: 0.8852611940298507
[2m[36m(func pid=103380)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=103380)[0m f1_macro: 0.13946927241197798
[2m[36m(func pid=103380)[0m f1_weighted: 0.24081927525234897
[2m[36m(func pid=103380)[0m f1_per_class: [0.121, 0.373, 0.216, 0.04, 0.0, 0.031, 0.52, 0.0, 0.094, 0.0]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 13.9589 | Steps: 2 | Val loss: 17.9200 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.2495 | Steps: 2 | Val loss: 2.1277 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.7557 | Steps: 2 | Val loss: 2.3686 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4295 | Steps: 2 | Val loss: 2.0823 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=110192)[0m top1: 0.11567164179104478
[2m[36m(func pid=110192)[0m top5: 0.5960820895522388
[2m[36m(func pid=110192)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=110192)[0m f1_macro: 0.1163941124253185
[2m[36m(func pid=110192)[0m f1_weighted: 0.08990717237198037
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.0, 0.296, 0.23, 0.085, 0.0, 0.0, 0.334, 0.065, 0.154]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.26399253731343286
[2m[36m(func pid=109192)[0m top5: 0.8675373134328358
[2m[36m(func pid=109192)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=109192)[0m f1_macro: 0.26719632023165496
[2m[36m(func pid=109192)[0m f1_weighted: 0.28830058907949774
[2m[36m(func pid=109192)[0m f1_per_class: [0.277, 0.289, 0.375, 0.326, 0.098, 0.352, 0.214, 0.498, 0.152, 0.091]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.16744402985074627
[2m[36m(func pid=103390)[0m top5: 0.7238805970149254
[2m[36m(func pid=103390)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=103390)[0m f1_macro: 0.13764529856493574
[2m[36m(func pid=103390)[0m f1_weighted: 0.14188843591007597
[2m[36m(func pid=103390)[0m f1_per_class: [0.136, 0.0, 0.319, 0.38, 0.072, 0.12, 0.0, 0.277, 0.0, 0.072]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:10:55 (running for 00:18:04.77)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.43  |      0.152 |                   46 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.756 |      0.138 |                   44 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.249 |      0.267 |                   22 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 13.959 |      0.116 |                   18 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.3138992537313433
[2m[36m(func pid=103380)[0m top5: 0.8941231343283582
[2m[36m(func pid=103380)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=103380)[0m f1_macro: 0.15222061592633107
[2m[36m(func pid=103380)[0m f1_weighted: 0.2617545490796778
[2m[36m(func pid=103380)[0m f1_per_class: [0.095, 0.362, 0.202, 0.092, 0.0, 0.08, 0.529, 0.0, 0.088, 0.074]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 11.0475 | Steps: 2 | Val loss: 23.3319 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1177 | Steps: 2 | Val loss: 2.1018 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7097 | Steps: 2 | Val loss: 2.3597 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4321 | Steps: 2 | Val loss: 2.0819 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=110192)[0m top1: 0.12126865671641791
[2m[36m(func pid=110192)[0m top5: 0.6077425373134329
[2m[36m(func pid=110192)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=110192)[0m f1_macro: 0.12347473720482886
[2m[36m(func pid=110192)[0m f1_weighted: 0.13692256908820405
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.024, 0.333, 0.376, 0.021, 0.0, 0.0, 0.438, 0.0, 0.043]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.3460820895522388
[2m[36m(func pid=109192)[0m top5: 0.8782649253731343
[2m[36m(func pid=109192)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=109192)[0m f1_macro: 0.27309778891737135
[2m[36m(func pid=109192)[0m f1_weighted: 0.31649095740413985
[2m[36m(func pid=109192)[0m f1_per_class: [0.273, 0.07, 0.357, 0.555, 0.094, 0.38, 0.208, 0.519, 0.131, 0.145]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:11:00 (running for 00:18:09.81)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.43  |      0.152 |                   46 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.71  |      0.146 |                   45 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.118 |      0.273 |                   23 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 11.048 |      0.123 |                   19 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m top1: 0.17537313432835822
[2m[36m(func pid=103390)[0m top5: 0.7336753731343284
[2m[36m(func pid=103390)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=103390)[0m f1_macro: 0.14583738825040043
[2m[36m(func pid=103390)[0m f1_weighted: 0.15005462002112405
[2m[36m(func pid=103390)[0m f1_per_class: [0.148, 0.016, 0.319, 0.402, 0.075, 0.091, 0.0, 0.292, 0.051, 0.064]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m top1: 0.31296641791044777
[2m[36m(func pid=103380)[0m top5: 0.9020522388059702
[2m[36m(func pid=103380)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=103380)[0m f1_macro: 0.1629433828098692
[2m[36m(func pid=103380)[0m f1_weighted: 0.272818279301274
[2m[36m(func pid=103380)[0m f1_per_class: [0.094, 0.332, 0.179, 0.132, 0.0, 0.133, 0.523, 0.0, 0.094, 0.143]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 11.4630 | Steps: 2 | Val loss: 24.6071 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4443 | Steps: 2 | Val loss: 2.1991 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3704 | Steps: 2 | Val loss: 2.0813 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6880 | Steps: 2 | Val loss: 2.3427 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=110192)[0m top1: 0.03404850746268657
[2m[36m(func pid=110192)[0m top5: 0.8194962686567164
[2m[36m(func pid=110192)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=110192)[0m f1_macro: 0.02042125315602559
[2m[36m(func pid=110192)[0m f1_weighted: 0.026802741654503307
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.142, 0.0, 0.0, 0.024, 0.0, 0.006, 0.0, 0.0, 0.033]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.3255597014925373
[2m[36m(func pid=109192)[0m top5: 0.8838619402985075
[2m[36m(func pid=109192)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=109192)[0m f1_macro: 0.2512628317038347
[2m[36m(func pid=109192)[0m f1_weighted: 0.28067119936989005
[2m[36m(func pid=109192)[0m f1_per_class: [0.261, 0.031, 0.364, 0.541, 0.077, 0.351, 0.146, 0.481, 0.078, 0.182]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:11:06 (running for 00:18:14.95)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.37  |      0.178 |                   48 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.71  |      0.146 |                   45 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.444 |      0.251 |                   24 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 11.463 |      0.02  |                   20 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.31529850746268656
[2m[36m(func pid=103380)[0m top5: 0.9043843283582089
[2m[36m(func pid=103380)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=103380)[0m f1_macro: 0.17801395616358356
[2m[36m(func pid=103380)[0m f1_weighted: 0.2896410498963252
[2m[36m(func pid=103380)[0m f1_per_class: [0.102, 0.32, 0.177, 0.187, 0.024, 0.172, 0.516, 0.0, 0.105, 0.176]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.1767723880597015
[2m[36m(func pid=103390)[0m top5: 0.7388059701492538
[2m[36m(func pid=103390)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=103390)[0m f1_macro: 0.15676800725151546
[2m[36m(func pid=103390)[0m f1_weighted: 0.16012965297749476
[2m[36m(func pid=103390)[0m f1_per_class: [0.17, 0.07, 0.301, 0.394, 0.077, 0.084, 0.0, 0.335, 0.084, 0.053]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 13.0325 | Steps: 2 | Val loss: 12.7494 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.2477 | Steps: 2 | Val loss: 2.2752 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3908 | Steps: 2 | Val loss: 2.0812 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7385 | Steps: 2 | Val loss: 2.3208 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=110192)[0m top1: 0.2019589552238806
[2m[36m(func pid=110192)[0m top5: 0.8125
[2m[36m(func pid=110192)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=110192)[0m f1_macro: 0.1924354762716921
[2m[36m(func pid=110192)[0m f1_weighted: 0.24435291152531752
[2m[36m(func pid=110192)[0m f1_per_class: [0.337, 0.18, 0.4, 0.32, 0.036, 0.175, 0.306, 0.0, 0.051, 0.119]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.30177238805970147
[2m[36m(func pid=109192)[0m top5: 0.8796641791044776
[2m[36m(func pid=109192)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=109192)[0m f1_macro: 0.2705535521120349
[2m[36m(func pid=109192)[0m f1_weighted: 0.30142849846426917
[2m[36m(func pid=109192)[0m f1_per_class: [0.246, 0.26, 0.381, 0.48, 0.07, 0.33, 0.147, 0.477, 0.098, 0.217]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:11:11 (running for 00:18:20.17)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.391 |      0.18  |                   49 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.688 |      0.157 |                   46 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.248 |      0.271 |                   25 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 13.032 |      0.192 |                   21 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.32322761194029853
[2m[36m(func pid=103380)[0m top5: 0.9085820895522388
[2m[36m(func pid=103380)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=103380)[0m f1_macro: 0.17964156996591793
[2m[36m(func pid=103380)[0m f1_weighted: 0.3138151596529621
[2m[36m(func pid=103380)[0m f1_per_class: [0.038, 0.299, 0.172, 0.278, 0.035, 0.22, 0.51, 0.016, 0.08, 0.146]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.1455223880597015
[2m[36m(func pid=103390)[0m top5: 0.7583955223880597
[2m[36m(func pid=103390)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=103390)[0m f1_macro: 0.15425255585850497
[2m[36m(func pid=103390)[0m f1_weighted: 0.13156007576053613
[2m[36m(func pid=103390)[0m f1_per_class: [0.198, 0.133, 0.286, 0.233, 0.077, 0.111, 0.0, 0.366, 0.089, 0.05]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 5.9424 | Steps: 2 | Val loss: 15.1388 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.0161 | Steps: 2 | Val loss: 2.4272 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3706 | Steps: 2 | Val loss: 2.0840 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.9634 | Steps: 2 | Val loss: 2.2961 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=110192)[0m top1: 0.21082089552238806
[2m[36m(func pid=110192)[0m top5: 0.8120335820895522
[2m[36m(func pid=110192)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=110192)[0m f1_macro: 0.1787867941568943
[2m[36m(func pid=110192)[0m f1_weighted: 0.17374783539183636
[2m[36m(func pid=110192)[0m f1_per_class: [0.192, 0.0, 0.387, 0.284, 0.323, 0.274, 0.172, 0.0, 0.08, 0.077]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.24440298507462688
[2m[36m(func pid=109192)[0m top5: 0.8638059701492538
[2m[36m(func pid=109192)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=109192)[0m f1_macro: 0.25057048821323513
[2m[36m(func pid=109192)[0m f1_weighted: 0.21675315139682008
[2m[36m(func pid=109192)[0m f1_per_class: [0.227, 0.359, 0.424, 0.038, 0.078, 0.336, 0.216, 0.462, 0.136, 0.231]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:11:16 (running for 00:18:25.31)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.371 |      0.187 |                   50 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.739 |      0.154 |                   47 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.016 |      0.251 |                   26 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  5.942 |      0.179 |                   22 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.3283582089552239
[2m[36m(func pid=103380)[0m top5: 0.9076492537313433
[2m[36m(func pid=103380)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=103380)[0m f1_macro: 0.18703032491384847
[2m[36m(func pid=103380)[0m f1_weighted: 0.3307188147341222
[2m[36m(func pid=103380)[0m f1_per_class: [0.042, 0.283, 0.165, 0.345, 0.043, 0.264, 0.495, 0.047, 0.057, 0.131]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.1394589552238806
[2m[36m(func pid=103390)[0m top5: 0.7714552238805971
[2m[36m(func pid=103390)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=103390)[0m f1_macro: 0.1666528715147168
[2m[36m(func pid=103390)[0m f1_weighted: 0.12367784805046346
[2m[36m(func pid=103390)[0m f1_per_class: [0.227, 0.192, 0.275, 0.117, 0.069, 0.197, 0.0, 0.422, 0.106, 0.061]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.0468 | Steps: 2 | Val loss: 2.5191 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 8.7793 | Steps: 2 | Val loss: 14.3029 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3501 | Steps: 2 | Val loss: 2.0898 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5867 | Steps: 2 | Val loss: 2.2533 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=109192)[0m top1: 0.27238805970149255
[2m[36m(func pid=109192)[0m top5: 0.8726679104477612
[2m[36m(func pid=109192)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=109192)[0m f1_macro: 0.25274979875497283
[2m[36m(func pid=109192)[0m f1_weighted: 0.25647608502332475
[2m[36m(func pid=109192)[0m f1_per_class: [0.231, 0.369, 0.438, 0.0, 0.074, 0.346, 0.396, 0.368, 0.123, 0.184]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.322294776119403
[2m[36m(func pid=110192)[0m top5: 0.8148320895522388
[2m[36m(func pid=110192)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=110192)[0m f1_macro: 0.24204526654377703
[2m[36m(func pid=110192)[0m f1_weighted: 0.3102985711167414
[2m[36m(func pid=110192)[0m f1_per_class: [0.204, 0.0, 0.333, 0.447, 0.256, 0.52, 0.378, 0.0, 0.11, 0.172]
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:11:21 (running for 00:18:30.40)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.35  |      0.197 |                   51 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.963 |      0.167 |                   48 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.047 |      0.253 |                   27 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  8.779 |      0.242 |                   23 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.3204291044776119
[2m[36m(func pid=103380)[0m top5: 0.8950559701492538
[2m[36m(func pid=103380)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=103380)[0m f1_macro: 0.19711531907158591
[2m[36m(func pid=103380)[0m f1_weighted: 0.3343014907574058
[2m[36m(func pid=103380)[0m f1_per_class: [0.043, 0.268, 0.168, 0.377, 0.061, 0.276, 0.461, 0.155, 0.038, 0.124]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.15111940298507462
[2m[36m(func pid=103390)[0m top5: 0.7966417910447762
[2m[36m(func pid=103390)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=103390)[0m f1_macro: 0.18953840446358347
[2m[36m(func pid=103390)[0m f1_weighted: 0.135669349337009
[2m[36m(func pid=103390)[0m f1_per_class: [0.294, 0.236, 0.301, 0.093, 0.071, 0.258, 0.0, 0.468, 0.106, 0.068]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.9339 | Steps: 2 | Val loss: 2.3782 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 8.3249 | Steps: 2 | Val loss: 18.5342 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3937 | Steps: 2 | Val loss: 2.0949 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6918 | Steps: 2 | Val loss: 2.1850 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=110192)[0m top1: 0.21128731343283583
[2m[36m(func pid=110192)[0m top5: 0.7369402985074627
[2m[36m(func pid=110192)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=110192)[0m f1_macro: 0.16789720004023542
[2m[36m(func pid=110192)[0m f1_weighted: 0.14975913436524557
[2m[36m(func pid=110192)[0m f1_per_class: [0.174, 0.0, 0.253, 0.408, 0.24, 0.0, 0.003, 0.408, 0.102, 0.091]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.27472014925373134
[2m[36m(func pid=109192)[0m top5: 0.8824626865671642
[2m[36m(func pid=109192)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=109192)[0m f1_macro: 0.24513917821393436
[2m[36m(func pid=109192)[0m f1_weighted: 0.27726120639932217
[2m[36m(func pid=109192)[0m f1_per_class: [0.238, 0.374, 0.387, 0.01, 0.062, 0.307, 0.482, 0.314, 0.104, 0.174]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:11:26 (running for 00:18:35.50)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.394 |      0.194 |                   52 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.587 |      0.19  |                   49 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.934 |      0.245 |                   28 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  8.325 |      0.168 |                   24 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.3101679104477612
[2m[36m(func pid=103380)[0m top5: 0.8754664179104478
[2m[36m(func pid=103380)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=103380)[0m f1_macro: 0.1944499357354267
[2m[36m(func pid=103380)[0m f1_weighted: 0.3310684912792731
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.251, 0.155, 0.425, 0.052, 0.265, 0.4, 0.284, 0.022, 0.089]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.17723880597014927
[2m[36m(func pid=103390)[0m top5: 0.820429104477612
[2m[36m(func pid=103390)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=103390)[0m f1_macro: 0.21805282812159632
[2m[36m(func pid=103390)[0m f1_weighted: 0.17682148556211946
[2m[36m(func pid=103390)[0m f1_per_class: [0.31, 0.271, 0.344, 0.084, 0.065, 0.308, 0.098, 0.495, 0.116, 0.09]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 9.3382 | Steps: 2 | Val loss: 22.0743 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.0148 | Steps: 2 | Val loss: 2.1950 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3900 | Steps: 2 | Val loss: 2.1035 | Batch size: 32 | lr: 0.0001 | Duration: 2.64s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6059 | Steps: 2 | Val loss: 2.1315 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=110192)[0m top1: 0.14412313432835822
[2m[36m(func pid=110192)[0m top5: 0.777518656716418
[2m[36m(func pid=110192)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=110192)[0m f1_macro: 0.12127940416046028
[2m[36m(func pid=110192)[0m f1_weighted: 0.10505860421731844
[2m[36m(func pid=110192)[0m f1_per_class: [0.093, 0.005, 0.156, 0.3, 0.246, 0.0, 0.0, 0.207, 0.068, 0.137]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.3208955223880597
[2m[36m(func pid=109192)[0m top5: 0.8899253731343284
[2m[36m(func pid=109192)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=109192)[0m f1_macro: 0.2908222247838928
[2m[36m(func pid=109192)[0m f1_weighted: 0.37628479579168156
[2m[36m(func pid=109192)[0m f1_per_class: [0.272, 0.351, 0.412, 0.403, 0.064, 0.284, 0.446, 0.406, 0.134, 0.135]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:11:31 (running for 00:18:40.57)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.39  |      0.191 |                   53 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.692 |      0.218 |                   50 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.015 |      0.291 |                   29 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  9.338 |      0.121 |                   25 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.29524253731343286
[2m[36m(func pid=103380)[0m top5: 0.8530783582089553
[2m[36m(func pid=103380)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=103380)[0m f1_macro: 0.1913257020513262
[2m[36m(func pid=103380)[0m f1_weighted: 0.31051516838246424
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.251, 0.145, 0.454, 0.074, 0.251, 0.297, 0.371, 0.0, 0.07]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.23134328358208955
[2m[36m(func pid=103390)[0m top5: 0.8493470149253731
[2m[36m(func pid=103390)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=103390)[0m f1_macro: 0.25433381912185593
[2m[36m(func pid=103390)[0m f1_weighted: 0.2605629626852466
[2m[36m(func pid=103390)[0m f1_per_class: [0.297, 0.288, 0.349, 0.13, 0.069, 0.33, 0.311, 0.519, 0.123, 0.127]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 12.5761 | Steps: 2 | Val loss: 23.3277 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0440 | Steps: 2 | Val loss: 2.1216 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3458 | Steps: 2 | Val loss: 2.1106 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6642 | Steps: 2 | Val loss: 2.0607 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:11:36 (running for 00:18:45.59)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.39  |      0.191 |                   53 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.606 |      0.254 |                   51 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.044 |      0.302 |                   30 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.576 |      0.167 |                   26 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.38619402985074625
[2m[36m(func pid=109192)[0m top5: 0.9034514925373134
[2m[36m(func pid=109192)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=109192)[0m f1_macro: 0.3023565224321549
[2m[36m(func pid=109192)[0m f1_weighted: 0.3924402165064009
[2m[36m(func pid=109192)[0m f1_per_class: [0.364, 0.15, 0.353, 0.57, 0.079, 0.338, 0.421, 0.475, 0.128, 0.146]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2294776119402985
[2m[36m(func pid=110192)[0m top5: 0.7416044776119403
[2m[36m(func pid=110192)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=110192)[0m f1_macro: 0.1669102362506934
[2m[36m(func pid=110192)[0m f1_weighted: 0.16640144375430044
[2m[36m(func pid=110192)[0m f1_per_class: [0.137, 0.445, 0.122, 0.0, 0.23, 0.0, 0.201, 0.358, 0.091, 0.087]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.2667910447761194
[2m[36m(func pid=103380)[0m top5: 0.8157649253731343
[2m[36m(func pid=103380)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=103380)[0m f1_macro: 0.1682906933179124
[2m[36m(func pid=103380)[0m f1_weighted: 0.25554988466253453
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.221, 0.14, 0.48, 0.088, 0.203, 0.124, 0.366, 0.0, 0.059]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.26632462686567165
[2m[36m(func pid=103390)[0m top5: 0.8768656716417911
[2m[36m(func pid=103390)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=103390)[0m f1_macro: 0.2691943925811572
[2m[36m(func pid=103390)[0m f1_weighted: 0.3047999694583788
[2m[36m(func pid=103390)[0m f1_per_class: [0.322, 0.284, 0.364, 0.21, 0.07, 0.333, 0.403, 0.409, 0.125, 0.172]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8689 | Steps: 2 | Val loss: 2.1954 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 10.0892 | Steps: 2 | Val loss: 30.7429 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3520 | Steps: 2 | Val loss: 2.1190 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5305 | Steps: 2 | Val loss: 2.0013 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=109192)[0m top1: 0.3843283582089552
[2m[36m(func pid=109192)[0m top5: 0.9001865671641791
[2m[36m(func pid=109192)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=109192)[0m f1_macro: 0.26767562611787893
[2m[36m(func pid=109192)[0m f1_weighted: 0.37834631490174453
[2m[36m(func pid=109192)[0m f1_per_class: [0.2, 0.167, 0.265, 0.58, 0.106, 0.363, 0.369, 0.474, 0.044, 0.109]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:11:41 (running for 00:18:50.59)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.346 |      0.168 |                   54 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.664 |      0.269 |                   52 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.869 |      0.268 |                   31 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.576 |      0.167 |                   26 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=110192)[0m top1: 0.26725746268656714
[2m[36m(func pid=110192)[0m top5: 0.7714552238805971
[2m[36m(func pid=110192)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=110192)[0m f1_macro: 0.12007242963828504
[2m[36m(func pid=110192)[0m f1_weighted: 0.19857742467362077
[2m[36m(func pid=110192)[0m f1_per_class: [0.065, 0.388, 0.128, 0.0, 0.132, 0.0, 0.43, 0.0, 0.0, 0.059]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.23833955223880596
[2m[36m(func pid=103380)[0m top5: 0.7826492537313433
[2m[36m(func pid=103380)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=103380)[0m f1_macro: 0.14285731706949883
[2m[36m(func pid=103380)[0m f1_weighted: 0.21174056801179544
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.185, 0.136, 0.485, 0.08, 0.157, 0.021, 0.316, 0.0, 0.048]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.3162313432835821
[2m[36m(func pid=103390)[0m top5: 0.9020522388059702
[2m[36m(func pid=103390)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=103390)[0m f1_macro: 0.2861980225737066
[2m[36m(func pid=103390)[0m f1_weighted: 0.3557729537009745
[2m[36m(func pid=103390)[0m f1_per_class: [0.324, 0.299, 0.377, 0.329, 0.072, 0.335, 0.466, 0.331, 0.139, 0.189]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.6982 | Steps: 2 | Val loss: 2.2478 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3324 | Steps: 2 | Val loss: 2.1270 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 25.8904 | Steps: 2 | Val loss: 28.3987 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:11:46 (running for 00:18:55.78)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.352 |      0.143 |                   55 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.531 |      0.286 |                   53 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.698 |      0.277 |                   32 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 10.089 |      0.12  |                   27 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5108 | Steps: 2 | Val loss: 1.9493 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=109192)[0m top1: 0.3833955223880597
[2m[36m(func pid=109192)[0m top5: 0.8913246268656716
[2m[36m(func pid=109192)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=109192)[0m f1_macro: 0.27714865621460916
[2m[36m(func pid=109192)[0m f1_weighted: 0.3870051678884206
[2m[36m(func pid=109192)[0m f1_per_class: [0.085, 0.399, 0.212, 0.512, 0.155, 0.436, 0.31, 0.427, 0.081, 0.155]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.21548507462686567
[2m[36m(func pid=110192)[0m top5: 0.8027052238805971
[2m[36m(func pid=110192)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=110192)[0m f1_macro: 0.12768527417440495
[2m[36m(func pid=110192)[0m f1_weighted: 0.18073082472704283
[2m[36m(func pid=110192)[0m f1_per_class: [0.152, 0.385, 0.171, 0.0, 0.066, 0.0, 0.357, 0.0, 0.085, 0.062]
[2m[36m(func pid=103380)[0m top1: 0.22201492537313433
[2m[36m(func pid=103380)[0m top5: 0.7476679104477612
[2m[36m(func pid=103380)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=103380)[0m f1_macro: 0.13138995171075232
[2m[36m(func pid=103380)[0m f1_weighted: 0.19525127596142988
[2m[36m(func pid=103380)[0m f1_per_class: [0.0, 0.167, 0.138, 0.477, 0.074, 0.128, 0.003, 0.276, 0.0, 0.05]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.3712686567164179
[2m[36m(func pid=103390)[0m top5: 0.9165111940298507
[2m[36m(func pid=103390)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=103390)[0m f1_macro: 0.2948291756907012
[2m[36m(func pid=103390)[0m f1_weighted: 0.39835021344910365
[2m[36m(func pid=103390)[0m f1_per_class: [0.322, 0.297, 0.407, 0.47, 0.073, 0.319, 0.505, 0.229, 0.116, 0.211]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7610 | Steps: 2 | Val loss: 2.4924 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 12.6098 | Steps: 2 | Val loss: 27.4980 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3351 | Steps: 2 | Val loss: 2.1362 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=103380)[0m top1: 0.20569029850746268
[2m[36m(func pid=103380)[0m top5: 0.7238805970149254
[2m[36m(func pid=103380)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=103380)[0m f1_macro: 0.12056424162946693
[2m[36m(func pid=103380)[0m f1_weighted: 0.17674484725965822
[2m[36m(func pid=103380)[0m f1_per_class: [0.041, 0.141, 0.134, 0.462, 0.075, 0.058, 0.0, 0.245, 0.0, 0.05]
[2m[36m(func pid=109192)[0m top1: 0.332089552238806
[2m[36m(func pid=109192)[0m top5: 0.8763992537313433
[2m[36m(func pid=109192)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=109192)[0m f1_macro: 0.2691917204730505
[2m[36m(func pid=109192)[0m f1_weighted: 0.30387960404797243
[2m[36m(func pid=109192)[0m f1_per_class: [0.16, 0.451, 0.173, 0.167, 0.179, 0.435, 0.312, 0.4, 0.147, 0.267]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:11:52 (running for 00:19:01.06)
Memory usage on this node: 24.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.335 |      0.121 |                   57 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.511 |      0.295 |                   54 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.698 |      0.277 |                   32 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 25.89  |      0.128 |                   28 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.09468283582089553
[2m[36m(func pid=110192)[0m top5: 0.7798507462686567
[2m[36m(func pid=110192)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=110192)[0m f1_macro: 0.11052769835911458
[2m[36m(func pid=110192)[0m f1_weighted: 0.1048725572229621
[2m[36m(func pid=110192)[0m f1_per_class: [0.104, 0.153, 0.4, 0.152, 0.038, 0.0, 0.092, 0.0, 0.084, 0.081]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4487 | Steps: 2 | Val loss: 1.9144 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=103390)[0m top1: 0.384794776119403
[2m[36m(func pid=103390)[0m top5: 0.9188432835820896
[2m[36m(func pid=103390)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=103390)[0m f1_macro: 0.28170651884838016
[2m[36m(func pid=103390)[0m f1_weighted: 0.399038103792243
[2m[36m(func pid=103390)[0m f1_per_class: [0.283, 0.26, 0.385, 0.499, 0.073, 0.3, 0.523, 0.19, 0.078, 0.226]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3280 | Steps: 2 | Val loss: 2.1492 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 12.0155 | Steps: 2 | Val loss: 24.2936 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9357 | Steps: 2 | Val loss: 2.8362 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 00:11:57 (running for 00:19:06.26)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.328 |      0.117 |                   58 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.449 |      0.282 |                   55 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.761 |      0.269 |                   33 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.61  |      0.111 |                   29 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.19776119402985073
[2m[36m(func pid=103380)[0m top5: 0.7024253731343284
[2m[36m(func pid=103380)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=103380)[0m f1_macro: 0.11741584432680792
[2m[36m(func pid=103380)[0m f1_weighted: 0.16922629263502992
[2m[36m(func pid=103380)[0m f1_per_class: [0.077, 0.112, 0.129, 0.463, 0.082, 0.039, 0.0, 0.22, 0.0, 0.053]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.23274253731343283
[2m[36m(func pid=110192)[0m top5: 0.7280783582089553
[2m[36m(func pid=110192)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=110192)[0m f1_macro: 0.1619277681211689
[2m[36m(func pid=110192)[0m f1_weighted: 0.17446734134983363
[2m[36m(func pid=110192)[0m f1_per_class: [0.193, 0.0, 0.4, 0.452, 0.08, 0.315, 0.006, 0.0, 0.077, 0.097]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.333955223880597
[2m[36m(func pid=109192)[0m top5: 0.8619402985074627
[2m[36m(func pid=109192)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=109192)[0m f1_macro: 0.26225529721676694
[2m[36m(func pid=109192)[0m f1_weighted: 0.2827232357799131
[2m[36m(func pid=109192)[0m f1_per_class: [0.183, 0.459, 0.176, 0.0, 0.191, 0.433, 0.385, 0.457, 0.13, 0.208]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.5675 | Steps: 2 | Val loss: 1.9155 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=103390)[0m top1: 0.38526119402985076
[2m[36m(func pid=103390)[0m top5: 0.9207089552238806
[2m[36m(func pid=103390)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=103390)[0m f1_macro: 0.2697409084507319
[2m[36m(func pid=103390)[0m f1_weighted: 0.39116426000728344
[2m[36m(func pid=103390)[0m f1_per_class: [0.24, 0.221, 0.385, 0.508, 0.074, 0.259, 0.53, 0.19, 0.079, 0.213]
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3177 | Steps: 2 | Val loss: 2.1612 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 8.1654 | Steps: 2 | Val loss: 23.6791 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.9185 | Steps: 2 | Val loss: 3.1038 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=109192)[0m top1: 0.31716417910447764
[2m[36m(func pid=109192)[0m top5: 0.871268656716418
[2m[36m(func pid=109192)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=109192)[0m f1_macro: 0.2443819598121145
[2m[36m(func pid=109192)[0m f1_weighted: 0.2872076315504077
[2m[36m(func pid=109192)[0m f1_per_class: [0.104, 0.482, 0.196, 0.0, 0.14, 0.397, 0.413, 0.457, 0.097, 0.158]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:12:02 (running for 00:19:11.51)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.328 |      0.117 |                   58 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.568 |      0.27  |                   56 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.919 |      0.244 |                   35 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.016 |      0.162 |                   30 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.18050373134328357
[2m[36m(func pid=103380)[0m top5: 0.6842350746268657
[2m[36m(func pid=103380)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=103380)[0m f1_macro: 0.11142572542416658
[2m[36m(func pid=103380)[0m f1_weighted: 0.15361916372052
[2m[36m(func pid=103380)[0m f1_per_class: [0.102, 0.088, 0.129, 0.43, 0.083, 0.024, 0.0, 0.2, 0.0, 0.059]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2966417910447761
[2m[36m(func pid=110192)[0m top5: 0.8115671641791045
[2m[36m(func pid=110192)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=110192)[0m f1_macro: 0.21875723968299415
[2m[36m(func pid=110192)[0m f1_weighted: 0.20531795397852332
[2m[36m(func pid=110192)[0m f1_per_class: [0.333, 0.005, 0.381, 0.505, 0.194, 0.379, 0.0, 0.047, 0.135, 0.208]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5422 | Steps: 2 | Val loss: 1.9332 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3033 | Steps: 2 | Val loss: 2.1724 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6520 | Steps: 2 | Val loss: 3.2398 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 10.9298 | Steps: 2 | Val loss: 18.9685 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=103390)[0m top1: 0.3787313432835821
[2m[36m(func pid=103390)[0m top5: 0.9165111940298507
[2m[36m(func pid=103390)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=103390)[0m f1_macro: 0.25593900528765334
[2m[36m(func pid=103390)[0m f1_weighted: 0.37671941415975124
[2m[36m(func pid=103390)[0m f1_per_class: [0.211, 0.136, 0.377, 0.51, 0.078, 0.25, 0.533, 0.229, 0.028, 0.208]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:12:07 (running for 00:19:16.84)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.318 |      0.111 |                   59 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.542 |      0.256 |                   57 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.652 |      0.242 |                   36 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  8.165 |      0.219 |                   31 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.16277985074626866
[2m[36m(func pid=103380)[0m top5: 0.6716417910447762
[2m[36m(func pid=103380)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=103380)[0m f1_macro: 0.10818614845121244
[2m[36m(func pid=103380)[0m f1_weighted: 0.1367123620077385
[2m[36m(func pid=103380)[0m f1_per_class: [0.158, 0.068, 0.131, 0.384, 0.083, 0.016, 0.0, 0.186, 0.0, 0.056]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.292910447761194
[2m[36m(func pid=109192)[0m top5: 0.8810634328358209
[2m[36m(func pid=109192)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=109192)[0m f1_macro: 0.24161550216643843
[2m[36m(func pid=109192)[0m f1_weighted: 0.295563420526968
[2m[36m(func pid=109192)[0m f1_per_class: [0.08, 0.499, 0.212, 0.003, 0.092, 0.367, 0.446, 0.404, 0.155, 0.158]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.29244402985074625
[2m[36m(func pid=110192)[0m top5: 0.90625
[2m[36m(func pid=110192)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=110192)[0m f1_macro: 0.19543310324491298
[2m[36m(func pid=110192)[0m f1_weighted: 0.22545055464499564
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.082, 0.235, 0.521, 0.2, 0.334, 0.012, 0.25, 0.146, 0.174]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.8364 | Steps: 2 | Val loss: 1.9796 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3145 | Steps: 2 | Val loss: 2.1868 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6982 | Steps: 2 | Val loss: 3.2297 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 11.4808 | Steps: 2 | Val loss: 9.8822 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=103390)[0m top1: 0.373134328358209
[2m[36m(func pid=103390)[0m top5: 0.9095149253731343
[2m[36m(func pid=103390)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=103390)[0m f1_macro: 0.2645654893469134
[2m[36m(func pid=103390)[0m f1_weighted: 0.3793980867785381
[2m[36m(func pid=103390)[0m f1_per_class: [0.18, 0.115, 0.4, 0.501, 0.079, 0.285, 0.531, 0.342, 0.0, 0.213]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:12:13 (running for 00:19:22.06)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.303 |      0.108 |                   60 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.836 |      0.265 |                   58 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.698 |      0.24  |                   37 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 10.93  |      0.195 |                   32 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.2574626865671642
[2m[36m(func pid=109192)[0m top5: 0.886660447761194
[2m[36m(func pid=109192)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=109192)[0m f1_macro: 0.24036976467515608
[2m[36m(func pid=109192)[0m f1_weighted: 0.3057034115174562
[2m[36m(func pid=109192)[0m f1_per_class: [0.081, 0.471, 0.234, 0.141, 0.068, 0.252, 0.413, 0.398, 0.127, 0.217]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.14878731343283583
[2m[36m(func pid=103380)[0m top5: 0.6576492537313433
[2m[36m(func pid=103380)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=103380)[0m f1_macro: 0.09808144778778194
[2m[36m(func pid=103380)[0m f1_weighted: 0.12193704254614494
[2m[36m(func pid=103380)[0m f1_per_class: [0.13, 0.036, 0.131, 0.358, 0.087, 0.008, 0.0, 0.177, 0.0, 0.054]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.40578358208955223
[2m[36m(func pid=110192)[0m top5: 0.933768656716418
[2m[36m(func pid=110192)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=110192)[0m f1_macro: 0.30868572510027914
[2m[36m(func pid=110192)[0m f1_weighted: 0.39002070679022965
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.46, 0.417, 0.245, 0.227, 0.473, 0.523, 0.356, 0.154, 0.233]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0858 | Steps: 2 | Val loss: 1.9839 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1745 | Steps: 2 | Val loss: 3.1228 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3063 | Steps: 2 | Val loss: 2.1982 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 4.2153 | Steps: 2 | Val loss: 17.3982 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=103390)[0m top1: 0.3614738805970149
[2m[36m(func pid=103390)[0m top5: 0.894589552238806
[2m[36m(func pid=103390)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=103390)[0m f1_macro: 0.27153939025819934
[2m[36m(func pid=103390)[0m f1_weighted: 0.3752450119388899
[2m[36m(func pid=103390)[0m f1_per_class: [0.176, 0.103, 0.415, 0.498, 0.073, 0.339, 0.496, 0.394, 0.0, 0.222]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.3003731343283582
[2m[36m(func pid=109192)[0m top5: 0.8913246268656716
[2m[36m(func pid=109192)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=109192)[0m f1_macro: 0.25371530698749195
[2m[36m(func pid=109192)[0m f1_weighted: 0.3476430930767932
[2m[36m(func pid=109192)[0m f1_per_class: [0.101, 0.3, 0.222, 0.476, 0.074, 0.219, 0.344, 0.424, 0.126, 0.25]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:12:18 (running for 00:19:27.32)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.306 |      0.091 |                   62 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  2.086 |      0.272 |                   59 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.175 |      0.254 |                   38 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 11.481 |      0.309 |                   33 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.13526119402985073
[2m[36m(func pid=103380)[0m top5: 0.6422574626865671
[2m[36m(func pid=103380)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=103380)[0m f1_macro: 0.0909268423405318
[2m[36m(func pid=103380)[0m f1_weighted: 0.10707477760157384
[2m[36m(func pid=103380)[0m f1_per_class: [0.133, 0.016, 0.136, 0.322, 0.088, 0.0, 0.0, 0.169, 0.0, 0.045]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.3306902985074627
[2m[36m(func pid=110192)[0m top5: 0.8563432835820896
[2m[36m(func pid=110192)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=110192)[0m f1_macro: 0.21869141122845512
[2m[36m(func pid=110192)[0m f1_weighted: 0.2503184647641638
[2m[36m(func pid=110192)[0m f1_per_class: [0.125, 0.421, 0.37, 0.0, 0.18, 0.0, 0.482, 0.451, 0.0, 0.156]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.6433 | Steps: 2 | Val loss: 2.0317 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7354 | Steps: 2 | Val loss: 2.9579 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2841 | Steps: 2 | Val loss: 2.2083 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 11.1543 | Steps: 2 | Val loss: 23.5688 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=103390)[0m top1: 0.3516791044776119
[2m[36m(func pid=103390)[0m top5: 0.8805970149253731
[2m[36m(func pid=103390)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=103390)[0m f1_macro: 0.268489463550185
[2m[36m(func pid=103390)[0m f1_weighted: 0.36810865544291155
[2m[36m(func pid=103390)[0m f1_per_class: [0.16, 0.094, 0.344, 0.489, 0.073, 0.345, 0.467, 0.479, 0.028, 0.207]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.3591417910447761
[2m[36m(func pid=109192)[0m top5: 0.8689365671641791
[2m[36m(func pid=109192)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=109192)[0m f1_macro: 0.25872427001969167
[2m[36m(func pid=109192)[0m f1_weighted: 0.3434673237972078
[2m[36m(func pid=109192)[0m f1_per_class: [0.176, 0.245, 0.237, 0.539, 0.097, 0.254, 0.28, 0.463, 0.111, 0.185]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:12:23 (running for 00:19:32.47)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.284 |      0.086 |                   63 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.643 |      0.268 |                   60 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.735 |      0.259 |                   39 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  4.215 |      0.219 |                   34 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.12453358208955224
[2m[36m(func pid=103380)[0m top5: 0.6273320895522388
[2m[36m(func pid=103380)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=103380)[0m f1_macro: 0.08649326484293166
[2m[36m(func pid=103380)[0m f1_weighted: 0.09514580210771263
[2m[36m(func pid=103380)[0m f1_per_class: [0.126, 0.005, 0.146, 0.287, 0.088, 0.0, 0.0, 0.163, 0.0, 0.049]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.16744402985074627
[2m[36m(func pid=110192)[0m top5: 0.7653917910447762
[2m[36m(func pid=110192)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=110192)[0m f1_macro: 0.16356608422145416
[2m[36m(func pid=110192)[0m f1_weighted: 0.10606827569088591
[2m[36m(func pid=110192)[0m f1_per_class: [0.333, 0.4, 0.383, 0.0, 0.135, 0.0, 0.049, 0.182, 0.0, 0.152]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4938 | Steps: 2 | Val loss: 2.0910 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.0335 | Steps: 2 | Val loss: 2.7050 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2885 | Steps: 2 | Val loss: 2.2202 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 15.9856 | Steps: 2 | Val loss: 22.9355 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=103390)[0m top1: 0.3269589552238806
[2m[36m(func pid=103390)[0m top5: 0.8689365671641791
[2m[36m(func pid=103390)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=103390)[0m f1_macro: 0.25279224954140256
[2m[36m(func pid=103390)[0m f1_weighted: 0.34227271401954285
[2m[36m(func pid=103390)[0m f1_per_class: [0.154, 0.115, 0.282, 0.485, 0.071, 0.353, 0.376, 0.45, 0.027, 0.214]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.40671641791044777
[2m[36m(func pid=109192)[0m top5: 0.8698694029850746
[2m[36m(func pid=109192)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=109192)[0m f1_macro: 0.2913515523128729
[2m[36m(func pid=109192)[0m f1_weighted: 0.38747897333322934
[2m[36m(func pid=109192)[0m f1_per_class: [0.229, 0.391, 0.244, 0.549, 0.143, 0.342, 0.299, 0.471, 0.072, 0.174]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.11520522388059702
[2m[36m(func pid=103380)[0m top5: 0.6194029850746269
[2m[36m(func pid=103380)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=103380)[0m f1_macro: 0.08188981340400764
[2m[36m(func pid=103380)[0m f1_weighted: 0.08314680305589342
[2m[36m(func pid=103380)[0m f1_per_class: [0.118, 0.0, 0.153, 0.248, 0.088, 0.0, 0.0, 0.16, 0.0, 0.051]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:12:28 (running for 00:19:37.78)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.289 |      0.082 |                   64 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.494 |      0.253 |                   61 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.033 |      0.291 |                   40 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 15.986 |      0.165 |                   36 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=110192)[0m top1: 0.17257462686567165
[2m[36m(func pid=110192)[0m top5: 0.75
[2m[36m(func pid=110192)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=110192)[0m f1_macro: 0.16549688058706316
[2m[36m(func pid=110192)[0m f1_weighted: 0.13406975761897832
[2m[36m(func pid=110192)[0m f1_per_class: [0.323, 0.313, 0.407, 0.0, 0.09, 0.0, 0.187, 0.231, 0.0, 0.103]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.3896 | Steps: 2 | Val loss: 2.1336 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5660 | Steps: 2 | Val loss: 2.4059 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3002 | Steps: 2 | Val loss: 2.2315 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 13.6802 | Steps: 2 | Val loss: 20.8660 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=103390)[0m top1: 0.28777985074626866
[2m[36m(func pid=103390)[0m top5: 0.8628731343283582
[2m[36m(func pid=103390)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=103390)[0m f1_macro: 0.23746403565862523
[2m[36m(func pid=103390)[0m f1_weighted: 0.29287954041887904
[2m[36m(func pid=103390)[0m f1_per_class: [0.146, 0.162, 0.262, 0.473, 0.071, 0.357, 0.197, 0.391, 0.098, 0.219]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m top1: 0.10494402985074627
[2m[36m(func pid=103380)[0m top5: 0.6142723880597015
[2m[36m(func pid=103380)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=103380)[0m f1_macro: 0.07783979002529746
[2m[36m(func pid=103380)[0m f1_weighted: 0.0686148799633332
[2m[36m(func pid=103380)[0m f1_per_class: [0.116, 0.0, 0.166, 0.197, 0.086, 0.0, 0.0, 0.157, 0.0, 0.057]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.42257462686567165
[2m[36m(func pid=109192)[0m top5: 0.8843283582089553
[2m[36m(func pid=109192)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=109192)[0m f1_macro: 0.32513742184863537
[2m[36m(func pid=109192)[0m f1_weighted: 0.42559230737209686
[2m[36m(func pid=109192)[0m f1_per_class: [0.269, 0.541, 0.297, 0.435, 0.149, 0.422, 0.418, 0.389, 0.159, 0.171]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:12:34 (running for 00:19:42.93)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.3   |      0.078 |                   65 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.39  |      0.237 |                   62 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.566 |      0.325 |                   41 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 13.68  |      0.212 |                   37 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=110192)[0m top1: 0.27705223880597013
[2m[36m(func pid=110192)[0m top5: 0.8591417910447762
[2m[36m(func pid=110192)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=110192)[0m f1_macro: 0.21238826546679285
[2m[36m(func pid=110192)[0m f1_weighted: 0.27026149434698293
[2m[36m(func pid=110192)[0m f1_per_class: [0.179, 0.268, 0.393, 0.11, 0.07, 0.0, 0.531, 0.477, 0.0, 0.096]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.3405 | Steps: 2 | Val loss: 2.1813 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2422 | Steps: 2 | Val loss: 2.2410 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5056 | Steps: 2 | Val loss: 2.6451 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 6.6357 | Steps: 2 | Val loss: 20.7512 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=103390)[0m top1: 0.2677238805970149
[2m[36m(func pid=103390)[0m top5: 0.8526119402985075
[2m[36m(func pid=103390)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=103390)[0m f1_macro: 0.2268510658428728
[2m[36m(func pid=103390)[0m f1_weighted: 0.25957690935892286
[2m[36m(func pid=103390)[0m f1_per_class: [0.145, 0.211, 0.247, 0.457, 0.072, 0.381, 0.067, 0.351, 0.136, 0.2]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m top1: 0.09888059701492537
[2m[36m(func pid=103380)[0m top5: 0.6124067164179104
[2m[36m(func pid=103380)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=103380)[0m f1_macro: 0.07491106928666033
[2m[36m(func pid=103380)[0m f1_weighted: 0.059001910879198226
[2m[36m(func pid=103380)[0m f1_per_class: [0.108, 0.0, 0.179, 0.162, 0.086, 0.0, 0.0, 0.158, 0.0, 0.056]
[2m[36m(func pid=103380)[0m 
== Status ==
Current time: 2024-01-07 00:12:39 (running for 00:19:47.94)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.242 |      0.075 |                   66 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.34  |      0.227 |                   63 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.506 |      0.281 |                   42 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 13.68  |      0.212 |                   37 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.3605410447761194
[2m[36m(func pid=109192)[0m top5: 0.8815298507462687
[2m[36m(func pid=109192)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=109192)[0m f1_macro: 0.2812336728395142
[2m[36m(func pid=109192)[0m f1_weighted: 0.3087510229466125
[2m[36m(func pid=109192)[0m f1_per_class: [0.271, 0.453, 0.328, 0.035, 0.188, 0.442, 0.456, 0.34, 0.113, 0.185]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2677238805970149
[2m[36m(func pid=110192)[0m top5: 0.8941231343283582
[2m[36m(func pid=110192)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=110192)[0m f1_macro: 0.1906836116186598
[2m[36m(func pid=110192)[0m f1_weighted: 0.2984710554458727
[2m[36m(func pid=110192)[0m f1_per_class: [0.126, 0.305, 0.386, 0.298, 0.059, 0.0, 0.501, 0.12, 0.0, 0.113]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3713 | Steps: 2 | Val loss: 2.2539 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2794 | Steps: 2 | Val loss: 2.2529 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7602 | Steps: 2 | Val loss: 2.6996 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.1301 | Steps: 2 | Val loss: 22.2302 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=103390)[0m top1: 0.22994402985074627
[2m[36m(func pid=103390)[0m top5: 0.8446828358208955
[2m[36m(func pid=103390)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=103390)[0m f1_macro: 0.20180944679198687
[2m[36m(func pid=103390)[0m f1_weighted: 0.21610945849148533
[2m[36m(func pid=103390)[0m f1_per_class: [0.132, 0.259, 0.216, 0.353, 0.074, 0.382, 0.003, 0.325, 0.096, 0.178]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:12:44 (running for 00:19:52.94)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.279 |      0.072 |                   67 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.371 |      0.202 |                   64 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.506 |      0.281 |                   42 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  6.636 |      0.191 |                   38 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.09328358208955224
[2m[36m(func pid=103380)[0m top5: 0.6110074626865671
[2m[36m(func pid=103380)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=103380)[0m f1_macro: 0.07182946279780736
[2m[36m(func pid=103380)[0m f1_weighted: 0.05083763829289514
[2m[36m(func pid=103380)[0m f1_per_class: [0.099, 0.0, 0.188, 0.133, 0.08, 0.0, 0.0, 0.159, 0.0, 0.059]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.3582089552238806
[2m[36m(func pid=109192)[0m top5: 0.8647388059701493
[2m[36m(func pid=109192)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=109192)[0m f1_macro: 0.2877690623791034
[2m[36m(func pid=109192)[0m f1_weighted: 0.3045970761190052
[2m[36m(func pid=109192)[0m f1_per_class: [0.279, 0.447, 0.364, 0.01, 0.206, 0.435, 0.47, 0.352, 0.08, 0.235]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.28171641791044777
[2m[36m(func pid=110192)[0m top5: 0.8544776119402985
[2m[36m(func pid=110192)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=110192)[0m f1_macro: 0.2355665271273574
[2m[36m(func pid=110192)[0m f1_weighted: 0.314977353541063
[2m[36m(func pid=110192)[0m f1_per_class: [0.129, 0.468, 0.361, 0.391, 0.083, 0.317, 0.25, 0.062, 0.094, 0.2]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.9794 | Steps: 2 | Val loss: 2.3339 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.3428 | Steps: 2 | Val loss: 2.2564 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.5873 | Steps: 2 | Val loss: 2.5139 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.0790 | Steps: 2 | Val loss: 30.0012 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:12:49 (running for 00:19:58.06)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.343 |      0.073 |                   68 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.371 |      0.202 |                   64 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.76  |      0.288 |                   43 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.13  |      0.236 |                   39 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.08955223880597014
[2m[36m(func pid=103380)[0m top5: 0.6147388059701493
[2m[36m(func pid=103380)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=103380)[0m f1_macro: 0.0725346958172561
[2m[36m(func pid=103380)[0m f1_weighted: 0.043492084429475636
[2m[36m(func pid=103380)[0m f1_per_class: [0.091, 0.0, 0.214, 0.105, 0.074, 0.0, 0.0, 0.164, 0.0, 0.077]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=103390)[0m top1: 0.20289179104477612
[2m[36m(func pid=103390)[0m top5: 0.8348880597014925
[2m[36m(func pid=103390)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=103390)[0m f1_macro: 0.18696830042849405
[2m[36m(func pid=103390)[0m f1_weighted: 0.18067959858379187
[2m[36m(func pid=103390)[0m f1_per_class: [0.129, 0.297, 0.191, 0.2, 0.077, 0.387, 0.003, 0.327, 0.108, 0.15]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.3306902985074627
[2m[36m(func pid=109192)[0m top5: 0.8582089552238806
[2m[36m(func pid=109192)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=109192)[0m f1_macro: 0.285241279544177
[2m[36m(func pid=109192)[0m f1_weighted: 0.3322271921676705
[2m[36m(func pid=109192)[0m f1_per_class: [0.258, 0.407, 0.392, 0.172, 0.119, 0.412, 0.439, 0.398, 0.078, 0.176]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.25419776119402987
[2m[36m(func pid=110192)[0m top5: 0.7341417910447762
[2m[36m(func pid=110192)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=110192)[0m f1_macro: 0.22671573573974638
[2m[36m(func pid=110192)[0m f1_weighted: 0.24005427605153457
[2m[36m(func pid=110192)[0m f1_per_class: [0.16, 0.517, 0.361, 0.326, 0.187, 0.421, 0.0, 0.0, 0.081, 0.214]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2343 | Steps: 2 | Val loss: 2.2600 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5364 | Steps: 2 | Val loss: 2.7772 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.3291 | Steps: 2 | Val loss: 2.3477 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 10.5222 | Steps: 2 | Val loss: 33.9243 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:12:54 (running for 00:20:03.24)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.234 |      0.077 |                   69 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.979 |      0.187 |                   65 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  1.587 |      0.285 |                   44 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  3.079 |      0.227 |                   40 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.08908582089552239
[2m[36m(func pid=103380)[0m top5: 0.6180037313432836
[2m[36m(func pid=103380)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=103380)[0m f1_macro: 0.07685707429280407
[2m[36m(func pid=103380)[0m f1_weighted: 0.03880263162666578
[2m[36m(func pid=103380)[0m f1_per_class: [0.123, 0.0, 0.247, 0.085, 0.072, 0.0, 0.0, 0.166, 0.0, 0.075]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m top1: 0.29151119402985076
[2m[36m(func pid=109192)[0m top5: 0.8451492537313433
[2m[36m(func pid=109192)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=109192)[0m f1_macro: 0.2595681831255285
[2m[36m(func pid=109192)[0m f1_weighted: 0.3195863536821009
[2m[36m(func pid=109192)[0m f1_per_class: [0.231, 0.118, 0.353, 0.329, 0.075, 0.385, 0.418, 0.463, 0.093, 0.13]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2873134328358209
[2m[36m(func pid=110192)[0m top5: 0.6819029850746269
[2m[36m(func pid=110192)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=110192)[0m f1_macro: 0.2162646542424646
[2m[36m(func pid=110192)[0m f1_weighted: 0.25607995528692834
[2m[36m(func pid=110192)[0m f1_per_class: [0.137, 0.516, 0.386, 0.398, 0.25, 0.412, 0.0, 0.0, 0.063, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.18983208955223882
[2m[36m(func pid=103390)[0m top5: 0.8316231343283582
[2m[36m(func pid=103390)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=103390)[0m f1_macro: 0.17852761383196258
[2m[36m(func pid=103390)[0m f1_weighted: 0.14811010403168162
[2m[36m(func pid=103390)[0m f1_per_class: [0.146, 0.312, 0.2, 0.068, 0.079, 0.389, 0.003, 0.349, 0.104, 0.134]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7052 | Steps: 2 | Val loss: 3.0648 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2121 | Steps: 2 | Val loss: 2.2663 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 12.4034 | Steps: 2 | Val loss: 34.8041 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.2602 | Steps: 2 | Val loss: 2.3811 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:12:59 (running for 00:20:08.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.234 |      0.077 |                   69 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.329 |      0.179 |                   66 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.705 |      0.248 |                   46 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 10.522 |      0.216 |                   41 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.2751865671641791
[2m[36m(func pid=109192)[0m top5: 0.8199626865671642
[2m[36m(func pid=109192)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=109192)[0m f1_macro: 0.24798767024169513
[2m[36m(func pid=109192)[0m f1_weighted: 0.30532188801007415
[2m[36m(func pid=109192)[0m f1_per_class: [0.246, 0.016, 0.367, 0.368, 0.055, 0.34, 0.411, 0.45, 0.101, 0.126]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.08861940298507463
[2m[36m(func pid=103380)[0m top5: 0.6156716417910447
[2m[36m(func pid=103380)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=103380)[0m f1_macro: 0.07764907342420796
[2m[36m(func pid=103380)[0m f1_weighted: 0.036782204002025774
[2m[36m(func pid=103380)[0m f1_per_class: [0.121, 0.0, 0.262, 0.077, 0.068, 0.0, 0.0, 0.172, 0.0, 0.076]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2966417910447761
[2m[36m(func pid=110192)[0m top5: 0.6823694029850746
[2m[36m(func pid=110192)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=110192)[0m f1_macro: 0.20544894192398339
[2m[36m(func pid=110192)[0m f1_weighted: 0.2532904141781325
[2m[36m(func pid=110192)[0m f1_per_class: [0.124, 0.487, 0.393, 0.411, 0.194, 0.414, 0.0, 0.0, 0.032, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.19169776119402984
[2m[36m(func pid=103390)[0m top5: 0.8227611940298507
[2m[36m(func pid=103390)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=103390)[0m f1_macro: 0.1811534235427291
[2m[36m(func pid=103390)[0m f1_weighted: 0.14016182702074317
[2m[36m(func pid=103390)[0m f1_per_class: [0.169, 0.325, 0.2, 0.023, 0.084, 0.39, 0.003, 0.378, 0.11, 0.129]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2752 | Steps: 2 | Val loss: 2.2767 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8178 | Steps: 2 | Val loss: 3.2517 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 13.6454 | Steps: 2 | Val loss: 31.7238 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.3093 | Steps: 2 | Val loss: 2.3941 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=109192)[0m top1: 0.279384328358209
[2m[36m(func pid=109192)[0m top5: 0.7994402985074627
[2m[36m(func pid=109192)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=109192)[0m f1_macro: 0.24337831314633349
[2m[36m(func pid=109192)[0m f1_weighted: 0.30252413554045615
[2m[36m(func pid=109192)[0m f1_per_class: [0.265, 0.0, 0.36, 0.429, 0.05, 0.227, 0.395, 0.45, 0.106, 0.152]
== Status ==
Current time: 2024-01-07 00:13:04 (running for 00:20:13.46)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.212 |      0.078 |                   70 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.26  |      0.181 |                   67 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.818 |      0.243 |                   47 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.403 |      0.205 |                   42 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.08582089552238806
[2m[36m(func pid=103380)[0m top5: 0.6147388059701493
[2m[36m(func pid=103380)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=103380)[0m f1_macro: 0.07896394133702045
[2m[36m(func pid=103380)[0m f1_weighted: 0.029554372564516693
[2m[36m(func pid=103380)[0m f1_per_class: [0.125, 0.0, 0.286, 0.048, 0.065, 0.0, 0.0, 0.177, 0.0, 0.088]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2980410447761194
[2m[36m(func pid=110192)[0m top5: 0.7439365671641791
[2m[36m(func pid=110192)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=110192)[0m f1_macro: 0.21046386305956322
[2m[36m(func pid=110192)[0m f1_weighted: 0.2619651323909296
[2m[36m(func pid=110192)[0m f1_per_class: [0.11, 0.489, 0.344, 0.402, 0.226, 0.421, 0.024, 0.062, 0.026, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.19776119402985073
[2m[36m(func pid=103390)[0m top5: 0.8176305970149254
[2m[36m(func pid=103390)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=103390)[0m f1_macro: 0.1895232154217501
[2m[36m(func pid=103390)[0m f1_weighted: 0.14042352726065355
[2m[36m(func pid=103390)[0m f1_per_class: [0.196, 0.334, 0.204, 0.003, 0.089, 0.392, 0.006, 0.409, 0.128, 0.134]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2403 | Steps: 2 | Val loss: 2.2795 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7809 | Steps: 2 | Val loss: 3.2694 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 7.9111 | Steps: 2 | Val loss: 27.2204 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.2589 | Steps: 2 | Val loss: 2.4079 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:13:09 (running for 00:20:18.58)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.275 |      0.079 |                   71 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.309 |      0.19  |                   68 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.781 |      0.245 |                   48 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 13.645 |      0.21  |                   43 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.29990671641791045
[2m[36m(func pid=109192)[0m top5: 0.7845149253731343
[2m[36m(func pid=109192)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=109192)[0m f1_macro: 0.2451994316871146
[2m[36m(func pid=109192)[0m f1_weighted: 0.30741049479702826
[2m[36m(func pid=109192)[0m f1_per_class: [0.268, 0.0, 0.327, 0.488, 0.055, 0.246, 0.353, 0.418, 0.113, 0.185]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.08722014925373134
[2m[36m(func pid=103380)[0m top5: 0.6156716417910447
[2m[36m(func pid=103380)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=103380)[0m f1_macro: 0.08209910686414515
[2m[36m(func pid=103380)[0m f1_weighted: 0.03105984606682821
[2m[36m(func pid=103380)[0m f1_per_class: [0.127, 0.0, 0.31, 0.052, 0.061, 0.0, 0.0, 0.185, 0.0, 0.087]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2737873134328358
[2m[36m(func pid=110192)[0m top5: 0.8703358208955224
[2m[36m(func pid=110192)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=110192)[0m f1_macro: 0.21581741459767648
[2m[36m(func pid=110192)[0m f1_weighted: 0.2893880332062675
[2m[36m(func pid=110192)[0m f1_per_class: [0.08, 0.531, 0.286, 0.221, 0.126, 0.375, 0.273, 0.092, 0.027, 0.148]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.19869402985074627
[2m[36m(func pid=103390)[0m top5: 0.8166977611940298
[2m[36m(func pid=103390)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=103390)[0m f1_macro: 0.19265084188702683
[2m[36m(func pid=103390)[0m f1_weighted: 0.14325345141338897
[2m[36m(func pid=103390)[0m f1_per_class: [0.209, 0.342, 0.21, 0.0, 0.09, 0.37, 0.016, 0.445, 0.124, 0.121]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.8889 | Steps: 2 | Val loss: 3.1211 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2648 | Steps: 2 | Val loss: 2.2850 | Batch size: 32 | lr: 0.0001 | Duration: 2.61s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.5197 | Steps: 2 | Val loss: 27.8769 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2876 | Steps: 2 | Val loss: 2.4015 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=109192)[0m top1: 0.3353544776119403
[2m[36m(func pid=109192)[0m top5: 0.7943097014925373
[2m[36m(func pid=109192)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=109192)[0m f1_macro: 0.25787519137257625
[2m[36m(func pid=109192)[0m f1_weighted: 0.3265559487115491
[2m[36m(func pid=109192)[0m f1_per_class: [0.251, 0.0, 0.34, 0.539, 0.072, 0.314, 0.347, 0.401, 0.115, 0.2]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:13:14 (running for 00:20:23.63)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.24  |      0.082 |                   72 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.259 |      0.193 |                   69 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.889 |      0.258 |                   49 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  7.911 |      0.216 |                   44 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=103380)[0m top1: 0.08582089552238806
[2m[36m(func pid=103380)[0m top5: 0.6198694029850746
[2m[36m(func pid=103380)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=103380)[0m f1_macro: 0.08336911168826015
[2m[36m(func pid=103380)[0m f1_weighted: 0.028918263863940235
[2m[36m(func pid=103380)[0m f1_per_class: [0.121, 0.0, 0.344, 0.042, 0.057, 0.0, 0.0, 0.195, 0.0, 0.074]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2905783582089552
[2m[36m(func pid=110192)[0m top5: 0.8666044776119403
[2m[36m(func pid=110192)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=110192)[0m f1_macro: 0.18710209374906608
[2m[36m(func pid=110192)[0m f1_weighted: 0.27665845790483834
[2m[36m(func pid=110192)[0m f1_per_class: [0.067, 0.54, 0.204, 0.013, 0.084, 0.048, 0.522, 0.239, 0.0, 0.154]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.20382462686567165
[2m[36m(func pid=103390)[0m top5: 0.8227611940298507
[2m[36m(func pid=103390)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=103390)[0m f1_macro: 0.19693025243410484
[2m[36m(func pid=103390)[0m f1_weighted: 0.15615448295055903
[2m[36m(func pid=103390)[0m f1_per_class: [0.193, 0.344, 0.218, 0.0, 0.094, 0.371, 0.054, 0.476, 0.109, 0.111]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6620 | Steps: 2 | Val loss: 2.8506 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2026 | Steps: 2 | Val loss: 2.2881 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1117 | Steps: 2 | Val loss: 31.8130 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3434 | Steps: 2 | Val loss: 2.3550 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 00:13:19 (running for 00:20:28.63)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.265 |      0.083 |                   73 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.288 |      0.197 |                   70 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.662 |      0.272 |                   50 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.52  |      0.187 |                   45 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.365205223880597
[2m[36m(func pid=109192)[0m top5: 0.8264925373134329
[2m[36m(func pid=109192)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=109192)[0m f1_macro: 0.2719541765578776
[2m[36m(func pid=109192)[0m f1_weighted: 0.34784113284027013
[2m[36m(func pid=109192)[0m f1_per_class: [0.229, 0.005, 0.364, 0.548, 0.104, 0.395, 0.374, 0.428, 0.089, 0.185]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.08582089552238806
[2m[36m(func pid=103380)[0m top5: 0.6226679104477612
[2m[36m(func pid=103380)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=103380)[0m f1_macro: 0.08776291786807178
[2m[36m(func pid=103380)[0m f1_weighted: 0.029687189525360473
[2m[36m(func pid=103380)[0m f1_per_class: [0.113, 0.0, 0.379, 0.042, 0.054, 0.0, 0.0, 0.206, 0.0, 0.083]
[2m[36m(func pid=103380)[0m 
[2m[36m(func pid=110192)[0m top1: 0.26865671641791045
[2m[36m(func pid=110192)[0m top5: 0.7047574626865671
[2m[36m(func pid=110192)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=110192)[0m f1_macro: 0.18795384833802903
[2m[36m(func pid=110192)[0m f1_weighted: 0.25826578680366297
[2m[36m(func pid=110192)[0m f1_per_class: [0.138, 0.382, 0.174, 0.0, 0.076, 0.0, 0.536, 0.455, 0.036, 0.084]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.21315298507462688
[2m[36m(func pid=103390)[0m top5: 0.8269589552238806
[2m[36m(func pid=103390)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=103390)[0m f1_macro: 0.2086917066206735
[2m[36m(func pid=103390)[0m f1_weighted: 0.17421295747014387
[2m[36m(func pid=103390)[0m f1_per_class: [0.205, 0.337, 0.244, 0.0, 0.092, 0.379, 0.112, 0.485, 0.112, 0.12]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5748 | Steps: 2 | Val loss: 2.5468 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=103380)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2246 | Steps: 2 | Val loss: 2.2938 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.9104 | Steps: 2 | Val loss: 42.3688 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.2171 | Steps: 2 | Val loss: 2.3243 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 00:13:24 (running for 00:20:33.82)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2515
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00008 | RUNNING    | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.203 |      0.088 |                   74 |
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.343 |      0.209 |                   71 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.575 |      0.303 |                   51 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  1.112 |      0.188 |                   46 |
| train_51d3e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109192)[0m top1: 0.39738805970149255
[2m[36m(func pid=109192)[0m top5: 0.8596082089552238
[2m[36m(func pid=109192)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=109192)[0m f1_macro: 0.3026270406439295
[2m[36m(func pid=109192)[0m f1_weighted: 0.3812970306666795
[2m[36m(func pid=109192)[0m f1_per_class: [0.211, 0.053, 0.364, 0.542, 0.161, 0.461, 0.427, 0.463, 0.119, 0.225]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103380)[0m top1: 0.08488805970149253
[2m[36m(func pid=103380)[0m top5: 0.6282649253731343
[2m[36m(func pid=103380)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=103380)[0m f1_macro: 0.08804102527223498
[2m[36m(func pid=103380)[0m f1_weighted: 0.02857136452000646
[2m[36m(func pid=103380)[0m f1_per_class: [0.106, 0.0, 0.379, 0.036, 0.05, 0.0, 0.0, 0.219, 0.0, 0.089]
[2m[36m(func pid=110192)[0m top1: 0.1333955223880597
[2m[36m(func pid=110192)[0m top5: 0.6319962686567164
[2m[36m(func pid=110192)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=110192)[0m f1_macro: 0.1119357862712573
[2m[36m(func pid=110192)[0m f1_weighted: 0.12115497991903237
[2m[36m(func pid=110192)[0m f1_per_class: [0.044, 0.149, 0.19, 0.0, 0.08, 0.0, 0.239, 0.354, 0.022, 0.041]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.23041044776119404
[2m[36m(func pid=103390)[0m top5: 0.8330223880597015
[2m[36m(func pid=103390)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=103390)[0m f1_macro: 0.2235284456633158
[2m[36m(func pid=103390)[0m f1_weighted: 0.21111718057351192
[2m[36m(func pid=103390)[0m f1_per_class: [0.209, 0.334, 0.259, 0.003, 0.089, 0.372, 0.232, 0.508, 0.114, 0.115]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.8580 | Steps: 2 | Val loss: 2.2942 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 12.6404 | Steps: 2 | Val loss: 46.9404 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.2697 | Steps: 2 | Val loss: 2.2877 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=109192)[0m top1: 0.4230410447761194
[2m[36m(func pid=109192)[0m top5: 0.8936567164179104
[2m[36m(func pid=109192)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=109192)[0m f1_macro: 0.3403255564931823
[2m[36m(func pid=109192)[0m f1_weighted: 0.4337260635230539
[2m[36m(func pid=109192)[0m f1_per_class: [0.193, 0.343, 0.348, 0.515, 0.242, 0.476, 0.46, 0.419, 0.127, 0.281]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.08908582089552239
[2m[36m(func pid=110192)[0m top5: 0.636660447761194
[2m[36m(func pid=110192)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=110192)[0m f1_macro: 0.07393530441856944
[2m[36m(func pid=110192)[0m f1_weighted: 0.04170913700613607
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.13, 0.198, 0.0, 0.086, 0.0, 0.009, 0.22, 0.044, 0.052]
[2m[36m(func pid=103390)[0m top1: 0.23973880597014927
[2m[36m(func pid=103390)[0m top5: 0.8428171641791045
[2m[36m(func pid=103390)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=103390)[0m f1_macro: 0.22971092700840373
[2m[36m(func pid=103390)[0m f1_weighted: 0.2313977223700029
[2m[36m(func pid=103390)[0m f1_per_class: [0.215, 0.335, 0.278, 0.023, 0.086, 0.363, 0.291, 0.475, 0.108, 0.124]
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.0583 | Steps: 2 | Val loss: 2.2308 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:13:30 (running for 00:20:38.95)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.217 |      0.224 |                   72 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.858 |      0.34  |                   52 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  3.91  |      0.112 |                   47 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=120862)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=120862)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=120862)[0m Configuration completed!
[2m[36m(func pid=120862)[0m New optimizer parameters:
[2m[36m(func pid=120862)[0m SGD (
[2m[36m(func pid=120862)[0m Parameter Group 0
[2m[36m(func pid=120862)[0m     dampening: 0
[2m[36m(func pid=120862)[0m     differentiable: False
[2m[36m(func pid=120862)[0m     foreach: None
[2m[36m(func pid=120862)[0m     lr: 0.0001
[2m[36m(func pid=120862)[0m     maximize: False
[2m[36m(func pid=120862)[0m     momentum: 0.9
[2m[36m(func pid=120862)[0m     nesterov: False
[2m[36m(func pid=120862)[0m     weight_decay: 0.0001
[2m[36m(func pid=120862)[0m )
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:13:35 (running for 00:20:44.25)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.27  |      0.23  |                   73 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  2.058 |      0.352 |                   53 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.64  |      0.074 |                   48 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.439365671641791
[2m[36m(func pid=109192)[0m top5: 0.9048507462686567
[2m[36m(func pid=109192)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=109192)[0m f1_macro: 0.3517468514312668
[2m[36m(func pid=109192)[0m f1_weighted: 0.4482239146556486
[2m[36m(func pid=109192)[0m f1_per_class: [0.22, 0.521, 0.357, 0.455, 0.269, 0.474, 0.478, 0.31, 0.146, 0.286]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.7596 | Steps: 2 | Val loss: 2.2679 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 18.0408 | Steps: 2 | Val loss: 36.3140 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3279 | Steps: 2 | Val loss: 2.3675 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0902 | Steps: 2 | Val loss: 2.3718 | Batch size: 32 | lr: 0.0001 | Duration: 4.49s
[2m[36m(func pid=103390)[0m top1: 0.24860074626865672
[2m[36m(func pid=103390)[0m top5: 0.8521455223880597
[2m[36m(func pid=103390)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=103390)[0m f1_macro: 0.23274263504003448
[2m[36m(func pid=103390)[0m f1_weighted: 0.25321831243290244
[2m[36m(func pid=103390)[0m f1_per_class: [0.201, 0.337, 0.297, 0.084, 0.082, 0.356, 0.318, 0.433, 0.103, 0.117]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.1837686567164179
[2m[36m(func pid=110192)[0m top5: 0.6721082089552238
[2m[36m(func pid=110192)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=110192)[0m f1_macro: 0.14153725999229583
[2m[36m(func pid=110192)[0m f1_weighted: 0.15980630881156335
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.418, 0.185, 0.0, 0.092, 0.0, 0.221, 0.273, 0.09, 0.138]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.42257462686567165
[2m[36m(func pid=109192)[0m top5: 0.9090485074626866
[2m[36m(func pid=109192)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=109192)[0m f1_macro: 0.3207482588056988
[2m[36m(func pid=109192)[0m f1_weighted: 0.41062211932921916
[2m[36m(func pid=109192)[0m f1_per_class: [0.264, 0.521, 0.333, 0.317, 0.216, 0.467, 0.498, 0.253, 0.137, 0.2]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:13:40 (running for 00:20:49.48)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.76  |      0.233 |                   74 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.328 |      0.321 |                   54 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 18.041 |      0.142 |                   49 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.06716417910447761
[2m[36m(func pid=120862)[0m top5: 0.3894589552238806
[2m[36m(func pid=120862)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=120862)[0m f1_macro: 0.017415388843449293
[2m[36m(func pid=120862)[0m f1_weighted: 0.022012285030203683
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.005, 0.0, 0.051, 0.0, 0.0, 0.0, 0.118, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 5.0990 | Steps: 2 | Val loss: 26.3088 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.2905 | Steps: 2 | Val loss: 2.2114 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3912 | Steps: 2 | Val loss: 2.6532 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0925 | Steps: 2 | Val loss: 2.3279 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=110192)[0m top1: 0.34281716417910446
[2m[36m(func pid=110192)[0m top5: 0.8362873134328358
[2m[36m(func pid=110192)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=110192)[0m f1_macro: 0.24250179687442014
[2m[36m(func pid=110192)[0m f1_weighted: 0.3106813924459034
[2m[36m(func pid=110192)[0m f1_per_class: [0.085, 0.552, 0.204, 0.0, 0.138, 0.098, 0.568, 0.418, 0.124, 0.238]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.2751865671641791
[2m[36m(func pid=103390)[0m top5: 0.8717350746268657
[2m[36m(func pid=103390)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=103390)[0m f1_macro: 0.25104713419731517
[2m[36m(func pid=103390)[0m f1_weighted: 0.2996462313277786
[2m[36m(func pid=103390)[0m f1_per_class: [0.186, 0.358, 0.344, 0.199, 0.079, 0.359, 0.361, 0.386, 0.111, 0.129]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:13:45 (running for 00:20:54.71)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.29  |      0.251 |                   75 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.391 |      0.278 |                   55 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  5.099 |      0.243 |                   50 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  3.09  |      0.017 |                    1 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.386660447761194
[2m[36m(func pid=109192)[0m top5: 0.8950559701492538
[2m[36m(func pid=109192)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=109192)[0m f1_macro: 0.278464549769169
[2m[36m(func pid=109192)[0m f1_weighted: 0.3573893340883556
[2m[36m(func pid=109192)[0m f1_per_class: [0.103, 0.481, 0.353, 0.194, 0.146, 0.453, 0.484, 0.199, 0.155, 0.216]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.14085820895522388
[2m[36m(func pid=120862)[0m top5: 0.5298507462686567
[2m[36m(func pid=120862)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=120862)[0m f1_macro: 0.04188813017055637
[2m[36m(func pid=120862)[0m f1_weighted: 0.07924612513963185
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.0, 0.0, 0.249, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.1512 | Steps: 2 | Val loss: 2.1578 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.8665 | Steps: 2 | Val loss: 18.8730 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3974 | Steps: 2 | Val loss: 2.8836 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1184 | Steps: 2 | Val loss: 2.3177 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=103390)[0m top1: 0.31343283582089554
[2m[36m(func pid=103390)[0m top5: 0.8871268656716418
[2m[36m(func pid=103390)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=103390)[0m f1_macro: 0.27630210705690617
[2m[36m(func pid=103390)[0m f1_weighted: 0.3527384528023654
[2m[36m(func pid=103390)[0m f1_per_class: [0.188, 0.37, 0.379, 0.336, 0.077, 0.359, 0.404, 0.362, 0.134, 0.152]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.38759328358208955
[2m[36m(func pid=110192)[0m top5: 0.914179104477612
[2m[36m(func pid=110192)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=110192)[0m f1_macro: 0.2817730175638446
[2m[36m(func pid=110192)[0m f1_weighted: 0.3693015580195537
[2m[36m(func pid=110192)[0m f1_per_class: [0.214, 0.563, 0.262, 0.125, 0.215, 0.425, 0.565, 0.119, 0.115, 0.214]
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:13:50 (running for 00:20:59.84)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.151 |      0.276 |                   76 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.397 |      0.277 |                   56 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.866 |      0.282 |                   51 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  3.092 |      0.042 |                    2 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.37453358208955223
[2m[36m(func pid=109192)[0m top5: 0.8838619402985075
[2m[36m(func pid=109192)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=109192)[0m f1_macro: 0.2768927984344844
[2m[36m(func pid=109192)[0m f1_weighted: 0.3473892013305886
[2m[36m(func pid=109192)[0m f1_per_class: [0.077, 0.472, 0.353, 0.179, 0.174, 0.41, 0.473, 0.26, 0.185, 0.186]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.1791044776119403
[2m[36m(func pid=120862)[0m top5: 0.5671641791044776
[2m[36m(func pid=120862)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=120862)[0m f1_macro: 0.046820868910030636
[2m[36m(func pid=120862)[0m f1_weighted: 0.09598244726314628
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.0, 0.0, 0.312, 0.0, 0.0, 0.0, 0.157, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.4986 | Steps: 2 | Val loss: 2.0954 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.9464 | Steps: 2 | Val loss: 16.5526 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5913 | Steps: 2 | Val loss: 2.8844 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0359 | Steps: 2 | Val loss: 2.3160 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=103390)[0m top1: 0.34375
[2m[36m(func pid=103390)[0m top5: 0.9015858208955224
[2m[36m(func pid=103390)[0m f1_micro: 0.34375
[2m[36m(func pid=103390)[0m f1_macro: 0.28528389518318653
[2m[36m(func pid=103390)[0m f1_weighted: 0.38753740809000725
[2m[36m(func pid=103390)[0m f1_per_class: [0.184, 0.354, 0.353, 0.429, 0.075, 0.366, 0.445, 0.325, 0.151, 0.17]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m top1: 0.43097014925373134
[2m[36m(func pid=110192)[0m top5: 0.9468283582089553
[2m[36m(func pid=110192)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=110192)[0m f1_macro: 0.32874607464369915
[2m[36m(func pid=110192)[0m f1_weighted: 0.43369441435591843
[2m[36m(func pid=110192)[0m f1_per_class: [0.331, 0.558, 0.407, 0.43, 0.222, 0.469, 0.481, 0.062, 0.113, 0.214]
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:13:56 (running for 00:21:05.03)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.499 |      0.285 |                   77 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.591 |      0.27  |                   57 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.946 |      0.329 |                   52 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  3.118 |      0.047 |                    3 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.36613805970149255
[2m[36m(func pid=109192)[0m top5: 0.8638059701492538
[2m[36m(func pid=109192)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=109192)[0m f1_macro: 0.2699897278134308
[2m[36m(func pid=109192)[0m f1_weighted: 0.36665390918680363
[2m[36m(func pid=109192)[0m f1_per_class: [0.078, 0.501, 0.339, 0.3, 0.117, 0.344, 0.432, 0.293, 0.174, 0.122]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.1912313432835821
[2m[36m(func pid=120862)[0m top5: 0.5788246268656716
[2m[36m(func pid=120862)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=120862)[0m f1_macro: 0.0405181150015283
[2m[36m(func pid=120862)[0m f1_weighted: 0.10022355392237542
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.005, 0.0, 0.345, 0.0, 0.0, 0.0, 0.055, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 4.0066 | Steps: 2 | Val loss: 21.2632 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.2945 | Steps: 2 | Val loss: 2.0503 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8533 | Steps: 2 | Val loss: 3.0328 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0447 | Steps: 2 | Val loss: 2.3181 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=110192)[0m top1: 0.41651119402985076
[2m[36m(func pid=110192)[0m top5: 0.9193097014925373
[2m[36m(func pid=110192)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=110192)[0m f1_macro: 0.2860851175273461
[2m[36m(func pid=110192)[0m f1_weighted: 0.37813419292439404
[2m[36m(func pid=110192)[0m f1_per_class: [0.317, 0.484, 0.412, 0.541, 0.273, 0.427, 0.259, 0.078, 0.07, 0.0]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.353544776119403
[2m[36m(func pid=103390)[0m top5: 0.909981343283582
[2m[36m(func pid=103390)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=103390)[0m f1_macro: 0.2894770970978755
[2m[36m(func pid=103390)[0m f1_weighted: 0.3918713925513731
[2m[36m(func pid=103390)[0m f1_per_class: [0.191, 0.286, 0.375, 0.471, 0.073, 0.361, 0.456, 0.354, 0.149, 0.179]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:14:01 (running for 00:21:10.14)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.294 |      0.289 |                   78 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.591 |      0.27  |                   57 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  4.007 |      0.286 |                   53 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  3.045 |      0.039 |                    5 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.34888059701492535
[2m[36m(func pid=109192)[0m top5: 0.8432835820895522
[2m[36m(func pid=109192)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=109192)[0m f1_macro: 0.2669470800423247
[2m[36m(func pid=109192)[0m f1_weighted: 0.37634639190756075
[2m[36m(func pid=109192)[0m f1_per_class: [0.145, 0.495, 0.319, 0.472, 0.068, 0.238, 0.342, 0.326, 0.144, 0.121]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.20055970149253732
[2m[36m(func pid=120862)[0m top5: 0.5778917910447762
[2m[36m(func pid=120862)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=120862)[0m f1_macro: 0.039483462930447556
[2m[36m(func pid=120862)[0m f1_weighted: 0.10171317787117018
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.0, 0.0, 0.357, 0.0, 0.0, 0.0, 0.038, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 10.1957 | Steps: 2 | Val loss: 27.1261 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.5698 | Steps: 2 | Val loss: 2.0083 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5964 | Steps: 2 | Val loss: 3.4999 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0104 | Steps: 2 | Val loss: 2.3211 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=110192)[0m top1: 0.4099813432835821
[2m[36m(func pid=110192)[0m top5: 0.8568097014925373
[2m[36m(func pid=110192)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=110192)[0m f1_macro: 0.27989420979235446
[2m[36m(func pid=110192)[0m f1_weighted: 0.3506760303321246
[2m[36m(func pid=110192)[0m f1_per_class: [0.274, 0.459, 0.32, 0.545, 0.3, 0.45, 0.154, 0.176, 0.043, 0.077]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.36007462686567165
[2m[36m(func pid=103390)[0m top5: 0.9132462686567164
[2m[36m(func pid=103390)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=103390)[0m f1_macro: 0.2849323868914952
[2m[36m(func pid=103390)[0m f1_weighted: 0.38944398187529705
[2m[36m(func pid=103390)[0m f1_per_class: [0.208, 0.233, 0.348, 0.504, 0.071, 0.348, 0.453, 0.344, 0.146, 0.194]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:14:06 (running for 00:21:15.22)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.57  |      0.285 |                   79 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.853 |      0.267 |                   58 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 10.196 |      0.28  |                   54 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  3.045 |      0.039 |                    5 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.2868470149253731
[2m[36m(func pid=109192)[0m top5: 0.8222947761194029
[2m[36m(func pid=109192)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=109192)[0m f1_macro: 0.23498083554133484
[2m[36m(func pid=109192)[0m f1_weighted: 0.3115181890885213
[2m[36m(func pid=109192)[0m f1_per_class: [0.163, 0.238, 0.301, 0.503, 0.047, 0.134, 0.262, 0.42, 0.176, 0.106]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.21082089552238806
[2m[36m(func pid=120862)[0m top5: 0.5774253731343284
[2m[36m(func pid=120862)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=120862)[0m f1_macro: 0.039329033568635416
[2m[36m(func pid=120862)[0m f1_weighted: 0.10425034696508823
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.0, 0.0, 0.369, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 9.7271 | Steps: 2 | Val loss: 29.9476 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3715 | Steps: 2 | Val loss: 3.9551 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.0651 | Steps: 2 | Val loss: 1.9948 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9805 | Steps: 2 | Val loss: 2.3257 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=110192)[0m top1: 0.34701492537313433
[2m[36m(func pid=110192)[0m top5: 0.8260261194029851
[2m[36m(func pid=110192)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=110192)[0m f1_macro: 0.2683444416087317
[2m[36m(func pid=110192)[0m f1_weighted: 0.3338604152372784
[2m[36m(func pid=110192)[0m f1_per_class: [0.154, 0.472, 0.348, 0.49, 0.189, 0.423, 0.143, 0.238, 0.078, 0.148]
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:14:11 (running for 00:21:20.35)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.57  |      0.285 |                   79 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.372 |      0.187 |                   60 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  9.727 |      0.268 |                   55 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  3.01  |      0.039 |                    6 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.2178171641791045
[2m[36m(func pid=109192)[0m top5: 0.8180970149253731
[2m[36m(func pid=109192)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=109192)[0m f1_macro: 0.18738401268995675
[2m[36m(func pid=109192)[0m f1_weighted: 0.24414296082620543
[2m[36m(func pid=109192)[0m f1_per_class: [0.117, 0.057, 0.31, 0.426, 0.042, 0.127, 0.231, 0.441, 0.022, 0.099]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.21735074626865672
[2m[36m(func pid=120862)[0m top5: 0.5755597014925373
[2m[36m(func pid=120862)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=120862)[0m f1_macro: 0.05649445893192394
[2m[36m(func pid=120862)[0m f1_weighted: 0.1083264443895146
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.005, 0.125, 0.375, 0.0, 0.0, 0.0, 0.028, 0.0, 0.032]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m top1: 0.3619402985074627
[2m[36m(func pid=103390)[0m top5: 0.9146455223880597
[2m[36m(func pid=103390)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=103390)[0m f1_macro: 0.28150670524994154
[2m[36m(func pid=103390)[0m f1_weighted: 0.3806381089128915
[2m[36m(func pid=103390)[0m f1_per_class: [0.212, 0.176, 0.348, 0.521, 0.07, 0.345, 0.439, 0.362, 0.121, 0.219]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 7.6021 | Steps: 2 | Val loss: 35.2298 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5486 | Steps: 2 | Val loss: 4.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9268 | Steps: 2 | Val loss: 2.3314 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.1800 | Steps: 2 | Val loss: 1.9738 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=110192)[0m top1: 0.22761194029850745
[2m[36m(func pid=110192)[0m top5: 0.8442164179104478
[2m[36m(func pid=110192)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=110192)[0m f1_macro: 0.24655328265242482
[2m[36m(func pid=110192)[0m f1_weighted: 0.2782623420185346
[2m[36m(func pid=110192)[0m f1_per_class: [0.081, 0.496, 0.417, 0.221, 0.105, 0.332, 0.219, 0.304, 0.091, 0.2]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.16371268656716417
[2m[36m(func pid=109192)[0m top5: 0.8134328358208955
[2m[36m(func pid=109192)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=109192)[0m f1_macro: 0.17106056382440496
[2m[36m(func pid=109192)[0m f1_weighted: 0.19470622426072767
[2m[36m(func pid=109192)[0m f1_per_class: [0.081, 0.021, 0.306, 0.293, 0.044, 0.153, 0.198, 0.461, 0.026, 0.128]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:14:16 (running for 00:21:25.59)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.065 |      0.282 |                   80 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.549 |      0.171 |                   61 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  7.602 |      0.247 |                   56 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.981 |      0.056 |                    7 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.22201492537313433
[2m[36m(func pid=120862)[0m top5: 0.5625
[2m[36m(func pid=120862)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=120862)[0m f1_macro: 0.05142681033621105
[2m[36m(func pid=120862)[0m f1_weighted: 0.1119787041062269
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.016, 0.055, 0.383, 0.0, 0.0, 0.0, 0.031, 0.0, 0.03]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m top1: 0.36100746268656714
[2m[36m(func pid=103390)[0m top5: 0.9123134328358209
[2m[36m(func pid=103390)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=103390)[0m f1_macro: 0.2756596478811949
[2m[36m(func pid=103390)[0m f1_weighted: 0.36934719058547094
[2m[36m(func pid=103390)[0m f1_per_class: [0.231, 0.135, 0.348, 0.527, 0.07, 0.343, 0.42, 0.364, 0.127, 0.192]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 5.1209 | Steps: 2 | Val loss: 35.3808 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7920 | Steps: 2 | Val loss: 4.2101 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9360 | Steps: 2 | Val loss: 2.3355 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.2984 | Steps: 2 | Val loss: 1.9691 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=110192)[0m top1: 0.19309701492537312
[2m[36m(func pid=110192)[0m top5: 0.8362873134328358
[2m[36m(func pid=110192)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=110192)[0m f1_macro: 0.2156846134222651
[2m[36m(func pid=110192)[0m f1_weighted: 0.22932242321128146
[2m[36m(func pid=110192)[0m f1_per_class: [0.083, 0.481, 0.424, 0.11, 0.061, 0.047, 0.25, 0.459, 0.097, 0.145]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.15951492537313433
[2m[36m(func pid=109192)[0m top5: 0.8297574626865671
[2m[36m(func pid=109192)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=109192)[0m f1_macro: 0.17484470070610922
[2m[36m(func pid=109192)[0m f1_weighted: 0.18571589095248323
[2m[36m(func pid=109192)[0m f1_per_class: [0.072, 0.005, 0.314, 0.213, 0.054, 0.261, 0.214, 0.432, 0.027, 0.154]
[2m[36m(func pid=109192)[0m 
== Status ==
Current time: 2024-01-07 00:14:21 (running for 00:21:30.75)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.18  |      0.276 |                   81 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.792 |      0.175 |                   62 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  5.121 |      0.216 |                   57 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.927 |      0.051 |                    8 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.21828358208955223
[2m[36m(func pid=120862)[0m top5: 0.5527052238805971
[2m[36m(func pid=120862)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=120862)[0m f1_macro: 0.051666491767196274
[2m[36m(func pid=120862)[0m f1_weighted: 0.1139785164909003
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.021, 0.044, 0.386, 0.0, 0.0, 0.0, 0.034, 0.0, 0.031]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m top1: 0.37033582089552236
[2m[36m(func pid=103390)[0m top5: 0.9071828358208955
[2m[36m(func pid=103390)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=103390)[0m f1_macro: 0.29079437557500976
[2m[36m(func pid=103390)[0m f1_weighted: 0.3779033855399522
[2m[36m(func pid=103390)[0m f1_per_class: [0.245, 0.157, 0.367, 0.535, 0.074, 0.348, 0.415, 0.407, 0.124, 0.235]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.0957 | Steps: 2 | Val loss: 32.6977 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4306 | Steps: 2 | Val loss: 3.6982 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8800 | Steps: 2 | Val loss: 2.3396 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.1237 | Steps: 2 | Val loss: 1.9648 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=110192)[0m top1: 0.18423507462686567
[2m[36m(func pid=110192)[0m top5: 0.7989738805970149
[2m[36m(func pid=110192)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=110192)[0m f1_macro: 0.19707213777704807
[2m[36m(func pid=110192)[0m f1_weighted: 0.17377356149840603
[2m[36m(func pid=110192)[0m f1_per_class: [0.135, 0.392, 0.4, 0.086, 0.055, 0.0, 0.139, 0.517, 0.119, 0.129]
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:14:26 (running for 00:21:35.76)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.298 |      0.291 |                   82 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.431 |      0.213 |                   63 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.096 |      0.197 |                   58 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.936 |      0.052 |                    9 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.22574626865671643
[2m[36m(func pid=109192)[0m top5: 0.8484141791044776
[2m[36m(func pid=109192)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=109192)[0m f1_macro: 0.21331471538748326
[2m[36m(func pid=109192)[0m f1_weighted: 0.2593343857132176
[2m[36m(func pid=109192)[0m f1_per_class: [0.077, 0.143, 0.286, 0.293, 0.086, 0.358, 0.272, 0.43, 0.0, 0.189]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.19869402985074627
[2m[36m(func pid=120862)[0m top5: 0.5433768656716418
[2m[36m(func pid=120862)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=120862)[0m f1_macro: 0.04361145074658473
[2m[36m(func pid=120862)[0m f1_weighted: 0.1078532561493062
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.005, 0.035, 0.379, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m top1: 0.36800373134328357
[2m[36m(func pid=103390)[0m top5: 0.9053171641791045
[2m[36m(func pid=103390)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=103390)[0m f1_macro: 0.29188445587037115
[2m[36m(func pid=103390)[0m f1_weighted: 0.37477754669159324
[2m[36m(func pid=103390)[0m f1_per_class: [0.247, 0.17, 0.36, 0.533, 0.077, 0.343, 0.394, 0.439, 0.13, 0.226]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 7.0447 | Steps: 2 | Val loss: 36.1394 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3371 | Steps: 2 | Val loss: 3.1082 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8696 | Steps: 2 | Val loss: 2.3426 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.2123 | Steps: 2 | Val loss: 1.9550 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=110192)[0m top1: 0.16837686567164178
[2m[36m(func pid=110192)[0m top5: 0.773320895522388
[2m[36m(func pid=110192)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=110192)[0m f1_macro: 0.16068785704677385
[2m[36m(func pid=110192)[0m f1_weighted: 0.1223264418573487
[2m[36m(func pid=110192)[0m f1_per_class: [0.196, 0.352, 0.247, 0.007, 0.066, 0.0, 0.073, 0.48, 0.096, 0.09]
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:14:32 (running for 00:21:41.04)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.124 |      0.292 |                   83 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.337 |      0.262 |                   64 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  7.045 |      0.161 |                   59 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.88  |      0.044 |                   10 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.31716417910447764
[2m[36m(func pid=109192)[0m top5: 0.8726679104477612
[2m[36m(func pid=109192)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=109192)[0m f1_macro: 0.26184930176530974
[2m[36m(func pid=109192)[0m f1_weighted: 0.350836633762992
[2m[36m(func pid=109192)[0m f1_per_class: [0.095, 0.374, 0.244, 0.383, 0.157, 0.391, 0.353, 0.391, 0.0, 0.232]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.17490671641791045
[2m[36m(func pid=120862)[0m top5: 0.5405783582089553
[2m[36m(func pid=120862)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=120862)[0m f1_macro: 0.04099024557745824
[2m[36m(func pid=120862)[0m f1_weighted: 0.10317251290496478
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.0, 0.032, 0.367, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m top1: 0.37220149253731344
[2m[36m(func pid=103390)[0m top5: 0.9029850746268657
[2m[36m(func pid=103390)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=103390)[0m f1_macro: 0.30088927005662536
[2m[36m(func pid=103390)[0m f1_weighted: 0.38496489424721575
[2m[36m(func pid=103390)[0m f1_per_class: [0.265, 0.217, 0.346, 0.533, 0.08, 0.359, 0.391, 0.451, 0.134, 0.233]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 4.3994 | Steps: 2 | Val loss: 41.2390 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2034 | Steps: 2 | Val loss: 2.6807 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8746 | Steps: 2 | Val loss: 2.3432 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.1341 | Steps: 2 | Val loss: 1.9539 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=110192)[0m top1: 0.17210820895522388
[2m[36m(func pid=110192)[0m top5: 0.753731343283582
[2m[36m(func pid=110192)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=110192)[0m f1_macro: 0.1431863027127196
[2m[36m(func pid=110192)[0m f1_weighted: 0.12272716695771946
[2m[36m(func pid=110192)[0m f1_per_class: [0.043, 0.354, 0.242, 0.0, 0.092, 0.0, 0.098, 0.447, 0.095, 0.061]
[2m[36m(func pid=110192)[0m 
== Status ==
Current time: 2024-01-07 00:14:37 (running for 00:21:46.21)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.212 |      0.301 |                   84 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.203 |      0.326 |                   65 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  4.399 |      0.143 |                   60 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.87  |      0.041 |                   11 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.41697761194029853
[2m[36m(func pid=109192)[0m top5: 0.8889925373134329
[2m[36m(func pid=109192)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=109192)[0m f1_macro: 0.3264505978292193
[2m[36m(func pid=109192)[0m f1_weighted: 0.43576193128703444
[2m[36m(func pid=109192)[0m f1_per_class: [0.165, 0.462, 0.22, 0.49, 0.286, 0.441, 0.45, 0.365, 0.133, 0.254]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.14598880597014927
[2m[36m(func pid=120862)[0m top5: 0.5401119402985075
[2m[36m(func pid=120862)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=120862)[0m f1_macro: 0.03809495931337403
[2m[36m(func pid=120862)[0m f1_weighted: 0.09562822697985579
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.005, 0.026, 0.336, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m top1: 0.36986940298507465
[2m[36m(func pid=103390)[0m top5: 0.8959888059701493
[2m[36m(func pid=103390)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=103390)[0m f1_macro: 0.3131444097695625
[2m[36m(func pid=103390)[0m f1_weighted: 0.38943756531256224
[2m[36m(func pid=103390)[0m f1_per_class: [0.293, 0.278, 0.377, 0.512, 0.081, 0.367, 0.376, 0.487, 0.152, 0.206]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 9.8773 | Steps: 2 | Val loss: 40.7577 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3955 | Steps: 2 | Val loss: 2.4637 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8502 | Steps: 2 | Val loss: 2.3453 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.0065 | Steps: 2 | Val loss: 1.9827 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=110192)[0m top1: 0.20848880597014927
[2m[36m(func pid=110192)[0m top5: 0.7700559701492538
[2m[36m(func pid=110192)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=110192)[0m f1_macro: 0.16033383977751353
[2m[36m(func pid=110192)[0m f1_weighted: 0.17508881941726026
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.362, 0.239, 0.0, 0.127, 0.0, 0.27, 0.452, 0.092, 0.061]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.11007462686567164
[2m[36m(func pid=120862)[0m top5: 0.5373134328358209
[2m[36m(func pid=120862)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=120862)[0m f1_macro: 0.03303004803214858
[2m[36m(func pid=120862)[0m f1_weighted: 0.08215077629477761
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.005, 0.022, 0.288, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:14:42 (running for 00:21:51.36)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.134 |      0.313 |                   85 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.396 |      0.33  |                   66 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  9.877 |      0.16  |                   61 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.85  |      0.033 |                   13 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.44216417910447764
[2m[36m(func pid=109192)[0m top5: 0.9090485074626866
[2m[36m(func pid=109192)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=109192)[0m f1_macro: 0.33028409821058957
[2m[36m(func pid=109192)[0m f1_weighted: 0.45286857520868845
[2m[36m(func pid=109192)[0m f1_per_class: [0.174, 0.548, 0.229, 0.44, 0.229, 0.467, 0.498, 0.341, 0.154, 0.222]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.36427238805970147
[2m[36m(func pid=103390)[0m top5: 0.8857276119402985
[2m[36m(func pid=103390)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=103390)[0m f1_macro: 0.32163738485311216
[2m[36m(func pid=103390)[0m f1_weighted: 0.38739676021879016
[2m[36m(func pid=103390)[0m f1_per_class: [0.302, 0.366, 0.393, 0.479, 0.086, 0.372, 0.342, 0.505, 0.157, 0.213]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 12.9350 | Steps: 2 | Val loss: 35.0846 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8215 | Steps: 2 | Val loss: 2.3466 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6923 | Steps: 2 | Val loss: 2.5576 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.1831 | Steps: 2 | Val loss: 1.9976 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=110192)[0m top1: 0.30130597014925375
[2m[36m(func pid=110192)[0m top5: 0.7966417910447762
[2m[36m(func pid=110192)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=110192)[0m f1_macro: 0.21412574118962197
[2m[36m(func pid=110192)[0m f1_weighted: 0.27400708428292403
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.407, 0.214, 0.0, 0.144, 0.248, 0.481, 0.431, 0.107, 0.109]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07882462686567164
[2m[36m(func pid=120862)[0m top5: 0.5377798507462687
[2m[36m(func pid=120862)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=120862)[0m f1_macro: 0.028393901548619337
[2m[36m(func pid=120862)[0m f1_weighted: 0.06973383479018982
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.01, 0.018, 0.24, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:14:47 (running for 00:21:56.51)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.006 |      0.322 |                   86 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.692 |      0.291 |                   67 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 12.935 |      0.214 |                   62 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.821 |      0.028 |                   14 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.4230410447761194
[2m[36m(func pid=109192)[0m top5: 0.9272388059701493
[2m[36m(func pid=109192)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=109192)[0m f1_macro: 0.2908725866770089
[2m[36m(func pid=109192)[0m f1_weighted: 0.4201460259984117
[2m[36m(func pid=109192)[0m f1_per_class: [0.085, 0.557, 0.25, 0.3, 0.0, 0.46, 0.523, 0.365, 0.161, 0.207]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.34421641791044777
[2m[36m(func pid=103390)[0m top5: 0.8843283582089553
[2m[36m(func pid=103390)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=103390)[0m f1_macro: 0.3119979516797127
[2m[36m(func pid=103390)[0m f1_weighted: 0.36410131861473954
[2m[36m(func pid=103390)[0m f1_per_class: [0.31, 0.405, 0.393, 0.42, 0.091, 0.383, 0.3, 0.476, 0.157, 0.186]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8130 | Steps: 2 | Val loss: 2.3478 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 5.9684 | Steps: 2 | Val loss: 28.1525 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 3.2948 | Steps: 2 | Val loss: 2.7382 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.9675 | Steps: 2 | Val loss: 2.0281 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=120862)[0m top1: 0.05643656716417911
[2m[36m(func pid=120862)[0m top5: 0.5340485074626866
[2m[36m(func pid=120862)[0m f1_micro: 0.05643656716417911
[2m[36m(func pid=120862)[0m f1_macro: 0.023330395360330453
[2m[36m(func pid=120862)[0m f1_weighted: 0.05609258114742179
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.01, 0.016, 0.191, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:14:52 (running for 00:22:01.60)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.183 |      0.312 |                   87 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.692 |      0.291 |                   67 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  5.968 |      0.241 |                   63 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.813 |      0.023 |                   15 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=110192)[0m top1: 0.34701492537313433
[2m[36m(func pid=110192)[0m top5: 0.8694029850746269
[2m[36m(func pid=110192)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=110192)[0m f1_macro: 0.2406885017982741
[2m[36m(func pid=110192)[0m f1_weighted: 0.3130830235079698
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.408, 0.188, 0.026, 0.161, 0.427, 0.533, 0.338, 0.109, 0.217]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m top1: 0.4025186567164179
[2m[36m(func pid=109192)[0m top5: 0.9309701492537313
[2m[36m(func pid=109192)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=109192)[0m f1_macro: 0.28149016329620374
[2m[36m(func pid=109192)[0m f1_weighted: 0.38577973898690837
[2m[36m(func pid=109192)[0m f1_per_class: [0.044, 0.544, 0.282, 0.217, 0.0, 0.442, 0.491, 0.413, 0.166, 0.214]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.31669776119402987
[2m[36m(func pid=103390)[0m top5: 0.8773320895522388
[2m[36m(func pid=103390)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=103390)[0m f1_macro: 0.2903025164910803
[2m[36m(func pid=103390)[0m f1_weighted: 0.32181016575760485
[2m[36m(func pid=103390)[0m f1_per_class: [0.297, 0.428, 0.379, 0.332, 0.098, 0.397, 0.232, 0.434, 0.144, 0.16]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8334 | Steps: 2 | Val loss: 2.3479 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4619 | Steps: 2 | Val loss: 2.6577 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 4.7060 | Steps: 2 | Val loss: 25.2944 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.3350 | Steps: 2 | Val loss: 2.0463 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:14:57 (running for 00:22:06.61)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  0.967 |      0.29  |                   88 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  3.295 |      0.281 |                   68 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  5.968 |      0.241 |                   63 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.833 |      0.02  |                   16 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.3903917910447761
[2m[36m(func pid=109192)[0m top5: 0.9249067164179104
[2m[36m(func pid=109192)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=109192)[0m f1_macro: 0.2790037676004617
[2m[36m(func pid=109192)[0m f1_weighted: 0.37330249896576695
[2m[36m(func pid=109192)[0m f1_per_class: [0.044, 0.526, 0.333, 0.245, 0.0, 0.432, 0.443, 0.379, 0.173, 0.214]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.044309701492537316
[2m[36m(func pid=120862)[0m top5: 0.5396455223880597
[2m[36m(func pid=120862)[0m f1_micro: 0.044309701492537316
[2m[36m(func pid=120862)[0m f1_macro: 0.01961380940342726
[2m[36m(func pid=120862)[0m f1_weighted: 0.04788103320558433
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.026, 0.015, 0.155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m top1: 0.37406716417910446
[2m[36m(func pid=110192)[0m top5: 0.9127798507462687
[2m[36m(func pid=110192)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=110192)[0m f1_macro: 0.25280725826524825
[2m[36m(func pid=110192)[0m f1_weighted: 0.36351625755720784
[2m[36m(func pid=110192)[0m f1_per_class: [0.0, 0.325, 0.19, 0.311, 0.163, 0.402, 0.514, 0.222, 0.086, 0.314]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.292910447761194
[2m[36m(func pid=103390)[0m top5: 0.8736007462686567
[2m[36m(func pid=103390)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=103390)[0m f1_macro: 0.2725222718316872
[2m[36m(func pid=103390)[0m f1_weighted: 0.2809842921244283
[2m[36m(func pid=103390)[0m f1_per_class: [0.29, 0.414, 0.373, 0.237, 0.105, 0.404, 0.197, 0.414, 0.122, 0.17]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6157 | Steps: 2 | Val loss: 2.5546 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7912 | Steps: 2 | Val loss: 2.3483 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.7770 | Steps: 2 | Val loss: 26.0432 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:15:02 (running for 00:22:11.68)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.335 |      0.273 |                   89 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.462 |      0.279 |                   69 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  4.706 |      0.253 |                   64 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.791 |      0.018 |                   17 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.03451492537313433
[2m[36m(func pid=120862)[0m top5: 0.5382462686567164
[2m[36m(func pid=120862)[0m f1_micro: 0.03451492537313433
[2m[36m(func pid=120862)[0m f1_macro: 0.018165740092103776
[2m[36m(func pid=120862)[0m f1_weighted: 0.03940042657980868
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.036, 0.014, 0.116, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.6189 | Steps: 2 | Val loss: 2.1302 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=109192)[0m top1: 0.3908582089552239
[2m[36m(func pid=109192)[0m top5: 0.9169776119402985
[2m[36m(func pid=109192)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=109192)[0m f1_macro: 0.2873092343068295
[2m[36m(func pid=109192)[0m f1_weighted: 0.37974912625412527
[2m[36m(func pid=109192)[0m f1_per_class: [0.085, 0.538, 0.367, 0.333, 0.0, 0.43, 0.376, 0.368, 0.162, 0.214]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=110192)[0m top1: 0.3824626865671642
[2m[36m(func pid=110192)[0m top5: 0.9123134328358209
[2m[36m(func pid=110192)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=110192)[0m f1_macro: 0.239921296961165
[2m[36m(func pid=110192)[0m f1_weighted: 0.3594241623719323
[2m[36m(func pid=110192)[0m f1_per_class: [0.125, 0.122, 0.187, 0.451, 0.172, 0.393, 0.499, 0.174, 0.024, 0.25]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8068 | Steps: 2 | Val loss: 2.3490 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=103390)[0m top1: 0.27052238805970147
[2m[36m(func pid=103390)[0m top5: 0.8675373134328358
[2m[36m(func pid=103390)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=103390)[0m f1_macro: 0.25199178488537294
[2m[36m(func pid=103390)[0m f1_weighted: 0.23566968576408195
[2m[36m(func pid=103390)[0m f1_per_class: [0.269, 0.405, 0.333, 0.139, 0.116, 0.403, 0.146, 0.398, 0.127, 0.185]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6114 | Steps: 2 | Val loss: 2.4257 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 5.8269 | Steps: 2 | Val loss: 25.3832 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=120862)[0m top1: 0.03125
[2m[36m(func pid=120862)[0m top5: 0.5331156716417911
[2m[36m(func pid=120862)[0m f1_micro: 0.03125
[2m[36m(func pid=120862)[0m f1_macro: 0.016813324875787884
[2m[36m(func pid=120862)[0m f1_weighted: 0.0371159092336302
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.056, 0.014, 0.098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:15:07 (running for 00:22:16.83)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.619 |      0.252 |                   90 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.616 |      0.287 |                   70 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.777 |      0.24  |                   65 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.807 |      0.017 |                   18 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.4006529850746269
[2m[36m(func pid=109192)[0m top5: 0.9090485074626866
[2m[36m(func pid=109192)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=109192)[0m f1_macro: 0.32110479437122474
[2m[36m(func pid=109192)[0m f1_weighted: 0.3978170085646282
[2m[36m(func pid=109192)[0m f1_per_class: [0.2, 0.562, 0.386, 0.456, 0.136, 0.39, 0.31, 0.373, 0.157, 0.242]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.2306 | Steps: 2 | Val loss: 2.1493 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=110192)[0m top1: 0.4183768656716418
[2m[36m(func pid=110192)[0m top5: 0.9029850746268657
[2m[36m(func pid=110192)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=110192)[0m f1_macro: 0.2365665062323618
[2m[36m(func pid=110192)[0m f1_weighted: 0.3768753346364616
[2m[36m(func pid=110192)[0m f1_per_class: [0.148, 0.042, 0.19, 0.545, 0.183, 0.433, 0.514, 0.129, 0.0, 0.182]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1617 | Steps: 2 | Val loss: 2.4954 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7482 | Steps: 2 | Val loss: 2.3497 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=103390)[0m top1: 0.26399253731343286
[2m[36m(func pid=103390)[0m top5: 0.8605410447761194
[2m[36m(func pid=103390)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=103390)[0m f1_macro: 0.24530187074583737
[2m[36m(func pid=103390)[0m f1_weighted: 0.22091020067931036
[2m[36m(func pid=103390)[0m f1_per_class: [0.254, 0.397, 0.333, 0.102, 0.126, 0.416, 0.133, 0.388, 0.124, 0.179]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 4.2446 | Steps: 2 | Val loss: 26.6883 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:15:13 (running for 00:22:22.13)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.231 |      0.245 |                   91 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.162 |      0.314 |                   72 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  5.827 |      0.237 |                   66 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.807 |      0.017 |                   18 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.3885261194029851
[2m[36m(func pid=109192)[0m top5: 0.8941231343283582
[2m[36m(func pid=109192)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=109192)[0m f1_macro: 0.3135425029110266
[2m[36m(func pid=109192)[0m f1_weighted: 0.3955877633824069
[2m[36m(func pid=109192)[0m f1_per_class: [0.371, 0.497, 0.383, 0.533, 0.089, 0.313, 0.296, 0.381, 0.075, 0.197]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.03031716417910448
[2m[36m(func pid=120862)[0m top5: 0.5377798507462687
[2m[36m(func pid=120862)[0m f1_micro: 0.03031716417910448
[2m[36m(func pid=120862)[0m f1_macro: 0.01737355043770087
[2m[36m(func pid=120862)[0m f1_weighted: 0.037160111405612
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.07, 0.014, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.1039 | Steps: 2 | Val loss: 2.1743 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=110192)[0m top1: 0.44216417910447764
[2m[36m(func pid=110192)[0m top5: 0.8810634328358209
[2m[36m(func pid=110192)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=110192)[0m f1_macro: 0.2664469125304917
[2m[36m(func pid=110192)[0m f1_weighted: 0.40158785254608453
[2m[36m(func pid=110192)[0m f1_per_class: [0.212, 0.021, 0.242, 0.562, 0.188, 0.486, 0.543, 0.245, 0.027, 0.14]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2646 | Steps: 2 | Val loss: 2.8678 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8042 | Steps: 2 | Val loss: 2.3526 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=103390)[0m top1: 0.26072761194029853
[2m[36m(func pid=103390)[0m top5: 0.855410447761194
[2m[36m(func pid=103390)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=103390)[0m f1_macro: 0.2417140080910279
[2m[36m(func pid=103390)[0m f1_weighted: 0.21352311796713086
[2m[36m(func pid=103390)[0m f1_per_class: [0.242, 0.392, 0.333, 0.057, 0.13, 0.429, 0.151, 0.386, 0.119, 0.179]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.8165 | Steps: 2 | Val loss: 31.1191 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:15:18 (running for 00:22:27.15)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.104 |      0.242 |                   92 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.265 |      0.277 |                   73 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  4.245 |      0.266 |                   67 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.748 |      0.017 |                   19 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.35447761194029853
[2m[36m(func pid=109192)[0m top5: 0.8754664179104478
[2m[36m(func pid=109192)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=109192)[0m f1_macro: 0.2770286425997225
[2m[36m(func pid=109192)[0m f1_weighted: 0.36668957099897
[2m[36m(func pid=109192)[0m f1_per_class: [0.294, 0.24, 0.409, 0.557, 0.057, 0.207, 0.373, 0.392, 0.044, 0.198]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.029384328358208957
[2m[36m(func pid=120862)[0m top5: 0.5307835820895522
[2m[36m(func pid=120862)[0m f1_micro: 0.029384328358208953
[2m[36m(func pid=120862)[0m f1_macro: 0.017428077649675255
[2m[36m(func pid=120862)[0m f1_weighted: 0.03650615567457935
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.078, 0.014, 0.082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m top1: 0.37080223880597013
[2m[36m(func pid=110192)[0m top5: 0.8684701492537313
[2m[36m(func pid=110192)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=110192)[0m f1_macro: 0.2656240414409865
[2m[36m(func pid=110192)[0m f1_weighted: 0.36736735442143625
[2m[36m(func pid=110192)[0m f1_per_class: [0.136, 0.027, 0.328, 0.484, 0.144, 0.404, 0.493, 0.43, 0.067, 0.143]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.0346 | Steps: 2 | Val loss: 2.1892 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3392 | Steps: 2 | Val loss: 3.3776 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7493 | Steps: 2 | Val loss: 2.3528 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=103390)[0m top1: 0.26492537313432835
[2m[36m(func pid=103390)[0m top5: 0.8540111940298507
[2m[36m(func pid=103390)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=103390)[0m f1_macro: 0.24302088198030614
[2m[36m(func pid=103390)[0m f1_weighted: 0.22564005147910787
[2m[36m(func pid=103390)[0m f1_per_class: [0.226, 0.392, 0.319, 0.048, 0.137, 0.423, 0.201, 0.41, 0.106, 0.169]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 3.2690 | Steps: 2 | Val loss: 39.5445 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 00:15:23 (running for 00:22:32.26)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.035 |      0.243 |                   93 |
| train_51d3e_00010 | RUNNING    | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.339 |      0.244 |                   74 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  1.817 |      0.266 |                   68 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.804 |      0.017 |                   20 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109192)[0m top1: 0.31669776119402987
[2m[36m(func pid=109192)[0m top5: 0.875
[2m[36m(func pid=109192)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=109192)[0m f1_macro: 0.24376304286712017
[2m[36m(func pid=109192)[0m f1_weighted: 0.3338978520672116
[2m[36m(func pid=109192)[0m f1_per_class: [0.226, 0.042, 0.432, 0.521, 0.045, 0.156, 0.433, 0.421, 0.025, 0.136]
[2m[36m(func pid=109192)[0m 
[2m[36m(func pid=120862)[0m top1: 0.027985074626865673
[2m[36m(func pid=120862)[0m top5: 0.5298507462686567
[2m[36m(func pid=120862)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=120862)[0m f1_macro: 0.017200489471941006
[2m[36m(func pid=120862)[0m f1_weighted: 0.034637579703848585
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.089, 0.014, 0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m top1: 0.2583955223880597
[2m[36m(func pid=110192)[0m top5: 0.8521455223880597
[2m[36m(func pid=110192)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=110192)[0m f1_macro: 0.23033842511188768
[2m[36m(func pid=110192)[0m f1_weighted: 0.2812941828711342
[2m[36m(func pid=110192)[0m f1_per_class: [0.094, 0.027, 0.4, 0.353, 0.106, 0.207, 0.391, 0.488, 0.092, 0.146]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.0642 | Steps: 2 | Val loss: 2.1993 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=109192)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7269 | Steps: 2 | Val loss: 3.7451 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7710 | Steps: 2 | Val loss: 2.3536 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 10.5901 | Steps: 2 | Val loss: 46.3545 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=103390)[0m top1: 0.269589552238806
[2m[36m(func pid=103390)[0m top5: 0.8530783582089553
[2m[36m(func pid=103390)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=103390)[0m f1_macro: 0.24611208611168672
[2m[36m(func pid=103390)[0m f1_weighted: 0.24315673214541136
[2m[36m(func pid=103390)[0m f1_per_class: [0.204, 0.384, 0.306, 0.06, 0.137, 0.431, 0.249, 0.42, 0.106, 0.164]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=109192)[0m top1: 0.28218283582089554
[2m[36m(func pid=109192)[0m top5: 0.8717350746268657
[2m[36m(func pid=109192)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=109192)[0m f1_macro: 0.2293441713749365
[2m[36m(func pid=109192)[0m f1_weighted: 0.3186879884941249
[2m[36m(func pid=109192)[0m f1_per_class: [0.16, 0.042, 0.432, 0.463, 0.041, 0.181, 0.445, 0.346, 0.026, 0.156]
== Status ==
Current time: 2024-01-07 00:15:28 (running for 00:22:37.46)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 3 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.064 |      0.246 |                   94 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  3.269 |      0.23  |                   69 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.749 |      0.017 |                   21 |
| train_51d3e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 1 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.026119402985074626
[2m[36m(func pid=120862)[0m top5: 0.519589552238806
[2m[36m(func pid=120862)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=120862)[0m f1_macro: 0.01653393324625846
[2m[36m(func pid=120862)[0m f1_weighted: 0.03212388593928704
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.096, 0.014, 0.056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m top1: 0.15625
[2m[36m(func pid=110192)[0m top5: 0.8227611940298507
[2m[36m(func pid=110192)[0m f1_micro: 0.15625
[2m[36m(func pid=110192)[0m f1_macro: 0.17481301679697142
[2m[36m(func pid=110192)[0m f1_weighted: 0.1659666088459454
[2m[36m(func pid=110192)[0m f1_per_class: [0.074, 0.048, 0.392, 0.246, 0.079, 0.024, 0.166, 0.47, 0.107, 0.143]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.0372 | Steps: 2 | Val loss: 2.1953 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7397 | Steps: 2 | Val loss: 2.3524 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.5668 | Steps: 2 | Val loss: 47.0019 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=103390)[0m top1: 0.27705223880597013
[2m[36m(func pid=103390)[0m top5: 0.8582089552238806
[2m[36m(func pid=103390)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=103390)[0m f1_macro: 0.25139030352372493
[2m[36m(func pid=103390)[0m f1_weighted: 0.2655387417764472
[2m[36m(func pid=103390)[0m f1_per_class: [0.205, 0.371, 0.293, 0.074, 0.132, 0.43, 0.317, 0.443, 0.095, 0.153]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=120862)[0m top1: 0.026119402985074626
[2m[36m(func pid=120862)[0m top5: 0.5205223880597015
[2m[36m(func pid=120862)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=120862)[0m f1_macro: 0.016435487695968413
[2m[36m(func pid=120862)[0m f1_weighted: 0.03132816543584677
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.1, 0.014, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m top1: 0.12733208955223882
[2m[36m(func pid=110192)[0m top5: 0.816231343283582
[2m[36m(func pid=110192)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=110192)[0m f1_macro: 0.1684559506734999
[2m[36m(func pid=110192)[0m f1_weighted: 0.12296778851477022
[2m[36m(func pid=110192)[0m f1_per_class: [0.079, 0.17, 0.409, 0.134, 0.072, 0.0, 0.06, 0.5, 0.086, 0.174]
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.9965 | Steps: 2 | Val loss: 2.1971 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 00:15:34 (running for 00:22:42.93)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.037 |      0.251 |                   95 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 | 10.59  |      0.175 |                   70 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.74  |      0.016 |                   23 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=126068)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=126068)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=126068)[0m Configuration completed!
[2m[36m(func pid=126068)[0m New optimizer parameters:
[2m[36m(func pid=126068)[0m SGD (
[2m[36m(func pid=126068)[0m Parameter Group 0
[2m[36m(func pid=126068)[0m     dampening: 0
[2m[36m(func pid=126068)[0m     differentiable: False
[2m[36m(func pid=126068)[0m     foreach: None
[2m[36m(func pid=126068)[0m     lr: 0.001
[2m[36m(func pid=126068)[0m     maximize: False
[2m[36m(func pid=126068)[0m     momentum: 0.9
[2m[36m(func pid=126068)[0m     nesterov: False
[2m[36m(func pid=126068)[0m     weight_decay: 0.0001
[2m[36m(func pid=126068)[0m )
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7313 | Steps: 2 | Val loss: 2.3519 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=103390)[0m top1: 0.2868470149253731
[2m[36m(func pid=103390)[0m top5: 0.863339552238806
[2m[36m(func pid=103390)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=103390)[0m f1_macro: 0.25956711688276934
[2m[36m(func pid=103390)[0m f1_weighted: 0.28844704521428743
[2m[36m(func pid=103390)[0m f1_per_class: [0.194, 0.371, 0.278, 0.103, 0.129, 0.435, 0.36, 0.461, 0.111, 0.153]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:15:39 (running for 00:22:48.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  0.997 |      0.26  |                   96 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  3.567 |      0.168 |                   71 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.731 |      0.017 |                   24 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.02751865671641791
[2m[36m(func pid=120862)[0m top5: 0.5237873134328358
[2m[36m(func pid=120862)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=120862)[0m f1_macro: 0.017246154819486346
[2m[36m(func pid=120862)[0m f1_weighted: 0.03323280903482614
[2m[36m(func pid=120862)[0m f1_per_class: [0.0, 0.103, 0.014, 0.055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3138 | Steps: 2 | Val loss: 46.3970 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.9477 | Steps: 2 | Val loss: 2.1986 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0862 | Steps: 2 | Val loss: 2.3699 | Batch size: 32 | lr: 0.001 | Duration: 4.60s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7400 | Steps: 2 | Val loss: 2.3544 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=110192)[0m top1: 0.14458955223880596
[2m[36m(func pid=110192)[0m top5: 0.7985074626865671
[2m[36m(func pid=110192)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=110192)[0m f1_macro: 0.1780333392718762
[2m[36m(func pid=110192)[0m f1_weighted: 0.1228289639791324
[2m[36m(func pid=110192)[0m f1_per_class: [0.091, 0.39, 0.39, 0.032, 0.077, 0.0, 0.024, 0.51, 0.096, 0.169]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.3041044776119403
[2m[36m(func pid=103390)[0m top5: 0.8684701492537313
[2m[36m(func pid=103390)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=103390)[0m f1_macro: 0.2722710613287034
[2m[36m(func pid=103390)[0m f1_weighted: 0.32590158836670285
[2m[36m(func pid=103390)[0m f1_per_class: [0.182, 0.372, 0.278, 0.183, 0.119, 0.431, 0.409, 0.489, 0.095, 0.164]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=126068)[0m top1: 0.07136194029850747
[2m[36m(func pid=126068)[0m top5: 0.32649253731343286
[2m[36m(func pid=126068)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=126068)[0m f1_macro: 0.01970942998180352
[2m[36m(func pid=126068)[0m f1_weighted: 0.02803044809984818
[2m[36m(func pid=126068)[0m f1_per_class: [0.0, 0.0, 0.0, 0.075, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
== Status ==
Current time: 2024-01-07 00:15:44 (running for 00:22:53.14)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  0.948 |      0.272 |                   97 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.314 |      0.178 |                   72 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.74  |      0.02  |                   25 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  3.086 |      0.02  |                    1 |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.02751865671641791
[2m[36m(func pid=120862)[0m top5: 0.5172574626865671
[2m[36m(func pid=120862)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=120862)[0m f1_macro: 0.02013133481201603
[2m[36m(func pid=120862)[0m f1_weighted: 0.0316744446235478
[2m[36m(func pid=120862)[0m f1_per_class: [0.039, 0.098, 0.015, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 4.1889 | Steps: 2 | Val loss: 42.9142 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.0111 | Steps: 2 | Val loss: 2.1760 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0106 | Steps: 2 | Val loss: 2.3286 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7477 | Steps: 2 | Val loss: 2.3540 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=110192)[0m top1: 0.20289179104477612
[2m[36m(func pid=110192)[0m top5: 0.7915111940298507
[2m[36m(func pid=110192)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=110192)[0m f1_macro: 0.19437773816588338
[2m[36m(func pid=110192)[0m f1_weighted: 0.14373138528361346
[2m[36m(func pid=110192)[0m f1_per_class: [0.145, 0.505, 0.41, 0.02, 0.098, 0.016, 0.033, 0.499, 0.092, 0.127]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.3031716417910448
[2m[36m(func pid=103390)[0m top5: 0.8763992537313433
[2m[36m(func pid=103390)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=103390)[0m f1_macro: 0.2717576383007544
[2m[36m(func pid=103390)[0m f1_weighted: 0.33243867493534346
[2m[36m(func pid=103390)[0m f1_per_class: [0.189, 0.328, 0.278, 0.231, 0.111, 0.424, 0.416, 0.471, 0.1, 0.169]
[2m[36m(func pid=103390)[0m 
[2m[36m(func pid=126068)[0m top1: 0.12313432835820895
[2m[36m(func pid=126068)[0m top5: 0.490205223880597
[2m[36m(func pid=126068)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=126068)[0m f1_macro: 0.05638298796492267
[2m[36m(func pid=126068)[0m f1_weighted: 0.08266991744880653
[2m[36m(func pid=126068)[0m f1_per_class: [0.0, 0.0, 0.027, 0.233, 0.0, 0.0, 0.0, 0.304, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=120862)[0m top1: 0.028917910447761194
[2m[36m(func pid=120862)[0m top5: 0.5144589552238806
[2m[36m(func pid=120862)[0m f1_micro: 0.028917910447761194
[2m[36m(func pid=120862)[0m f1_macro: 0.020404889708402266
[2m[36m(func pid=120862)[0m f1_weighted: 0.03284802746569903
[2m[36m(func pid=120862)[0m f1_per_class: [0.035, 0.103, 0.015, 0.051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:15:49 (running for 00:22:58.24)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.011 |      0.272 |                   98 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  4.189 |      0.194 |                   73 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.748 |      0.02  |                   26 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  3.011 |      0.056 |                    2 |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.0171 | Steps: 2 | Val loss: 42.5182 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.8908 | Steps: 2 | Val loss: 2.1675 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8920 | Steps: 2 | Val loss: 2.3243 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7339 | Steps: 2 | Val loss: 2.3514 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=110192)[0m top1: 0.2453358208955224
[2m[36m(func pid=110192)[0m top5: 0.7924440298507462
[2m[36m(func pid=110192)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=110192)[0m f1_macro: 0.22322050166663088
[2m[36m(func pid=110192)[0m f1_weighted: 0.16458957261436685
[2m[36m(func pid=110192)[0m f1_per_class: [0.234, 0.428, 0.5, 0.013, 0.098, 0.132, 0.098, 0.503, 0.128, 0.099]
[2m[36m(func pid=110192)[0m 
[2m[36m(func pid=103390)[0m top1: 0.3125
[2m[36m(func pid=103390)[0m top5: 0.8880597014925373
[2m[36m(func pid=103390)[0m f1_micro: 0.3125
[2m[36m(func pid=103390)[0m f1_macro: 0.2719641609214942
[2m[36m(func pid=103390)[0m f1_weighted: 0.35192213293191643
[2m[36m(func pid=103390)[0m f1_per_class: [0.165, 0.323, 0.256, 0.311, 0.103, 0.412, 0.422, 0.447, 0.097, 0.185]
[2m[36m(func pid=103390)[0m 
== Status ==
Current time: 2024-01-07 00:15:54 (running for 00:23:03.37)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00009 | RUNNING    | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  0.891 |      0.272 |                   99 |
| train_51d3e_00011 | RUNNING    | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  1.017 |      0.223 |                   74 |
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.748 |      0.02  |                   26 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.892 |      0.014 |                    3 |
| train_51d3e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.03544776119402985
[2m[36m(func pid=120862)[0m top5: 0.5158582089552238
[2m[36m(func pid=120862)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=120862)[0m f1_macro: 0.024626921746340198
[2m[36m(func pid=120862)[0m f1_weighted: 0.039637193698159275
[2m[36m(func pid=120862)[0m f1_per_class: [0.042, 0.129, 0.016, 0.059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.013992537313432836
[2m[36m(func pid=126068)[0m top5: 0.5494402985074627
[2m[36m(func pid=126068)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=126068)[0m f1_macro: 0.013883247352824975
[2m[36m(func pid=126068)[0m f1_weighted: 0.014941906740536632
[2m[36m(func pid=126068)[0m f1_per_class: [0.0, 0.0, 0.012, 0.034, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=110192)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.9075 | Steps: 2 | Val loss: 45.1832 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=103390)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.0030 | Steps: 2 | Val loss: 2.1610 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7176 | Steps: 2 | Val loss: 2.3504 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8893 | Steps: 2 | Val loss: 2.3295 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=110192)[0m top1: 0.26026119402985076
[2m[36m(func pid=110192)[0m top5: 0.8111007462686567
[2m[36m(func pid=110192)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=110192)[0m f1_macro: 0.21645897397641017
[2m[36m(func pid=110192)[0m f1_weighted: 0.19358974401665774
[2m[36m(func pid=110192)[0m f1_per_class: [0.118, 0.375, 0.4, 0.01, 0.127, 0.241, 0.199, 0.487, 0.142, 0.066]
[2m[36m(func pid=103390)[0m top1: 0.31669776119402987
[2m[36m(func pid=103390)[0m top5: 0.8950559701492538
[2m[36m(func pid=103390)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=103390)[0m f1_macro: 0.26659552384217644
[2m[36m(func pid=103390)[0m f1_weighted: 0.35857592848559255
[2m[36m(func pid=103390)[0m f1_per_class: [0.161, 0.29, 0.247, 0.358, 0.1, 0.403, 0.434, 0.393, 0.094, 0.185]
[2m[36m(func pid=120862)[0m top1: 0.03544776119402985
[2m[36m(func pid=120862)[0m top5: 0.5135261194029851
[2m[36m(func pid=120862)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=120862)[0m f1_macro: 0.024101064586621125
[2m[36m(func pid=120862)[0m f1_weighted: 0.03813068978450669
[2m[36m(func pid=120862)[0m f1_per_class: [0.039, 0.137, 0.016, 0.049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=126068)[0m top1: 0.006063432835820896
[2m[36m(func pid=126068)[0m top5: 0.5410447761194029
[2m[36m(func pid=126068)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=126068)[0m f1_macro: 0.0012064965197215777
[2m[36m(func pid=126068)[0m f1_weighted: 7.315510613983447e-05
[2m[36m(func pid=126068)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 00:15:59 (running for 00:23:08.69)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 3 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.734 |      0.025 |                   27 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.892 |      0.014 |                    3 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 TERMINATED)


== Status ==
Current time: 2024-01-07 00:16:06 (running for 00:23:15.64)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 3 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.734 |      0.025 |                   27 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.889 |      0.001 |                    4 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 TERMINATED)


[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127473)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=127473)[0m Configuration completed!
[2m[36m(func pid=127473)[0m New optimizer parameters:
[2m[36m(func pid=127473)[0m SGD (
[2m[36m(func pid=127473)[0m Parameter Group 0
[2m[36m(func pid=127473)[0m     dampening: 0
[2m[36m(func pid=127473)[0m     differentiable: False
[2m[36m(func pid=127473)[0m     foreach: None
[2m[36m(func pid=127473)[0m     lr: 0.01
[2m[36m(func pid=127473)[0m     maximize: False
[2m[36m(func pid=127473)[0m     momentum: 0.9
[2m[36m(func pid=127473)[0m     nesterov: False
[2m[36m(func pid=127473)[0m     weight_decay: 0.0001
[2m[36m(func pid=127473)[0m )
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7158 | Steps: 2 | Val loss: 2.3488 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7648 | Steps: 2 | Val loss: 2.3297 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0227 | Steps: 2 | Val loss: 2.4074 | Batch size: 32 | lr: 0.01 | Duration: 4.61s
[2m[36m(func pid=120862)[0m top1: 0.03871268656716418
[2m[36m(func pid=120862)[0m top5: 0.5111940298507462
[2m[36m(func pid=120862)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=120862)[0m f1_macro: 0.024481563488479182
[2m[36m(func pid=120862)[0m f1_weighted: 0.040796806475593805
[2m[36m(func pid=120862)[0m f1_per_class: [0.032, 0.134, 0.017, 0.061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.006063432835820896
[2m[36m(func pid=126068)[0m top5: 0.5377798507462687
[2m[36m(func pid=126068)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=126068)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=126068)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=126068)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=127473)[0m top1: 0.006063432835820896
[2m[36m(func pid=127473)[0m top5: 0.39365671641791045
[2m[36m(func pid=127473)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=127473)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=127473)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=127473)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7760 | Steps: 2 | Val loss: 2.3480 | Batch size: 32 | lr: 0.0001 | Duration: 2.61s
== Status ==
Current time: 2024-01-07 00:16:12 (running for 00:23:20.92)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.716 |      0.024 |                   29 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.889 |      0.001 |                    4 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127909)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=127909)[0m Configuration completed!
[2m[36m(func pid=127909)[0m New optimizer parameters:
[2m[36m(func pid=127909)[0m SGD (
[2m[36m(func pid=127909)[0m Parameter Group 0
[2m[36m(func pid=127909)[0m     dampening: 0
[2m[36m(func pid=127909)[0m     differentiable: False
[2m[36m(func pid=127909)[0m     foreach: None
[2m[36m(func pid=127909)[0m     lr: 0.1
[2m[36m(func pid=127909)[0m     maximize: False
[2m[36m(func pid=127909)[0m     momentum: 0.9
[2m[36m(func pid=127909)[0m     nesterov: False
[2m[36m(func pid=127909)[0m     weight_decay: 0.0001
[2m[36m(func pid=127909)[0m )
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:16:17 (running for 00:23:25.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.776 |      0.026 |                   30 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.765 |      0.001 |                    5 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  3.023 |      0.001 |                    1 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.042444029850746266
[2m[36m(func pid=120862)[0m top5: 0.5093283582089553
[2m[36m(func pid=120862)[0m f1_micro: 0.042444029850746266
[2m[36m(func pid=120862)[0m f1_macro: 0.02561916232395848
[2m[36m(func pid=120862)[0m f1_weighted: 0.04374562821140721
[2m[36m(func pid=120862)[0m f1_per_class: [0.028, 0.144, 0.018, 0.066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7519 | Steps: 2 | Val loss: 2.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7140 | Steps: 2 | Val loss: 2.3182 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.8833 | Steps: 2 | Val loss: 2.4307 | Batch size: 32 | lr: 0.1 | Duration: 4.34s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8009 | Steps: 2 | Val loss: 2.3478 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=126068)[0m top1: 0.006063432835820896
[2m[36m(func pid=126068)[0m top5: 0.5368470149253731
[2m[36m(func pid=126068)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=126068)[0m f1_macro: 0.0012177985948477752
[2m[36m(func pid=126068)[0m f1_weighted: 7.384039987416548e-05
[2m[36m(func pid=126068)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.006063432835820896
[2m[36m(func pid=127473)[0m top5: 0.6063432835820896
[2m[36m(func pid=127473)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=127473)[0m f1_macro: 0.0012404580152671756
[2m[36m(func pid=127473)[0m f1_weighted: 7.521433861228211e-05
[2m[36m(func pid=127473)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:16:22 (running for 00:23:30.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.776 |      0.026 |                   30 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.752 |      0.001 |                    6 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.714 |      0.001 |                    2 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.883 |      0.015 |                    1 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.03544776119402985
[2m[36m(func pid=127909)[0m top5: 0.6431902985074627
[2m[36m(func pid=127909)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=127909)[0m f1_macro: 0.015380110771333073
[2m[36m(func pid=127909)[0m f1_weighted: 0.006937556359726452
[2m[36m(func pid=127909)[0m f1_per_class: [0.095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.047, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.049906716417910446
[2m[36m(func pid=120862)[0m top5: 0.5097947761194029
[2m[36m(func pid=120862)[0m f1_micro: 0.04990671641791045
[2m[36m(func pid=120862)[0m f1_macro: 0.030235740050793814
[2m[36m(func pid=120862)[0m f1_weighted: 0.05017038195600082
[2m[36m(func pid=120862)[0m f1_per_class: [0.031, 0.16, 0.02, 0.075, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7460 | Steps: 2 | Val loss: 2.3108 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0897 | Steps: 2 | Val loss: 2.1814 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 4.9060 | Steps: 2 | Val loss: 2.6096 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6924 | Steps: 2 | Val loss: 2.3420 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=126068)[0m top1: 0.020522388059701493
[2m[36m(func pid=126068)[0m top5: 0.5377798507462687
[2m[36m(func pid=126068)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=126068)[0m f1_macro: 0.01246320201635541
[2m[36m(func pid=126068)[0m f1_weighted: 0.013745304021675972
[2m[36m(func pid=126068)[0m f1_per_class: [0.057, 0.02, 0.015, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.2248134328358209
[2m[36m(func pid=127473)[0m top5: 0.8680037313432836
[2m[36m(func pid=127473)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=127473)[0m f1_macro: 0.11407167876153161
[2m[36m(func pid=127473)[0m f1_weighted: 0.197448638572692
[2m[36m(func pid=127473)[0m f1_per_class: [0.116, 0.0, 0.291, 0.176, 0.0, 0.0, 0.474, 0.0, 0.083, 0.0]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:16:27 (running for 00:23:35.97)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.801 |      0.03  |                   31 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.746 |      0.012 |                    7 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  3.09  |      0.114 |                    3 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  4.906 |      0.091 |                    2 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.11613805970149253
[2m[36m(func pid=127909)[0m top5: 0.5293843283582089
[2m[36m(func pid=127909)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=127909)[0m f1_macro: 0.09124983411328622
[2m[36m(func pid=127909)[0m f1_weighted: 0.12224777958258472
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.17, 0.278, 0.323, 0.113, 0.0, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.05830223880597015
[2m[36m(func pid=120862)[0m top5: 0.511660447761194
[2m[36m(func pid=120862)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=120862)[0m f1_macro: 0.03356376525647046
[2m[36m(func pid=120862)[0m f1_weighted: 0.055728922062043294
[2m[36m(func pid=120862)[0m f1_per_class: [0.027, 0.165, 0.023, 0.089, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6892 | Steps: 2 | Val loss: 2.2910 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6869 | Steps: 2 | Val loss: 2.0977 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 5.7086 | Steps: 2 | Val loss: 4.0083 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.7118 | Steps: 2 | Val loss: 2.3402 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=126068)[0m top1: 0.05550373134328358
[2m[36m(func pid=126068)[0m top5: 0.5307835820895522
[2m[36m(func pid=126068)[0m f1_micro: 0.05550373134328358
[2m[36m(func pid=126068)[0m f1_macro: 0.029494099449807458
[2m[36m(func pid=126068)[0m f1_weighted: 0.041069789457724847
[2m[36m(func pid=126068)[0m f1_per_class: [0.065, 0.121, 0.037, 0.064, 0.0, 0.008, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.2980410447761194
[2m[36m(func pid=127473)[0m top5: 0.9043843283582089
[2m[36m(func pid=127473)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=127473)[0m f1_macro: 0.09495751912542785
[2m[36m(func pid=127473)[0m f1_weighted: 0.2265267933202902
[2m[36m(func pid=127473)[0m f1_per_class: [0.085, 0.109, 0.0, 0.402, 0.0, 0.062, 0.291, 0.0, 0.0, 0.0]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.06110074626865672
[2m[36m(func pid=127909)[0m top5: 0.4878731343283582
[2m[36m(func pid=127909)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=127909)[0m f1_macro: 0.028080678038837036
[2m[36m(func pid=127909)[0m f1_weighted: 0.0074792096005329
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.0, 0.169, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:16:32 (running for 00:23:41.14)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.692 |      0.034 |                   32 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.689 |      0.029 |                    8 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.687 |      0.095 |                    4 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  5.709 |      0.028 |                    3 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.06203358208955224
[2m[36m(func pid=120862)[0m top5: 0.5088619402985075
[2m[36m(func pid=120862)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=120862)[0m f1_macro: 0.03552004582395579
[2m[36m(func pid=120862)[0m f1_weighted: 0.05783763928494569
[2m[36m(func pid=120862)[0m f1_per_class: [0.028, 0.154, 0.026, 0.1, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6603 | Steps: 2 | Val loss: 2.2723 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.5085 | Steps: 2 | Val loss: 2.2089 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 7.5283 | Steps: 2 | Val loss: 3.6483 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6991 | Steps: 2 | Val loss: 2.3366 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=126068)[0m top1: 0.0648320895522388
[2m[36m(func pid=126068)[0m top5: 0.6231343283582089
[2m[36m(func pid=126068)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=126068)[0m f1_macro: 0.07892308555722724
[2m[36m(func pid=126068)[0m f1_weighted: 0.0474291178775873
[2m[36m(func pid=126068)[0m f1_per_class: [0.069, 0.111, 0.375, 0.057, 0.114, 0.06, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.05970149253731343
[2m[36m(func pid=127473)[0m top5: 0.5932835820895522
[2m[36m(func pid=127473)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=127473)[0m f1_macro: 0.07496371246036412
[2m[36m(func pid=127473)[0m f1_weighted: 0.03850569016102907
[2m[36m(func pid=127473)[0m f1_per_class: [0.036, 0.0, 0.345, 0.016, 0.058, 0.265, 0.0, 0.0, 0.0, 0.029]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:16:37 (running for 00:23:46.30)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.712 |      0.036 |                   33 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.66  |      0.079 |                    9 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.509 |      0.075 |                    5 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  7.528 |      0.035 |                    4 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.03544776119402985
[2m[36m(func pid=127909)[0m top5: 0.5205223880597015
[2m[36m(func pid=127909)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=127909)[0m f1_macro: 0.035218776993417335
[2m[36m(func pid=127909)[0m f1_weighted: 0.0038090587630163013
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.06529850746268656
[2m[36m(func pid=120862)[0m top5: 0.5111940298507462
[2m[36m(func pid=120862)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=120862)[0m f1_macro: 0.03748158132008329
[2m[36m(func pid=120862)[0m f1_weighted: 0.05984285675905563
[2m[36m(func pid=120862)[0m f1_per_class: [0.025, 0.155, 0.029, 0.103, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6345 | Steps: 2 | Val loss: 2.2510 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.3923 | Steps: 2 | Val loss: 2.3187 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 8.5224 | Steps: 2 | Val loss: 4.1502 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7603 | Steps: 2 | Val loss: 2.3353 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=126068)[0m top1: 0.08488805970149253
[2m[36m(func pid=126068)[0m top5: 0.7080223880597015
[2m[36m(func pid=126068)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=126068)[0m f1_macro: 0.08731280008512779
[2m[36m(func pid=126068)[0m f1_weighted: 0.07381770564970229
[2m[36m(func pid=126068)[0m f1_per_class: [0.078, 0.128, 0.357, 0.041, 0.079, 0.113, 0.078, 0.0, 0.0, 0.0]
[2m[36m(func pid=126068)[0m 
== Status ==
Current time: 2024-01-07 00:16:42 (running for 00:23:51.35)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.699 |      0.037 |                   34 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.635 |      0.087 |                   10 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.392 |      0.052 |                    6 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  7.528 |      0.035 |                    4 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127473)[0m top1: 0.021921641791044777
[2m[36m(func pid=127473)[0m top5: 0.37826492537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.021921641791044777
[2m[36m(func pid=127473)[0m f1_macro: 0.05216821653607455
[2m[36m(func pid=127473)[0m f1_weighted: 0.007934943359562864
[2m[36m(func pid=127473)[0m f1_per_class: [0.038, 0.0, 0.357, 0.0, 0.027, 0.008, 0.0, 0.061, 0.0, 0.03]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.12546641791044777
[2m[36m(func pid=127909)[0m top5: 0.605410447761194
[2m[36m(func pid=127909)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=127909)[0m f1_macro: 0.03308964510272672
[2m[36m(func pid=127909)[0m f1_weighted: 0.04530497539147402
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.11, 0.0, 0.0, 0.0, 0.215, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.06669776119402986
[2m[36m(func pid=120862)[0m top5: 0.5111940298507462
[2m[36m(func pid=120862)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=120862)[0m f1_macro: 0.03774247265650268
[2m[36m(func pid=120862)[0m f1_weighted: 0.06026609874299895
[2m[36m(func pid=120862)[0m f1_per_class: [0.023, 0.157, 0.031, 0.104, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6243 | Steps: 2 | Val loss: 2.2293 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 8.7871 | Steps: 2 | Val loss: 9.1395 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3395 | Steps: 2 | Val loss: 2.3171 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6900 | Steps: 2 | Val loss: 2.3300 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=126068)[0m top1: 0.13199626865671643
[2m[36m(func pid=126068)[0m top5: 0.7126865671641791
[2m[36m(func pid=126068)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=126068)[0m f1_macro: 0.10681448876193685
[2m[36m(func pid=126068)[0m f1_weighted: 0.1295136078523438
[2m[36m(func pid=126068)[0m f1_per_class: [0.098, 0.172, 0.324, 0.017, 0.041, 0.152, 0.244, 0.0, 0.018, 0.0]
[2m[36m(func pid=126068)[0m 
== Status ==
Current time: 2024-01-07 00:16:47 (running for 00:23:56.59)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.76  |      0.038 |                   35 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.624 |      0.107 |                   11 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.34  |      0.027 |                    7 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  8.522 |      0.033 |                    5 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127473)[0m top1: 0.022388059701492536
[2m[36m(func pid=127473)[0m top5: 0.3763992537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=127473)[0m f1_macro: 0.027185438547258654
[2m[36m(func pid=127473)[0m f1_weighted: 0.01611060016333696
[2m[36m(func pid=127473)[0m f1_per_class: [0.102, 0.054, 0.0, 0.0, 0.019, 0.0, 0.0, 0.075, 0.0, 0.022]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.013059701492537313
[2m[36m(func pid=127909)[0m top5: 0.447294776119403
[2m[36m(func pid=127909)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=127909)[0m f1_macro: 0.016027998155526916
[2m[36m(func pid=127909)[0m f1_weighted: 0.003084268878716718
[2m[36m(func pid=127909)[0m f1_per_class: [0.145, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07369402985074627
[2m[36m(func pid=120862)[0m top5: 0.5139925373134329
[2m[36m(func pid=120862)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=120862)[0m f1_macro: 0.04144055599591564
[2m[36m(func pid=120862)[0m f1_weighted: 0.06506705593265925
[2m[36m(func pid=120862)[0m f1_per_class: [0.022, 0.172, 0.034, 0.108, 0.0, 0.0, 0.0, 0.078, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5685 | Steps: 2 | Val loss: 2.2148 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.1990 | Steps: 2 | Val loss: 2.2161 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 13.0706 | Steps: 2 | Val loss: 10.5127 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6999 | Steps: 2 | Val loss: 2.3272 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=126068)[0m top1: 0.16977611940298507
[2m[36m(func pid=126068)[0m top5: 0.7024253731343284
[2m[36m(func pid=126068)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=126068)[0m f1_macro: 0.13365793833434955
[2m[36m(func pid=126068)[0m f1_weighted: 0.15962469913972582
[2m[36m(func pid=126068)[0m f1_per_class: [0.12, 0.225, 0.35, 0.016, 0.047, 0.205, 0.286, 0.0, 0.088, 0.0]
[2m[36m(func pid=126068)[0m 
== Status ==
Current time: 2024-01-07 00:16:52 (running for 00:24:01.84)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.69  |      0.041 |                   36 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.568 |      0.134 |                   12 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.34  |      0.027 |                    7 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 | 13.071 |      0.03  |                    7 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127473)[0m top1: 0.05550373134328358
[2m[36m(func pid=127473)[0m top5: 0.5638992537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.05550373134328358
[2m[36m(func pid=127473)[0m f1_macro: 0.04996508114295396
[2m[36m(func pid=127473)[0m f1_weighted: 0.04789547512270914
[2m[36m(func pid=127473)[0m f1_per_class: [0.14, 0.194, 0.0, 0.0, 0.022, 0.055, 0.0, 0.089, 0.0, 0.0]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.11567164179104478
[2m[36m(func pid=127909)[0m top5: 0.36007462686567165
[2m[36m(func pid=127909)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=127909)[0m f1_macro: 0.029644245939159607
[2m[36m(func pid=127909)[0m f1_weighted: 0.027691818887032735
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.076, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07229477611940298
[2m[36m(func pid=120862)[0m top5: 0.5163246268656716
[2m[36m(func pid=120862)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=120862)[0m f1_macro: 0.04222958935649841
[2m[36m(func pid=120862)[0m f1_weighted: 0.06332812141418094
[2m[36m(func pid=120862)[0m f1_per_class: [0.024, 0.167, 0.036, 0.102, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5380 | Steps: 2 | Val loss: 2.2014 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 8.9436 | Steps: 2 | Val loss: 10.3781 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.1923 | Steps: 2 | Val loss: 2.0967 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6726 | Steps: 2 | Val loss: 2.3237 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:16:58 (running for 00:24:06.95)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.7   |      0.042 |                   37 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.538 |      0.123 |                   13 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.199 |      0.05  |                    8 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 | 13.071 |      0.03  |                    7 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m top1: 0.16277985074626866
[2m[36m(func pid=126068)[0m top5: 0.7234141791044776
[2m[36m(func pid=126068)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=126068)[0m f1_macro: 0.12335352230930814
[2m[36m(func pid=126068)[0m f1_weighted: 0.14922744476550423
[2m[36m(func pid=126068)[0m f1_per_class: [0.1, 0.233, 0.292, 0.009, 0.042, 0.206, 0.254, 0.0, 0.097, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.17210820895522388
[2m[36m(func pid=127909)[0m top5: 0.38759328358208955
[2m[36m(func pid=127909)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=127909)[0m f1_macro: 0.029378980891719746
[2m[36m(func pid=127909)[0m f1_weighted: 0.05056363782203632
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.11520522388059702
[2m[36m(func pid=127473)[0m top5: 0.7807835820895522
[2m[36m(func pid=127473)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=127473)[0m f1_macro: 0.06656014449212909
[2m[36m(func pid=127473)[0m f1_weighted: 0.08515883969917014
[2m[36m(func pid=127473)[0m f1_per_class: [0.083, 0.26, 0.0, 0.0, 0.035, 0.142, 0.072, 0.0, 0.0, 0.074]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07462686567164178
[2m[36m(func pid=120862)[0m top5: 0.5228544776119403
[2m[36m(func pid=120862)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=120862)[0m f1_macro: 0.04308384448960434
[2m[36m(func pid=120862)[0m f1_weighted: 0.06490233157095959
[2m[36m(func pid=120862)[0m f1_per_class: [0.023, 0.171, 0.039, 0.106, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5270 | Steps: 2 | Val loss: 2.1921 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 9.5390 | Steps: 2 | Val loss: 4.9955 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3634 | Steps: 2 | Val loss: 2.0454 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6739 | Steps: 2 | Val loss: 2.3199 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:17:03 (running for 00:24:12.33)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.673 |      0.043 |                   38 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.538 |      0.123 |                   13 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.192 |      0.067 |                    9 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  9.539 |      0.05  |                    9 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m top1: 0.16184701492537312
[2m[36m(func pid=126068)[0m top5: 0.7490671641791045
[2m[36m(func pid=126068)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=126068)[0m f1_macro: 0.1251515508295196
[2m[36m(func pid=126068)[0m f1_weighted: 0.1496572557374005
[2m[36m(func pid=126068)[0m f1_per_class: [0.038, 0.247, 0.333, 0.03, 0.042, 0.253, 0.213, 0.0, 0.093, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.07649253731343283
[2m[36m(func pid=127909)[0m top5: 0.5746268656716418
[2m[36m(func pid=127909)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=127909)[0m f1_macro: 0.049836258384444464
[2m[36m(func pid=127909)[0m f1_weighted: 0.09020753137051592
[2m[36m(func pid=127909)[0m f1_per_class: [0.17, 0.0, 0.0, 0.31, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07882462686567164
[2m[36m(func pid=120862)[0m top5: 0.5279850746268657
[2m[36m(func pid=120862)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=120862)[0m f1_macro: 0.04603513043124584
[2m[36m(func pid=120862)[0m f1_weighted: 0.06895602423870797
[2m[36m(func pid=120862)[0m f1_per_class: [0.022, 0.175, 0.042, 0.115, 0.0, 0.0, 0.0, 0.107, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127473)[0m top1: 0.1455223880597015
[2m[36m(func pid=127473)[0m top5: 0.8041044776119403
[2m[36m(func pid=127473)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=127473)[0m f1_macro: 0.08798332547391098
[2m[36m(func pid=127473)[0m f1_weighted: 0.15243255788456372
[2m[36m(func pid=127473)[0m f1_per_class: [0.0, 0.228, 0.0, 0.007, 0.032, 0.189, 0.295, 0.0, 0.0, 0.129]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5068 | Steps: 2 | Val loss: 2.1879 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 8.5852 | Steps: 2 | Val loss: 4.1197 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6773 | Steps: 2 | Val loss: 2.3181 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0447 | Steps: 2 | Val loss: 2.1117 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:17:08 (running for 00:24:17.47)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.674 |      0.046 |                   39 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.527 |      0.125 |                   14 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.363 |      0.088 |                   10 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  8.585 |      0.079 |                   10 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.3125
[2m[36m(func pid=127909)[0m top5: 0.8185634328358209
[2m[36m(func pid=127909)[0m f1_micro: 0.3125
[2m[36m(func pid=127909)[0m f1_macro: 0.07884232243486118
[2m[36m(func pid=127909)[0m f1_weighted: 0.17066037947106746
[2m[36m(func pid=127909)[0m f1_per_class: [0.158, 0.0, 0.0, 0.053, 0.0, 0.106, 0.472, 0.0, 0.0, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m top1: 0.14925373134328357
[2m[36m(func pid=126068)[0m top5: 0.7635261194029851
[2m[36m(func pid=126068)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=126068)[0m f1_macro: 0.11882481684895121
[2m[36m(func pid=126068)[0m f1_weighted: 0.14259561555832848
[2m[36m(func pid=126068)[0m f1_per_class: [0.042, 0.248, 0.31, 0.089, 0.041, 0.231, 0.143, 0.0, 0.084, 0.0]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07695895522388059
[2m[36m(func pid=120862)[0m top5: 0.534981343283582
[2m[36m(func pid=120862)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=120862)[0m f1_macro: 0.04634879031779692
[2m[36m(func pid=120862)[0m f1_weighted: 0.0673096153508095
[2m[36m(func pid=120862)[0m f1_per_class: [0.021, 0.171, 0.043, 0.109, 0.0, 0.0, 0.0, 0.119, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127473)[0m top1: 0.07929104477611941
[2m[36m(func pid=127473)[0m top5: 0.7896455223880597
[2m[36m(func pid=127473)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=127473)[0m f1_macro: 0.05773697933705487
[2m[36m(func pid=127473)[0m f1_weighted: 0.11121862183078536
[2m[36m(func pid=127473)[0m f1_per_class: [0.042, 0.115, 0.0, 0.227, 0.019, 0.0, 0.087, 0.0, 0.0, 0.087]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 6.2604 | Steps: 2 | Val loss: 10.8815 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6801 | Steps: 2 | Val loss: 2.3150 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5163 | Steps: 2 | Val loss: 2.1850 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0634 | Steps: 2 | Val loss: 2.2254 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 00:17:13 (running for 00:24:22.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.677 |      0.046 |                   40 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.507 |      0.119 |                   15 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.045 |      0.058 |                   11 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  6.26  |      0.007 |                   11 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.013526119402985074
[2m[36m(func pid=127909)[0m top5: 0.6291977611940298
[2m[36m(func pid=127909)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=127909)[0m f1_macro: 0.006617695696783149
[2m[36m(func pid=127909)[0m f1_weighted: 0.0031642139542194876
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.024]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07882462686567164
[2m[36m(func pid=120862)[0m top5: 0.5499067164179104
[2m[36m(func pid=120862)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=120862)[0m f1_macro: 0.04828064865762634
[2m[36m(func pid=120862)[0m f1_weighted: 0.06912574083692591
[2m[36m(func pid=120862)[0m f1_per_class: [0.021, 0.177, 0.044, 0.109, 0.0, 0.0, 0.0, 0.132, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.1478544776119403
[2m[36m(func pid=126068)[0m top5: 0.7588619402985075
[2m[36m(func pid=126068)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=126068)[0m f1_macro: 0.12591452716303653
[2m[36m(func pid=126068)[0m f1_weighted: 0.15188200969689708
[2m[36m(func pid=126068)[0m f1_per_class: [0.038, 0.249, 0.256, 0.154, 0.037, 0.218, 0.099, 0.112, 0.055, 0.04]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.058768656716417914
[2m[36m(func pid=127473)[0m top5: 0.6996268656716418
[2m[36m(func pid=127473)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=127473)[0m f1_macro: 0.0556987741728268
[2m[36m(func pid=127473)[0m f1_weighted: 0.08101472084457322
[2m[36m(func pid=127473)[0m f1_per_class: [0.077, 0.053, 0.0, 0.213, 0.017, 0.0, 0.0, 0.171, 0.026, 0.0]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 6.5197 | Steps: 2 | Val loss: 8.1591 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.6789 | Steps: 2 | Val loss: 2.3149 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4650 | Steps: 2 | Val loss: 2.1835 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0084 | Steps: 2 | Val loss: 2.2687 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=127909)[0m top1: 0.1525186567164179
[2m[36m(func pid=127909)[0m top5: 0.6333955223880597
[2m[36m(func pid=127909)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=127909)[0m f1_macro: 0.054906000837892266
[2m[36m(func pid=127909)[0m f1_weighted: 0.07199976265368681
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.063, 0.042]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:19 (running for 00:24:28.19)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.68  |      0.048 |                   41 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.465 |      0.135 |                   17 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.063 |      0.056 |                   12 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  6.52  |      0.055 |                   12 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m top1: 0.13899253731343283
[2m[36m(func pid=126068)[0m top5: 0.7374067164179104
[2m[36m(func pid=126068)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=126068)[0m f1_macro: 0.1349746574583242
[2m[36m(func pid=126068)[0m f1_weighted: 0.1549795694945174
[2m[36m(func pid=126068)[0m f1_per_class: [0.086, 0.216, 0.247, 0.191, 0.035, 0.178, 0.084, 0.24, 0.043, 0.029]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=120862)[0m top1: 0.07882462686567164
[2m[36m(func pid=120862)[0m top5: 0.5583022388059702
[2m[36m(func pid=120862)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=120862)[0m f1_macro: 0.04823311928970018
[2m[36m(func pid=120862)[0m f1_weighted: 0.06974810756973863
[2m[36m(func pid=120862)[0m f1_per_class: [0.022, 0.175, 0.043, 0.109, 0.0, 0.0, 0.003, 0.13, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127473)[0m top1: 0.07369402985074627
[2m[36m(func pid=127473)[0m top5: 0.6674440298507462
[2m[36m(func pid=127473)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=127473)[0m f1_macro: 0.09948337850559008
[2m[36m(func pid=127473)[0m f1_weighted: 0.09146729004526027
[2m[36m(func pid=127473)[0m f1_per_class: [0.043, 0.016, 0.32, 0.254, 0.019, 0.0, 0.0, 0.225, 0.026, 0.092]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 5.7430 | Steps: 2 | Val loss: 6.1459 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6692 | Steps: 2 | Val loss: 2.3132 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4917 | Steps: 2 | Val loss: 2.1820 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9983 | Steps: 2 | Val loss: 2.2313 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=127909)[0m top1: 0.08162313432835822
[2m[36m(func pid=127909)[0m top5: 0.5083955223880597
[2m[36m(func pid=127909)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=127909)[0m f1_macro: 0.06687180736020096
[2m[36m(func pid=127909)[0m f1_weighted: 0.06797611707860532
[2m[36m(func pid=127909)[0m f1_per_class: [0.198, 0.0, 0.0, 0.0, 0.051, 0.106, 0.161, 0.0, 0.082, 0.072]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:24 (running for 00:24:33.44)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.669 |      0.05  |                   43 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.465 |      0.135 |                   17 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  2.008 |      0.099 |                   13 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  5.743 |      0.067 |                   13 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.08208955223880597
[2m[36m(func pid=120862)[0m top5: 0.5620335820895522
[2m[36m(func pid=120862)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=120862)[0m f1_macro: 0.05024132969144596
[2m[36m(func pid=120862)[0m f1_weighted: 0.07159881320776022
[2m[36m(func pid=120862)[0m f1_per_class: [0.022, 0.188, 0.043, 0.106, 0.0, 0.0, 0.003, 0.14, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.13759328358208955
[2m[36m(func pid=126068)[0m top5: 0.7444029850746269
[2m[36m(func pid=126068)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=126068)[0m f1_macro: 0.13573912447131287
[2m[36m(func pid=126068)[0m f1_weighted: 0.15532327175696867
[2m[36m(func pid=126068)[0m f1_per_class: [0.129, 0.195, 0.253, 0.218, 0.038, 0.137, 0.085, 0.262, 0.0, 0.041]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.08582089552238806
[2m[36m(func pid=127473)[0m top5: 0.7014925373134329
[2m[36m(func pid=127473)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=127473)[0m f1_macro: 0.10397925010531597
[2m[36m(func pid=127473)[0m f1_weighted: 0.10162478148804203
[2m[36m(func pid=127473)[0m f1_per_class: [0.0, 0.044, 0.296, 0.253, 0.027, 0.0, 0.0, 0.349, 0.027, 0.044]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.2972 | Steps: 2 | Val loss: 6.4473 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7065 | Steps: 2 | Val loss: 2.3137 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4310 | Steps: 2 | Val loss: 2.1739 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8883 | Steps: 2 | Val loss: 2.1206 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=127909)[0m top1: 0.0708955223880597
[2m[36m(func pid=127909)[0m top5: 0.6683768656716418
[2m[36m(func pid=127909)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=127909)[0m f1_macro: 0.07277047158799825
[2m[36m(func pid=127909)[0m f1_weighted: 0.08227062960540862
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.0, 0.32, 0.272, 0.022, 0.0, 0.0, 0.062, 0.0, 0.052]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:29 (running for 00:24:38.57)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.707 |      0.051 |                   44 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.492 |      0.136 |                   18 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.998 |      0.104 |                   14 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.297 |      0.073 |                   14 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.0830223880597015
[2m[36m(func pid=120862)[0m top5: 0.5666977611940298
[2m[36m(func pid=120862)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=120862)[0m f1_macro: 0.05066758453405871
[2m[36m(func pid=120862)[0m f1_weighted: 0.07225370547343558
[2m[36m(func pid=120862)[0m f1_per_class: [0.022, 0.193, 0.043, 0.105, 0.0, 0.0, 0.003, 0.141, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.14319029850746268
[2m[36m(func pid=126068)[0m top5: 0.7658582089552238
[2m[36m(func pid=126068)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=126068)[0m f1_macro: 0.13928072510971173
[2m[36m(func pid=126068)[0m f1_weighted: 0.1605955924101191
[2m[36m(func pid=126068)[0m f1_per_class: [0.124, 0.165, 0.282, 0.236, 0.041, 0.103, 0.11, 0.29, 0.0, 0.041]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.134794776119403
[2m[36m(func pid=127473)[0m top5: 0.8311567164179104
[2m[36m(func pid=127473)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=127473)[0m f1_macro: 0.12285534840437709
[2m[36m(func pid=127473)[0m f1_weighted: 0.15412810187317466
[2m[36m(func pid=127473)[0m f1_per_class: [0.125, 0.107, 0.111, 0.38, 0.041, 0.037, 0.0, 0.352, 0.028, 0.047]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.3691 | Steps: 2 | Val loss: 4.8409 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6659 | Steps: 2 | Val loss: 2.3105 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4654 | Steps: 2 | Val loss: 2.1751 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.8431 | Steps: 2 | Val loss: 2.0663 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=127909)[0m top1: 0.17444029850746268
[2m[36m(func pid=127909)[0m top5: 0.7639925373134329
[2m[36m(func pid=127909)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=127909)[0m f1_macro: 0.07072736240987139
[2m[36m(func pid=127909)[0m f1_weighted: 0.08838295808411213
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.36, 0.0, 0.0, 0.0, 0.0, 0.033, 0.28, 0.0, 0.034]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:34 (running for 00:24:43.67)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.666 |      0.052 |                   45 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.431 |      0.139 |                   19 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.888 |      0.123 |                   15 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.369 |      0.071 |                   15 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.08582089552238806
[2m[36m(func pid=120862)[0m top5: 0.5722947761194029
[2m[36m(func pid=120862)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=120862)[0m f1_macro: 0.0519414550342891
[2m[36m(func pid=120862)[0m f1_weighted: 0.07460591023525891
[2m[36m(func pid=120862)[0m f1_per_class: [0.022, 0.197, 0.045, 0.11, 0.0, 0.0, 0.003, 0.142, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.134794776119403
[2m[36m(func pid=126068)[0m top5: 0.757929104477612
[2m[36m(func pid=126068)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=126068)[0m f1_macro: 0.14260080535453948
[2m[36m(func pid=126068)[0m f1_weighted: 0.1427072249838473
[2m[36m(func pid=126068)[0m f1_per_class: [0.161, 0.133, 0.377, 0.224, 0.041, 0.092, 0.079, 0.291, 0.0, 0.027]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.1912313432835821
[2m[36m(func pid=127473)[0m top5: 0.8260261194029851
[2m[36m(func pid=127473)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=127473)[0m f1_macro: 0.16001901917524075
[2m[36m(func pid=127473)[0m f1_weighted: 0.19773324591250924
[2m[36m(func pid=127473)[0m f1_per_class: [0.261, 0.253, 0.0, 0.373, 0.073, 0.173, 0.003, 0.395, 0.0, 0.069]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.2769 | Steps: 2 | Val loss: 3.8027 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.6959 | Steps: 2 | Val loss: 2.3111 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4776 | Steps: 2 | Val loss: 2.1709 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7879 | Steps: 2 | Val loss: 2.0050 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=127909)[0m top1: 0.23600746268656717
[2m[36m(func pid=127909)[0m top5: 0.761660447761194
[2m[36m(func pid=127909)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=127909)[0m f1_macro: 0.1497878594163115
[2m[36m(func pid=127909)[0m f1_weighted: 0.2350976346546944
[2m[36m(func pid=127909)[0m f1_per_class: [0.296, 0.0, 0.0, 0.168, 0.0, 0.14, 0.498, 0.238, 0.11, 0.049]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:39 (running for 00:24:48.77)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.696 |      0.053 |                   46 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.465 |      0.143 |                   20 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.843 |      0.16  |                   16 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.277 |      0.15  |                   16 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.0853544776119403
[2m[36m(func pid=120862)[0m top5: 0.5774253731343284
[2m[36m(func pid=120862)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=120862)[0m f1_macro: 0.052853505761679506
[2m[36m(func pid=120862)[0m f1_weighted: 0.0749259554083932
[2m[36m(func pid=120862)[0m f1_per_class: [0.022, 0.193, 0.045, 0.111, 0.0, 0.0, 0.003, 0.154, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.14272388059701493
[2m[36m(func pid=126068)[0m top5: 0.7416044776119403
[2m[36m(func pid=126068)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=126068)[0m f1_macro: 0.14385767730739388
[2m[36m(func pid=126068)[0m f1_weighted: 0.14294999662337052
[2m[36m(func pid=126068)[0m f1_per_class: [0.147, 0.149, 0.392, 0.258, 0.04, 0.055, 0.051, 0.307, 0.0, 0.039]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.21548507462686567
[2m[36m(func pid=127473)[0m top5: 0.8297574626865671
[2m[36m(func pid=127473)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=127473)[0m f1_macro: 0.20683581878806354
[2m[36m(func pid=127473)[0m f1_weighted: 0.20374372466031437
[2m[36m(func pid=127473)[0m f1_per_class: [0.318, 0.279, 0.32, 0.31, 0.077, 0.254, 0.03, 0.348, 0.026, 0.107]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6265 | Steps: 2 | Val loss: 6.8398 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.6420 | Steps: 2 | Val loss: 2.3069 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3828 | Steps: 2 | Val loss: 2.1632 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.9804 | Steps: 2 | Val loss: 1.9902 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=127909)[0m top1: 0.07602611940298508
[2m[36m(func pid=127909)[0m top5: 0.6273320895522388
[2m[36m(func pid=127909)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=127909)[0m f1_macro: 0.08328118514361738
[2m[36m(func pid=127909)[0m f1_weighted: 0.046601102798178376
[2m[36m(func pid=127909)[0m f1_per_class: [0.104, 0.0, 0.0, 0.029, 0.104, 0.06, 0.0, 0.485, 0.0, 0.05]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:45 (running for 00:24:53.99)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.642 |      0.055 |                   47 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.478 |      0.144 |                   21 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.788 |      0.207 |                   17 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.626 |      0.083 |                   17 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.08768656716417911
[2m[36m(func pid=120862)[0m top5: 0.5848880597014925
[2m[36m(func pid=120862)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=120862)[0m f1_macro: 0.054796617464416865
[2m[36m(func pid=120862)[0m f1_weighted: 0.07685362190506702
[2m[36m(func pid=120862)[0m f1_per_class: [0.021, 0.197, 0.048, 0.114, 0.0, 0.0, 0.003, 0.166, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.17350746268656717
[2m[36m(func pid=126068)[0m top5: 0.7318097014925373
[2m[36m(func pid=126068)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=126068)[0m f1_macro: 0.15146437282157646
[2m[36m(func pid=126068)[0m f1_weighted: 0.16576545680076105
[2m[36m(func pid=126068)[0m f1_per_class: [0.143, 0.123, 0.357, 0.356, 0.042, 0.071, 0.043, 0.316, 0.0, 0.064]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.2523320895522388
[2m[36m(func pid=127473)[0m top5: 0.8619402985074627
[2m[36m(func pid=127473)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=127473)[0m f1_macro: 0.2167987998088463
[2m[36m(func pid=127473)[0m f1_weighted: 0.24684141160034279
[2m[36m(func pid=127473)[0m f1_per_class: [0.245, 0.116, 0.345, 0.491, 0.049, 0.198, 0.103, 0.463, 0.027, 0.132]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.9588 | Steps: 2 | Val loss: 7.5060 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.6530 | Steps: 2 | Val loss: 2.3036 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.3832 | Steps: 2 | Val loss: 2.1596 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7363 | Steps: 2 | Val loss: 1.9925 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=127909)[0m top1: 0.15391791044776118
[2m[36m(func pid=127909)[0m top5: 0.5181902985074627
[2m[36m(func pid=127909)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=127909)[0m f1_macro: 0.1423826315464837
[2m[36m(func pid=127909)[0m f1_weighted: 0.1562126903885259
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.0, 0.333, 0.447, 0.034, 0.0, 0.0, 0.42, 0.122, 0.067]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:50 (running for 00:24:59.15)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.653 |      0.053 |                   48 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.383 |      0.151 |                   22 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.98  |      0.217 |                   18 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.959 |      0.142 |                   18 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.08722014925373134
[2m[36m(func pid=120862)[0m top5: 0.5904850746268657
[2m[36m(func pid=120862)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=120862)[0m f1_macro: 0.05344655845863204
[2m[36m(func pid=120862)[0m f1_weighted: 0.07577634164987733
[2m[36m(func pid=120862)[0m f1_per_class: [0.02, 0.199, 0.051, 0.112, 0.0, 0.0, 0.003, 0.15, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.18330223880597016
[2m[36m(func pid=126068)[0m top5: 0.7332089552238806
[2m[36m(func pid=126068)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=126068)[0m f1_macro: 0.15679888471505393
[2m[36m(func pid=126068)[0m f1_weighted: 0.16779802980232048
[2m[36m(func pid=126068)[0m f1_per_class: [0.135, 0.095, 0.349, 0.39, 0.042, 0.115, 0.015, 0.319, 0.0, 0.107]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.25046641791044777
[2m[36m(func pid=127473)[0m top5: 0.8507462686567164
[2m[36m(func pid=127473)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=127473)[0m f1_macro: 0.20907776530626854
[2m[36m(func pid=127473)[0m f1_weighted: 0.21396636729020851
[2m[36m(func pid=127473)[0m f1_per_class: [0.215, 0.0, 0.32, 0.501, 0.048, 0.238, 0.033, 0.472, 0.0, 0.263]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.7464 | Steps: 2 | Val loss: 4.5939 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6348 | Steps: 2 | Val loss: 2.3018 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.3708 | Steps: 2 | Val loss: 2.1541 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7221 | Steps: 2 | Val loss: 2.0249 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=127909)[0m top1: 0.22201492537313433
[2m[36m(func pid=127909)[0m top5: 0.7817164179104478
[2m[36m(func pid=127909)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=127909)[0m f1_macro: 0.14753414888533958
[2m[36m(func pid=127909)[0m f1_weighted: 0.23068833552648343
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.0, 0.34, 0.501, 0.044, 0.172, 0.216, 0.0, 0.104, 0.099]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:17:55 (running for 00:25:04.39)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.635 |      0.054 |                   49 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.383 |      0.157 |                   23 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.736 |      0.209 |                   19 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.746 |      0.148 |                   19 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.08675373134328358
[2m[36m(func pid=120862)[0m top5: 0.5970149253731343
[2m[36m(func pid=120862)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=120862)[0m f1_macro: 0.05362335552270812
[2m[36m(func pid=120862)[0m f1_weighted: 0.07538333222521741
[2m[36m(func pid=120862)[0m f1_per_class: [0.024, 0.194, 0.052, 0.113, 0.0, 0.0, 0.003, 0.151, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.18236940298507462
[2m[36m(func pid=126068)[0m top5: 0.7597947761194029
[2m[36m(func pid=126068)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=126068)[0m f1_macro: 0.15203673617646402
[2m[36m(func pid=126068)[0m f1_weighted: 0.16961282224456867
[2m[36m(func pid=126068)[0m f1_per_class: [0.113, 0.081, 0.268, 0.387, 0.042, 0.146, 0.022, 0.325, 0.0, 0.136]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.22761194029850745
[2m[36m(func pid=127473)[0m top5: 0.8409514925373134
[2m[36m(func pid=127473)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=127473)[0m f1_macro: 0.21319603483543137
[2m[36m(func pid=127473)[0m f1_weighted: 0.20896344770475908
[2m[36m(func pid=127473)[0m f1_per_class: [0.266, 0.026, 0.39, 0.502, 0.037, 0.182, 0.019, 0.45, 0.043, 0.217]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.1147 | Steps: 2 | Val loss: 5.8952 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.6515 | Steps: 2 | Val loss: 2.2987 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3701 | Steps: 2 | Val loss: 2.1495 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6680 | Steps: 2 | Val loss: 2.0856 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=127909)[0m top1: 0.24720149253731344
[2m[36m(func pid=127909)[0m top5: 0.8246268656716418
[2m[36m(func pid=127909)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=127909)[0m f1_macro: 0.13440975177088302
[2m[36m(func pid=127909)[0m f1_weighted: 0.174422678711194
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.345, 0.118, 0.0, 0.0, 0.171, 0.237, 0.402, 0.0, 0.071]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:18:00 (running for 00:25:09.53)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.651 |      0.053 |                   50 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.371 |      0.152 |                   24 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.722 |      0.213 |                   20 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.115 |      0.134 |                   20 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.08815298507462686
[2m[36m(func pid=120862)[0m top5: 0.6040111940298507
[2m[36m(func pid=120862)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=120862)[0m f1_macro: 0.053276483462596204
[2m[36m(func pid=120862)[0m f1_weighted: 0.07647118882743255
[2m[36m(func pid=120862)[0m f1_per_class: [0.023, 0.197, 0.054, 0.118, 0.0, 0.0, 0.003, 0.138, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.1958955223880597
[2m[36m(func pid=126068)[0m top5: 0.78125
[2m[36m(func pid=126068)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=126068)[0m f1_macro: 0.15543732823804454
[2m[36m(func pid=126068)[0m f1_weighted: 0.18343131510393004
[2m[36m(func pid=126068)[0m f1_per_class: [0.11, 0.072, 0.239, 0.406, 0.041, 0.205, 0.034, 0.33, 0.0, 0.118]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.15298507462686567
[2m[36m(func pid=127473)[0m top5: 0.8306902985074627
[2m[36m(func pid=127473)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=127473)[0m f1_macro: 0.1744982929838132
[2m[36m(func pid=127473)[0m f1_weighted: 0.18192669030849604
[2m[36m(func pid=127473)[0m f1_per_class: [0.085, 0.205, 0.319, 0.209, 0.033, 0.077, 0.148, 0.447, 0.148, 0.074]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.2057 | Steps: 2 | Val loss: 5.4911 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6321 | Steps: 2 | Val loss: 2.2938 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3877 | Steps: 2 | Val loss: 2.1464 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.6827 | Steps: 2 | Val loss: 2.1237 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=127909)[0m top1: 0.19449626865671643
[2m[36m(func pid=127909)[0m top5: 0.6926305970149254
[2m[36m(func pid=127909)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=127909)[0m f1_macro: 0.12020784956445105
[2m[36m(func pid=127909)[0m f1_weighted: 0.14377213104920297
[2m[36m(func pid=127909)[0m f1_per_class: [0.127, 0.0, 0.0, 0.394, 0.07, 0.0, 0.0, 0.467, 0.09, 0.053]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:18:05 (running for 00:25:14.56)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.651 |      0.053 |                   50 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.37  |      0.155 |                   25 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.668 |      0.174 |                   21 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.206 |      0.12  |                   21 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.09188432835820895
[2m[36m(func pid=120862)[0m top5: 0.6142723880597015
[2m[36m(func pid=120862)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=120862)[0m f1_macro: 0.05683024240777339
[2m[36m(func pid=120862)[0m f1_weighted: 0.08078368707652975
[2m[36m(func pid=120862)[0m f1_per_class: [0.024, 0.203, 0.054, 0.122, 0.0, 0.0, 0.006, 0.16, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.208955223880597
[2m[36m(func pid=126068)[0m top5: 0.7971082089552238
[2m[36m(func pid=126068)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=126068)[0m f1_macro: 0.16387913927968006
[2m[36m(func pid=126068)[0m f1_weighted: 0.20367626209698783
[2m[36m(func pid=126068)[0m f1_per_class: [0.101, 0.109, 0.212, 0.418, 0.043, 0.231, 0.054, 0.364, 0.0, 0.108]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.11287313432835822
[2m[36m(func pid=127473)[0m top5: 0.8185634328358209
[2m[36m(func pid=127473)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=127473)[0m f1_macro: 0.13862507348902658
[2m[36m(func pid=127473)[0m f1_weighted: 0.09495413571896626
[2m[36m(func pid=127473)[0m f1_per_class: [0.128, 0.223, 0.321, 0.0, 0.055, 0.0, 0.074, 0.458, 0.081, 0.046]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8584 | Steps: 2 | Val loss: 5.1464 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6457 | Steps: 2 | Val loss: 2.2915 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3685 | Steps: 2 | Val loss: 2.1488 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 00:18:10 (running for 00:25:19.68)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.632 |      0.057 |                   51 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.388 |      0.164 |                   26 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.683 |      0.139 |                   22 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.858 |      0.191 |                   22 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.8395 | Steps: 2 | Val loss: 2.0506 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=127909)[0m top1: 0.20662313432835822
[2m[36m(func pid=127909)[0m top5: 0.7486007462686567
[2m[36m(func pid=127909)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=127909)[0m f1_macro: 0.1911212372495263
[2m[36m(func pid=127909)[0m f1_weighted: 0.160468811366663
[2m[36m(func pid=127909)[0m f1_per_class: [0.346, 0.0, 0.308, 0.405, 0.096, 0.008, 0.012, 0.488, 0.101, 0.148]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m top1: 0.09188432835820895
[2m[36m(func pid=120862)[0m top5: 0.6203358208955224
[2m[36m(func pid=120862)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=120862)[0m f1_macro: 0.0567351540205111
[2m[36m(func pid=120862)[0m f1_weighted: 0.08037108720476703
[2m[36m(func pid=120862)[0m f1_per_class: [0.024, 0.202, 0.055, 0.121, 0.0, 0.0, 0.006, 0.16, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.21455223880597016
[2m[36m(func pid=126068)[0m top5: 0.8143656716417911
[2m[36m(func pid=126068)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=126068)[0m f1_macro: 0.17561982613916535
[2m[36m(func pid=126068)[0m f1_weighted: 0.22717888676585124
[2m[36m(func pid=126068)[0m f1_per_class: [0.095, 0.153, 0.185, 0.388, 0.044, 0.251, 0.121, 0.398, 0.0, 0.121]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.1599813432835821
[2m[36m(func pid=127473)[0m top5: 0.8465485074626866
[2m[36m(func pid=127473)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=127473)[0m f1_macro: 0.19196877890653483
[2m[36m(func pid=127473)[0m f1_weighted: 0.16069257071708695
[2m[36m(func pid=127473)[0m f1_per_class: [0.327, 0.253, 0.41, 0.003, 0.069, 0.0, 0.256, 0.455, 0.101, 0.044]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.4088 | Steps: 2 | Val loss: 4.3210 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6694 | Steps: 2 | Val loss: 2.2915 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5104 | Steps: 2 | Val loss: 2.1571 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 00:18:16 (running for 00:25:24.98)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.646 |      0.057 |                   52 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.369 |      0.176 |                   27 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.84  |      0.192 |                   23 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.409 |      0.128 |                   23 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.09375
[2m[36m(func pid=120862)[0m top5: 0.6226679104477612
[2m[36m(func pid=120862)[0m f1_micro: 0.09375
[2m[36m(func pid=120862)[0m f1_macro: 0.05814014007209008
[2m[36m(func pid=120862)[0m f1_weighted: 0.08266109408033338
[2m[36m(func pid=120862)[0m f1_per_class: [0.026, 0.204, 0.057, 0.127, 0.0, 0.0, 0.006, 0.161, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.1837686567164179
[2m[36m(func pid=127909)[0m top5: 0.8418843283582089
[2m[36m(func pid=127909)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=127909)[0m f1_macro: 0.1279666651292292
[2m[36m(func pid=127909)[0m f1_weighted: 0.16600032709462592
[2m[36m(func pid=127909)[0m f1_per_class: [0.044, 0.152, 0.328, 0.042, 0.076, 0.277, 0.31, 0.0, 0.0, 0.05]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.5924 | Steps: 2 | Val loss: 1.9971 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=126068)[0m top1: 0.19962686567164178
[2m[36m(func pid=126068)[0m top5: 0.8218283582089553
[2m[36m(func pid=126068)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=126068)[0m f1_macro: 0.17991925458706332
[2m[36m(func pid=126068)[0m f1_weighted: 0.2316260384295442
[2m[36m(func pid=126068)[0m f1_per_class: [0.087, 0.164, 0.175, 0.311, 0.047, 0.255, 0.198, 0.394, 0.036, 0.133]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.269589552238806
[2m[36m(func pid=127473)[0m top5: 0.8684701492537313
[2m[36m(func pid=127473)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=127473)[0m f1_macro: 0.22377345011186675
[2m[36m(func pid=127473)[0m f1_weighted: 0.3187670959782028
[2m[36m(func pid=127473)[0m f1_per_class: [0.19, 0.113, 0.286, 0.458, 0.084, 0.202, 0.395, 0.357, 0.096, 0.057]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.6269 | Steps: 2 | Val loss: 2.2881 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.6211 | Steps: 2 | Val loss: 6.1889 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3562 | Steps: 2 | Val loss: 2.1577 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127909)[0m top1: 0.24253731343283583
[2m[36m(func pid=127909)[0m top5: 0.8311567164179104
[2m[36m(func pid=127909)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=127909)[0m f1_macro: 0.17293306760130162
[2m[36m(func pid=127909)[0m f1_weighted: 0.1623166006239926
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.404, 0.34, 0.0, 0.061, 0.124, 0.161, 0.449, 0.0, 0.19]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:18:21 (running for 00:25:30.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.627 |      0.062 |                   54 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.51  |      0.18  |                   28 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.592 |      0.224 |                   24 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.621 |      0.173 |                   24 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.09468283582089553
[2m[36m(func pid=120862)[0m top5: 0.628731343283582
[2m[36m(func pid=120862)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=120862)[0m f1_macro: 0.06216487780087397
[2m[36m(func pid=120862)[0m f1_weighted: 0.08407271800478681
[2m[36m(func pid=120862)[0m f1_per_class: [0.026, 0.199, 0.058, 0.135, 0.0, 0.0, 0.009, 0.139, 0.0, 0.056]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.5790 | Steps: 2 | Val loss: 1.8771 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=126068)[0m top1: 0.177705223880597
[2m[36m(func pid=126068)[0m top5: 0.8152985074626866
[2m[36m(func pid=126068)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=126068)[0m f1_macro: 0.18224450601045789
[2m[36m(func pid=126068)[0m f1_weighted: 0.21069684711863818
[2m[36m(func pid=126068)[0m f1_per_class: [0.092, 0.21, 0.21, 0.16, 0.048, 0.255, 0.238, 0.366, 0.1, 0.143]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.35867537313432835
[2m[36m(func pid=127473)[0m top5: 0.9015858208955224
[2m[36m(func pid=127473)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=127473)[0m f1_macro: 0.24329235715460387
[2m[36m(func pid=127473)[0m f1_weighted: 0.34957012422982314
[2m[36m(func pid=127473)[0m f1_per_class: [0.206, 0.011, 0.4, 0.515, 0.089, 0.39, 0.47, 0.162, 0.028, 0.163]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7702 | Steps: 2 | Val loss: 3.5996 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6202 | Steps: 2 | Val loss: 2.2864 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=127909)[0m top1: 0.33488805970149255
[2m[36m(func pid=127909)[0m top5: 0.8717350746268657
[2m[36m(func pid=127909)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=127909)[0m f1_macro: 0.2167772445190297
[2m[36m(func pid=127909)[0m f1_weighted: 0.28366954755286794
[2m[36m(func pid=127909)[0m f1_per_class: [0.302, 0.091, 0.262, 0.514, 0.137, 0.063, 0.278, 0.402, 0.05, 0.069]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:18:26 (running for 00:25:35.31)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.62  |      0.065 |                   55 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.356 |      0.182 |                   29 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.579 |      0.243 |                   25 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.77  |      0.217 |                   25 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.09748134328358209
[2m[36m(func pid=120862)[0m top5: 0.6291977611940298
[2m[36m(func pid=120862)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=120862)[0m f1_macro: 0.06471236928324237
[2m[36m(func pid=120862)[0m f1_weighted: 0.087472696420647
[2m[36m(func pid=120862)[0m f1_per_class: [0.029, 0.196, 0.059, 0.145, 0.0, 0.0, 0.009, 0.154, 0.0, 0.054]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3357 | Steps: 2 | Val loss: 2.1579 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.5900 | Steps: 2 | Val loss: 1.8138 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=126068)[0m top1: 0.16977611940298507
[2m[36m(func pid=126068)[0m top5: 0.8232276119402985
[2m[36m(func pid=126068)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=126068)[0m f1_macro: 0.1791771240137525
[2m[36m(func pid=126068)[0m f1_weighted: 0.19694803913226291
[2m[36m(func pid=126068)[0m f1_per_class: [0.103, 0.239, 0.234, 0.104, 0.051, 0.27, 0.229, 0.331, 0.081, 0.148]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6208 | Steps: 2 | Val loss: 2.2856 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.4922 | Steps: 2 | Val loss: 4.1992 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=127473)[0m top1: 0.3833955223880597
[2m[36m(func pid=127473)[0m top5: 0.9235074626865671
[2m[36m(func pid=127473)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=127473)[0m f1_macro: 0.2646082816478067
[2m[36m(func pid=127473)[0m f1_weighted: 0.3668694701103169
[2m[36m(func pid=127473)[0m f1_per_class: [0.27, 0.085, 0.324, 0.551, 0.091, 0.409, 0.419, 0.255, 0.058, 0.186]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=120862)[0m top1: 0.09794776119402986
[2m[36m(func pid=120862)[0m top5: 0.6319962686567164
[2m[36m(func pid=120862)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=120862)[0m f1_macro: 0.06460353126726945
[2m[36m(func pid=120862)[0m f1_weighted: 0.08896596366292786
[2m[36m(func pid=120862)[0m f1_per_class: [0.029, 0.187, 0.06, 0.153, 0.0, 0.0, 0.012, 0.153, 0.0, 0.051]
== Status ==
Current time: 2024-01-07 00:18:31 (running for 00:25:40.56)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.621 |      0.065 |                   56 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.336 |      0.179 |                   30 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.59  |      0.265 |                   26 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.77  |      0.217 |                   25 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.291044776119403
[2m[36m(func pid=127909)[0m top5: 0.8460820895522388
[2m[36m(func pid=127909)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=127909)[0m f1_macro: 0.23416763028279206
[2m[36m(func pid=127909)[0m f1_weighted: 0.25044150301179335
[2m[36m(func pid=127909)[0m f1_per_class: [0.391, 0.163, 0.333, 0.471, 0.0, 0.389, 0.053, 0.257, 0.124, 0.16]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5262 | Steps: 2 | Val loss: 2.1686 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.6827 | Steps: 2 | Val loss: 1.8806 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=126068)[0m top1: 0.15485074626865672
[2m[36m(func pid=126068)[0m top5: 0.824160447761194
[2m[36m(func pid=126068)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=126068)[0m f1_macro: 0.16298851145197124
[2m[36m(func pid=126068)[0m f1_weighted: 0.17116696452632668
[2m[36m(func pid=126068)[0m f1_per_class: [0.091, 0.25, 0.216, 0.044, 0.052, 0.268, 0.206, 0.274, 0.082, 0.148]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6538 | Steps: 2 | Val loss: 2.2838 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=127473)[0m top1: 0.30830223880597013
[2m[36m(func pid=127473)[0m top5: 0.9029850746268657
[2m[36m(func pid=127473)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=127473)[0m f1_macro: 0.2612286922552503
[2m[36m(func pid=127473)[0m f1_weighted: 0.3300560662602862
[2m[36m(func pid=127473)[0m f1_per_class: [0.123, 0.351, 0.218, 0.24, 0.074, 0.358, 0.421, 0.398, 0.179, 0.25]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.7694 | Steps: 2 | Val loss: 3.1707 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=120862)[0m top1: 0.09888059701492537
[2m[36m(func pid=120862)[0m top5: 0.6361940298507462
[2m[36m(func pid=120862)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=120862)[0m f1_macro: 0.06550366411241407
[2m[36m(func pid=120862)[0m f1_weighted: 0.09133185510839523
[2m[36m(func pid=120862)[0m f1_per_class: [0.028, 0.186, 0.062, 0.156, 0.0, 0.0, 0.018, 0.153, 0.0, 0.051]
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:18:36 (running for 00:25:45.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.654 |      0.066 |                   57 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.526 |      0.163 |                   31 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.683 |      0.261 |                   27 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.492 |      0.234 |                   26 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.3558768656716418
[2m[36m(func pid=127909)[0m top5: 0.8213619402985075
[2m[36m(func pid=127909)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=127909)[0m f1_macro: 0.2590862120449918
[2m[36m(func pid=127909)[0m f1_weighted: 0.30554136282121036
[2m[36m(func pid=127909)[0m f1_per_class: [0.283, 0.502, 0.333, 0.484, 0.0, 0.381, 0.012, 0.457, 0.056, 0.083]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3230 | Steps: 2 | Val loss: 2.1615 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.5381 | Steps: 2 | Val loss: 1.9867 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.7583 | Steps: 2 | Val loss: 2.2861 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=126068)[0m top1: 0.1623134328358209
[2m[36m(func pid=126068)[0m top5: 0.8194962686567164
[2m[36m(func pid=126068)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=126068)[0m f1_macro: 0.167688815440332
[2m[36m(func pid=126068)[0m f1_weighted: 0.17145179834807506
[2m[36m(func pid=126068)[0m f1_per_class: [0.109, 0.255, 0.242, 0.029, 0.052, 0.266, 0.216, 0.282, 0.078, 0.148]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0069 | Steps: 2 | Val loss: 2.7838 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=127473)[0m top1: 0.22901119402985073
[2m[36m(func pid=127473)[0m top5: 0.8619402985074627
[2m[36m(func pid=127473)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=127473)[0m f1_macro: 0.2216470085733552
[2m[36m(func pid=127473)[0m f1_weighted: 0.21907861286179278
[2m[36m(func pid=127473)[0m f1_per_class: [0.161, 0.338, 0.227, 0.047, 0.052, 0.243, 0.269, 0.449, 0.181, 0.25]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:18:41 (running for 00:25:50.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.758 |      0.068 |                   58 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.323 |      0.168 |                   32 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.538 |      0.222 |                   28 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.769 |      0.259 |                   27 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.10121268656716417
[2m[36m(func pid=120862)[0m top5: 0.6478544776119403
[2m[36m(func pid=120862)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=120862)[0m f1_macro: 0.0676471469385995
[2m[36m(func pid=120862)[0m f1_weighted: 0.09642245171602844
[2m[36m(func pid=120862)[0m f1_per_class: [0.028, 0.193, 0.062, 0.154, 0.0, 0.0, 0.033, 0.154, 0.0, 0.053]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.41744402985074625
[2m[36m(func pid=127909)[0m top5: 0.9053171641791045
[2m[36m(func pid=127909)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=127909)[0m f1_macro: 0.2814318017695522
[2m[36m(func pid=127909)[0m f1_weighted: 0.42077777337847505
[2m[36m(func pid=127909)[0m f1_per_class: [0.275, 0.529, 0.439, 0.515, 0.058, 0.0, 0.527, 0.326, 0.0, 0.146]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4038 | Steps: 2 | Val loss: 2.1596 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.4960 | Steps: 2 | Val loss: 2.0473 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7331 | Steps: 2 | Val loss: 2.2857 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=126068)[0m top1: 0.17257462686567165
[2m[36m(func pid=126068)[0m top5: 0.8260261194029851
[2m[36m(func pid=126068)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=126068)[0m f1_macro: 0.1684903008453474
[2m[36m(func pid=126068)[0m f1_weighted: 0.1790961581705624
[2m[36m(func pid=126068)[0m f1_per_class: [0.1, 0.268, 0.234, 0.038, 0.056, 0.262, 0.23, 0.274, 0.075, 0.148]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.6026 | Steps: 2 | Val loss: 3.9835 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=127473)[0m top1: 0.22574626865671643
[2m[36m(func pid=127473)[0m top5: 0.8451492537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=127473)[0m f1_macro: 0.21129891612498736
[2m[36m(func pid=127473)[0m f1_weighted: 0.24083626071716838
[2m[36m(func pid=127473)[0m f1_per_class: [0.205, 0.339, 0.278, 0.192, 0.046, 0.067, 0.277, 0.464, 0.132, 0.114]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:18:47 (running for 00:25:56.03)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.733 |      0.065 |                   59 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.404 |      0.168 |                   33 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.496 |      0.211 |                   29 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.007 |      0.281 |                   28 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.10167910447761194
[2m[36m(func pid=120862)[0m top5: 0.6520522388059702
[2m[36m(func pid=120862)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=120862)[0m f1_macro: 0.06469971291231494
[2m[36m(func pid=120862)[0m f1_weighted: 0.0992705181658696
[2m[36m(func pid=120862)[0m f1_per_class: [0.027, 0.189, 0.064, 0.153, 0.0, 0.0, 0.045, 0.169, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.21688432835820895
[2m[36m(func pid=127909)[0m top5: 0.8666044776119403
[2m[36m(func pid=127909)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=127909)[0m f1_macro: 0.18918656361968367
[2m[36m(func pid=127909)[0m f1_weighted: 0.24940291982799986
[2m[36m(func pid=127909)[0m f1_per_class: [0.167, 0.314, 0.306, 0.126, 0.035, 0.0, 0.472, 0.119, 0.139, 0.214]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2770 | Steps: 2 | Val loss: 2.1512 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.8481 | Steps: 2 | Val loss: 2.0114 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.7560 | Steps: 2 | Val loss: 2.2856 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9222 | Steps: 2 | Val loss: 3.9926 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=126068)[0m top1: 0.19029850746268656
[2m[36m(func pid=126068)[0m top5: 0.8344216417910447
[2m[36m(func pid=126068)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=126068)[0m f1_macro: 0.1882822295295802
[2m[36m(func pid=126068)[0m f1_weighted: 0.19970489190361979
[2m[36m(func pid=126068)[0m f1_per_class: [0.106, 0.259, 0.265, 0.065, 0.063, 0.292, 0.264, 0.268, 0.078, 0.222]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.2868470149253731
[2m[36m(func pid=127473)[0m top5: 0.8549440298507462
[2m[36m(func pid=127473)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=127473)[0m f1_macro: 0.23700546802533254
[2m[36m(func pid=127473)[0m f1_weighted: 0.30601884137585905
[2m[36m(func pid=127473)[0m f1_per_class: [0.173, 0.193, 0.417, 0.484, 0.058, 0.131, 0.284, 0.502, 0.049, 0.079]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:18:52 (running for 00:26:01.04)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.756 |      0.061 |                   60 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.277 |      0.188 |                   34 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.848 |      0.237 |                   30 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.603 |      0.189 |                   29 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.10027985074626866
[2m[36m(func pid=120862)[0m top5: 0.6576492537313433
[2m[36m(func pid=120862)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=120862)[0m f1_macro: 0.06093394867451474
[2m[36m(func pid=120862)[0m f1_weighted: 0.09878367087695515
[2m[36m(func pid=120862)[0m f1_per_class: [0.027, 0.182, 0.063, 0.152, 0.0, 0.0, 0.056, 0.129, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.2630597014925373
[2m[36m(func pid=127909)[0m top5: 0.8031716417910447
[2m[36m(func pid=127909)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=127909)[0m f1_macro: 0.2559684946137997
[2m[36m(func pid=127909)[0m f1_weighted: 0.22709776039161897
[2m[36m(func pid=127909)[0m f1_per_class: [0.171, 0.52, 0.206, 0.173, 0.269, 0.341, 0.039, 0.424, 0.115, 0.301]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2717 | Steps: 2 | Val loss: 2.1393 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.5260 | Steps: 2 | Val loss: 1.9272 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6256 | Steps: 2 | Val loss: 2.2774 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0340 | Steps: 2 | Val loss: 3.6150 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=126068)[0m top1: 0.21128731343283583
[2m[36m(func pid=126068)[0m top5: 0.8386194029850746
[2m[36m(func pid=126068)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=126068)[0m f1_macro: 0.19987081407193524
[2m[36m(func pid=126068)[0m f1_weighted: 0.22420978941438366
[2m[36m(func pid=126068)[0m f1_per_class: [0.109, 0.244, 0.306, 0.115, 0.07, 0.321, 0.296, 0.28, 0.063, 0.195]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.3185634328358209
[2m[36m(func pid=127473)[0m top5: 0.8591417910447762
[2m[36m(func pid=127473)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=127473)[0m f1_macro: 0.2495605976378717
[2m[36m(func pid=127473)[0m f1_weighted: 0.27916496171193594
[2m[36m(func pid=127473)[0m f1_per_class: [0.271, 0.136, 0.387, 0.541, 0.074, 0.321, 0.096, 0.466, 0.088, 0.117]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:18:57 (running for 00:26:06.32)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.626 |      0.064 |                   61 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.272 |      0.2   |                   35 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.526 |      0.25  |                   31 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.922 |      0.256 |                   30 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.10774253731343283
[2m[36m(func pid=120862)[0m top5: 0.6669776119402985
[2m[36m(func pid=120862)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=120862)[0m f1_macro: 0.06438241733618444
[2m[36m(func pid=120862)[0m f1_weighted: 0.10609954797083458
[2m[36m(func pid=120862)[0m f1_per_class: [0.027, 0.188, 0.07, 0.168, 0.0, 0.0, 0.062, 0.129, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.3568097014925373
[2m[36m(func pid=127909)[0m top5: 0.8647388059701493
[2m[36m(func pid=127909)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=127909)[0m f1_macro: 0.2962809865806381
[2m[36m(func pid=127909)[0m f1_weighted: 0.4042153597514748
[2m[36m(func pid=127909)[0m f1_per_class: [0.117, 0.389, 0.407, 0.451, 0.2, 0.392, 0.458, 0.323, 0.111, 0.114]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2564 | Steps: 2 | Val loss: 2.1337 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.5320 | Steps: 2 | Val loss: 1.8859 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.7002 | Steps: 2 | Val loss: 2.2765 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0787 | Steps: 2 | Val loss: 2.7553 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=126068)[0m top1: 0.2234141791044776
[2m[36m(func pid=126068)[0m top5: 0.8297574626865671
[2m[36m(func pid=126068)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=126068)[0m f1_macro: 0.20145860381071423
[2m[36m(func pid=126068)[0m f1_weighted: 0.2440546102409625
[2m[36m(func pid=126068)[0m f1_per_class: [0.103, 0.184, 0.293, 0.204, 0.071, 0.323, 0.315, 0.29, 0.035, 0.196]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.30223880597014924
[2m[36m(func pid=127473)[0m top5: 0.8666044776119403
[2m[36m(func pid=127473)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=127473)[0m f1_macro: 0.28890454471558336
[2m[36m(func pid=127473)[0m f1_weighted: 0.29396008281416214
[2m[36m(func pid=127473)[0m f1_per_class: [0.333, 0.377, 0.424, 0.419, 0.094, 0.391, 0.085, 0.424, 0.169, 0.173]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:19:02 (running for 00:26:11.43)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.7   |      0.064 |                   62 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.256 |      0.201 |                   36 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.532 |      0.289 |                   32 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.034 |      0.296 |                   31 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.10774253731343283
[2m[36m(func pid=120862)[0m top5: 0.6697761194029851
[2m[36m(func pid=120862)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=120862)[0m f1_macro: 0.06421835758227633
[2m[36m(func pid=120862)[0m f1_weighted: 0.10604687875295914
[2m[36m(func pid=120862)[0m f1_per_class: [0.03, 0.174, 0.071, 0.173, 0.0, 0.0, 0.065, 0.129, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.425839552238806
[2m[36m(func pid=127909)[0m top5: 0.8908582089552238
[2m[36m(func pid=127909)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=127909)[0m f1_macro: 0.3061892684294768
[2m[36m(func pid=127909)[0m f1_weighted: 0.4312293531746599
[2m[36m(func pid=127909)[0m f1_per_class: [0.249, 0.531, 0.348, 0.524, 0.033, 0.18, 0.449, 0.439, 0.139, 0.171]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3504 | Steps: 2 | Val loss: 2.1321 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.6109 | Steps: 2 | Val loss: 1.9081 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6375 | Steps: 2 | Val loss: 2.2732 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5184 | Steps: 2 | Val loss: 3.8372 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=126068)[0m top1: 0.22527985074626866
[2m[36m(func pid=126068)[0m top5: 0.8148320895522388
[2m[36m(func pid=126068)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=126068)[0m f1_macro: 0.18752896203505764
[2m[36m(func pid=126068)[0m f1_weighted: 0.2398503471088868
[2m[36m(func pid=126068)[0m f1_per_class: [0.085, 0.092, 0.247, 0.289, 0.072, 0.327, 0.275, 0.305, 0.021, 0.161]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.2667910447761194
[2m[36m(func pid=127473)[0m top5: 0.8568097014925373
[2m[36m(func pid=127473)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=127473)[0m f1_macro: 0.2600907436740477
[2m[36m(func pid=127473)[0m f1_weighted: 0.2327947855188817
[2m[36m(func pid=127473)[0m f1_per_class: [0.341, 0.369, 0.364, 0.26, 0.117, 0.399, 0.042, 0.362, 0.165, 0.182]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:19:07 (running for 00:26:16.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.638 |      0.072 |                   63 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.35  |      0.188 |                   37 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.611 |      0.26  |                   33 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.079 |      0.306 |                   32 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.11007462686567164
[2m[36m(func pid=120862)[0m top5: 0.6767723880597015
[2m[36m(func pid=120862)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=120862)[0m f1_macro: 0.07219874209631534
[2m[36m(func pid=120862)[0m f1_weighted: 0.11062384916313284
[2m[36m(func pid=120862)[0m f1_per_class: [0.029, 0.179, 0.07, 0.173, 0.0, 0.0, 0.073, 0.143, 0.0, 0.056]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.29850746268656714
[2m[36m(func pid=127909)[0m top5: 0.832089552238806
[2m[36m(func pid=127909)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=127909)[0m f1_macro: 0.23282331165905537
[2m[36m(func pid=127909)[0m f1_weighted: 0.21273104297941134
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.495, 0.328, 0.159, 0.142, 0.369, 0.015, 0.443, 0.164, 0.214]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3056 | Steps: 2 | Val loss: 2.1227 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5463 | Steps: 2 | Val loss: 1.9317 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6134 | Steps: 2 | Val loss: 2.2686 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6754 | Steps: 2 | Val loss: 3.5203 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=127473)[0m top1: 0.302705223880597
[2m[36m(func pid=127473)[0m top5: 0.8591417910447762
[2m[36m(func pid=127473)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=127473)[0m f1_macro: 0.2490472576976157
[2m[36m(func pid=127473)[0m f1_weighted: 0.30147885296255383
[2m[36m(func pid=127473)[0m f1_per_class: [0.206, 0.352, 0.282, 0.222, 0.107, 0.406, 0.342, 0.36, 0.022, 0.19]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=126068)[0m top1: 0.23880597014925373
[2m[36m(func pid=126068)[0m top5: 0.8115671641791045
[2m[36m(func pid=126068)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=126068)[0m f1_macro: 0.18798353475964572
[2m[36m(func pid=126068)[0m f1_weighted: 0.24200163808658884
[2m[36m(func pid=126068)[0m f1_per_class: [0.099, 0.031, 0.25, 0.36, 0.049, 0.332, 0.244, 0.329, 0.022, 0.164]
[2m[36m(func pid=126068)[0m 
== Status ==
Current time: 2024-01-07 00:19:12 (running for 00:26:21.53)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.613 |      0.067 |                   64 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.306 |      0.188 |                   38 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.546 |      0.249 |                   34 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.518 |      0.233 |                   33 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.11147388059701492
[2m[36m(func pid=120862)[0m top5: 0.6856343283582089
[2m[36m(func pid=120862)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=120862)[0m f1_macro: 0.06673595870915446
[2m[36m(func pid=120862)[0m f1_weighted: 0.1118794779162014
[2m[36m(func pid=120862)[0m f1_per_class: [0.028, 0.169, 0.076, 0.183, 0.0, 0.008, 0.076, 0.129, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.31669776119402987
[2m[36m(func pid=127909)[0m top5: 0.8703358208955224
[2m[36m(func pid=127909)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=127909)[0m f1_macro: 0.24071723326851463
[2m[36m(func pid=127909)[0m f1_weighted: 0.3446314886411135
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.302, 0.242, 0.449, 0.062, 0.327, 0.344, 0.317, 0.137, 0.227]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2840 | Steps: 2 | Val loss: 2.1158 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.4734 | Steps: 2 | Val loss: 2.0739 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6058 | Steps: 2 | Val loss: 2.2643 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2118 | Steps: 2 | Val loss: 3.9228 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=126068)[0m top1: 0.2775186567164179
[2m[36m(func pid=126068)[0m top5: 0.8055037313432836
[2m[36m(func pid=126068)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=126068)[0m f1_macro: 0.20260270444590134
[2m[36m(func pid=126068)[0m f1_weighted: 0.2855624752643038
[2m[36m(func pid=126068)[0m f1_per_class: [0.098, 0.011, 0.229, 0.414, 0.074, 0.366, 0.338, 0.338, 0.022, 0.137]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.25326492537313433
[2m[36m(func pid=127473)[0m top5: 0.8596082089552238
[2m[36m(func pid=127473)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=127473)[0m f1_macro: 0.20624926088894818
[2m[36m(func pid=127473)[0m f1_weighted: 0.2750235676463734
[2m[36m(func pid=127473)[0m f1_per_class: [0.11, 0.256, 0.237, 0.103, 0.071, 0.374, 0.449, 0.352, 0.0, 0.111]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=120862)[0m top1: 0.1142723880597015
[2m[36m(func pid=120862)[0m top5: 0.6921641791044776
[2m[36m(func pid=120862)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=120862)[0m f1_macro: 0.07326678645825355
[2m[36m(func pid=120862)[0m f1_weighted: 0.11413865394684682
[2m[36m(func pid=120862)[0m f1_per_class: [0.028, 0.169, 0.08, 0.188, 0.0, 0.008, 0.076, 0.129, 0.0, 0.056]
[2m[36m(func pid=120862)[0m 
== Status ==
Current time: 2024-01-07 00:19:17 (running for 00:26:26.55)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.606 |      0.073 |                   65 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.284 |      0.203 |                   39 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.473 |      0.206 |                   35 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.675 |      0.241 |                   34 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.2947761194029851
[2m[36m(func pid=127909)[0m top5: 0.8563432835820896
[2m[36m(func pid=127909)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=127909)[0m f1_macro: 0.21172433698967827
[2m[36m(func pid=127909)[0m f1_weighted: 0.28514057316805785
[2m[36m(func pid=127909)[0m f1_per_class: [0.173, 0.378, 0.34, 0.072, 0.08, 0.283, 0.509, 0.104, 0.074, 0.104]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2358 | Steps: 2 | Val loss: 2.1134 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.2975 | Steps: 2 | Val loss: 2.1665 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6142 | Steps: 2 | Val loss: 2.2615 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4833 | Steps: 2 | Val loss: 4.4925 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=126068)[0m top1: 0.2891791044776119
[2m[36m(func pid=126068)[0m top5: 0.808768656716418
[2m[36m(func pid=126068)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=126068)[0m f1_macro: 0.20333818258094896
[2m[36m(func pid=126068)[0m f1_weighted: 0.2968692757786744
[2m[36m(func pid=126068)[0m f1_per_class: [0.093, 0.005, 0.214, 0.441, 0.089, 0.368, 0.355, 0.329, 0.024, 0.115]
[2m[36m(func pid=126068)[0m 
== Status ==
Current time: 2024-01-07 00:19:22 (running for 00:26:31.56)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.606 |      0.073 |                   65 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.236 |      0.203 |                   40 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.297 |      0.181 |                   36 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.212 |      0.212 |                   35 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.11613805970149253
[2m[36m(func pid=120862)[0m top5: 0.7010261194029851
[2m[36m(func pid=120862)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=120862)[0m f1_macro: 0.0733363277867879
[2m[36m(func pid=120862)[0m f1_weighted: 0.11450578004764309
[2m[36m(func pid=120862)[0m f1_per_class: [0.029, 0.165, 0.079, 0.195, 0.0, 0.008, 0.073, 0.128, 0.0, 0.057]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127473)[0m top1: 0.22388059701492538
[2m[36m(func pid=127473)[0m top5: 0.8572761194029851
[2m[36m(func pid=127473)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=127473)[0m f1_macro: 0.18144082466332787
[2m[36m(func pid=127473)[0m f1_weighted: 0.24747895354387467
[2m[36m(func pid=127473)[0m f1_per_class: [0.092, 0.294, 0.19, 0.032, 0.046, 0.249, 0.477, 0.148, 0.093, 0.194]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.2490671641791045
[2m[36m(func pid=127909)[0m top5: 0.8451492537313433
[2m[36m(func pid=127909)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=127909)[0m f1_macro: 0.2009236092450931
[2m[36m(func pid=127909)[0m f1_weighted: 0.22180776563403012
[2m[36m(func pid=127909)[0m f1_per_class: [0.104, 0.462, 0.125, 0.032, 0.107, 0.257, 0.234, 0.436, 0.117, 0.136]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.3485 | Steps: 2 | Val loss: 2.2021 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6123 | Steps: 2 | Val loss: 2.2602 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3828 | Steps: 2 | Val loss: 2.1155 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7418 | Steps: 2 | Val loss: 4.6203 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:19:27 (running for 00:26:36.88)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.614 |      0.073 |                   66 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.236 |      0.203 |                   40 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.349 |      0.182 |                   37 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.483 |      0.201 |                   36 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.11847014925373134
[2m[36m(func pid=120862)[0m top5: 0.7024253731343284
[2m[36m(func pid=120862)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=120862)[0m f1_macro: 0.07540738972322707
[2m[36m(func pid=120862)[0m f1_weighted: 0.11735577905059236
[2m[36m(func pid=120862)[0m f1_per_class: [0.029, 0.169, 0.079, 0.197, 0.0, 0.008, 0.076, 0.141, 0.0, 0.056]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.29244402985074625
[2m[36m(func pid=126068)[0m top5: 0.8083022388059702
[2m[36m(func pid=126068)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=126068)[0m f1_macro: 0.2008504198298854
[2m[36m(func pid=126068)[0m f1_weighted: 0.30181403176436716
[2m[36m(func pid=126068)[0m f1_per_class: [0.096, 0.0, 0.183, 0.472, 0.091, 0.325, 0.353, 0.389, 0.024, 0.075]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.18003731343283583
[2m[36m(func pid=127473)[0m top5: 0.8456156716417911
[2m[36m(func pid=127473)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=127473)[0m f1_macro: 0.18163809952182003
[2m[36m(func pid=127473)[0m f1_weighted: 0.20615796144056692
[2m[36m(func pid=127473)[0m f1_per_class: [0.131, 0.318, 0.188, 0.013, 0.041, 0.114, 0.36, 0.297, 0.094, 0.261]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.1982276119402985
[2m[36m(func pid=127909)[0m top5: 0.820429104477612
[2m[36m(func pid=127909)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=127909)[0m f1_macro: 0.20834419316876723
[2m[36m(func pid=127909)[0m f1_weighted: 0.1894068916238351
[2m[36m(func pid=127909)[0m f1_per_class: [0.08, 0.085, 0.415, 0.339, 0.117, 0.373, 0.015, 0.398, 0.1, 0.163]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.2981 | Steps: 2 | Val loss: 2.0858 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.2864 | Steps: 2 | Val loss: 2.1129 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6327 | Steps: 2 | Val loss: 2.2577 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4783 | Steps: 2 | Val loss: 3.8443 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 00:19:33 (running for 00:26:42.28)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.612 |      0.075 |                   67 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.383 |      0.201 |                   41 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.298 |      0.238 |                   38 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.742 |      0.208 |                   37 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.1259328358208955
[2m[36m(func pid=120862)[0m top5: 0.7089552238805971
[2m[36m(func pid=120862)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=120862)[0m f1_macro: 0.07779400601245776
[2m[36m(func pid=120862)[0m f1_weighted: 0.12458637561266271
[2m[36m(func pid=120862)[0m f1_per_class: [0.031, 0.167, 0.078, 0.213, 0.0, 0.008, 0.086, 0.139, 0.0, 0.056]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.28824626865671643
[2m[36m(func pid=126068)[0m top5: 0.8111007462686567
[2m[36m(func pid=126068)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=126068)[0m f1_macro: 0.19536435332590196
[2m[36m(func pid=126068)[0m f1_weighted: 0.2966056469369426
[2m[36m(func pid=126068)[0m f1_per_class: [0.097, 0.0, 0.182, 0.488, 0.088, 0.258, 0.342, 0.414, 0.025, 0.061]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.24347014925373134
[2m[36m(func pid=127473)[0m top5: 0.8488805970149254
[2m[36m(func pid=127473)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=127473)[0m f1_macro: 0.23808705276364747
[2m[36m(func pid=127473)[0m f1_weighted: 0.29450235261839824
[2m[36m(func pid=127473)[0m f1_per_class: [0.162, 0.399, 0.196, 0.308, 0.054, 0.244, 0.26, 0.407, 0.101, 0.25]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.37779850746268656
[2m[36m(func pid=127909)[0m top5: 0.8344216417910447
[2m[36m(func pid=127909)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=127909)[0m f1_macro: 0.263378946740601
[2m[36m(func pid=127909)[0m f1_weighted: 0.3657036536396322
[2m[36m(func pid=127909)[0m f1_per_class: [0.139, 0.011, 0.218, 0.515, 0.181, 0.49, 0.444, 0.386, 0.048, 0.203]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.5943 | Steps: 2 | Val loss: 2.2554 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2671 | Steps: 2 | Val loss: 2.1026 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2937 | Steps: 2 | Val loss: 1.9626 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8489 | Steps: 2 | Val loss: 4.3053 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 00:19:38 (running for 00:26:47.73)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.633 |      0.078 |                   68 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.267 |      0.193 |                   43 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.298 |      0.238 |                   38 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.478 |      0.263 |                   38 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.1357276119402985
[2m[36m(func pid=120862)[0m top5: 0.710820895522388
[2m[36m(func pid=120862)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=120862)[0m f1_macro: 0.08154033808090287
[2m[36m(func pid=120862)[0m f1_weighted: 0.13249153613988698
[2m[36m(func pid=120862)[0m f1_per_class: [0.032, 0.174, 0.078, 0.235, 0.0, 0.008, 0.086, 0.15, 0.0, 0.053]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.29197761194029853
[2m[36m(func pid=126068)[0m top5: 0.8125
[2m[36m(func pid=126068)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=126068)[0m f1_macro: 0.1929498260712125
[2m[36m(func pid=126068)[0m f1_weighted: 0.29276297981796323
[2m[36m(func pid=126068)[0m f1_per_class: [0.091, 0.0, 0.2, 0.499, 0.084, 0.214, 0.335, 0.418, 0.026, 0.063]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.3516791044776119
[2m[36m(func pid=127473)[0m top5: 0.8451492537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=127473)[0m f1_macro: 0.27540235501413596
[2m[36m(func pid=127473)[0m f1_weighted: 0.3354720435516059
[2m[36m(func pid=127473)[0m f1_per_class: [0.207, 0.346, 0.237, 0.522, 0.096, 0.365, 0.158, 0.504, 0.106, 0.213]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.29011194029850745
[2m[36m(func pid=127909)[0m top5: 0.8442164179104478
[2m[36m(func pid=127909)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=127909)[0m f1_macro: 0.25301033729159605
[2m[36m(func pid=127909)[0m f1_weighted: 0.3083469396896413
[2m[36m(func pid=127909)[0m f1_per_class: [0.178, 0.154, 0.158, 0.265, 0.131, 0.445, 0.407, 0.394, 0.124, 0.274]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.5962 | Steps: 2 | Val loss: 2.2541 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2152 | Steps: 2 | Val loss: 2.0931 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.3196 | Steps: 2 | Val loss: 1.9278 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6705 | Steps: 2 | Val loss: 4.9634 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:19:44 (running for 00:26:52.92)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.596 |      0.085 |                   70 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.267 |      0.193 |                   43 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.294 |      0.275 |                   39 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.849 |      0.253 |                   39 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.14132462686567165
[2m[36m(func pid=120862)[0m top5: 0.7126865671641791
[2m[36m(func pid=120862)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=120862)[0m f1_macro: 0.08458967567802975
[2m[36m(func pid=120862)[0m f1_weighted: 0.13787220541861905
[2m[36m(func pid=120862)[0m f1_per_class: [0.033, 0.172, 0.076, 0.245, 0.0, 0.008, 0.092, 0.171, 0.0, 0.049]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.29617537313432835
[2m[36m(func pid=126068)[0m top5: 0.8092350746268657
[2m[36m(func pid=126068)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=126068)[0m f1_macro: 0.2004119251501726
[2m[36m(func pid=126068)[0m f1_weighted: 0.29356891512439315
[2m[36m(func pid=126068)[0m f1_per_class: [0.134, 0.0, 0.259, 0.501, 0.074, 0.181, 0.344, 0.413, 0.025, 0.073]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.3558768656716418
[2m[36m(func pid=127473)[0m top5: 0.8624067164179104
[2m[36m(func pid=127473)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=127473)[0m f1_macro: 0.2932183534894518
[2m[36m(func pid=127473)[0m f1_weighted: 0.33126083020120484
[2m[36m(func pid=127473)[0m f1_per_class: [0.212, 0.404, 0.355, 0.495, 0.148, 0.414, 0.117, 0.476, 0.115, 0.195]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.24347014925373134
[2m[36m(func pid=127909)[0m top5: 0.7999067164179104
[2m[36m(func pid=127909)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=127909)[0m f1_macro: 0.22706748140667404
[2m[36m(func pid=127909)[0m f1_weighted: 0.22545816074347627
[2m[36m(func pid=127909)[0m f1_per_class: [0.081, 0.507, 0.214, 0.026, 0.083, 0.335, 0.177, 0.497, 0.161, 0.189]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.1941 | Steps: 2 | Val loss: 2.0901 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.5975 | Steps: 2 | Val loss: 2.2509 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.4174 | Steps: 2 | Val loss: 1.9556 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2003 | Steps: 2 | Val loss: 6.6130 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 00:19:49 (running for 00:26:58.27)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.596 |      0.085 |                   70 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.194 |      0.196 |                   45 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.32  |      0.293 |                   40 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.67  |      0.227 |                   40 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.14458955223880596
[2m[36m(func pid=120862)[0m top5: 0.7164179104477612
[2m[36m(func pid=120862)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=120862)[0m f1_macro: 0.07740779276553722
[2m[36m(func pid=120862)[0m f1_weighted: 0.13807217354689283
[2m[36m(func pid=120862)[0m f1_per_class: [0.036, 0.174, 0.076, 0.251, 0.0, 0.008, 0.094, 0.135, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.28404850746268656
[2m[36m(func pid=126068)[0m top5: 0.8120335820895522
[2m[36m(func pid=126068)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=126068)[0m f1_macro: 0.1956470474553289
[2m[36m(func pid=126068)[0m f1_weighted: 0.27316060427831507
[2m[36m(func pid=126068)[0m f1_per_class: [0.139, 0.005, 0.272, 0.498, 0.074, 0.162, 0.282, 0.411, 0.026, 0.087]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.31902985074626866
[2m[36m(func pid=127473)[0m top5: 0.8726679104477612
[2m[36m(func pid=127473)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=127473)[0m f1_macro: 0.28319481172270666
[2m[36m(func pid=127473)[0m f1_weighted: 0.335067051550696
[2m[36m(func pid=127473)[0m f1_per_class: [0.148, 0.396, 0.408, 0.295, 0.158, 0.421, 0.338, 0.43, 0.068, 0.17]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.21222014925373134
[2m[36m(func pid=127909)[0m top5: 0.8283582089552238
[2m[36m(func pid=127909)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=127909)[0m f1_macro: 0.19141371385986555
[2m[36m(func pid=127909)[0m f1_weighted: 0.19970917701862403
[2m[36m(func pid=127909)[0m f1_per_class: [0.072, 0.553, 0.387, 0.003, 0.081, 0.296, 0.173, 0.208, 0.0, 0.14]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2379 | Steps: 2 | Val loss: 2.0968 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.5804 | Steps: 2 | Val loss: 2.2489 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2457 | Steps: 2 | Val loss: 1.9613 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.8880 | Steps: 2 | Val loss: 4.5284 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 00:19:54 (running for 00:27:03.63)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.58  |      0.078 |                   72 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.194 |      0.196 |                   45 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.417 |      0.283 |                   41 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.2   |      0.191 |                   41 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.14692164179104478
[2m[36m(func pid=120862)[0m top5: 0.7215485074626866
[2m[36m(func pid=120862)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=120862)[0m f1_macro: 0.07832389533186389
[2m[36m(func pid=120862)[0m f1_weighted: 0.14070989282267343
[2m[36m(func pid=120862)[0m f1_per_class: [0.036, 0.172, 0.076, 0.254, 0.0, 0.008, 0.102, 0.135, 0.0, 0.0]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.2635261194029851
[2m[36m(func pid=126068)[0m top5: 0.8097014925373134
[2m[36m(func pid=126068)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=126068)[0m f1_macro: 0.1873257389187239
[2m[36m(func pid=126068)[0m f1_weighted: 0.262888387158658
[2m[36m(func pid=126068)[0m f1_per_class: [0.147, 0.026, 0.286, 0.482, 0.062, 0.154, 0.266, 0.361, 0.0, 0.09]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.3101679104477612
[2m[36m(func pid=127473)[0m top5: 0.8591417910447762
[2m[36m(func pid=127473)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=127473)[0m f1_macro: 0.27109628825962356
[2m[36m(func pid=127473)[0m f1_weighted: 0.30433681441057764
[2m[36m(func pid=127473)[0m f1_per_class: [0.154, 0.364, 0.439, 0.057, 0.11, 0.429, 0.479, 0.366, 0.116, 0.196]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.33722014925373134
[2m[36m(func pid=127909)[0m top5: 0.8805970149253731
[2m[36m(func pid=127909)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=127909)[0m f1_macro: 0.1985007418716326
[2m[36m(func pid=127909)[0m f1_weighted: 0.26847602545926313
[2m[36m(func pid=127909)[0m f1_per_class: [0.189, 0.037, 0.218, 0.522, 0.1, 0.314, 0.192, 0.275, 0.0, 0.139]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5852 | Steps: 2 | Val loss: 2.2478 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1920 | Steps: 2 | Val loss: 2.0960 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 3.8843 | Steps: 2 | Val loss: 6.8292 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.1744 | Steps: 2 | Val loss: 1.9825 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 00:20:00 (running for 00:27:08.93)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.58  |      0.078 |                   72 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.238 |      0.187 |                   46 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.246 |      0.271 |                   42 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.888 |      0.199 |                   42 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.15205223880597016
[2m[36m(func pid=120862)[0m top5: 0.7215485074626866
[2m[36m(func pid=120862)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=120862)[0m f1_macro: 0.08711769103775399
[2m[36m(func pid=120862)[0m f1_weighted: 0.14990768324415424
[2m[36m(func pid=120862)[0m f1_per_class: [0.036, 0.171, 0.076, 0.257, 0.0, 0.008, 0.125, 0.154, 0.0, 0.043]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=126068)[0m top1: 0.25
[2m[36m(func pid=126068)[0m top5: 0.8115671641791045
[2m[36m(func pid=126068)[0m f1_micro: 0.25
[2m[36m(func pid=126068)[0m f1_macro: 0.19523078811917033
[2m[36m(func pid=126068)[0m f1_weighted: 0.25922608978972767
[2m[36m(func pid=126068)[0m f1_per_class: [0.134, 0.046, 0.349, 0.436, 0.055, 0.131, 0.29, 0.367, 0.0, 0.143]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.18889925373134328
[2m[36m(func pid=127909)[0m top5: 0.7527985074626866
[2m[36m(func pid=127909)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=127909)[0m f1_macro: 0.17627724642608222
[2m[36m(func pid=127909)[0m f1_weighted: 0.1510724051100385
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.0, 0.161, 0.221, 0.098, 0.399, 0.025, 0.483, 0.121, 0.256]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.2728544776119403
[2m[36m(func pid=127473)[0m top5: 0.8409514925373134
[2m[36m(func pid=127473)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=127473)[0m f1_macro: 0.25551643403386864
[2m[36m(func pid=127473)[0m f1_weighted: 0.27563949602765836
[2m[36m(func pid=127473)[0m f1_per_class: [0.23, 0.322, 0.408, 0.023, 0.063, 0.364, 0.461, 0.371, 0.103, 0.211]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.5955 | Steps: 2 | Val loss: 2.2458 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1810 | Steps: 2 | Val loss: 2.0985 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1762 | Steps: 2 | Val loss: 1.9957 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.6039 | Steps: 2 | Val loss: 4.8780 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:20:05 (running for 00:27:14.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00012 | RUNNING    | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.595 |      0.086 |                   74 |
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.192 |      0.195 |                   47 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.174 |      0.256 |                   43 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.884 |      0.176 |                   43 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.15485074626865672
[2m[36m(func pid=120862)[0m top5: 0.726679104477612
[2m[36m(func pid=120862)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=120862)[0m f1_macro: 0.08613923705059762
[2m[36m(func pid=120862)[0m f1_weighted: 0.1531444041617895
[2m[36m(func pid=120862)[0m f1_per_class: [0.038, 0.17, 0.077, 0.259, 0.0, 0.008, 0.14, 0.127, 0.0, 0.043]
[2m[36m(func pid=120862)[0m 
[2m[36m(func pid=127909)[0m top1: 0.314365671641791
[2m[36m(func pid=127909)[0m top5: 0.7490671641791045
[2m[36m(func pid=127909)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=127909)[0m f1_macro: 0.21488173568169597
[2m[36m(func pid=127909)[0m f1_weighted: 0.26076360173384955
[2m[36m(func pid=127909)[0m f1_per_class: [0.156, 0.141, 0.265, 0.068, 0.166, 0.425, 0.498, 0.207, 0.0, 0.224]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m top1: 0.24207089552238806
[2m[36m(func pid=126068)[0m top5: 0.8134328358208955
[2m[36m(func pid=126068)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=126068)[0m f1_macro: 0.19624962701157583
[2m[36m(func pid=126068)[0m f1_weighted: 0.2603169630459301
[2m[36m(func pid=126068)[0m f1_per_class: [0.127, 0.051, 0.333, 0.427, 0.046, 0.09, 0.315, 0.358, 0.0, 0.214]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.23694029850746268
[2m[36m(func pid=127473)[0m top5: 0.8572761194029851
[2m[36m(func pid=127473)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=127473)[0m f1_macro: 0.24166418833943584
[2m[36m(func pid=127473)[0m f1_weighted: 0.2670230672769005
[2m[36m(func pid=127473)[0m f1_per_class: [0.292, 0.299, 0.379, 0.093, 0.039, 0.145, 0.452, 0.408, 0.119, 0.19]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=120862)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6051 | Steps: 2 | Val loss: 2.2490 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 4.2332 | Steps: 2 | Val loss: 6.1126 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.2150 | Steps: 2 | Val loss: 1.9520 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1648 | Steps: 2 | Val loss: 2.1044 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 00:20:10 (running for 00:27:19.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 3 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.181 |      0.196 |                   48 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.176 |      0.242 |                   44 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.604 |      0.215 |                   44 |
| train_51d3e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120862)[0m top1: 0.15578358208955223
[2m[36m(func pid=120862)[0m top5: 0.7220149253731343
[2m[36m(func pid=120862)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=120862)[0m f1_macro: 0.08816447920042832
[2m[36m(func pid=120862)[0m f1_weighted: 0.15498985931994066
[2m[36m(func pid=120862)[0m f1_per_class: [0.038, 0.168, 0.073, 0.262, 0.0, 0.008, 0.139, 0.152, 0.0, 0.041]
[2m[36m(func pid=127909)[0m top1: 0.27098880597014924
[2m[36m(func pid=127909)[0m top5: 0.8423507462686567
[2m[36m(func pid=127909)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=127909)[0m f1_macro: 0.21775856300675261
[2m[36m(func pid=127909)[0m f1_weighted: 0.23796050326995283
[2m[36m(func pid=127909)[0m f1_per_class: [0.106, 0.522, 0.222, 0.207, 0.0, 0.373, 0.042, 0.504, 0.0, 0.2]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.29617537313432835
[2m[36m(func pid=127473)[0m top5: 0.8894589552238806
[2m[36m(func pid=127473)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=127473)[0m f1_macro: 0.2592818607978268
[2m[36m(func pid=127473)[0m f1_weighted: 0.3446352326017175
[2m[36m(func pid=127473)[0m f1_per_class: [0.261, 0.161, 0.344, 0.468, 0.038, 0.152, 0.443, 0.378, 0.152, 0.195]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=126068)[0m top1: 0.22574626865671643
[2m[36m(func pid=126068)[0m top5: 0.8152985074626866
[2m[36m(func pid=126068)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=126068)[0m f1_macro: 0.18807673929054053
[2m[36m(func pid=126068)[0m f1_weighted: 0.2538516413342186
[2m[36m(func pid=126068)[0m f1_per_class: [0.123, 0.06, 0.324, 0.384, 0.039, 0.052, 0.342, 0.375, 0.0, 0.182]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 5.4154 | Steps: 2 | Val loss: 6.5448 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.3051 | Steps: 2 | Val loss: 1.9203 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2597 | Steps: 2 | Val loss: 2.1172 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=127909)[0m top1: 0.363339552238806
[2m[36m(func pid=127909)[0m top5: 0.6744402985074627
[2m[36m(func pid=127909)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=127909)[0m f1_macro: 0.24122370266852883
[2m[36m(func pid=127909)[0m f1_weighted: 0.278214333983638
[2m[36m(func pid=127909)[0m f1_per_class: [0.204, 0.513, 0.328, 0.483, 0.091, 0.148, 0.0, 0.515, 0.0, 0.129]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.33955223880597013
[2m[36m(func pid=127473)[0m top5: 0.8880597014925373
[2m[36m(func pid=127473)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=127473)[0m f1_macro: 0.2549225367488943
[2m[36m(func pid=127473)[0m f1_weighted: 0.3341980079568249
[2m[36m(func pid=127473)[0m f1_per_class: [0.242, 0.047, 0.272, 0.546, 0.059, 0.338, 0.335, 0.379, 0.104, 0.226]
[2m[36m(func pid=126068)[0m top1: 0.2140858208955224
[2m[36m(func pid=126068)[0m top5: 0.8176305970149254
[2m[36m(func pid=126068)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=126068)[0m f1_macro: 0.17655086204758477
[2m[36m(func pid=126068)[0m f1_weighted: 0.2513979441224823
[2m[36m(func pid=126068)[0m f1_per_class: [0.106, 0.1, 0.265, 0.316, 0.037, 0.016, 0.394, 0.358, 0.0, 0.172]
== Status ==
Current time: 2024-01-07 00:20:16 (running for 00:27:25.02)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.165 |      0.188 |                   49 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.215 |      0.259 |                   45 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  5.415 |      0.241 |                   46 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.8953 | Steps: 2 | Val loss: 8.1432 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=138124)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=138124)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=138124)[0m Configuration completed!
[2m[36m(func pid=138124)[0m New optimizer parameters:
[2m[36m(func pid=138124)[0m SGD (
[2m[36m(func pid=138124)[0m Parameter Group 0
[2m[36m(func pid=138124)[0m     dampening: 0
[2m[36m(func pid=138124)[0m     differentiable: False
[2m[36m(func pid=138124)[0m     foreach: None
[2m[36m(func pid=138124)[0m     lr: 0.0001
[2m[36m(func pid=138124)[0m     maximize: False
[2m[36m(func pid=138124)[0m     momentum: 0.99
[2m[36m(func pid=138124)[0m     nesterov: False
[2m[36m(func pid=138124)[0m     weight_decay: 1e-05
[2m[36m(func pid=138124)[0m )
[2m[36m(func pid=138124)[0m 
== Status ==
Current time: 2024-01-07 00:20:21 (running for 00:27:30.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.26  |      0.177 |                   50 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.305 |      0.255 |                   46 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.895 |      0.125 |                   47 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.08022388059701492
[2m[36m(func pid=127909)[0m top5: 0.5830223880597015
[2m[36m(func pid=127909)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=127909)[0m f1_macro: 0.12514286136850045
[2m[36m(func pid=127909)[0m f1_weighted: 0.052398095329690565
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.005, 0.333, 0.0, 0.037, 0.15, 0.0, 0.463, 0.102, 0.161]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1586 | Steps: 2 | Val loss: 2.1247 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1891 | Steps: 2 | Val loss: 1.9423 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0767 | Steps: 2 | Val loss: 2.3707 | Batch size: 32 | lr: 0.0001 | Duration: 4.51s
[2m[36m(func pid=126068)[0m top1: 0.20615671641791045
[2m[36m(func pid=126068)[0m top5: 0.820429104477612
[2m[36m(func pid=126068)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=126068)[0m f1_macro: 0.16713027572698014
[2m[36m(func pid=126068)[0m f1_weighted: 0.2500912049201987
[2m[36m(func pid=126068)[0m f1_per_class: [0.101, 0.161, 0.253, 0.261, 0.036, 0.016, 0.411, 0.357, 0.0, 0.077]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.3558768656716418
[2m[36m(func pid=127473)[0m top5: 0.8698694029850746
[2m[36m(func pid=127473)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=127473)[0m f1_macro: 0.2641130020210591
[2m[36m(func pid=127473)[0m f1_weighted: 0.3458316450426147
[2m[36m(func pid=127473)[0m f1_per_class: [0.202, 0.164, 0.237, 0.549, 0.102, 0.387, 0.284, 0.394, 0.138, 0.186]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.0992 | Steps: 2 | Val loss: 7.3536 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=138124)[0m top1: 0.06669776119402986
[2m[36m(func pid=138124)[0m top5: 0.38619402985074625
[2m[36m(func pid=138124)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=138124)[0m f1_macro: 0.016846992349870644
[2m[36m(func pid=138124)[0m f1_weighted: 0.02093017342293207
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.0, 0.051, 0.0, 0.0, 0.0, 0.118, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
== Status ==
Current time: 2024-01-07 00:20:26 (running for 00:27:35.53)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.159 |      0.167 |                   51 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.189 |      0.264 |                   47 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.099 |      0.19  |                   48 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  3.077 |      0.017 |                    1 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.29757462686567165
[2m[36m(func pid=127909)[0m top5: 0.6152052238805971
[2m[36m(func pid=127909)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=127909)[0m f1_macro: 0.1897746723754687
[2m[36m(func pid=127909)[0m f1_weighted: 0.21023195209529807
[2m[36m(func pid=127909)[0m f1_per_class: [0.31, 0.011, 0.421, 0.0, 0.102, 0.397, 0.506, 0.015, 0.0, 0.137]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1724 | Steps: 2 | Val loss: 2.1383 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1646 | Steps: 2 | Val loss: 1.9986 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0626 | Steps: 2 | Val loss: 2.3266 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=126068)[0m top1: 0.1837686567164179
[2m[36m(func pid=126068)[0m top5: 0.8115671641791045
[2m[36m(func pid=126068)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=126068)[0m f1_macro: 0.1601203403140655
[2m[36m(func pid=126068)[0m f1_weighted: 0.22529097357552824
[2m[36m(func pid=126068)[0m f1_per_class: [0.103, 0.182, 0.239, 0.183, 0.033, 0.015, 0.388, 0.341, 0.037, 0.08]
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3655 | Steps: 2 | Val loss: 7.4709 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.3306902985074627
[2m[36m(func pid=127473)[0m top5: 0.863339552238806
[2m[36m(func pid=127473)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=127473)[0m f1_macro: 0.2770625314784565
[2m[36m(func pid=127473)[0m f1_weighted: 0.3413316193846008
[2m[36m(func pid=127473)[0m f1_per_class: [0.191, 0.451, 0.224, 0.331, 0.16, 0.415, 0.296, 0.398, 0.117, 0.187]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=138124)[0m top1: 0.14132462686567165
[2m[36m(func pid=138124)[0m top5: 0.5251865671641791
[2m[36m(func pid=138124)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=138124)[0m f1_macro: 0.041939936057583116
[2m[36m(func pid=138124)[0m f1_weighted: 0.07941964603509818
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m top1: 0.2798507462686567
[2m[36m(func pid=127909)[0m top5: 0.6632462686567164
[2m[36m(func pid=127909)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=127909)[0m f1_macro: 0.22056373195415943
[2m[36m(func pid=127909)[0m f1_weighted: 0.18783983817402278
[2m[36m(func pid=127909)[0m f1_per_class: [0.194, 0.53, 0.409, 0.0, 0.0, 0.373, 0.084, 0.329, 0.0, 0.286]
== Status ==
Current time: 2024-01-07 00:20:31 (running for 00:27:40.70)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.172 |      0.16  |                   52 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.165 |      0.277 |                   48 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.366 |      0.221 |                   49 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  3.063 |      0.042 |                    2 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1702 | Steps: 2 | Val loss: 2.1510 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1247 | Steps: 2 | Val loss: 2.0603 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0501 | Steps: 2 | Val loss: 2.3161 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4459 | Steps: 2 | Val loss: 7.0861 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=126068)[0m top1: 0.15811567164179105
[2m[36m(func pid=126068)[0m top5: 0.8078358208955224
[2m[36m(func pid=126068)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=126068)[0m f1_macro: 0.1527405840101686
[2m[36m(func pid=126068)[0m f1_weighted: 0.1932577336974801
[2m[36m(func pid=126068)[0m f1_per_class: [0.106, 0.2, 0.234, 0.096, 0.033, 0.036, 0.341, 0.346, 0.041, 0.095]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.30970149253731344
[2m[36m(func pid=127473)[0m top5: 0.8605410447761194
[2m[36m(func pid=127473)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=127473)[0m f1_macro: 0.2754265832144703
[2m[36m(func pid=127473)[0m f1_weighted: 0.27111684438351996
[2m[36m(func pid=127473)[0m f1_per_class: [0.189, 0.463, 0.272, 0.013, 0.212, 0.436, 0.329, 0.44, 0.121, 0.279]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=138124)[0m top1: 0.18423507462686567
[2m[36m(func pid=138124)[0m top5: 0.5708955223880597
[2m[36m(func pid=138124)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=138124)[0m f1_macro: 0.04777354803382484
[2m[36m(func pid=138124)[0m f1_weighted: 0.0982928679019594
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.158, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
== Status ==
Current time: 2024-01-07 00:20:36 (running for 00:27:45.81)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.17  |      0.153 |                   53 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.125 |      0.275 |                   49 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.446 |      0.186 |                   50 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  3.05  |      0.048 |                    3 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.2635261194029851
[2m[36m(func pid=127909)[0m top5: 0.6814365671641791
[2m[36m(func pid=127909)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=127909)[0m f1_macro: 0.18609234949080128
[2m[36m(func pid=127909)[0m f1_weighted: 0.15391919094957887
[2m[36m(func pid=127909)[0m f1_per_class: [0.218, 0.474, 0.195, 0.0, 0.0, 0.375, 0.0, 0.3, 0.141, 0.158]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1377 | Steps: 2 | Val loss: 2.1605 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1533 | Steps: 2 | Val loss: 2.1304 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0585 | Steps: 2 | Val loss: 2.3157 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.2478 | Steps: 2 | Val loss: 7.7897 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=126068)[0m top1: 0.15111940298507462
[2m[36m(func pid=126068)[0m top5: 0.8027052238805971
[2m[36m(func pid=126068)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=126068)[0m f1_macro: 0.1613156723566424
[2m[36m(func pid=126068)[0m f1_weighted: 0.1716725181273525
[2m[36m(func pid=126068)[0m f1_per_class: [0.111, 0.234, 0.229, 0.01, 0.035, 0.095, 0.286, 0.416, 0.101, 0.095]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.30130597014925375
[2m[36m(func pid=127473)[0m top5: 0.8423507462686567
[2m[36m(func pid=127473)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=127473)[0m f1_macro: 0.26460192517053593
[2m[36m(func pid=127473)[0m f1_weighted: 0.27736397667291957
[2m[36m(func pid=127473)[0m f1_per_class: [0.135, 0.484, 0.301, 0.003, 0.142, 0.401, 0.359, 0.478, 0.142, 0.2]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=138124)[0m top1: 0.19402985074626866
[2m[36m(func pid=138124)[0m top5: 0.5778917910447762
[2m[36m(func pid=138124)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=138124)[0m f1_macro: 0.04012836095703938
[2m[36m(func pid=138124)[0m f1_weighted: 0.10022329834057281
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.0, 0.348, 0.0, 0.0, 0.0, 0.053, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
== Status ==
Current time: 2024-01-07 00:20:42 (running for 00:27:50.95)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.138 |      0.161 |                   54 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.153 |      0.265 |                   50 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.248 |      0.133 |                   51 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  3.059 |      0.04  |                    4 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.2621268656716418
[2m[36m(func pid=127909)[0m top5: 0.7714552238805971
[2m[36m(func pid=127909)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=127909)[0m f1_macro: 0.13277390679676265
[2m[36m(func pid=127909)[0m f1_weighted: 0.1616547469654581
[2m[36m(func pid=127909)[0m f1_per_class: [0.154, 0.058, 0.121, 0.452, 0.106, 0.016, 0.006, 0.255, 0.031, 0.129]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.1283 | Steps: 2 | Val loss: 2.1724 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0978 | Steps: 2 | Val loss: 2.1658 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.9895 | Steps: 2 | Val loss: 6.8863 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9958 | Steps: 2 | Val loss: 2.3192 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=126068)[0m top1: 0.15298507462686567
[2m[36m(func pid=126068)[0m top5: 0.7919776119402985
[2m[36m(func pid=126068)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=126068)[0m f1_macro: 0.1638034821301137
[2m[36m(func pid=126068)[0m f1_weighted: 0.16165896534018245
[2m[36m(func pid=126068)[0m f1_per_class: [0.139, 0.248, 0.222, 0.0, 0.036, 0.157, 0.226, 0.438, 0.089, 0.083]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.22154850746268656
[2m[36m(func pid=127473)[0m top5: 0.8451492537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=127473)[0m f1_macro: 0.22491270412192219
[2m[36m(func pid=127473)[0m f1_weighted: 0.2276177984692894
[2m[36m(func pid=127473)[0m f1_per_class: [0.125, 0.365, 0.314, 0.132, 0.064, 0.319, 0.186, 0.428, 0.127, 0.189]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:20:47 (running for 00:27:56.01)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.128 |      0.164 |                   55 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.098 |      0.225 |                   51 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.989 |      0.141 |                   52 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  3.059 |      0.04  |                    4 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.17723880597014927
[2m[36m(func pid=127909)[0m top5: 0.8386194029850746
[2m[36m(func pid=127909)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=127909)[0m f1_macro: 0.14070445752359811
[2m[36m(func pid=127909)[0m f1_weighted: 0.19627865850537168
[2m[36m(func pid=127909)[0m f1_per_class: [0.067, 0.1, 0.212, 0.059, 0.056, 0.303, 0.411, 0.0, 0.0, 0.2]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m top1: 0.2042910447761194
[2m[36m(func pid=138124)[0m top5: 0.5764925373134329
[2m[36m(func pid=138124)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=138124)[0m f1_macro: 0.03917297474481823
[2m[36m(func pid=138124)[0m f1_weighted: 0.1021938918108704
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.0, 0.36, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.2933 | Steps: 2 | Val loss: 2.1763 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1609 | Steps: 2 | Val loss: 2.0926 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8518 | Steps: 2 | Val loss: 6.7760 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0074 | Steps: 2 | Val loss: 2.3257 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=126068)[0m top1: 0.15904850746268656
[2m[36m(func pid=126068)[0m top5: 0.7863805970149254
[2m[36m(func pid=126068)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=126068)[0m f1_macro: 0.16983869117544473
[2m[36m(func pid=126068)[0m f1_weighted: 0.15182629340091355
[2m[36m(func pid=126068)[0m f1_per_class: [0.141, 0.27, 0.214, 0.0, 0.041, 0.22, 0.155, 0.437, 0.079, 0.143]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m top1: 0.23833955223880596
[2m[36m(func pid=127473)[0m top5: 0.8577425373134329
[2m[36m(func pid=127473)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=127473)[0m f1_macro: 0.21765726238701988
[2m[36m(func pid=127473)[0m f1_weighted: 0.2546506194190049
[2m[36m(func pid=127473)[0m f1_per_class: [0.17, 0.178, 0.262, 0.402, 0.049, 0.211, 0.176, 0.405, 0.118, 0.206]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.21361940298507462
[2m[36m(func pid=127909)[0m top5: 0.7709888059701493
[2m[36m(func pid=127909)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=127909)[0m f1_macro: 0.24480887684648617
[2m[36m(func pid=127909)[0m f1_weighted: 0.21302147247950226
[2m[36m(func pid=127909)[0m f1_per_class: [0.092, 0.466, 0.476, 0.0, 0.113, 0.328, 0.211, 0.349, 0.083, 0.329]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:20:52 (running for 00:28:01.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.293 |      0.17  |                   56 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.161 |      0.218 |                   52 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.852 |      0.245 |                   53 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.996 |      0.039 |                    5 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.2178171641791045
[2m[36m(func pid=138124)[0m top5: 0.5746268656716418
[2m[36m(func pid=138124)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=138124)[0m f1_macro: 0.04976158353925992
[2m[36m(func pid=138124)[0m f1_weighted: 0.10825199963215863
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.09, 0.38, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1426 | Steps: 2 | Val loss: 2.1648 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0358 | Steps: 2 | Val loss: 2.0183 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.7149 | Steps: 2 | Val loss: 4.2219 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9238 | Steps: 2 | Val loss: 2.3308 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=127473)[0m top1: 0.30970149253731344
[2m[36m(func pid=127473)[0m top5: 0.8736007462686567
[2m[36m(func pid=127473)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=127473)[0m f1_macro: 0.2365174833272173
[2m[36m(func pid=127473)[0m f1_weighted: 0.3039300819309091
[2m[36m(func pid=127473)[0m f1_per_class: [0.19, 0.042, 0.218, 0.528, 0.056, 0.292, 0.258, 0.465, 0.149, 0.168]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=126068)[0m top1: 0.17444029850746268
[2m[36m(func pid=126068)[0m top5: 0.7947761194029851
[2m[36m(func pid=126068)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=126068)[0m f1_macro: 0.18164787403473465
[2m[36m(func pid=126068)[0m f1_weighted: 0.15048178046485644
[2m[36m(func pid=126068)[0m f1_per_class: [0.169, 0.28, 0.229, 0.0, 0.05, 0.265, 0.122, 0.444, 0.084, 0.174]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.23833955223880596
[2m[36m(func pid=127909)[0m top5: 0.8563432835820896
[2m[36m(func pid=127909)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=127909)[0m f1_macro: 0.25798941394849056
[2m[36m(func pid=127909)[0m f1_weighted: 0.24845122572812253
[2m[36m(func pid=127909)[0m f1_per_class: [0.087, 0.38, 0.48, 0.419, 0.121, 0.17, 0.022, 0.478, 0.104, 0.32]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:20:57 (running for 00:28:06.71)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.143 |      0.182 |                   57 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.036 |      0.237 |                   53 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.715 |      0.258 |                   54 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.924 |      0.053 |                    7 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.21688432835820895
[2m[36m(func pid=138124)[0m top5: 0.5652985074626866
[2m[36m(func pid=138124)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=138124)[0m f1_macro: 0.05286597566745229
[2m[36m(func pid=138124)[0m f1_weighted: 0.11532551206994185
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.005, 0.05, 0.4, 0.0, 0.0, 0.0, 0.036, 0.0, 0.037]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0238 | Steps: 2 | Val loss: 1.9469 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.1235 | Steps: 2 | Val loss: 2.1391 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4527 | Steps: 2 | Val loss: 4.3004 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8434 | Steps: 2 | Val loss: 2.3368 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=127473)[0m top1: 0.3474813432835821
[2m[36m(func pid=127473)[0m top5: 0.8833955223880597
[2m[36m(func pid=127473)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=127473)[0m f1_macro: 0.2532165468690432
[2m[36m(func pid=127473)[0m f1_weighted: 0.34424387652180255
[2m[36m(func pid=127473)[0m f1_per_class: [0.218, 0.047, 0.216, 0.544, 0.067, 0.337, 0.357, 0.471, 0.132, 0.144]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=126068)[0m top1: 0.19169776119402984
[2m[36m(func pid=126068)[0m top5: 0.8078358208955224
[2m[36m(func pid=126068)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=126068)[0m f1_macro: 0.19227025730703887
[2m[36m(func pid=126068)[0m f1_weighted: 0.1703269995233791
[2m[36m(func pid=126068)[0m f1_per_class: [0.19, 0.282, 0.242, 0.01, 0.053, 0.293, 0.165, 0.452, 0.084, 0.152]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.3358208955223881
[2m[36m(func pid=127909)[0m top5: 0.8745335820895522
[2m[36m(func pid=127909)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=127909)[0m f1_macro: 0.22250056272153942
[2m[36m(func pid=127909)[0m f1_weighted: 0.2955253947339436
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.078, 0.355, 0.565, 0.13, 0.268, 0.224, 0.32, 0.102, 0.183]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:21:02 (running for 00:28:11.91)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.124 |      0.192 |                   58 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.024 |      0.253 |                   54 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.453 |      0.223 |                   55 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.843 |      0.045 |                    8 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.17490671641791045
[2m[36m(func pid=138124)[0m top5: 0.5541044776119403
[2m[36m(func pid=138124)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=138124)[0m f1_macro: 0.04501951074122475
[2m[36m(func pid=138124)[0m f1_weighted: 0.11304808130576227
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.005, 0.025, 0.397, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.0115 | Steps: 2 | Val loss: 1.9043 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2275 | Steps: 2 | Val loss: 4.1121 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.1719 | Steps: 2 | Val loss: 2.1217 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8549 | Steps: 2 | Val loss: 2.3433 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=127473)[0m top1: 0.3568097014925373
[2m[36m(func pid=127473)[0m top5: 0.8936567164179104
[2m[36m(func pid=127473)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=127473)[0m f1_macro: 0.27489310602647327
[2m[36m(func pid=127473)[0m f1_weighted: 0.3851626436043683
[2m[36m(func pid=127473)[0m f1_per_class: [0.242, 0.207, 0.265, 0.511, 0.066, 0.359, 0.443, 0.35, 0.144, 0.162]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.35634328358208955
[2m[36m(func pid=127909)[0m top5: 0.835820895522388
[2m[36m(func pid=127909)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=127909)[0m f1_macro: 0.2492965025591416
[2m[36m(func pid=127909)[0m f1_weighted: 0.34022686283783565
[2m[36m(func pid=127909)[0m f1_per_class: [0.231, 0.236, 0.167, 0.248, 0.112, 0.323, 0.57, 0.16, 0.131, 0.317]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m top1: 0.21548507462686567
[2m[36m(func pid=126068)[0m top5: 0.8236940298507462
[2m[36m(func pid=126068)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=126068)[0m f1_macro: 0.20451542906882744
[2m[36m(func pid=126068)[0m f1_weighted: 0.22528835796368404
[2m[36m(func pid=126068)[0m f1_per_class: [0.161, 0.275, 0.237, 0.128, 0.057, 0.303, 0.244, 0.436, 0.087, 0.115]
[2m[36m(func pid=126068)[0m 
== Status ==
Current time: 2024-01-07 00:21:08 (running for 00:28:17.14)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.172 |      0.205 |                   59 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.012 |      0.275 |                   55 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.228 |      0.249 |                   56 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.855 |      0.033 |                    9 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.09654850746268656
[2m[36m(func pid=138124)[0m top5: 0.5396455223880597
[2m[36m(func pid=138124)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=138124)[0m f1_macro: 0.03277140940604667
[2m[36m(func pid=138124)[0m f1_weighted: 0.08301066056090861
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.018, 0.294, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2417 | Steps: 2 | Val loss: 1.9518 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6490 | Steps: 2 | Val loss: 7.0548 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.0903 | Steps: 2 | Val loss: 2.1112 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8244 | Steps: 2 | Val loss: 2.3489 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=127473)[0m top1: 0.32369402985074625
[2m[36m(func pid=127473)[0m top5: 0.8936567164179104
[2m[36m(func pid=127473)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=127473)[0m f1_macro: 0.2826293427268012
[2m[36m(func pid=127473)[0m f1_weighted: 0.3573918378295165
[2m[36m(func pid=127473)[0m f1_per_class: [0.198, 0.384, 0.379, 0.304, 0.074, 0.37, 0.435, 0.366, 0.135, 0.182]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.24300373134328357
[2m[36m(func pid=127909)[0m top5: 0.7327425373134329
[2m[36m(func pid=127909)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=127909)[0m f1_macro: 0.21616858413319662
[2m[36m(func pid=127909)[0m f1_weighted: 0.24012936838641638
[2m[36m(func pid=127909)[0m f1_per_class: [0.1, 0.5, 0.244, 0.0, 0.106, 0.303, 0.317, 0.288, 0.024, 0.279]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:21:13 (running for 00:28:22.25)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.09  |      0.213 |                   60 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.242 |      0.283 |                   56 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.649 |      0.216 |                   57 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.855 |      0.033 |                    9 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m top1: 0.23134328358208955
[2m[36m(func pid=126068)[0m top5: 0.8157649253731343
[2m[36m(func pid=126068)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=126068)[0m f1_macro: 0.21275675235537217
[2m[36m(func pid=126068)[0m f1_weighted: 0.26068074511982925
[2m[36m(func pid=126068)[0m f1_per_class: [0.135, 0.247, 0.244, 0.25, 0.061, 0.309, 0.27, 0.409, 0.094, 0.109]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.041044776119402986
[2m[36m(func pid=138124)[0m top5: 0.542910447761194
[2m[36m(func pid=138124)[0m f1_micro: 0.041044776119402986
[2m[36m(func pid=138124)[0m f1_macro: 0.01815837793491425
[2m[36m(func pid=138124)[0m f1_weighted: 0.04683082176724302
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.014, 0.168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.0389 | Steps: 2 | Val loss: 1.9884 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8136 | Steps: 2 | Val loss: 8.2237 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1204 | Steps: 2 | Val loss: 2.1027 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7774 | Steps: 2 | Val loss: 2.3556 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=127473)[0m top1: 0.2868470149253731
[2m[36m(func pid=127473)[0m top5: 0.8969216417910447
[2m[36m(func pid=127473)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=127473)[0m f1_macro: 0.2756594148676225
[2m[36m(func pid=127473)[0m f1_weighted: 0.2768911050377115
[2m[36m(func pid=127473)[0m f1_per_class: [0.195, 0.397, 0.45, 0.099, 0.095, 0.37, 0.334, 0.408, 0.154, 0.255]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.1837686567164179
[2m[36m(func pid=127909)[0m top5: 0.7807835820895522
[2m[36m(func pid=127909)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=127909)[0m f1_macro: 0.1803389480555984
[2m[36m(func pid=127909)[0m f1_weighted: 0.1595581053122165
[2m[36m(func pid=127909)[0m f1_per_class: [0.084, 0.556, 0.212, 0.0, 0.063, 0.124, 0.054, 0.429, 0.096, 0.186]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:21:18 (running for 00:28:27.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.12  |      0.215 |                   61 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.039 |      0.276 |                   57 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.814 |      0.18  |                   58 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.824 |      0.018 |                   10 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m top1: 0.24347014925373134
[2m[36m(func pid=126068)[0m top5: 0.8194962686567164
[2m[36m(func pid=126068)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=126068)[0m f1_macro: 0.2146410624161575
[2m[36m(func pid=126068)[0m f1_weighted: 0.26707753892520536
[2m[36m(func pid=126068)[0m f1_per_class: [0.135, 0.194, 0.247, 0.349, 0.061, 0.318, 0.226, 0.414, 0.066, 0.136]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.015391791044776119
[2m[36m(func pid=138124)[0m top5: 0.5419776119402985
[2m[36m(func pid=138124)[0m f1_micro: 0.015391791044776119
[2m[36m(func pid=138124)[0m f1_macro: 0.006979492526937782
[2m[36m(func pid=138124)[0m f1_weighted: 0.016014881276620027
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.013, 0.057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6280 | Steps: 2 | Val loss: 6.3845 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0982 | Steps: 2 | Val loss: 1.9598 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8561 | Steps: 2 | Val loss: 2.3622 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1452 | Steps: 2 | Val loss: 2.1101 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=127473)[0m top1: 0.30550373134328357
[2m[36m(func pid=127473)[0m top5: 0.8899253731343284
[2m[36m(func pid=127473)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=127473)[0m f1_macro: 0.29312451876961537
[2m[36m(func pid=127473)[0m f1_weighted: 0.3172274454798241
[2m[36m(func pid=127473)[0m f1_per_class: [0.194, 0.407, 0.421, 0.214, 0.086, 0.377, 0.343, 0.456, 0.164, 0.269]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m top1: 0.29244402985074625
[2m[36m(func pid=127909)[0m top5: 0.804570895522388
[2m[36m(func pid=127909)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=127909)[0m f1_macro: 0.20280292257192917
[2m[36m(func pid=127909)[0m f1_weighted: 0.27315721958663897
[2m[36m(func pid=127909)[0m f1_per_class: [0.194, 0.531, 0.127, 0.453, 0.064, 0.0, 0.074, 0.419, 0.092, 0.075]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:21:24 (running for 00:28:33.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.12  |      0.215 |                   61 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.098 |      0.293 |                   58 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.628 |      0.203 |                   59 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.856 |      0.001 |                   12 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5424440298507462
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.0012200844673862037
[2m[36m(func pid=138124)[0m f1_weighted: 7.397900222024556e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.2439365671641791
[2m[36m(func pid=126068)[0m top5: 0.8106343283582089
[2m[36m(func pid=126068)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=126068)[0m f1_macro: 0.20114600876951436
[2m[36m(func pid=126068)[0m f1_weighted: 0.264842343419017
[2m[36m(func pid=126068)[0m f1_per_class: [0.107, 0.143, 0.216, 0.389, 0.06, 0.311, 0.221, 0.41, 0.04, 0.114]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6515 | Steps: 2 | Val loss: 5.6386 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0567 | Steps: 2 | Val loss: 1.9779 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7416 | Steps: 2 | Val loss: 2.3657 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.0800 | Steps: 2 | Val loss: 2.1105 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=127909)[0m top1: 0.3927238805970149
[2m[36m(func pid=127909)[0m top5: 0.9057835820895522
[2m[36m(func pid=127909)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=127909)[0m f1_macro: 0.2161219867943584
[2m[36m(func pid=127909)[0m f1_weighted: 0.385459073596909
[2m[36m(func pid=127909)[0m f1_per_class: [0.215, 0.485, 0.1, 0.507, 0.022, 0.188, 0.425, 0.078, 0.048, 0.093]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.302705223880597
[2m[36m(func pid=127473)[0m top5: 0.8740671641791045
[2m[36m(func pid=127473)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=127473)[0m f1_macro: 0.2870715470993207
[2m[36m(func pid=127473)[0m f1_weighted: 0.3334537343911775
[2m[36m(func pid=127473)[0m f1_per_class: [0.198, 0.354, 0.439, 0.36, 0.073, 0.368, 0.298, 0.476, 0.125, 0.179]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:21:29 (running for 00:28:38.56)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.145 |      0.201 |                   62 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.057 |      0.287 |                   59 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.651 |      0.216 |                   60 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.742 |      0.001 |                   13 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5424440298507462
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.0012093023255813954
[2m[36m(func pid=138124)[0m f1_weighted: 7.332523429364805e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.24113805970149255
[2m[36m(func pid=126068)[0m top5: 0.8106343283582089
[2m[36m(func pid=126068)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=126068)[0m f1_macro: 0.1981194903711131
[2m[36m(func pid=126068)[0m f1_weighted: 0.26155085403876366
[2m[36m(func pid=126068)[0m f1_per_class: [0.104, 0.107, 0.214, 0.396, 0.059, 0.312, 0.227, 0.389, 0.043, 0.13]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5752 | Steps: 2 | Val loss: 5.1444 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.0770 | Steps: 2 | Val loss: 1.9525 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7348 | Steps: 2 | Val loss: 2.3707 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.0801 | Steps: 2 | Val loss: 2.1074 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=127909)[0m top1: 0.3568097014925373
[2m[36m(func pid=127909)[0m top5: 0.9090485074626866
[2m[36m(func pid=127909)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=127909)[0m f1_macro: 0.29068602896520784
[2m[36m(func pid=127909)[0m f1_weighted: 0.37066437987718626
[2m[36m(func pid=127909)[0m f1_per_class: [0.213, 0.539, 0.099, 0.391, 0.314, 0.347, 0.334, 0.261, 0.115, 0.294]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.3306902985074627
[2m[36m(func pid=127473)[0m top5: 0.875
[2m[36m(func pid=127473)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=127473)[0m f1_macro: 0.28369514691921516
[2m[36m(func pid=127473)[0m f1_weighted: 0.35440124221829544
[2m[36m(func pid=127473)[0m f1_per_class: [0.246, 0.256, 0.349, 0.497, 0.068, 0.298, 0.321, 0.478, 0.139, 0.186]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:21:35 (running for 00:28:43.93)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.08  |      0.198 |                   63 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.077 |      0.284 |                   60 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.575 |      0.291 |                   61 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.735 |      0.001 |                   14 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5405783582089553
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=138124)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.23833955223880596
[2m[36m(func pid=126068)[0m top5: 0.8083022388059702
[2m[36m(func pid=126068)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=126068)[0m f1_macro: 0.1908301776334098
[2m[36m(func pid=126068)[0m f1_weighted: 0.2577390367035298
[2m[36m(func pid=126068)[0m f1_per_class: [0.105, 0.078, 0.212, 0.421, 0.055, 0.285, 0.224, 0.363, 0.042, 0.125]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.2624 | Steps: 2 | Val loss: 4.2616 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.9459 | Steps: 2 | Val loss: 1.9054 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7581 | Steps: 2 | Val loss: 2.3760 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.1060 | Steps: 2 | Val loss: 2.1114 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=127909)[0m top1: 0.3101679104477612
[2m[36m(func pid=127909)[0m top5: 0.8857276119402985
[2m[36m(func pid=127909)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=127909)[0m f1_macro: 0.23443247992161562
[2m[36m(func pid=127909)[0m f1_weighted: 0.26278039749184384
[2m[36m(func pid=127909)[0m f1_per_class: [0.231, 0.522, 0.385, 0.13, 0.078, 0.016, 0.334, 0.383, 0.112, 0.154]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.365205223880597
[2m[36m(func pid=127473)[0m top5: 0.8843283582089553
[2m[36m(func pid=127473)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=127473)[0m f1_macro: 0.2790910327147013
[2m[36m(func pid=127473)[0m f1_weighted: 0.36322772951408044
[2m[36m(func pid=127473)[0m f1_per_class: [0.241, 0.166, 0.253, 0.542, 0.086, 0.334, 0.345, 0.484, 0.157, 0.182]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:21:40 (running for 00:28:49.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.08  |      0.191 |                   64 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.946 |      0.279 |                   61 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.262 |      0.234 |                   62 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.758 |      0.001 |                   15 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5345149253731343
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=138124)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.22574626865671643
[2m[36m(func pid=126068)[0m top5: 0.8092350746268657
[2m[36m(func pid=126068)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=126068)[0m f1_macro: 0.18284626164547343
[2m[36m(func pid=126068)[0m f1_weighted: 0.24521640038540793
[2m[36m(func pid=126068)[0m f1_per_class: [0.104, 0.055, 0.212, 0.417, 0.049, 0.253, 0.213, 0.355, 0.042, 0.129]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 3.3095 | Steps: 2 | Val loss: 4.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2329 | Steps: 2 | Val loss: 1.8927 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0992 | Steps: 2 | Val loss: 2.1048 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7703 | Steps: 2 | Val loss: 2.3818 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127909)[0m top1: 0.36986940298507465
[2m[36m(func pid=127909)[0m top5: 0.8428171641791045
[2m[36m(func pid=127909)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=127909)[0m f1_macro: 0.2869347938915267
[2m[36m(func pid=127909)[0m f1_weighted: 0.3683709311856808
[2m[36m(func pid=127909)[0m f1_per_class: [0.231, 0.531, 0.462, 0.313, 0.082, 0.0, 0.504, 0.419, 0.162, 0.167]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.38013059701492535
[2m[36m(func pid=127473)[0m top5: 0.8964552238805971
[2m[36m(func pid=127473)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=127473)[0m f1_macro: 0.294277634325688
[2m[36m(func pid=127473)[0m f1_weighted: 0.40266783455510685
[2m[36m(func pid=127473)[0m f1_per_class: [0.219, 0.248, 0.297, 0.523, 0.092, 0.37, 0.45, 0.404, 0.153, 0.186]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:21:45 (running for 00:28:54.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.106 |      0.183 |                   65 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.233 |      0.294 |                   62 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  3.309 |      0.287 |                   63 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.77  |      0.001 |                   16 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5214552238805971
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=138124)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.2355410447761194
[2m[36m(func pid=126068)[0m top5: 0.8143656716417911
[2m[36m(func pid=126068)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=126068)[0m f1_macro: 0.18925726223673464
[2m[36m(func pid=126068)[0m f1_weighted: 0.2660707900174368
[2m[36m(func pid=126068)[0m f1_per_class: [0.113, 0.069, 0.21, 0.427, 0.042, 0.149, 0.296, 0.383, 0.077, 0.128]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.7804 | Steps: 2 | Val loss: 4.3689 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.9565 | Steps: 2 | Val loss: 1.9098 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=127909)[0m top1: 0.4253731343283582
[2m[36m(func pid=127909)[0m top5: 0.8456156716417911
[2m[36m(func pid=127909)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=127909)[0m f1_macro: 0.22485186689572187
[2m[36m(func pid=127909)[0m f1_weighted: 0.40402514140356455
[2m[36m(func pid=127909)[0m f1_per_class: [0.044, 0.454, 0.244, 0.51, 0.093, 0.0, 0.571, 0.12, 0.043, 0.169]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.1235 | Steps: 2 | Val loss: 2.0998 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7417 | Steps: 2 | Val loss: 2.3863 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=127473)[0m top1: 0.34048507462686567
[2m[36m(func pid=127473)[0m top5: 0.9006529850746269
[2m[36m(func pid=127473)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=127473)[0m f1_macro: 0.2951129059325176
[2m[36m(func pid=127473)[0m f1_weighted: 0.3867136142329434
[2m[36m(func pid=127473)[0m f1_per_class: [0.206, 0.382, 0.419, 0.387, 0.095, 0.372, 0.466, 0.314, 0.103, 0.207]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.3459 | Steps: 2 | Val loss: 4.9927 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 00:21:51 (running for 00:29:00.34)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.099 |      0.189 |                   66 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.957 |      0.295 |                   63 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.78  |      0.225 |                   64 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.742 |      0.001 |                   17 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=126068)[0m top1: 0.2439365671641791
[2m[36m(func pid=126068)[0m top5: 0.8166977611940298
[2m[36m(func pid=126068)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=126068)[0m f1_macro: 0.19186280178877274
[2m[36m(func pid=126068)[0m f1_weighted: 0.27983897687166814
[2m[36m(func pid=126068)[0m f1_per_class: [0.112, 0.06, 0.214, 0.429, 0.04, 0.088, 0.363, 0.401, 0.085, 0.126]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5186567164179104
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=138124)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9168 | Steps: 2 | Val loss: 1.9596 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=127909)[0m top1: 0.3596082089552239
[2m[36m(func pid=127909)[0m top5: 0.8759328358208955
[2m[36m(func pid=127909)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=127909)[0m f1_macro: 0.2537404467269818
[2m[36m(func pid=127909)[0m f1_weighted: 0.357128056266302
[2m[36m(func pid=127909)[0m f1_per_class: [0.194, 0.453, 0.145, 0.502, 0.112, 0.32, 0.236, 0.434, 0.0, 0.143]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.9420 | Steps: 2 | Val loss: 2.3916 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0495 | Steps: 2 | Val loss: 2.1083 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=127473)[0m top1: 0.30923507462686567
[2m[36m(func pid=127473)[0m top5: 0.8917910447761194
[2m[36m(func pid=127473)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=127473)[0m f1_macro: 0.2734757468952383
[2m[36m(func pid=127473)[0m f1_weighted: 0.3372279777388614
[2m[36m(func pid=127473)[0m f1_per_class: [0.193, 0.421, 0.424, 0.159, 0.081, 0.37, 0.494, 0.312, 0.108, 0.172]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6982 | Steps: 2 | Val loss: 7.2736 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:21:57 (running for 00:29:05.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.124 |      0.192 |                   67 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.917 |      0.273 |                   64 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.346 |      0.254 |                   65 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.942 |      0.001 |                   18 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5200559701492538
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=138124)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.23134328358208955
[2m[36m(func pid=126068)[0m top5: 0.8157649253731343
[2m[36m(func pid=126068)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=126068)[0m f1_macro: 0.185867955463236
[2m[36m(func pid=126068)[0m f1_weighted: 0.26569196322788574
[2m[36m(func pid=126068)[0m f1_per_class: [0.112, 0.055, 0.208, 0.4, 0.039, 0.03, 0.366, 0.397, 0.112, 0.139]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.0660 | Steps: 2 | Val loss: 1.9513 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=127909)[0m top1: 0.26492537313432835
[2m[36m(func pid=127909)[0m top5: 0.8432835820895522
[2m[36m(func pid=127909)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=127909)[0m f1_macro: 0.2232848623743271
[2m[36m(func pid=127909)[0m f1_weighted: 0.23060244084254067
[2m[36m(func pid=127909)[0m f1_per_class: [0.134, 0.5, 0.108, 0.276, 0.205, 0.307, 0.0, 0.416, 0.0, 0.286]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7554 | Steps: 2 | Val loss: 2.3927 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1948 | Steps: 2 | Val loss: 2.1246 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=127473)[0m top1: 0.2966417910447761
[2m[36m(func pid=127473)[0m top5: 0.894589552238806
[2m[36m(func pid=127473)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=127473)[0m f1_macro: 0.261310499320557
[2m[36m(func pid=127473)[0m f1_weighted: 0.3095891469015435
[2m[36m(func pid=127473)[0m f1_per_class: [0.194, 0.39, 0.45, 0.093, 0.074, 0.37, 0.488, 0.288, 0.085, 0.182]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.9101 | Steps: 2 | Val loss: 7.4445 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 00:22:02 (running for 00:29:11.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.05  |      0.186 |                   68 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.066 |      0.261 |                   65 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.698 |      0.223 |                   66 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.755 |      0.001 |                   19 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.006063432835820896
[2m[36m(func pid=138124)[0m top5: 0.5177238805970149
[2m[36m(func pid=138124)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=138124)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=138124)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.208955223880597
[2m[36m(func pid=126068)[0m top5: 0.8092350746268657
[2m[36m(func pid=126068)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=126068)[0m f1_macro: 0.1735777822372589
[2m[36m(func pid=126068)[0m f1_weighted: 0.24187508589448142
[2m[36m(func pid=126068)[0m f1_per_class: [0.102, 0.046, 0.186, 0.358, 0.039, 0.016, 0.338, 0.399, 0.091, 0.161]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.8774 | Steps: 2 | Val loss: 1.8969 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=127909)[0m top1: 0.19776119402985073
[2m[36m(func pid=127909)[0m top5: 0.871268656716418
[2m[36m(func pid=127909)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=127909)[0m f1_macro: 0.18902372772744463
[2m[36m(func pid=127909)[0m f1_weighted: 0.2400437670327121
[2m[36m(func pid=127909)[0m f1_per_class: [0.068, 0.471, 0.16, 0.201, 0.091, 0.0, 0.244, 0.384, 0.084, 0.188]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8378 | Steps: 2 | Val loss: 2.3925 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.0767 | Steps: 2 | Val loss: 2.1227 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=127473)[0m top1: 0.33302238805970147
[2m[36m(func pid=127473)[0m top5: 0.9011194029850746
[2m[36m(func pid=127473)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=127473)[0m f1_macro: 0.27881469502695494
[2m[36m(func pid=127473)[0m f1_weighted: 0.3626740498580643
[2m[36m(func pid=127473)[0m f1_per_class: [0.208, 0.413, 0.286, 0.289, 0.076, 0.371, 0.45, 0.373, 0.115, 0.208]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 5.1182 | Steps: 2 | Val loss: 7.3025 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:22:07 (running for 00:29:16.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.195 |      0.174 |                   69 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.877 |      0.279 |                   66 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.91  |      0.189 |                   67 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.838 |      0.002 |                   20 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.0065298507462686565
[2m[36m(func pid=138124)[0m top5: 0.5125932835820896
[2m[36m(func pid=138124)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=138124)[0m f1_macro: 0.001748718979202251
[2m[36m(func pid=138124)[0m f1_weighted: 0.0010035717311663328
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.19682835820895522
[2m[36m(func pid=126068)[0m top5: 0.8055037313432836
[2m[36m(func pid=126068)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=126068)[0m f1_macro: 0.16659082800650044
[2m[36m(func pid=126068)[0m f1_weighted: 0.22616972384662193
[2m[36m(func pid=126068)[0m f1_per_class: [0.116, 0.036, 0.186, 0.352, 0.037, 0.0, 0.303, 0.405, 0.082, 0.15]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.13386194029850745
[2m[36m(func pid=127909)[0m top5: 0.8222947761194029
[2m[36m(func pid=127909)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=127909)[0m f1_macro: 0.1863871814917566
[2m[36m(func pid=127909)[0m f1_weighted: 0.15033272070748707
[2m[36m(func pid=127909)[0m f1_per_class: [0.211, 0.338, 0.338, 0.116, 0.17, 0.0, 0.089, 0.375, 0.061, 0.167]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.8809 | Steps: 2 | Val loss: 1.9147 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8628 | Steps: 2 | Val loss: 2.3896 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.1191 | Steps: 2 | Val loss: 2.1304 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=127473)[0m top1: 0.3572761194029851
[2m[36m(func pid=127473)[0m top5: 0.8880597014925373
[2m[36m(func pid=127473)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=127473)[0m f1_macro: 0.28395984420432374
[2m[36m(func pid=127473)[0m f1_weighted: 0.38252004425425085
[2m[36m(func pid=127473)[0m f1_per_class: [0.184, 0.383, 0.161, 0.475, 0.092, 0.384, 0.339, 0.467, 0.119, 0.237]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.2549 | Steps: 2 | Val loss: 3.7488 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:22:13 (running for 00:29:22.18)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.077 |      0.167 |                   70 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.881 |      0.284 |                   67 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  5.118 |      0.186 |                   68 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.863 |      0.003 |                   21 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.00792910447761194
[2m[36m(func pid=138124)[0m top5: 0.5163246268656716
[2m[36m(func pid=138124)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=138124)[0m f1_macro: 0.0033439752986495195
[2m[36m(func pid=138124)[0m f1_weighted: 0.0037169348522462556
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.021, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.18470149253731344
[2m[36m(func pid=126068)[0m top5: 0.8027052238805971
[2m[36m(func pid=126068)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=126068)[0m f1_macro: 0.1588151763768258
[2m[36m(func pid=126068)[0m f1_weighted: 0.20838441041755718
[2m[36m(func pid=126068)[0m f1_per_class: [0.111, 0.036, 0.185, 0.348, 0.038, 0.0, 0.251, 0.377, 0.094, 0.148]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.3400186567164179
[2m[36m(func pid=127909)[0m top5: 0.8507462686567164
[2m[36m(func pid=127909)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=127909)[0m f1_macro: 0.2743799958024042
[2m[36m(func pid=127909)[0m f1_weighted: 0.33439203149993985
[2m[36m(func pid=127909)[0m f1_per_class: [0.358, 0.194, 0.5, 0.475, 0.136, 0.0, 0.44, 0.329, 0.158, 0.154]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.2440 | Steps: 2 | Val loss: 1.9132 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7080 | Steps: 2 | Val loss: 2.3801 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0539 | Steps: 2 | Val loss: 2.1392 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1108 | Steps: 2 | Val loss: 4.4099 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=127473)[0m top1: 0.3670708955223881
[2m[36m(func pid=127473)[0m top5: 0.8871268656716418
[2m[36m(func pid=127473)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=127473)[0m f1_macro: 0.2949951368570084
[2m[36m(func pid=127473)[0m f1_weighted: 0.3903041647190927
[2m[36m(func pid=127473)[0m f1_per_class: [0.171, 0.383, 0.242, 0.487, 0.108, 0.401, 0.347, 0.471, 0.112, 0.229]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:22:18 (running for 00:29:27.60)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.119 |      0.159 |                   71 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.244 |      0.295 |                   68 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.255 |      0.274 |                   69 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.708 |      0.01  |                   22 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.013992537313432836
[2m[36m(func pid=138124)[0m top5: 0.5335820895522388
[2m[36m(func pid=138124)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=138124)[0m f1_macro: 0.010181958151327181
[2m[36m(func pid=138124)[0m f1_weighted: 0.013644383574494014
[2m[36m(func pid=138124)[0m f1_per_class: [0.011, 0.077, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.16837686567164178
[2m[36m(func pid=126068)[0m top5: 0.8013059701492538
[2m[36m(func pid=126068)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=126068)[0m f1_macro: 0.15020453378129658
[2m[36m(func pid=126068)[0m f1_weighted: 0.18333890164053948
[2m[36m(func pid=126068)[0m f1_per_class: [0.099, 0.036, 0.183, 0.329, 0.037, 0.0, 0.184, 0.375, 0.103, 0.154]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.2905783582089552
[2m[36m(func pid=127909)[0m top5: 0.8563432835820896
[2m[36m(func pid=127909)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=127909)[0m f1_macro: 0.20962852630247203
[2m[36m(func pid=127909)[0m f1_weighted: 0.30226509830620224
[2m[36m(func pid=127909)[0m f1_per_class: [0.044, 0.09, 0.143, 0.395, 0.198, 0.399, 0.345, 0.418, 0.0, 0.063]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0729 | Steps: 2 | Val loss: 1.9134 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7269 | Steps: 2 | Val loss: 2.3725 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.0166 | Steps: 2 | Val loss: 2.1356 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 5.4144 | Steps: 2 | Val loss: 3.7359 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=127473)[0m top1: 0.3530783582089552
[2m[36m(func pid=127473)[0m top5: 0.9076492537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=127473)[0m f1_macro: 0.28448214469762034
[2m[36m(func pid=127473)[0m f1_weighted: 0.3903070463158038
[2m[36m(func pid=127473)[0m f1_per_class: [0.145, 0.392, 0.333, 0.429, 0.128, 0.404, 0.43, 0.339, 0.025, 0.22]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:22:23 (running for 00:29:32.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.054 |      0.15  |                   72 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.073 |      0.284 |                   69 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.111 |      0.21  |                   70 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.727 |      0.018 |                   23 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.026119402985074626
[2m[36m(func pid=138124)[0m top5: 0.5555037313432836
[2m[36m(func pid=138124)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=138124)[0m f1_macro: 0.017501808986053895
[2m[36m(func pid=138124)[0m f1_weighted: 0.02593474894837219
[2m[36m(func pid=138124)[0m f1_per_class: [0.011, 0.143, 0.018, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.16884328358208955
[2m[36m(func pid=126068)[0m top5: 0.8041044776119403
[2m[36m(func pid=126068)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=126068)[0m f1_macro: 0.149012510716436
[2m[36m(func pid=126068)[0m f1_weighted: 0.17923842007393428
[2m[36m(func pid=126068)[0m f1_per_class: [0.103, 0.041, 0.193, 0.353, 0.037, 0.016, 0.143, 0.362, 0.087, 0.156]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.3516791044776119
[2m[36m(func pid=127909)[0m top5: 0.9426305970149254
[2m[36m(func pid=127909)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=127909)[0m f1_macro: 0.2726792708452604
[2m[36m(func pid=127909)[0m f1_weighted: 0.3647564846339931
[2m[36m(func pid=127909)[0m f1_per_class: [0.362, 0.218, 0.474, 0.456, 0.065, 0.381, 0.461, 0.091, 0.026, 0.194]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.9861 | Steps: 2 | Val loss: 1.8806 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6934 | Steps: 2 | Val loss: 2.3597 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.0121 | Steps: 2 | Val loss: 2.1331 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6830 | Steps: 2 | Val loss: 6.4401 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=127473)[0m top1: 0.37779850746268656
[2m[36m(func pid=127473)[0m top5: 0.909981343283582
[2m[36m(func pid=127473)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=127473)[0m f1_macro: 0.31262957496575794
[2m[36m(func pid=127473)[0m f1_weighted: 0.4180961001538697
[2m[36m(func pid=127473)[0m f1_per_class: [0.157, 0.44, 0.385, 0.446, 0.133, 0.396, 0.469, 0.331, 0.137, 0.232]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:22:29 (running for 00:29:38.05)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.017 |      0.149 |                   73 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.986 |      0.313 |                   70 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  5.414 |      0.273 |                   71 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.693 |      0.025 |                   24 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.043843283582089554
[2m[36m(func pid=138124)[0m top5: 0.5750932835820896
[2m[36m(func pid=138124)[0m f1_micro: 0.043843283582089554
[2m[36m(func pid=138124)[0m f1_macro: 0.025397588034255285
[2m[36m(func pid=138124)[0m f1_weighted: 0.038773866571675294
[2m[36m(func pid=138124)[0m f1_per_class: [0.013, 0.185, 0.034, 0.0, 0.0, 0.0, 0.021, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=126068)[0m top1: 0.17117537313432835
[2m[36m(func pid=126068)[0m top5: 0.8073694029850746
[2m[36m(func pid=126068)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=126068)[0m f1_macro: 0.15005938255767423
[2m[36m(func pid=126068)[0m f1_weighted: 0.17422617163325382
[2m[36m(func pid=126068)[0m f1_per_class: [0.105, 0.045, 0.208, 0.375, 0.036, 0.038, 0.093, 0.365, 0.101, 0.135]
[2m[36m(func pid=126068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.2560634328358209
[2m[36m(func pid=127909)[0m top5: 0.9123134328358209
[2m[36m(func pid=127909)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=127909)[0m f1_macro: 0.16852329693472354
[2m[36m(func pid=127909)[0m f1_weighted: 0.27288342939613125
[2m[36m(func pid=127909)[0m f1_per_class: [0.167, 0.47, 0.177, 0.137, 0.04, 0.0, 0.464, 0.12, 0.109, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.8736 | Steps: 2 | Val loss: 1.7931 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7283 | Steps: 2 | Val loss: 2.3498 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6147 | Steps: 2 | Val loss: 10.0087 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=126068)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0412 | Steps: 2 | Val loss: 2.1277 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=127473)[0m top1: 0.3931902985074627
[2m[36m(func pid=127473)[0m top5: 0.9076492537313433
[2m[36m(func pid=127473)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=127473)[0m f1_macro: 0.3372802783221337
[2m[36m(func pid=127473)[0m f1_weighted: 0.4205210687347303
[2m[36m(func pid=127473)[0m f1_per_class: [0.299, 0.545, 0.45, 0.39, 0.124, 0.364, 0.461, 0.379, 0.142, 0.219]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:22:34 (running for 00:29:43.39)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00013 | RUNNING    | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.012 |      0.15  |                   74 |
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.874 |      0.337 |                   71 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.683 |      0.169 |                   72 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.728 |      0.03  |                   25 |
| train_51d3e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.048507462686567165
[2m[36m(func pid=138124)[0m top5: 0.5755597014925373
[2m[36m(func pid=138124)[0m f1_micro: 0.048507462686567165
[2m[36m(func pid=138124)[0m f1_macro: 0.03002345276305956
[2m[36m(func pid=138124)[0m f1_weighted: 0.04519341983354082
[2m[36m(func pid=138124)[0m f1_per_class: [0.015, 0.186, 0.057, 0.0, 0.0, 0.0, 0.042, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m top1: 0.19076492537313433
[2m[36m(func pid=127909)[0m top5: 0.8059701492537313
[2m[36m(func pid=127909)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=127909)[0m f1_macro: 0.12073340453698737
[2m[36m(func pid=127909)[0m f1_weighted: 0.11828632955837644
[2m[36m(func pid=127909)[0m f1_per_class: [0.097, 0.443, 0.11, 0.01, 0.0, 0.016, 0.025, 0.43, 0.077, 0.0]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=126068)[0m top1: 0.17164179104477612
[2m[36m(func pid=126068)[0m top5: 0.8092350746268657
[2m[36m(func pid=126068)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=126068)[0m f1_macro: 0.15275742773204884
[2m[36m(func pid=126068)[0m f1_weighted: 0.17468967468406527
[2m[36m(func pid=126068)[0m f1_per_class: [0.127, 0.112, 0.244, 0.358, 0.036, 0.051, 0.071, 0.366, 0.051, 0.111]
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9597 | Steps: 2 | Val loss: 1.8131 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6705 | Steps: 2 | Val loss: 2.3352 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 7.1699 | Steps: 2 | Val loss: 9.7847 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=127473)[0m top1: 0.36800373134328357
[2m[36m(func pid=127473)[0m top5: 0.8992537313432836
[2m[36m(func pid=127473)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=127473)[0m f1_macro: 0.3256082808607092
[2m[36m(func pid=127473)[0m f1_weighted: 0.39380366523506116
[2m[36m(func pid=127473)[0m f1_per_class: [0.4, 0.525, 0.344, 0.293, 0.088, 0.334, 0.467, 0.441, 0.169, 0.194]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=138124)[0m top1: 0.05550373134328358
[2m[36m(func pid=138124)[0m top5: 0.601679104477612
[2m[36m(func pid=138124)[0m f1_micro: 0.05550373134328358
[2m[36m(func pid=138124)[0m f1_macro: 0.03851353125198261
[2m[36m(func pid=138124)[0m f1_weighted: 0.05598669986580434
[2m[36m(func pid=138124)[0m f1_per_class: [0.016, 0.2, 0.081, 0.0, 0.0, 0.0, 0.067, 0.0, 0.021, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m top1: 0.23507462686567165
[2m[36m(func pid=127909)[0m top5: 0.8269589552238806
[2m[36m(func pid=127909)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=127909)[0m f1_macro: 0.1849512206888128
[2m[36m(func pid=127909)[0m f1_weighted: 0.22030943199337227
[2m[36m(func pid=127909)[0m f1_per_class: [0.054, 0.524, 0.142, 0.166, 0.0, 0.432, 0.015, 0.432, 0.084, 0.0]
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.1985 | Steps: 2 | Val loss: 1.8451 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7184 | Steps: 2 | Val loss: 2.3244 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:22:39 (running for 00:29:48.66)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.2455
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.96  |      0.326 |                   72 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.615 |      0.121 |                   73 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.671 |      0.039 |                   26 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127473)[0m top1: 0.3568097014925373
[2m[36m(func pid=127473)[0m top5: 0.8964552238805971
[2m[36m(func pid=127473)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=127473)[0m f1_macro: 0.2995122160246979
[2m[36m(func pid=127473)[0m f1_weighted: 0.37859571255214214
[2m[36m(func pid=127473)[0m f1_per_class: [0.289, 0.428, 0.289, 0.463, 0.082, 0.368, 0.313, 0.449, 0.113, 0.2]
[2m[36m(func pid=127473)[0m 
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=144068)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=144068)[0m Configuration completed!
[2m[36m(func pid=144068)[0m New optimizer parameters:
[2m[36m(func pid=144068)[0m SGD (
[2m[36m(func pid=144068)[0m Parameter Group 0
[2m[36m(func pid=144068)[0m     dampening: 0
[2m[36m(func pid=144068)[0m     differentiable: False
[2m[36m(func pid=144068)[0m     foreach: None
[2m[36m(func pid=144068)[0m     lr: 0.001
[2m[36m(func pid=144068)[0m     maximize: False
[2m[36m(func pid=144068)[0m     momentum: 0.99
[2m[36m(func pid=144068)[0m     nesterov: False
[2m[36m(func pid=144068)[0m     weight_decay: 1e-05
[2m[36m(func pid=144068)[0m )
[2m[36m(func pid=144068)[0m 
== Status ==
Current time: 2024-01-07 00:22:45 (running for 00:29:53.94)
Memory usage on this node: 23.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.2455
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  1.198 |      0.3   |                   73 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  7.17  |      0.185 |                   74 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.718 |      0.046 |                   27 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.06389925373134328
[2m[36m(func pid=138124)[0m top5: 0.6305970149253731
[2m[36m(func pid=138124)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=138124)[0m f1_macro: 0.046080719855810276
[2m[36m(func pid=138124)[0m f1_weighted: 0.06651313657357143
[2m[36m(func pid=138124)[0m f1_per_class: [0.023, 0.197, 0.118, 0.0, 0.0, 0.0, 0.103, 0.0, 0.019, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.7016 | Steps: 2 | Val loss: 5.1886 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9682 | Steps: 2 | Val loss: 1.9426 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7925 | Steps: 2 | Val loss: 2.3110 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0739 | Steps: 2 | Val loss: 2.3708 | Batch size: 32 | lr: 0.001 | Duration: 4.25s
[2m[36m(func pid=127909)[0m top1: 0.363339552238806
[2m[36m(func pid=127909)[0m top5: 0.9053171641791045
[2m[36m(func pid=127909)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=127909)[0m f1_macro: 0.2843910629907942
[2m[36m(func pid=127909)[0m f1_weighted: 0.37924464761572974
[2m[36m(func pid=127909)[0m f1_per_class: [0.145, 0.466, 0.253, 0.48, 0.208, 0.365, 0.306, 0.394, 0.04, 0.188]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.30177238805970147
[2m[36m(func pid=127473)[0m top5: 0.8908582089552238
[2m[36m(func pid=127473)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=127473)[0m f1_macro: 0.2460215426927125
[2m[36m(func pid=127473)[0m f1_weighted: 0.2703240128863349
[2m[36m(func pid=127473)[0m f1_per_class: [0.185, 0.11, 0.419, 0.506, 0.083, 0.371, 0.108, 0.446, 0.0, 0.232]
[2m[36m(func pid=127473)[0m 
== Status ==
Current time: 2024-01-07 00:22:50 (running for 00:29:59.26)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00014 | RUNNING    | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.968 |      0.246 |                   74 |
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.702 |      0.284 |                   75 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.792 |      0.059 |                   28 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m top1: 0.07882462686567164
[2m[36m(func pid=138124)[0m top5: 0.648320895522388
[2m[36m(func pid=138124)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=138124)[0m f1_macro: 0.0585719890133755
[2m[36m(func pid=138124)[0m f1_weighted: 0.0863579333876456
[2m[36m(func pid=138124)[0m f1_per_class: [0.03, 0.207, 0.168, 0.0, 0.0, 0.0, 0.163, 0.0, 0.018, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m top1: 0.07276119402985075
[2m[36m(func pid=144068)[0m top5: 0.3208955223880597
[2m[36m(func pid=144068)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=144068)[0m f1_macro: 0.020535581620621288
[2m[36m(func pid=144068)[0m f1_weighted: 0.03004027744516466
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.0, 0.0, 0.082, 0.0, 0.0, 0.0, 0.123, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.6174 | Steps: 2 | Val loss: 4.3956 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=127473)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9822 | Steps: 2 | Val loss: 2.0365 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7156 | Steps: 2 | Val loss: 2.3014 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0163 | Steps: 2 | Val loss: 2.3246 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=127909)[0m top1: 0.3493470149253731
[2m[36m(func pid=127909)[0m top5: 0.9011194029850746
[2m[36m(func pid=127909)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=127909)[0m f1_macro: 0.2786067268749828
[2m[36m(func pid=127909)[0m f1_weighted: 0.34620584940937627
[2m[36m(func pid=127909)[0m f1_per_class: [0.311, 0.428, 0.516, 0.254, 0.124, 0.0, 0.55, 0.37, 0.138, 0.094]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=127473)[0m top1: 0.2905783582089552
[2m[36m(func pid=127473)[0m top5: 0.8717350746268657
[2m[36m(func pid=127473)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=127473)[0m f1_macro: 0.2356038294760822
[2m[36m(func pid=127473)[0m f1_weighted: 0.24050113301164286
[2m[36m(func pid=127473)[0m f1_per_class: [0.165, 0.062, 0.41, 0.508, 0.098, 0.37, 0.03, 0.474, 0.0, 0.238]
[2m[36m(func pid=138124)[0m top1: 0.09561567164179105
[2m[36m(func pid=138124)[0m top5: 0.6637126865671642
[2m[36m(func pid=138124)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=138124)[0m f1_macro: 0.06904270325975569
[2m[36m(func pid=138124)[0m f1_weighted: 0.10894261642810119
[2m[36m(func pid=138124)[0m f1_per_class: [0.033, 0.215, 0.208, 0.0, 0.0, 0.0, 0.235, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m top1: 0.13199626865671643
[2m[36m(func pid=144068)[0m top5: 0.5102611940298507
[2m[36m(func pid=144068)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=144068)[0m f1_macro: 0.06077452068417424
[2m[36m(func pid=144068)[0m f1_weighted: 0.08943203324751768
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.0, 0.031, 0.253, 0.0, 0.0, 0.0, 0.324, 0.0, 0.0]
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.3507 | Steps: 2 | Val loss: 5.6416 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=127909)[0m top1: 0.22154850746268656
[2m[36m(func pid=127909)[0m top5: 0.8810634328358209
[2m[36m(func pid=127909)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=127909)[0m f1_macro: 0.2075584018301489
[2m[36m(func pid=127909)[0m f1_weighted: 0.2579292556726433
[2m[36m(func pid=127909)[0m f1_per_class: [0.235, 0.222, 0.25, 0.16, 0.065, 0.103, 0.428, 0.418, 0.116, 0.078]
== Status ==
Current time: 2024-01-07 00:22:55 (running for 00:30:04.51)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.617 |      0.279 |                   76 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.792 |      0.059 |                   28 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  3.074 |      0.021 |                    1 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 00:23:03 (running for 00:30:12.09)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.351 |      0.208 |                   77 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.792 |      0.059 |                   28 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  3.074 |      0.021 |                    1 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=145070)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=145070)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=145070)[0m Configuration completed!
[2m[36m(func pid=145070)[0m New optimizer parameters:
[2m[36m(func pid=145070)[0m SGD (
[2m[36m(func pid=145070)[0m Parameter Group 0
[2m[36m(func pid=145070)[0m     dampening: 0
[2m[36m(func pid=145070)[0m     differentiable: False
[2m[36m(func pid=145070)[0m     foreach: None
[2m[36m(func pid=145070)[0m     lr: 0.01
[2m[36m(func pid=145070)[0m     maximize: False
[2m[36m(func pid=145070)[0m     momentum: 0.99
[2m[36m(func pid=145070)[0m     nesterov: False
[2m[36m(func pid=145070)[0m     weight_decay: 1e-05
[2m[36m(func pid=145070)[0m )
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 2.3773 | Steps: 2 | Val loss: 5.5287 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6579 | Steps: 2 | Val loss: 2.2848 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9358 | Steps: 2 | Val loss: 2.3169 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0403 | Steps: 2 | Val loss: 2.4125 | Batch size: 32 | lr: 0.01 | Duration: 4.80s
== Status ==
Current time: 2024-01-07 00:23:08 (running for 00:30:17.11)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.351 |      0.208 |                   77 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.716 |      0.069 |                   29 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  3.016 |      0.061 |                    2 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.24253731343283583
[2m[36m(func pid=127909)[0m top5: 0.8680037313432836
[2m[36m(func pid=127909)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=127909)[0m f1_macro: 0.23449812064070508
[2m[36m(func pid=127909)[0m f1_weighted: 0.23455175741445794
[2m[36m(func pid=127909)[0m f1_per_class: [0.366, 0.118, 0.275, 0.418, 0.056, 0.308, 0.078, 0.396, 0.133, 0.197]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.01166044776119403
[2m[36m(func pid=144068)[0m top5: 0.5694962686567164
[2m[36m(func pid=144068)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=144068)[0m f1_macro: 0.004864524150419457
[2m[36m(func pid=144068)[0m f1_weighted: 0.01017204231706615
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.0, 0.012, 0.036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.1166044776119403
[2m[36m(func pid=138124)[0m top5: 0.6791044776119403
[2m[36m(func pid=138124)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=138124)[0m f1_macro: 0.08604174310056663
[2m[36m(func pid=138124)[0m f1_weighted: 0.1273244860903992
[2m[36m(func pid=138124)[0m f1_per_class: [0.047, 0.232, 0.297, 0.0, 0.0, 0.0, 0.284, 0.0, 0.0, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.006063432835820896
[2m[36m(func pid=145070)[0m top5: 0.3824626865671642
[2m[36m(func pid=145070)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=145070)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=145070)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.9040 | Steps: 2 | Val loss: 6.7295 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6001 | Steps: 2 | Val loss: 2.2645 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7646 | Steps: 2 | Val loss: 2.3178 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8028 | Steps: 2 | Val loss: 2.3451 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:23:13 (running for 00:30:22.58)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.904 |      0.204 |                   79 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.658 |      0.086 |                   30 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.936 |      0.005 |                    3 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  3.04  |      0.001 |                    1 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.292910447761194
[2m[36m(func pid=127909)[0m top5: 0.8875932835820896
[2m[36m(func pid=127909)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=127909)[0m f1_macro: 0.20397685734752952
[2m[36m(func pid=127909)[0m f1_weighted: 0.31085636706836633
[2m[36m(func pid=127909)[0m f1_per_class: [0.042, 0.211, 0.087, 0.462, 0.076, 0.346, 0.268, 0.389, 0.0, 0.159]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.006063432835820896
[2m[36m(func pid=144068)[0m top5: 0.570429104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=144068)[0m f1_macro: 0.001207617278216442
[2m[36m(func pid=144068)[0m f1_weighted: 7.322306257842233e-05
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.14505597014925373
[2m[36m(func pid=138124)[0m top5: 0.7005597014925373
[2m[36m(func pid=138124)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=138124)[0m f1_macro: 0.10537084416464695
[2m[36m(func pid=138124)[0m f1_weighted: 0.15368943996411413
[2m[36m(func pid=138124)[0m f1_per_class: [0.055, 0.271, 0.367, 0.0, 0.0, 0.0, 0.346, 0.0, 0.014, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.034981343283582086
[2m[36m(func pid=145070)[0m top5: 0.5811567164179104
[2m[36m(func pid=145070)[0m f1_micro: 0.034981343283582086
[2m[36m(func pid=145070)[0m f1_macro: 0.023408420951911292
[2m[36m(func pid=145070)[0m f1_weighted: 0.03805556783039869
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.221, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 2.3268 | Steps: 2 | Val loss: 7.7086 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9291 | Steps: 2 | Val loss: 2.3235 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6856 | Steps: 2 | Val loss: 2.2412 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8249 | Steps: 2 | Val loss: 2.2357 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:23:18 (running for 00:30:27.75)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.327 |      0.193 |                   80 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.6   |      0.105 |                   31 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.765 |      0.001 |                    4 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.803 |      0.023 |                    2 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.332089552238806
[2m[36m(func pid=127909)[0m top5: 0.8889925373134329
[2m[36m(func pid=127909)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=127909)[0m f1_macro: 0.19333138178195716
[2m[36m(func pid=127909)[0m f1_weighted: 0.32071302075381103
[2m[36m(func pid=127909)[0m f1_per_class: [0.068, 0.497, 0.074, 0.194, 0.132, 0.008, 0.522, 0.362, 0.0, 0.077]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.006063432835820896
[2m[36m(func pid=144068)[0m top5: 0.5569029850746269
[2m[36m(func pid=144068)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=144068)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=144068)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.17723880597014927
[2m[36m(func pid=138124)[0m top5: 0.7243470149253731
[2m[36m(func pid=138124)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=138124)[0m f1_macro: 0.11703976480759551
[2m[36m(func pid=138124)[0m f1_weighted: 0.17992579479690093
[2m[36m(func pid=138124)[0m f1_per_class: [0.063, 0.332, 0.353, 0.0, 0.0, 0.0, 0.398, 0.0, 0.025, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.25279850746268656
[2m[36m(func pid=145070)[0m top5: 0.6161380597014925
[2m[36m(func pid=145070)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=145070)[0m f1_macro: 0.12268230844113137
[2m[36m(func pid=145070)[0m f1_weighted: 0.20310900363128775
[2m[36m(func pid=145070)[0m f1_per_class: [0.095, 0.324, 0.327, 0.0, 0.0, 0.0, 0.481, 0.0, 0.0, 0.0]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 2.0145 | Steps: 2 | Val loss: 6.9752 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7940 | Steps: 2 | Val loss: 2.3131 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6815 | Steps: 2 | Val loss: 2.2314 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.5191 | Steps: 2 | Val loss: 2.1811 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 00:23:24 (running for 00:30:33.19)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.015 |      0.206 |                   81 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.686 |      0.117 |                   32 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.929 |      0.001 |                    5 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.825 |      0.123 |                    3 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.28591417910447764
[2m[36m(func pid=127909)[0m top5: 0.8628731343283582
[2m[36m(func pid=127909)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=127909)[0m f1_macro: 0.2059311102169255
[2m[36m(func pid=127909)[0m f1_weighted: 0.2510777155392322
[2m[36m(func pid=127909)[0m f1_per_class: [0.156, 0.484, 0.338, 0.074, 0.038, 0.0, 0.378, 0.435, 0.095, 0.062]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.007462686567164179
[2m[36m(func pid=144068)[0m top5: 0.5284514925373134
[2m[36m(func pid=144068)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=144068)[0m f1_macro: 0.006602353585112205
[2m[36m(func pid=144068)[0m f1_weighted: 0.0011749161622102951
[2m[36m(func pid=144068)[0m f1_per_class: [0.054, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.19402985074626866
[2m[36m(func pid=138124)[0m top5: 0.7350746268656716
[2m[36m(func pid=138124)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=138124)[0m f1_macro: 0.12385122808022772
[2m[36m(func pid=138124)[0m f1_weighted: 0.1919590149100908
[2m[36m(func pid=138124)[0m f1_per_class: [0.07, 0.361, 0.345, 0.0, 0.0, 0.0, 0.419, 0.0, 0.044, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.055970149253731345
[2m[36m(func pid=145070)[0m top5: 0.6898320895522388
[2m[36m(func pid=145070)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=145070)[0m f1_macro: 0.03530182197103714
[2m[36m(func pid=145070)[0m f1_weighted: 0.05822555778989001
[2m[36m(func pid=145070)[0m f1_per_class: [0.149, 0.0, 0.0, 0.0, 0.019, 0.0, 0.185, 0.0, 0.0, 0.0]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7334 | Steps: 2 | Val loss: 2.2845 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.5967 | Steps: 2 | Val loss: 10.2441 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5952 | Steps: 2 | Val loss: 2.2176 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.4634 | Steps: 2 | Val loss: 2.2176 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:23:29 (running for 00:30:38.37)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.015 |      0.206 |                   81 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.682 |      0.124 |                   33 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.733 |      0.029 |                    7 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.519 |      0.035 |                    4 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.04757462686567164
[2m[36m(func pid=144068)[0m top5: 0.542910447761194
[2m[36m(func pid=144068)[0m f1_micro: 0.04757462686567164
[2m[36m(func pid=144068)[0m f1_macro: 0.02890771762068453
[2m[36m(func pid=144068)[0m f1_weighted: 0.038806823540878395
[2m[36m(func pid=144068)[0m f1_per_class: [0.048, 0.219, 0.022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=127909)[0m top1: 0.10494402985074627
[2m[36m(func pid=127909)[0m top5: 0.8544776119402985
[2m[36m(func pid=127909)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=127909)[0m f1_macro: 0.15795796237089818
[2m[36m(func pid=127909)[0m f1_weighted: 0.11108788982568929
[2m[36m(func pid=127909)[0m f1_per_class: [0.15, 0.296, 0.375, 0.141, 0.212, 0.0, 0.0, 0.175, 0.054, 0.177]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m top1: 0.21082089552238806
[2m[36m(func pid=138124)[0m top5: 0.7453358208955224
[2m[36m(func pid=138124)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=138124)[0m f1_macro: 0.1282916983104458
[2m[36m(func pid=138124)[0m f1_weighted: 0.2013984333440797
[2m[36m(func pid=138124)[0m f1_per_class: [0.076, 0.361, 0.357, 0.0, 0.0, 0.0, 0.45, 0.0, 0.038, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.01585820895522388
[2m[36m(func pid=145070)[0m top5: 0.6305970149253731
[2m[36m(func pid=145070)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=145070)[0m f1_macro: 0.028796514948063772
[2m[36m(func pid=145070)[0m f1_weighted: 0.015065532828633606
[2m[36m(func pid=145070)[0m f1_per_class: [0.044, 0.0, 0.167, 0.045, 0.015, 0.0, 0.0, 0.0, 0.017, 0.0]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.9647 | Steps: 2 | Val loss: 5.8739 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7194 | Steps: 2 | Val loss: 2.2447 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5649 | Steps: 2 | Val loss: 2.2003 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.3343 | Steps: 2 | Val loss: 2.3346 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:23:34 (running for 00:30:43.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.965 |      0.283 |                   83 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.595 |      0.128 |                   34 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.733 |      0.029 |                    7 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.463 |      0.029 |                    5 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.35447761194029853
[2m[36m(func pid=127909)[0m top5: 0.8852611940298507
[2m[36m(func pid=127909)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=127909)[0m f1_macro: 0.28335951305199336
[2m[36m(func pid=127909)[0m f1_weighted: 0.32240537353979937
[2m[36m(func pid=127909)[0m f1_per_class: [0.176, 0.083, 0.414, 0.515, 0.213, 0.458, 0.258, 0.368, 0.103, 0.246]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.09048507462686567
[2m[36m(func pid=144068)[0m top5: 0.632929104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=144068)[0m f1_macro: 0.060841871031388176
[2m[36m(func pid=144068)[0m f1_weighted: 0.05025394889265851
[2m[36m(func pid=144068)[0m f1_per_class: [0.067, 0.215, 0.293, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.23647388059701493
[2m[36m(func pid=138124)[0m top5: 0.7625932835820896
[2m[36m(func pid=138124)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=138124)[0m f1_macro: 0.13795754827211829
[2m[36m(func pid=138124)[0m f1_weighted: 0.21921662536882652
[2m[36m(func pid=138124)[0m f1_per_class: [0.084, 0.394, 0.37, 0.0, 0.0, 0.0, 0.49, 0.0, 0.041, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.020522388059701493
[2m[36m(func pid=145070)[0m top5: 0.47574626865671643
[2m[36m(func pid=145070)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=145070)[0m f1_macro: 0.014781228896111784
[2m[36m(func pid=145070)[0m f1_weighted: 0.016149452492670816
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.0, 0.0, 0.038, 0.017, 0.0, 0.0, 0.093, 0.0, 0.0]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.6522 | Steps: 2 | Val loss: 4.5940 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6783 | Steps: 2 | Val loss: 2.2012 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5768 | Steps: 2 | Val loss: 2.1823 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3166 | Steps: 2 | Val loss: 2.3985 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:23:39 (running for 00:30:48.72)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.652 |      0.295 |                   84 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.565 |      0.138 |                   35 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.719 |      0.061 |                    8 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.334 |      0.015 |                    6 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.40578358208955223
[2m[36m(func pid=127909)[0m top5: 0.909981343283582
[2m[36m(func pid=127909)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=127909)[0m f1_macro: 0.29511473287808027
[2m[36m(func pid=127909)[0m f1_weighted: 0.38493809281436475
[2m[36m(func pid=127909)[0m f1_per_class: [0.259, 0.124, 0.393, 0.543, 0.171, 0.439, 0.447, 0.282, 0.0, 0.293]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.1599813432835821
[2m[36m(func pid=144068)[0m top5: 0.7649253731343284
[2m[36m(func pid=144068)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=144068)[0m f1_macro: 0.08200011594029306
[2m[36m(func pid=144068)[0m f1_weighted: 0.13403449403885065
[2m[36m(func pid=144068)[0m f1_per_class: [0.082, 0.254, 0.19, 0.0, 0.0, 0.0, 0.294, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.25466417910447764
[2m[36m(func pid=138124)[0m top5: 0.7817164179104478
[2m[36m(func pid=138124)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=138124)[0m f1_macro: 0.14214543110040434
[2m[36m(func pid=138124)[0m f1_weighted: 0.22512046955560117
[2m[36m(func pid=138124)[0m f1_per_class: [0.095, 0.405, 0.357, 0.0, 0.0, 0.0, 0.501, 0.0, 0.064, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.02378731343283582
[2m[36m(func pid=145070)[0m top5: 0.4771455223880597
[2m[36m(func pid=145070)[0m f1_micro: 0.02378731343283582
[2m[36m(func pid=145070)[0m f1_macro: 0.020958250527592516
[2m[36m(func pid=145070)[0m f1_weighted: 0.016112567066637797
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.0, 0.0, 0.053, 0.131, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4593 | Steps: 2 | Val loss: 4.9678 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6407 | Steps: 2 | Val loss: 2.1566 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5483 | Steps: 2 | Val loss: 2.1654 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.1444 | Steps: 2 | Val loss: 2.3763 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:23:44 (running for 00:30:53.81)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.459 |      0.246 |                   85 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.577 |      0.142 |                   36 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.678 |      0.082 |                    9 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.317 |      0.021 |                    7 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.32276119402985076
[2m[36m(func pid=127909)[0m top5: 0.8791977611940298
[2m[36m(func pid=127909)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=127909)[0m f1_macro: 0.24565091509768297
[2m[36m(func pid=127909)[0m f1_weighted: 0.3431981392991361
[2m[36m(func pid=127909)[0m f1_per_class: [0.298, 0.339, 0.229, 0.265, 0.091, 0.359, 0.485, 0.275, 0.0, 0.116]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.26865671641791045
[2m[36m(func pid=144068)[0m top5: 0.7882462686567164
[2m[36m(func pid=144068)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=144068)[0m f1_macro: 0.09151250613864735
[2m[36m(func pid=144068)[0m f1_weighted: 0.20074085298069386
[2m[36m(func pid=144068)[0m f1_per_class: [0.114, 0.32, 0.0, 0.0, 0.0, 0.0, 0.481, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.2630597014925373
[2m[36m(func pid=138124)[0m top5: 0.7952425373134329
[2m[36m(func pid=138124)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=138124)[0m f1_macro: 0.14025169383948485
[2m[36m(func pid=138124)[0m f1_weighted: 0.22262111785275807
[2m[36m(func pid=138124)[0m f1_per_class: [0.111, 0.389, 0.345, 0.0, 0.0, 0.0, 0.502, 0.0, 0.056, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.01632462686567164
[2m[36m(func pid=145070)[0m top5: 0.5186567164179104
[2m[36m(func pid=145070)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=145070)[0m f1_macro: 0.026864570567136353
[2m[36m(func pid=145070)[0m f1_weighted: 0.0071968670250106915
[2m[36m(func pid=145070)[0m f1_per_class: [0.12, 0.016, 0.0, 0.003, 0.105, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2772 | Steps: 2 | Val loss: 6.5551 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5749 | Steps: 2 | Val loss: 2.1149 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5659 | Steps: 2 | Val loss: 2.1501 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.0534 | Steps: 2 | Val loss: 2.2493 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=127909)[0m top1: 0.22294776119402984
[2m[36m(func pid=127909)[0m top5: 0.8512126865671642
[2m[36m(func pid=127909)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=127909)[0m f1_macro: 0.17027053396193073
[2m[36m(func pid=127909)[0m f1_weighted: 0.1786020034887421
[2m[36m(func pid=127909)[0m f1_per_class: [0.23, 0.426, 0.237, 0.007, 0.077, 0.104, 0.223, 0.297, 0.0, 0.103]
== Status ==
Current time: 2024-01-07 00:23:50 (running for 00:30:59.04)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.277 |      0.17  |                   86 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.548 |      0.14  |                   37 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.641 |      0.092 |                   10 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.144 |      0.027 |                    8 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.30970149253731344
[2m[36m(func pid=144068)[0m top5: 0.8302238805970149
[2m[36m(func pid=144068)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=144068)[0m f1_macro: 0.09525715475676796
[2m[36m(func pid=144068)[0m f1_weighted: 0.20731414843985463
[2m[36m(func pid=144068)[0m f1_per_class: [0.124, 0.335, 0.0, 0.0, 0.0, 0.0, 0.494, 0.0, 0.0, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.27472014925373134
[2m[36m(func pid=138124)[0m top5: 0.8083022388059702
[2m[36m(func pid=138124)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=138124)[0m f1_macro: 0.14263912476379076
[2m[36m(func pid=138124)[0m f1_weighted: 0.221233637040355
[2m[36m(func pid=138124)[0m f1_per_class: [0.122, 0.368, 0.364, 0.0, 0.0, 0.0, 0.507, 0.0, 0.066, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.07462686567164178
[2m[36m(func pid=145070)[0m top5: 0.757929104477612
[2m[36m(func pid=145070)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=145070)[0m f1_macro: 0.06304443748606849
[2m[36m(func pid=145070)[0m f1_weighted: 0.05025946430880083
[2m[36m(func pid=145070)[0m f1_per_class: [0.114, 0.144, 0.0, 0.0, 0.15, 0.19, 0.0, 0.0, 0.0, 0.033]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.8876 | Steps: 2 | Val loss: 5.9017 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5318 | Steps: 2 | Val loss: 2.0825 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5694 | Steps: 2 | Val loss: 2.1396 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=127909)[0m top1: 0.23647388059701493
[2m[36m(func pid=127909)[0m top5: 0.8526119402985075
[2m[36m(func pid=127909)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=127909)[0m f1_macro: 0.19476684341652603
[2m[36m(func pid=127909)[0m f1_weighted: 0.20994601288854475
[2m[36m(func pid=127909)[0m f1_per_class: [0.107, 0.431, 0.2, 0.082, 0.075, 0.231, 0.182, 0.38, 0.157, 0.104]
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.9867 | Steps: 2 | Val loss: 2.1475 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:23:55 (running for 00:31:04.17)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.888 |      0.195 |                   87 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.566 |      0.143 |                   38 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.575 |      0.095 |                   11 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.053 |      0.063 |                    9 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.3344216417910448
[2m[36m(func pid=144068)[0m top5: 0.8731343283582089
[2m[36m(func pid=144068)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=144068)[0m f1_macro: 0.10620250670052192
[2m[36m(func pid=144068)[0m f1_weighted: 0.21486178607877854
[2m[36m(func pid=144068)[0m f1_per_class: [0.193, 0.331, 0.0, 0.0, 0.0, 0.0, 0.514, 0.0, 0.025, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.2887126865671642
[2m[36m(func pid=138124)[0m top5: 0.8213619402985075
[2m[36m(func pid=138124)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=138124)[0m f1_macro: 0.14993610657758408
[2m[36m(func pid=138124)[0m f1_weighted: 0.22668630454511432
[2m[36m(func pid=138124)[0m f1_per_class: [0.138, 0.378, 0.381, 0.0, 0.0, 0.0, 0.515, 0.0, 0.087, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.16277985074626866
[2m[36m(func pid=145070)[0m top5: 0.7154850746268657
[2m[36m(func pid=145070)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=145070)[0m f1_macro: 0.07746001570203949
[2m[36m(func pid=145070)[0m f1_weighted: 0.07728474746254661
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.249, 0.0, 0.0, 0.103, 0.272, 0.003, 0.0, 0.0, 0.148]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2312 | Steps: 2 | Val loss: 5.2547 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5228 | Steps: 2 | Val loss: 2.0650 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5064 | Steps: 2 | Val loss: 2.1277 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=127909)[0m top1: 0.29011194029850745
[2m[36m(func pid=127909)[0m top5: 0.875
[2m[36m(func pid=127909)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=127909)[0m f1_macro: 0.23452376480510267
[2m[36m(func pid=127909)[0m f1_weighted: 0.3086248486785358
[2m[36m(func pid=127909)[0m f1_per_class: [0.174, 0.2, 0.171, 0.482, 0.092, 0.379, 0.223, 0.305, 0.156, 0.163]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:24:00 (running for 00:31:09.23)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.231 |      0.235 |                   88 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.569 |      0.15  |                   39 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.532 |      0.106 |                   12 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.987 |      0.077 |                   10 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1114 | Steps: 2 | Val loss: 2.1231 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=144068)[0m top1: 0.3316231343283582
[2m[36m(func pid=144068)[0m top5: 0.8838619402985075
[2m[36m(func pid=144068)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=144068)[0m f1_macro: 0.1164752576752786
[2m[36m(func pid=144068)[0m f1_weighted: 0.23274695729134387
[2m[36m(func pid=144068)[0m f1_per_class: [0.043, 0.333, 0.095, 0.006, 0.0, 0.149, 0.519, 0.0, 0.019, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.2905783582089552
[2m[36m(func pid=138124)[0m top5: 0.8311567164179104
[2m[36m(func pid=138124)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=138124)[0m f1_macro: 0.14925305441346504
[2m[36m(func pid=138124)[0m f1_weighted: 0.21969647070222573
[2m[36m(func pid=138124)[0m f1_per_class: [0.161, 0.345, 0.386, 0.0, 0.0, 0.0, 0.509, 0.0, 0.092, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.13246268656716417
[2m[36m(func pid=145070)[0m top5: 0.7154850746268657
[2m[36m(func pid=145070)[0m f1_micro: 0.13246268656716417
[2m[36m(func pid=145070)[0m f1_macro: 0.0895414177164672
[2m[36m(func pid=145070)[0m f1_weighted: 0.11094175560412693
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.24, 0.0, 0.0, 0.042, 0.237, 0.124, 0.0, 0.104, 0.148]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.5383 | Steps: 2 | Val loss: 5.2306 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4213 | Steps: 2 | Val loss: 2.0562 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5405 | Steps: 2 | Val loss: 2.1174 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5113 | Steps: 2 | Val loss: 2.2080 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:24:05 (running for 00:31:14.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.538 |      0.229 |                   89 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.506 |      0.149 |                   40 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.523 |      0.116 |                   13 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.111 |      0.09  |                   11 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.3292910447761194
[2m[36m(func pid=127909)[0m top5: 0.8861940298507462
[2m[36m(func pid=127909)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=127909)[0m f1_macro: 0.22872643904831086
[2m[36m(func pid=127909)[0m f1_weighted: 0.3163190545275092
[2m[36m(func pid=127909)[0m f1_per_class: [0.231, 0.067, 0.169, 0.504, 0.129, 0.413, 0.34, 0.016, 0.154, 0.265]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=144068)[0m top1: 0.3003731343283582
[2m[36m(func pid=144068)[0m top5: 0.8908582089552238
[2m[36m(func pid=144068)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=144068)[0m f1_macro: 0.16208406463062613
[2m[36m(func pid=144068)[0m f1_weighted: 0.296024664706247
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.267, 0.24, 0.22, 0.046, 0.327, 0.5, 0.0, 0.021, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.2994402985074627
[2m[36m(func pid=138124)[0m top5: 0.8367537313432836
[2m[36m(func pid=138124)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=138124)[0m f1_macro: 0.14089546180226706
[2m[36m(func pid=138124)[0m f1_weighted: 0.2193374022146453
[2m[36m(func pid=138124)[0m f1_per_class: [0.182, 0.339, 0.286, 0.0, 0.0, 0.0, 0.512, 0.0, 0.09, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.03871268656716418
[2m[36m(func pid=145070)[0m top5: 0.6982276119402985
[2m[36m(func pid=145070)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=145070)[0m f1_macro: 0.06759883602530288
[2m[36m(func pid=145070)[0m f1_weighted: 0.02727749603434495
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.0, 0.32, 0.0, 0.025, 0.026, 0.058, 0.0, 0.1, 0.148]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2974 | Steps: 2 | Val loss: 4.9325 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3748 | Steps: 2 | Val loss: 2.0652 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5203 | Steps: 2 | Val loss: 2.1113 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:24:10 (running for 00:31:19.67)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.297 |      0.256 |                   90 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.54  |      0.141 |                   41 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.421 |      0.162 |                   14 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.511 |      0.068 |                   12 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.37173507462686567
[2m[36m(func pid=127909)[0m top5: 0.8964552238805971
[2m[36m(func pid=127909)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=127909)[0m f1_macro: 0.25580012974778876
[2m[36m(func pid=127909)[0m f1_weighted: 0.3767849657681482
[2m[36m(func pid=127909)[0m f1_per_class: [0.209, 0.221, 0.175, 0.465, 0.17, 0.451, 0.484, 0.0, 0.103, 0.28]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9786 | Steps: 2 | Val loss: 2.3690 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=144068)[0m top1: 0.22667910447761194
[2m[36m(func pid=144068)[0m top5: 0.8847947761194029
[2m[36m(func pid=144068)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=144068)[0m f1_macro: 0.13556977024882427
[2m[36m(func pid=144068)[0m f1_weighted: 0.21747749374215286
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.196, 0.296, 0.435, 0.048, 0.252, 0.103, 0.0, 0.026, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.29617537313432835
[2m[36m(func pid=138124)[0m top5: 0.8446828358208955
[2m[36m(func pid=138124)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=138124)[0m f1_macro: 0.12961923818277804
[2m[36m(func pid=138124)[0m f1_weighted: 0.2135657242135732
[2m[36m(func pid=138124)[0m f1_per_class: [0.151, 0.313, 0.234, 0.0, 0.0, 0.0, 0.511, 0.0, 0.087, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.14832089552238806
[2m[36m(func pid=145070)[0m top5: 0.6333955223880597
[2m[36m(func pid=145070)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=145070)[0m f1_macro: 0.11855629918171537
[2m[36m(func pid=145070)[0m f1_weighted: 0.15430034969888662
[2m[36m(func pid=145070)[0m f1_per_class: [0.164, 0.0, 0.0, 0.453, 0.021, 0.0, 0.0, 0.393, 0.0, 0.154]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.2878 | Steps: 2 | Val loss: 5.2028 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3255 | Steps: 2 | Val loss: 2.0913 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4518 | Steps: 2 | Val loss: 2.1049 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:24:16 (running for 00:31:24.95)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.288 |      0.327 |                   91 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.52  |      0.13  |                   42 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.375 |      0.136 |                   15 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.979 |      0.119 |                   13 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.37033582089552236
[2m[36m(func pid=127909)[0m top5: 0.9001865671641791
[2m[36m(func pid=127909)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=127909)[0m f1_macro: 0.32661967501580125
[2m[36m(func pid=127909)[0m f1_weighted: 0.3885280357120065
[2m[36m(func pid=127909)[0m f1_per_class: [0.139, 0.56, 0.458, 0.25, 0.179, 0.451, 0.458, 0.352, 0.098, 0.32]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9064 | Steps: 2 | Val loss: 2.5150 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=144068)[0m top1: 0.20242537313432835
[2m[36m(func pid=144068)[0m top5: 0.7677238805970149
[2m[36m(func pid=144068)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=144068)[0m f1_macro: 0.1363067400163593
[2m[36m(func pid=144068)[0m f1_weighted: 0.1846112008552927
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.096, 0.333, 0.47, 0.035, 0.19, 0.0, 0.211, 0.028, 0.0]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.29850746268656714
[2m[36m(func pid=138124)[0m top5: 0.8493470149253731
[2m[36m(func pid=138124)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=138124)[0m f1_macro: 0.12317798612554576
[2m[36m(func pid=138124)[0m f1_weighted: 0.2114864275433795
[2m[36m(func pid=138124)[0m f1_per_class: [0.128, 0.287, 0.21, 0.003, 0.0, 0.008, 0.516, 0.0, 0.08, 0.0]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.14085820895522388
[2m[36m(func pid=145070)[0m top5: 0.5965485074626866
[2m[36m(func pid=145070)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=145070)[0m f1_macro: 0.11225080327644468
[2m[36m(func pid=145070)[0m f1_weighted: 0.14691913246827001
[2m[36m(func pid=145070)[0m f1_per_class: [0.156, 0.0, 0.0, 0.418, 0.023, 0.0, 0.0, 0.448, 0.0, 0.077]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3257 | Steps: 2 | Val loss: 8.8717 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2782 | Steps: 2 | Val loss: 2.1324 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4874 | Steps: 2 | Val loss: 2.1023 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:24:21 (running for 00:31:30.01)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.326 |      0.184 |                   92 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.452 |      0.123 |                   43 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.325 |      0.136 |                   16 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.906 |      0.112 |                   14 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.2439365671641791
[2m[36m(func pid=127909)[0m top5: 0.8125
[2m[36m(func pid=127909)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=127909)[0m f1_macro: 0.18415973967453755
[2m[36m(func pid=127909)[0m f1_weighted: 0.17185878011024602
[2m[36m(func pid=127909)[0m f1_per_class: [0.116, 0.533, 0.143, 0.0, 0.089, 0.361, 0.042, 0.343, 0.0, 0.214]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.6063 | Steps: 2 | Val loss: 2.6299 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=144068)[0m top1: 0.2103544776119403
[2m[36m(func pid=144068)[0m top5: 0.5219216417910447
[2m[36m(func pid=144068)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=144068)[0m f1_macro: 0.1579775763834801
[2m[36m(func pid=144068)[0m f1_weighted: 0.19357112525774403
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.05, 0.316, 0.52, 0.029, 0.099, 0.0, 0.434, 0.0, 0.133]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.3041044776119403
[2m[36m(func pid=138124)[0m top5: 0.8544776119402985
[2m[36m(func pid=138124)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=138124)[0m f1_macro: 0.13299275425915055
[2m[36m(func pid=138124)[0m f1_weighted: 0.221071087656015
[2m[36m(func pid=138124)[0m f1_per_class: [0.145, 0.283, 0.186, 0.035, 0.0, 0.008, 0.517, 0.0, 0.078, 0.077]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 2.5197 | Steps: 2 | Val loss: 6.1546 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=145070)[0m top1: 0.09095149253731344
[2m[36m(func pid=145070)[0m top5: 0.7476679104477612
[2m[36m(func pid=145070)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=145070)[0m f1_macro: 0.08532551058007444
[2m[36m(func pid=145070)[0m f1_weighted: 0.10049316601957954
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.172, 0.0, 0.161, 0.03, 0.0, 0.0, 0.437, 0.0, 0.054]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.3238 | Steps: 2 | Val loss: 2.1863 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4933 | Steps: 2 | Val loss: 2.0942 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:24:26 (running for 00:31:35.22)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  2.52  |      0.268 |                   93 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.487 |      0.133 |                   44 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.278 |      0.158 |                   17 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.606 |      0.085 |                   15 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.28824626865671643
[2m[36m(func pid=127909)[0m top5: 0.855410447761194
[2m[36m(func pid=127909)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=127909)[0m f1_macro: 0.26784320028032166
[2m[36m(func pid=127909)[0m f1_weighted: 0.2168701527128733
[2m[36m(func pid=127909)[0m f1_per_class: [0.314, 0.474, 0.48, 0.003, 0.062, 0.347, 0.169, 0.474, 0.14, 0.214]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7621 | Steps: 2 | Val loss: 2.6713 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=144068)[0m top1: 0.17957089552238806
[2m[36m(func pid=144068)[0m top5: 0.5088619402985075
[2m[36m(func pid=144068)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.13242826635487479
[2m[36m(func pid=144068)[0m f1_weighted: 0.15684948583521635
[2m[36m(func pid=144068)[0m f1_per_class: [0.0, 0.021, 0.381, 0.458, 0.031, 0.047, 0.0, 0.282, 0.0, 0.103]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.310634328358209
[2m[36m(func pid=138124)[0m top5: 0.8680037313432836
[2m[36m(func pid=138124)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=138124)[0m f1_macro: 0.13788627685663485
[2m[36m(func pid=138124)[0m f1_weighted: 0.23193578855718253
[2m[36m(func pid=138124)[0m f1_per_class: [0.16, 0.277, 0.182, 0.076, 0.0, 0.008, 0.517, 0.0, 0.082, 0.077]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.1823 | Steps: 2 | Val loss: 5.2889 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=145070)[0m top1: 0.07369402985074627
[2m[36m(func pid=145070)[0m top5: 0.7835820895522388
[2m[36m(func pid=145070)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=145070)[0m f1_macro: 0.06969414732897011
[2m[36m(func pid=145070)[0m f1_weighted: 0.05785358380938881
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.194, 0.0, 0.0, 0.055, 0.0, 0.0, 0.409, 0.0, 0.04]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.2930 | Steps: 2 | Val loss: 2.2400 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:24:31 (running for 00:31:40.32)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.182 |      0.23  |                   94 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.493 |      0.138 |                   45 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.324 |      0.132 |                   18 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.762 |      0.07  |                   16 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.27425373134328357
[2m[36m(func pid=127909)[0m top5: 0.8992537313432836
[2m[36m(func pid=127909)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=127909)[0m f1_macro: 0.2297533534778411
[2m[36m(func pid=127909)[0m f1_weighted: 0.24498629843680478
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.519, 0.319, 0.082, 0.12, 0.374, 0.232, 0.162, 0.136, 0.353]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4933 | Steps: 2 | Val loss: 2.0964 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=144068)[0m top1: 0.12686567164179105
[2m[36m(func pid=144068)[0m top5: 0.507929104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=144068)[0m f1_macro: 0.10608316979585612
[2m[36m(func pid=144068)[0m f1_weighted: 0.10320381696929376
[2m[36m(func pid=144068)[0m f1_per_class: [0.075, 0.011, 0.364, 0.298, 0.041, 0.023, 0.0, 0.187, 0.0, 0.062]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7197 | Steps: 2 | Val loss: 2.4878 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=138124)[0m top1: 0.31669776119402987
[2m[36m(func pid=138124)[0m top5: 0.875
[2m[36m(func pid=138124)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=138124)[0m f1_macro: 0.14704086242337
[2m[36m(func pid=138124)[0m f1_weighted: 0.2521760305068993
[2m[36m(func pid=138124)[0m f1_per_class: [0.136, 0.25, 0.151, 0.145, 0.019, 0.016, 0.532, 0.0, 0.084, 0.138]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.9878 | Steps: 2 | Val loss: 4.5658 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=145070)[0m top1: 0.07695895522388059
[2m[36m(func pid=145070)[0m top5: 0.8185634328358209
[2m[36m(func pid=145070)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=145070)[0m f1_macro: 0.09505351012456771
[2m[36m(func pid=145070)[0m f1_weighted: 0.08357806588480655
[2m[36m(func pid=145070)[0m f1_per_class: [0.107, 0.148, 0.0, 0.063, 0.086, 0.051, 0.033, 0.299, 0.124, 0.039]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2949 | Steps: 2 | Val loss: 2.2800 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127909)[0m top1: 0.3843283582089552
[2m[36m(func pid=127909)[0m top5: 0.9011194029850746
[2m[36m(func pid=127909)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=127909)[0m f1_macro: 0.2427134891987827
[2m[36m(func pid=127909)[0m f1_weighted: 0.38270583906618827
[2m[36m(func pid=127909)[0m f1_per_class: [0.0, 0.419, 0.208, 0.508, 0.128, 0.385, 0.387, 0.031, 0.068, 0.294]
[2m[36m(func pid=127909)[0m 
== Status ==
Current time: 2024-01-07 00:24:36 (running for 00:31:45.61)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.988 |      0.243 |                   95 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.493 |      0.147 |                   46 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.293 |      0.106 |                   19 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.72  |      0.095 |                   17 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4111 | Steps: 2 | Val loss: 2.0967 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4735 | Steps: 2 | Val loss: 2.3415 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=144068)[0m top1: 0.11007462686567164
[2m[36m(func pid=144068)[0m top5: 0.5097947761194029
[2m[36m(func pid=144068)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=144068)[0m f1_macro: 0.11630378491074309
[2m[36m(func pid=144068)[0m f1_weighted: 0.07865020851240742
[2m[36m(func pid=144068)[0m f1_per_class: [0.28, 0.0, 0.364, 0.216, 0.059, 0.0, 0.0, 0.155, 0.0, 0.09]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.3306902985074627
[2m[36m(func pid=138124)[0m top5: 0.8805970149253731
[2m[36m(func pid=138124)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=138124)[0m f1_macro: 0.1509383909425906
[2m[36m(func pid=138124)[0m f1_weighted: 0.2864196615440531
[2m[36m(func pid=138124)[0m f1_per_class: [0.038, 0.227, 0.144, 0.274, 0.038, 0.016, 0.547, 0.0, 0.067, 0.158]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.2463 | Steps: 2 | Val loss: 4.6544 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=145070)[0m top1: 0.13432835820895522
[2m[36m(func pid=145070)[0m top5: 0.7831156716417911
[2m[36m(func pid=145070)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=145070)[0m f1_macro: 0.11466661624546112
[2m[36m(func pid=145070)[0m f1_weighted: 0.13832514369726054
[2m[36m(func pid=145070)[0m f1_per_class: [0.222, 0.0, 0.111, 0.095, 0.072, 0.204, 0.262, 0.0, 0.114, 0.066]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3008 | Steps: 2 | Val loss: 2.2957 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:24:42 (running for 00:31:50.95)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  1.246 |      0.245 |                   96 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.411 |      0.151 |                   47 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.295 |      0.116 |                   20 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.474 |      0.115 |                   18 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.3689365671641791
[2m[36m(func pid=127909)[0m top5: 0.8903917910447762
[2m[36m(func pid=127909)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=127909)[0m f1_macro: 0.24535680371467045
[2m[36m(func pid=127909)[0m f1_weighted: 0.389617284283652
[2m[36m(func pid=127909)[0m f1_per_class: [0.16, 0.297, 0.202, 0.521, 0.094, 0.378, 0.437, 0.226, 0.0, 0.139]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3801 | Steps: 2 | Val loss: 2.0974 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.4057 | Steps: 2 | Val loss: 2.1412 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=144068)[0m top1: 0.10167910447761194
[2m[36m(func pid=144068)[0m top5: 0.5135261194029851
[2m[36m(func pid=144068)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=144068)[0m f1_macro: 0.11210090169022313
[2m[36m(func pid=144068)[0m f1_weighted: 0.06169335258994963
[2m[36m(func pid=144068)[0m f1_per_class: [0.275, 0.005, 0.381, 0.153, 0.073, 0.0, 0.0, 0.15, 0.0, 0.084]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.3423507462686567
[2m[36m(func pid=138124)[0m top5: 0.8843283582089553
[2m[36m(func pid=138124)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=138124)[0m f1_macro: 0.1569900545676563
[2m[36m(func pid=138124)[0m f1_weighted: 0.31027603597622483
[2m[36m(func pid=138124)[0m f1_per_class: [0.039, 0.222, 0.136, 0.354, 0.042, 0.023, 0.553, 0.0, 0.074, 0.127]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2646 | Steps: 2 | Val loss: 4.6574 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=145070)[0m top1: 0.291044776119403
[2m[36m(func pid=145070)[0m top5: 0.8208955223880597
[2m[36m(func pid=145070)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=145070)[0m f1_macro: 0.20848039846742764
[2m[36m(func pid=145070)[0m f1_weighted: 0.3091270061929376
[2m[36m(func pid=145070)[0m f1_per_class: [0.291, 0.0, 0.308, 0.468, 0.058, 0.241, 0.456, 0.0, 0.172, 0.091]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2297 | Steps: 2 | Val loss: 2.2937 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:24:47 (running for 00:31:56.15)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.265 |      0.257 |                   97 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.38  |      0.157 |                   48 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.301 |      0.112 |                   21 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.406 |      0.208 |                   19 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.3180970149253731
[2m[36m(func pid=127909)[0m top5: 0.8745335820895522
[2m[36m(func pid=127909)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=127909)[0m f1_macro: 0.25707654819509074
[2m[36m(func pid=127909)[0m f1_weighted: 0.3366840953532011
[2m[36m(func pid=127909)[0m f1_per_class: [0.275, 0.401, 0.297, 0.466, 0.073, 0.276, 0.261, 0.312, 0.0, 0.21]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3842 | Steps: 2 | Val loss: 2.0999 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.2700 | Steps: 2 | Val loss: 2.0450 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=144068)[0m top1: 0.09561567164179105
[2m[36m(func pid=144068)[0m top5: 0.5191231343283582
[2m[36m(func pid=144068)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=144068)[0m f1_macro: 0.10169063595067773
[2m[36m(func pid=144068)[0m f1_weighted: 0.04794578418926282
[2m[36m(func pid=144068)[0m f1_per_class: [0.222, 0.0, 0.368, 0.106, 0.074, 0.008, 0.0, 0.158, 0.0, 0.08]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m top1: 0.341884328358209
[2m[36m(func pid=138124)[0m top5: 0.8684701492537313
[2m[36m(func pid=138124)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=138124)[0m f1_macro: 0.15870729230052374
[2m[36m(func pid=138124)[0m f1_weighted: 0.3260879445024686
[2m[36m(func pid=138124)[0m f1_per_class: [0.042, 0.199, 0.13, 0.438, 0.056, 0.03, 0.536, 0.031, 0.054, 0.07]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.1562 | Steps: 2 | Val loss: 6.2326 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=145070)[0m top1: 0.33115671641791045
[2m[36m(func pid=145070)[0m top5: 0.8754664179104478
[2m[36m(func pid=145070)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=145070)[0m f1_macro: 0.2447246178410707
[2m[36m(func pid=145070)[0m f1_weighted: 0.34270895497061765
[2m[36m(func pid=145070)[0m f1_per_class: [0.319, 0.0, 0.381, 0.537, 0.046, 0.228, 0.46, 0.314, 0.026, 0.138]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1874 | Steps: 2 | Val loss: 2.2806 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 00:24:52 (running for 00:32:01.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.156 |      0.237 |                   98 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.384 |      0.159 |                   49 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.23  |      0.102 |                   22 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.27  |      0.245 |                   20 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.25886194029850745
[2m[36m(func pid=127909)[0m top5: 0.8843283582089553
[2m[36m(func pid=127909)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=127909)[0m f1_macro: 0.23716805838873783
[2m[36m(func pid=127909)[0m f1_weighted: 0.2744106579214686
[2m[36m(func pid=127909)[0m f1_per_class: [0.117, 0.54, 0.41, 0.249, 0.091, 0.201, 0.207, 0.33, 0.0, 0.226]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4361 | Steps: 2 | Val loss: 2.1088 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.3670 | Steps: 2 | Val loss: 2.0962 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=144068)[0m top1: 0.10354477611940298
[2m[36m(func pid=144068)[0m top5: 0.534981343283582
[2m[36m(func pid=144068)[0m f1_micro: 0.10354477611940298
[2m[36m(func pid=144068)[0m f1_macro: 0.10080358862884538
[2m[36m(func pid=144068)[0m f1_weighted: 0.06311243320224971
[2m[36m(func pid=144068)[0m f1_per_class: [0.192, 0.011, 0.333, 0.149, 0.073, 0.016, 0.0, 0.187, 0.0, 0.046]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2535 | Steps: 2 | Val loss: 6.3415 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=138124)[0m top1: 0.3260261194029851
[2m[36m(func pid=138124)[0m top5: 0.8535447761194029
[2m[36m(func pid=138124)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=138124)[0m f1_macro: 0.15783060730905868
[2m[36m(func pid=138124)[0m f1_weighted: 0.32657764526227273
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.18, 0.119, 0.472, 0.063, 0.072, 0.498, 0.083, 0.021, 0.07]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m top1: 0.26399253731343286
[2m[36m(func pid=145070)[0m top5: 0.8754664179104478
[2m[36m(func pid=145070)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=145070)[0m f1_macro: 0.22053948446103142
[2m[36m(func pid=145070)[0m f1_weighted: 0.24633088241197562
[2m[36m(func pid=145070)[0m f1_per_class: [0.269, 0.137, 0.367, 0.518, 0.039, 0.174, 0.075, 0.438, 0.027, 0.162]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1413 | Steps: 2 | Val loss: 2.2604 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 00:24:57 (running for 00:32:06.74)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00015 | RUNNING    | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.253 |      0.283 |                   99 |
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.436 |      0.158 |                   50 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.187 |      0.101 |                   23 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.367 |      0.221 |                   21 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127909)[0m top1: 0.29384328358208955
[2m[36m(func pid=127909)[0m top5: 0.8666044776119403
[2m[36m(func pid=127909)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=127909)[0m f1_macro: 0.2827944519591886
[2m[36m(func pid=127909)[0m f1_weighted: 0.30403486482046177
[2m[36m(func pid=127909)[0m f1_per_class: [0.117, 0.571, 0.4, 0.098, 0.173, 0.323, 0.344, 0.448, 0.125, 0.229]
[2m[36m(func pid=127909)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3472 | Steps: 2 | Val loss: 2.1142 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.2362 | Steps: 2 | Val loss: 2.3262 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=144068)[0m top1: 0.11800373134328358
[2m[36m(func pid=144068)[0m top5: 0.5769589552238806
[2m[36m(func pid=144068)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=144068)[0m f1_macro: 0.11674641216274348
[2m[36m(func pid=144068)[0m f1_weighted: 0.08612808628499878
[2m[36m(func pid=144068)[0m f1_per_class: [0.165, 0.016, 0.375, 0.207, 0.065, 0.039, 0.0, 0.251, 0.0, 0.049]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.19169776119402984
[2m[36m(func pid=145070)[0m top5: 0.8134328358208955
[2m[36m(func pid=145070)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=145070)[0m f1_macro: 0.17273493681424174
[2m[36m(func pid=145070)[0m f1_weighted: 0.12005900612923494
[2m[36m(func pid=145070)[0m f1_per_class: [0.246, 0.328, 0.306, 0.02, 0.063, 0.222, 0.0, 0.412, 0.0, 0.131]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=127909)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.1416 | Steps: 2 | Val loss: 6.4555 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=138124)[0m top1: 0.3045708955223881
[2m[36m(func pid=138124)[0m top5: 0.8302238805970149
[2m[36m(func pid=138124)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=138124)[0m f1_macro: 0.16629387631556944
[2m[36m(func pid=138124)[0m f1_weighted: 0.31938586253009515
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.156, 0.119, 0.503, 0.075, 0.075, 0.429, 0.228, 0.024, 0.055]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1665 | Steps: 2 | Val loss: 2.2377 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127909)[0m top1: 0.2574626865671642
[2m[36m(func pid=127909)[0m top5: 0.8652052238805971
[2m[36m(func pid=127909)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=127909)[0m f1_macro: 0.24707540631820812
[2m[36m(func pid=127909)[0m f1_weighted: 0.24531082642306506
[2m[36m(func pid=127909)[0m f1_per_class: [0.186, 0.542, 0.373, 0.026, 0.0, 0.411, 0.206, 0.437, 0.084, 0.207]
== Status ==
Current time: 2024-01-07 00:25:03 (running for 00:32:11.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 3 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.347 |      0.166 |                   51 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.141 |      0.117 |                   24 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.236 |      0.173 |                   22 |
| train_51d3e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1645 | Steps: 2 | Val loss: 2.6359 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4102 | Steps: 2 | Val loss: 2.1237 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=144068)[0m top1: 0.11380597014925373
[2m[36m(func pid=144068)[0m top5: 0.6539179104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=144068)[0m f1_macro: 0.12811438886880996
[2m[36m(func pid=144068)[0m f1_weighted: 0.09053656149188188
[2m[36m(func pid=144068)[0m f1_per_class: [0.153, 0.066, 0.387, 0.158, 0.056, 0.073, 0.0, 0.352, 0.0, 0.035]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.21875
[2m[36m(func pid=145070)[0m top5: 0.7639925373134329
[2m[36m(func pid=145070)[0m f1_micro: 0.21875
[2m[36m(func pid=145070)[0m f1_macro: 0.18879374591199446
[2m[36m(func pid=145070)[0m f1_weighted: 0.13459174913078698
[2m[36m(func pid=145070)[0m f1_per_class: [0.177, 0.371, 0.244, 0.0, 0.086, 0.265, 0.0, 0.482, 0.174, 0.088]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m top1: 0.25886194029850745
[2m[36m(func pid=138124)[0m top5: 0.7985074626865671
[2m[36m(func pid=138124)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=138124)[0m f1_macro: 0.15404811312665526
[2m[36m(func pid=138124)[0m f1_weighted: 0.2664438705564765
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.121, 0.116, 0.5, 0.076, 0.07, 0.254, 0.356, 0.0, 0.047]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1684 | Steps: 2 | Val loss: 2.2177 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.2816 | Steps: 2 | Val loss: 2.6969 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3626 | Steps: 2 | Val loss: 2.1305 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=144068)[0m top1: 0.11380597014925373
[2m[36m(func pid=144068)[0m top5: 0.7411380597014925
[2m[36m(func pid=144068)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=144068)[0m f1_macro: 0.1424241634993127
[2m[36m(func pid=144068)[0m f1_weighted: 0.09594000314887717
[2m[36m(func pid=144068)[0m f1_per_class: [0.143, 0.169, 0.389, 0.077, 0.049, 0.12, 0.0, 0.441, 0.0, 0.036]
[2m[36m(func pid=144068)[0m 
== Status ==
Current time: 2024-01-07 00:25:09 (running for 00:32:18.34)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.41  |      0.154 |                   52 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.168 |      0.142 |                   26 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.164 |      0.189 |                   23 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145070)[0m top1: 0.2178171641791045
[2m[36m(func pid=145070)[0m top5: 0.7397388059701493
[2m[36m(func pid=145070)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=145070)[0m f1_macro: 0.1998262684483937
[2m[36m(func pid=145070)[0m f1_weighted: 0.1474540518118373
[2m[36m(func pid=145070)[0m f1_per_class: [0.157, 0.417, 0.227, 0.0, 0.09, 0.281, 0.0, 0.549, 0.15, 0.126]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=150330)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=150330)[0m Configuration completed!
[2m[36m(func pid=150330)[0m New optimizer parameters:
[2m[36m(func pid=150330)[0m SGD (
[2m[36m(func pid=150330)[0m Parameter Group 0
[2m[36m(func pid=150330)[0m     dampening: 0
[2m[36m(func pid=150330)[0m     differentiable: False
[2m[36m(func pid=150330)[0m     foreach: None
[2m[36m(func pid=150330)[0m     lr: 0.1
[2m[36m(func pid=150330)[0m     maximize: False
[2m[36m(func pid=150330)[0m     momentum: 0.99
[2m[36m(func pid=150330)[0m     nesterov: False
[2m[36m(func pid=150330)[0m     weight_decay: 1e-05
[2m[36m(func pid=150330)[0m )
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.23180970149253732
[2m[36m(func pid=138124)[0m top5: 0.7691231343283582
[2m[36m(func pid=138124)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=138124)[0m f1_macro: 0.13746280360808574
[2m[36m(func pid=138124)[0m f1_weighted: 0.21454213666455593
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.085, 0.117, 0.501, 0.075, 0.072, 0.093, 0.391, 0.0, 0.041]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0548 | Steps: 2 | Val loss: 2.1879 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1140 | Steps: 2 | Val loss: 2.6132 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4050 | Steps: 2 | Val loss: 2.1370 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:25:14 (running for 00:32:23.41)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.363 |      0.137 |                   53 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.055 |      0.159 |                   27 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.282 |      0.2   |                   24 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.1310634328358209
[2m[36m(func pid=144068)[0m top5: 0.7980410447761194
[2m[36m(func pid=144068)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=144068)[0m f1_macro: 0.15948929355148223
[2m[36m(func pid=144068)[0m f1_weighted: 0.10612145197310795
[2m[36m(func pid=144068)[0m f1_per_class: [0.159, 0.229, 0.348, 0.02, 0.048, 0.204, 0.006, 0.494, 0.038, 0.05]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.2213 | Steps: 2 | Val loss: 2.7333 | Batch size: 32 | lr: 0.1 | Duration: 4.39s
[2m[36m(func pid=145070)[0m top1: 0.21315298507462688
[2m[36m(func pid=145070)[0m top5: 0.8292910447761194
[2m[36m(func pid=145070)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=145070)[0m f1_macro: 0.21898073233928478
[2m[36m(func pid=145070)[0m f1_weighted: 0.22460742687706528
[2m[36m(func pid=145070)[0m f1_per_class: [0.19, 0.39, 0.244, 0.246, 0.084, 0.3, 0.057, 0.464, 0.105, 0.108]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m top1: 0.21315298507462688
[2m[36m(func pid=138124)[0m top5: 0.7406716417910447
[2m[36m(func pid=138124)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=138124)[0m f1_macro: 0.12315603264956235
[2m[36m(func pid=138124)[0m f1_weighted: 0.1859617743209526
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.069, 0.121, 0.503, 0.073, 0.053, 0.018, 0.357, 0.0, 0.037]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.9949 | Steps: 2 | Val loss: 2.1561 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=150330)[0m top1: 0.020522388059701493
[2m[36m(func pid=150330)[0m top5: 0.6380597014925373
[2m[36m(func pid=150330)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=150330)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=150330)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=150330)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1974 | Steps: 2 | Val loss: 2.4536 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4157 | Steps: 2 | Val loss: 2.1482 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 00:25:20 (running for 00:32:28.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.405 |      0.123 |                   54 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.995 |      0.188 |                   28 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.114 |      0.219 |                   25 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  4.221 |      0.004 |                    1 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.19169776119402984
[2m[36m(func pid=144068)[0m top5: 0.8535447761194029
[2m[36m(func pid=144068)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=144068)[0m f1_macro: 0.18809861609955839
[2m[36m(func pid=144068)[0m f1_weighted: 0.197089225626408
[2m[36m(func pid=144068)[0m f1_per_class: [0.188, 0.287, 0.379, 0.003, 0.048, 0.217, 0.325, 0.257, 0.081, 0.096]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 7.4497 | Steps: 2 | Val loss: 2.4208 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=145070)[0m top1: 0.3218283582089552
[2m[36m(func pid=145070)[0m top5: 0.902518656716418
[2m[36m(func pid=145070)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=145070)[0m f1_macro: 0.2383780635209168
[2m[36m(func pid=145070)[0m f1_weighted: 0.34206439087870155
[2m[36m(func pid=145070)[0m f1_per_class: [0.164, 0.14, 0.289, 0.511, 0.083, 0.335, 0.376, 0.229, 0.149, 0.108]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m top1: 0.20289179104477612
[2m[36m(func pid=138124)[0m top5: 0.7103544776119403
[2m[36m(func pid=138124)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=138124)[0m f1_macro: 0.11502260220382077
[2m[36m(func pid=138124)[0m f1_weighted: 0.17175878456753912
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.051, 0.119, 0.491, 0.079, 0.047, 0.0, 0.33, 0.0, 0.033]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=150330)[0m top1: 0.01912313432835821
[2m[36m(func pid=150330)[0m top5: 0.7621268656716418
[2m[36m(func pid=150330)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=150330)[0m f1_macro: 0.007609613683860576
[2m[36m(func pid=150330)[0m f1_weighted: 0.014347244655690367
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.005, 0.0, 0.047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0635 | Steps: 2 | Val loss: 2.1360 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8838 | Steps: 2 | Val loss: 2.3456 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4255 | Steps: 2 | Val loss: 2.1611 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 00:25:25 (running for 00:32:34.07)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.416 |      0.115 |                   55 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.064 |      0.189 |                   29 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.197 |      0.238 |                   26 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  7.45  |      0.008 |                    2 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.23414179104477612
[2m[36m(func pid=144068)[0m top5: 0.8698694029850746
[2m[36m(func pid=144068)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=144068)[0m f1_macro: 0.1893936454875893
[2m[36m(func pid=144068)[0m f1_weighted: 0.23039399753012405
[2m[36m(func pid=144068)[0m f1_per_class: [0.211, 0.313, 0.314, 0.0, 0.055, 0.218, 0.465, 0.016, 0.088, 0.214]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 6.6096 | Steps: 2 | Val loss: 2.8413 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=145070)[0m top1: 0.3493470149253731
[2m[36m(func pid=145070)[0m top5: 0.9221082089552238
[2m[36m(func pid=145070)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=145070)[0m f1_macro: 0.22307206676776112
[2m[36m(func pid=145070)[0m f1_weighted: 0.3521052000020042
[2m[36m(func pid=145070)[0m f1_per_class: [0.173, 0.11, 0.301, 0.514, 0.085, 0.361, 0.458, 0.062, 0.027, 0.139]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m top1: 0.19309701492537312
[2m[36m(func pid=138124)[0m top5: 0.6847014925373134
[2m[36m(func pid=138124)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=138124)[0m f1_macro: 0.10527026020791896
[2m[36m(func pid=138124)[0m f1_weighted: 0.15748749462322628
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.021, 0.114, 0.473, 0.082, 0.024, 0.0, 0.303, 0.0, 0.036]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=150330)[0m top1: 0.06203358208955224
[2m[36m(func pid=150330)[0m top5: 0.6497201492537313
[2m[36m(func pid=150330)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=150330)[0m f1_macro: 0.032321561188536456
[2m[36m(func pid=150330)[0m f1_weighted: 0.00947601473086606
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.011, 0.202, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9744 | Steps: 2 | Val loss: 2.1057 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0086 | Steps: 2 | Val loss: 2.1376 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3700 | Steps: 2 | Val loss: 2.1730 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 13.3102 | Steps: 2 | Val loss: 4.0616 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:25:30 (running for 00:32:39.57)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.425 |      0.105 |                   56 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.974 |      0.186 |                   30 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.884 |      0.223 |                   27 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  6.61  |      0.032 |                    3 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.25093283582089554
[2m[36m(func pid=144068)[0m top5: 0.8717350746268657
[2m[36m(func pid=144068)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=144068)[0m f1_macro: 0.18579701888599404
[2m[36m(func pid=144068)[0m f1_weighted: 0.22984286002374021
[2m[36m(func pid=144068)[0m f1_per_class: [0.231, 0.324, 0.301, 0.0, 0.06, 0.08, 0.509, 0.0, 0.093, 0.258]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3824626865671642
[2m[36m(func pid=145070)[0m top5: 0.9221082089552238
[2m[36m(func pid=145070)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=145070)[0m f1_macro: 0.2613527318918698
[2m[36m(func pid=145070)[0m f1_weighted: 0.38259601371139274
[2m[36m(func pid=145070)[0m f1_per_class: [0.291, 0.312, 0.328, 0.524, 0.091, 0.379, 0.412, 0.107, 0.0, 0.17]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m top1: 0.18796641791044777
[2m[36m(func pid=138124)[0m top5: 0.6637126865671642
[2m[36m(func pid=138124)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=138124)[0m f1_macro: 0.1012270661349443
[2m[36m(func pid=138124)[0m f1_weighted: 0.15313333695242692
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.016, 0.114, 0.467, 0.077, 0.016, 0.0, 0.286, 0.0, 0.037]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=150330)[0m top1: 0.12033582089552239
[2m[36m(func pid=150330)[0m top5: 0.6152052238805971
[2m[36m(func pid=150330)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=150330)[0m f1_macro: 0.06767203742784086
[2m[36m(func pid=150330)[0m f1_weighted: 0.03583096142907877
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.0, 0.323, 0.0, 0.0, 0.273, 0.0, 0.0, 0.081, 0.0]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0787 | Steps: 2 | Val loss: 2.0901 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.4573 | Steps: 2 | Val loss: 2.2858 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 11.0531 | Steps: 2 | Val loss: 3.9971 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3621 | Steps: 2 | Val loss: 2.1822 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 00:25:35 (running for 00:32:44.89)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.37  |      0.101 |                   57 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.079 |      0.173 |                   31 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.009 |      0.261 |                   28 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 13.31  |      0.068 |                    4 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.25886194029850745
[2m[36m(func pid=144068)[0m top5: 0.8805970149253731
[2m[36m(func pid=144068)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=144068)[0m f1_macro: 0.17315716146236898
[2m[36m(func pid=144068)[0m f1_weighted: 0.2285567707849069
[2m[36m(func pid=144068)[0m f1_per_class: [0.252, 0.323, 0.272, 0.0, 0.071, 0.046, 0.521, 0.0, 0.105, 0.143]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3064365671641791
[2m[36m(func pid=145070)[0m top5: 0.9043843283582089
[2m[36m(func pid=145070)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=145070)[0m f1_macro: 0.2675648356964113
[2m[36m(func pid=145070)[0m f1_weighted: 0.2908885453809981
[2m[36m(func pid=145070)[0m f1_per_class: [0.29, 0.386, 0.351, 0.158, 0.106, 0.36, 0.342, 0.437, 0.025, 0.222]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.17024253731343283
[2m[36m(func pid=150330)[0m top5: 0.5601679104477612
[2m[36m(func pid=150330)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=150330)[0m f1_macro: 0.06990814539794139
[2m[36m(func pid=150330)[0m f1_weighted: 0.1431784342658148
[2m[36m(func pid=150330)[0m f1_per_class: [0.169, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.18236940298507462
[2m[36m(func pid=138124)[0m top5: 0.6431902985074627
[2m[36m(func pid=138124)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=138124)[0m f1_macro: 0.09703123065858008
[2m[36m(func pid=138124)[0m f1_weighted: 0.14643306362224956
[2m[36m(func pid=138124)[0m f1_per_class: [0.0, 0.01, 0.113, 0.457, 0.081, 0.0, 0.0, 0.266, 0.0, 0.043]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.9973 | Steps: 2 | Val loss: 2.0782 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9975 | Steps: 2 | Val loss: 2.6716 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 8.2698 | Steps: 2 | Val loss: 5.2045 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3306 | Steps: 2 | Val loss: 2.1913 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=144068)[0m top1: 0.2574626865671642
[2m[36m(func pid=144068)[0m top5: 0.8857276119402985
[2m[36m(func pid=144068)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=144068)[0m f1_macro: 0.1726172191302384
[2m[36m(func pid=144068)[0m f1_weighted: 0.22534081664756844
[2m[36m(func pid=144068)[0m f1_per_class: [0.263, 0.32, 0.275, 0.0, 0.07, 0.024, 0.519, 0.0, 0.107, 0.148]
== Status ==
Current time: 2024-01-07 00:25:41 (running for 00:32:50.14)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.362 |      0.097 |                   58 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.997 |      0.173 |                   32 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.457 |      0.268 |                   29 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 11.053 |      0.07  |                    5 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.23833955223880596
[2m[36m(func pid=145070)[0m top5: 0.8628731343283582
[2m[36m(func pid=145070)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=145070)[0m f1_macro: 0.19970659876768238
[2m[36m(func pid=145070)[0m f1_weighted: 0.15200072975421608
[2m[36m(func pid=145070)[0m f1_per_class: [0.087, 0.343, 0.303, 0.0, 0.108, 0.364, 0.051, 0.462, 0.075, 0.203]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.09281716417910447
[2m[36m(func pid=150330)[0m top5: 0.6156716417910447
[2m[36m(func pid=150330)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=150330)[0m f1_macro: 0.033703745509400015
[2m[36m(func pid=150330)[0m f1_weighted: 0.07742396892400212
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.253, 0.0, 0.055, 0.03]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.17630597014925373
[2m[36m(func pid=138124)[0m top5: 0.6245335820895522
[2m[36m(func pid=138124)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=138124)[0m f1_macro: 0.10113192490381243
[2m[36m(func pid=138124)[0m f1_weighted: 0.14206618265073617
[2m[36m(func pid=138124)[0m f1_per_class: [0.077, 0.005, 0.115, 0.444, 0.085, 0.0, 0.0, 0.239, 0.0, 0.047]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.9875 | Steps: 2 | Val loss: 2.0642 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.0592 | Steps: 2 | Val loss: 2.7214 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 10.7657 | Steps: 2 | Val loss: 11.0899 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 00:25:46 (running for 00:32:55.50)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.331 |      0.101 |                   59 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.987 |      0.17  |                   33 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.998 |      0.2   |                   30 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  8.27  |      0.034 |                    6 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3035 | Steps: 2 | Val loss: 2.1979 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=144068)[0m top1: 0.2523320895522388
[2m[36m(func pid=144068)[0m top5: 0.8913246268656716
[2m[36m(func pid=144068)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=144068)[0m f1_macro: 0.16985350631612087
[2m[36m(func pid=144068)[0m f1_weighted: 0.22320035417501374
[2m[36m(func pid=144068)[0m f1_per_class: [0.235, 0.309, 0.289, 0.003, 0.072, 0.024, 0.517, 0.0, 0.1, 0.148]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.21222014925373134
[2m[36m(func pid=145070)[0m top5: 0.824160447761194
[2m[36m(func pid=145070)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=145070)[0m f1_macro: 0.1997375403002408
[2m[36m(func pid=145070)[0m f1_weighted: 0.14080597055536956
[2m[36m(func pid=145070)[0m f1_per_class: [0.085, 0.357, 0.37, 0.0, 0.087, 0.377, 0.003, 0.407, 0.162, 0.149]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.024720149253731342
[2m[36m(func pid=150330)[0m top5: 0.6842350746268657
[2m[36m(func pid=150330)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=150330)[0m f1_macro: 0.014338211625886842
[2m[36m(func pid=150330)[0m f1_weighted: 0.021968418291632576
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.127, 0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.1646455223880597
[2m[36m(func pid=138124)[0m top5: 0.6068097014925373
[2m[36m(func pid=138124)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=138124)[0m f1_macro: 0.09938104228278277
[2m[36m(func pid=138124)[0m f1_weighted: 0.13219157755942515
[2m[36m(func pid=138124)[0m f1_per_class: [0.105, 0.005, 0.118, 0.41, 0.089, 0.0, 0.0, 0.221, 0.0, 0.045]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8993 | Steps: 2 | Val loss: 2.0441 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4647 | Steps: 2 | Val loss: 2.6853 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 13.6252 | Steps: 2 | Val loss: 9.0094 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 00:25:51 (running for 00:33:00.61)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.304 |      0.099 |                   60 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.899 |      0.184 |                   34 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.059 |      0.2   |                   31 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 10.766 |      0.014 |                    7 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2681902985074627
[2m[36m(func pid=144068)[0m top5: 0.8885261194029851
[2m[36m(func pid=144068)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=144068)[0m f1_macro: 0.18407981340957366
[2m[36m(func pid=144068)[0m f1_weighted: 0.25173523566462414
[2m[36m(func pid=144068)[0m f1_per_class: [0.23, 0.31, 0.314, 0.078, 0.071, 0.068, 0.526, 0.0, 0.095, 0.148]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3234 | Steps: 2 | Val loss: 2.2044 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=145070)[0m top1: 0.18190298507462688
[2m[36m(func pid=145070)[0m top5: 0.8344216417910447
[2m[36m(func pid=145070)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=145070)[0m f1_macro: 0.21822201051142712
[2m[36m(func pid=145070)[0m f1_weighted: 0.16407446500177628
[2m[36m(func pid=145070)[0m f1_per_class: [0.356, 0.168, 0.4, 0.183, 0.066, 0.357, 0.016, 0.398, 0.123, 0.115]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.1884328358208955
[2m[36m(func pid=150330)[0m top5: 0.7835820895522388
[2m[36m(func pid=150330)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=150330)[0m f1_macro: 0.058544122732559975
[2m[36m(func pid=150330)[0m f1_weighted: 0.15517376512232764
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.0, 0.0, 0.555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.16044776119402984
[2m[36m(func pid=138124)[0m top5: 0.5951492537313433
[2m[36m(func pid=138124)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=138124)[0m f1_macro: 0.10124894752834257
[2m[36m(func pid=138124)[0m f1_weighted: 0.1290419883914391
[2m[36m(func pid=138124)[0m f1_per_class: [0.137, 0.005, 0.124, 0.398, 0.091, 0.0, 0.0, 0.212, 0.0, 0.045]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.9351 | Steps: 2 | Val loss: 2.0442 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8408 | Steps: 2 | Val loss: 2.8252 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 12.6193 | Steps: 2 | Val loss: 9.9845 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 00:25:56 (running for 00:33:05.83)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.323 |      0.101 |                   61 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.935 |      0.21  |                   35 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.465 |      0.218 |                   32 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 13.625 |      0.059 |                    8 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.29011194029850745
[2m[36m(func pid=144068)[0m top5: 0.8861940298507462
[2m[36m(func pid=144068)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=144068)[0m f1_macro: 0.20980465024281872
[2m[36m(func pid=144068)[0m f1_weighted: 0.3020991094278902
[2m[36m(func pid=144068)[0m f1_per_class: [0.199, 0.262, 0.319, 0.249, 0.075, 0.176, 0.522, 0.0, 0.096, 0.2]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3192 | Steps: 2 | Val loss: 2.2084 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=145070)[0m top1: 0.24720149253731344
[2m[36m(func pid=145070)[0m top5: 0.8278917910447762
[2m[36m(func pid=145070)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=145070)[0m f1_macro: 0.22585253112393017
[2m[36m(func pid=145070)[0m f1_weighted: 0.2520390882905323
[2m[36m(func pid=145070)[0m f1_per_class: [0.212, 0.0, 0.348, 0.42, 0.066, 0.346, 0.191, 0.466, 0.093, 0.116]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.03171641791044776
[2m[36m(func pid=150330)[0m top5: 0.617070895522388
[2m[36m(func pid=150330)[0m f1_micro: 0.03171641791044776
[2m[36m(func pid=150330)[0m f1_macro: 0.01816608243133565
[2m[36m(func pid=150330)[0m f1_weighted: 0.02981623991645732
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.092, 0.0, 0.065, 0.025]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.15345149253731344
[2m[36m(func pid=138124)[0m top5: 0.5872201492537313
[2m[36m(func pid=138124)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=138124)[0m f1_macro: 0.09905536745154095
[2m[36m(func pid=138124)[0m f1_weighted: 0.12310689872782181
[2m[36m(func pid=138124)[0m f1_per_class: [0.13, 0.005, 0.136, 0.379, 0.096, 0.0, 0.0, 0.203, 0.0, 0.041]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8487 | Steps: 2 | Val loss: 2.0501 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.3233 | Steps: 2 | Val loss: 2.9285 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 11.9219 | Steps: 2 | Val loss: 10.9179 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:26:02 (running for 00:33:10.97)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.319 |      0.099 |                   62 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.849 |      0.237 |                   36 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.841 |      0.226 |                   33 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 12.619 |      0.018 |                    9 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.31529850746268656
[2m[36m(func pid=144068)[0m top5: 0.8782649253731343
[2m[36m(func pid=144068)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=144068)[0m f1_macro: 0.23668764749808907
[2m[36m(func pid=144068)[0m f1_weighted: 0.3421149987757201
[2m[36m(func pid=144068)[0m f1_per_class: [0.176, 0.186, 0.328, 0.385, 0.078, 0.323, 0.507, 0.047, 0.087, 0.25]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3655 | Steps: 2 | Val loss: 2.2172 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=145070)[0m top1: 0.32882462686567165
[2m[36m(func pid=145070)[0m top5: 0.8288246268656716
[2m[36m(func pid=145070)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=145070)[0m f1_macro: 0.24389445648480898
[2m[36m(func pid=145070)[0m f1_weighted: 0.3518520325677922
[2m[36m(func pid=145070)[0m f1_per_class: [0.165, 0.0, 0.235, 0.496, 0.071, 0.362, 0.462, 0.423, 0.088, 0.136]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.197294776119403
[2m[36m(func pid=150330)[0m top5: 0.5778917910447762
[2m[36m(func pid=150330)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=150330)[0m f1_macro: 0.1044215431777729
[2m[36m(func pid=150330)[0m f1_weighted: 0.0938831516731245
[2m[36m(func pid=150330)[0m f1_per_class: [0.196, 0.369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.449, 0.0, 0.03]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.14458955223880596
[2m[36m(func pid=138124)[0m top5: 0.5834888059701493
[2m[36m(func pid=138124)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=138124)[0m f1_macro: 0.09631212333114271
[2m[36m(func pid=138124)[0m f1_weighted: 0.11565447520926339
[2m[36m(func pid=138124)[0m f1_per_class: [0.131, 0.005, 0.14, 0.354, 0.093, 0.0, 0.0, 0.197, 0.0, 0.043]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7675 | Steps: 2 | Val loss: 2.0625 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.9391 | Steps: 2 | Val loss: 2.6366 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 22.8102 | Steps: 2 | Val loss: 13.3021 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:26:07 (running for 00:33:16.34)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.366 |      0.096 |                   63 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.767 |      0.277 |                   37 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.323 |      0.244 |                   34 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 11.922 |      0.104 |                   10 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.33115671641791045
[2m[36m(func pid=144068)[0m top5: 0.8717350746268657
[2m[36m(func pid=144068)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=144068)[0m f1_macro: 0.27749630296507005
[2m[36m(func pid=144068)[0m f1_weighted: 0.36005808745368606
[2m[36m(func pid=144068)[0m f1_per_class: [0.16, 0.143, 0.361, 0.466, 0.073, 0.361, 0.434, 0.382, 0.102, 0.294]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3495 | Steps: 2 | Val loss: 2.2224 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=145070)[0m top1: 0.39225746268656714
[2m[36m(func pid=145070)[0m top5: 0.8381529850746269
[2m[36m(func pid=145070)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=145070)[0m f1_macro: 0.26088846935138943
[2m[36m(func pid=145070)[0m f1_weighted: 0.37959710908778505
[2m[36m(func pid=145070)[0m f1_per_class: [0.272, 0.0, 0.32, 0.563, 0.084, 0.389, 0.51, 0.239, 0.072, 0.159]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.17723880597014927
[2m[36m(func pid=150330)[0m top5: 0.5676305970149254
[2m[36m(func pid=150330)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=150330)[0m f1_macro: 0.05320012982721338
[2m[36m(func pid=150330)[0m f1_weighted: 0.062214911862509485
[2m[36m(func pid=150330)[0m f1_per_class: [0.062, 0.295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.175, 0.0, 0.0]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m top1: 0.14039179104477612
[2m[36m(func pid=138124)[0m top5: 0.5778917910447762
[2m[36m(func pid=138124)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=138124)[0m f1_macro: 0.09297776161963253
[2m[36m(func pid=138124)[0m f1_weighted: 0.11220619779446701
[2m[36m(func pid=138124)[0m f1_per_class: [0.105, 0.0, 0.148, 0.347, 0.092, 0.0, 0.0, 0.193, 0.0, 0.045]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7687 | Steps: 2 | Val loss: 2.0879 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.6261 | Steps: 2 | Val loss: 2.3769 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 11.3782 | Steps: 2 | Val loss: 9.4398 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 00:26:12 (running for 00:33:21.65)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.349 |      0.093 |                   64 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.769 |      0.251 |                   38 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.939 |      0.261 |                   35 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 22.81  |      0.053 |                   11 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2896455223880597
[2m[36m(func pid=144068)[0m top5: 0.8236940298507462
[2m[36m(func pid=144068)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=144068)[0m f1_macro: 0.25050131038981416
[2m[36m(func pid=144068)[0m f1_weighted: 0.26072441543004804
[2m[36m(func pid=144068)[0m f1_per_class: [0.17, 0.096, 0.4, 0.488, 0.078, 0.375, 0.094, 0.438, 0.06, 0.308]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.4821 | Steps: 2 | Val loss: 2.2266 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=145070)[0m top1: 0.408115671641791
[2m[36m(func pid=145070)[0m top5: 0.8731343283582089
[2m[36m(func pid=145070)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=145070)[0m f1_macro: 0.28400205782648263
[2m[36m(func pid=145070)[0m f1_weighted: 0.39111919022549646
[2m[36m(func pid=145070)[0m f1_per_class: [0.375, 0.011, 0.34, 0.578, 0.091, 0.407, 0.509, 0.213, 0.15, 0.165]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.04151119402985075
[2m[36m(func pid=150330)[0m top5: 0.6870335820895522
[2m[36m(func pid=150330)[0m f1_micro: 0.04151119402985075
[2m[36m(func pid=150330)[0m f1_macro: 0.02310031354993393
[2m[36m(func pid=150330)[0m f1_weighted: 0.04188157656919866
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0, 0.024]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.9559 | Steps: 2 | Val loss: 2.1370 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=138124)[0m top1: 0.134794776119403
[2m[36m(func pid=138124)[0m top5: 0.5806902985074627
[2m[36m(func pid=138124)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=138124)[0m f1_macro: 0.09328504317276834
[2m[36m(func pid=138124)[0m f1_weighted: 0.10532021261295146
[2m[36m(func pid=138124)[0m f1_per_class: [0.113, 0.0, 0.166, 0.322, 0.099, 0.0, 0.0, 0.188, 0.0, 0.046]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.1181 | Steps: 2 | Val loss: 2.3956 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 5.0451 | Steps: 2 | Val loss: 15.5940 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 00:26:17 (running for 00:33:26.89)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.482 |      0.093 |                   65 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.956 |      0.219 |                   39 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.626 |      0.284 |                   36 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 11.378 |      0.023 |                   12 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2644589552238806
[2m[36m(func pid=144068)[0m top5: 0.7859141791044776
[2m[36m(func pid=144068)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.21942221794289826
[2m[36m(func pid=144068)[0m f1_weighted: 0.22042748289468553
[2m[36m(func pid=144068)[0m f1_per_class: [0.161, 0.069, 0.373, 0.484, 0.07, 0.366, 0.0, 0.371, 0.026, 0.273]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2620 | Steps: 2 | Val loss: 2.2277 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=145070)[0m top1: 0.33348880597014924
[2m[36m(func pid=145070)[0m top5: 0.8819962686567164
[2m[36m(func pid=145070)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=145070)[0m f1_macro: 0.25388491699961446
[2m[36m(func pid=145070)[0m f1_weighted: 0.35987667086732755
[2m[36m(func pid=145070)[0m f1_per_class: [0.044, 0.121, 0.293, 0.429, 0.094, 0.418, 0.477, 0.331, 0.143, 0.188]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.06203358208955224
[2m[36m(func pid=150330)[0m top5: 0.6301305970149254
[2m[36m(func pid=150330)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=150330)[0m f1_macro: 0.04086192422465346
[2m[36m(func pid=150330)[0m f1_weighted: 0.05250260295488527
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.0, 0.0, 0.133, 0.096, 0.119, 0.0, 0.0, 0.024, 0.038]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7210 | Steps: 2 | Val loss: 2.1823 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=138124)[0m top1: 0.12826492537313433
[2m[36m(func pid=138124)[0m top5: 0.5778917910447762
[2m[36m(func pid=138124)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=138124)[0m f1_macro: 0.09225247462772515
[2m[36m(func pid=138124)[0m f1_weighted: 0.09773646024327992
[2m[36m(func pid=138124)[0m f1_per_class: [0.111, 0.0, 0.186, 0.295, 0.096, 0.0, 0.0, 0.185, 0.0, 0.05]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1140 | Steps: 2 | Val loss: 2.8026 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 12.6011 | Steps: 2 | Val loss: 21.7559 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 00:26:23 (running for 00:33:32.10)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.262 |      0.092 |                   66 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.721 |      0.207 |                   40 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.118 |      0.254 |                   37 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  5.045 |      0.041 |                   13 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2523320895522388
[2m[36m(func pid=144068)[0m top5: 0.7490671641791045
[2m[36m(func pid=144068)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=144068)[0m f1_macro: 0.20741932817117287
[2m[36m(func pid=144068)[0m f1_weighted: 0.21669708222579945
[2m[36m(func pid=144068)[0m f1_per_class: [0.166, 0.083, 0.361, 0.48, 0.065, 0.342, 0.0, 0.332, 0.055, 0.19]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3101679104477612
[2m[36m(func pid=145070)[0m top5: 0.8773320895522388
[2m[36m(func pid=145070)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=145070)[0m f1_macro: 0.2524464608914601
[2m[36m(func pid=145070)[0m f1_weighted: 0.2768368686337883
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.432, 0.214, 0.007, 0.136, 0.421, 0.378, 0.502, 0.159, 0.276]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2688 | Steps: 2 | Val loss: 2.2288 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=150330)[0m top1: 0.03404850746268657
[2m[36m(func pid=150330)[0m top5: 0.5419776119402985
[2m[36m(func pid=150330)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=150330)[0m f1_macro: 0.032976151118563835
[2m[36m(func pid=150330)[0m f1_weighted: 0.03551974158202029
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.0, 0.0, 0.111, 0.018, 0.0, 0.0, 0.0, 0.102, 0.099]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7303 | Steps: 2 | Val loss: 2.2294 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=138124)[0m top1: 0.11893656716417911
[2m[36m(func pid=138124)[0m top5: 0.5783582089552238
[2m[36m(func pid=138124)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=138124)[0m f1_macro: 0.09212090665086231
[2m[36m(func pid=138124)[0m f1_weighted: 0.0845992915967378
[2m[36m(func pid=138124)[0m f1_per_class: [0.121, 0.0, 0.224, 0.247, 0.094, 0.0, 0.0, 0.182, 0.0, 0.052]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.9823 | Steps: 2 | Val loss: 3.6649 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 13.6224 | Steps: 2 | Val loss: 15.1256 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=144068)[0m top1: 0.2355410447761194
[2m[36m(func pid=144068)[0m top5: 0.7322761194029851
[2m[36m(func pid=144068)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=144068)[0m f1_macro: 0.19475551475853203
[2m[36m(func pid=144068)[0m f1_weighted: 0.20803569508363157
[2m[36m(func pid=144068)[0m f1_per_class: [0.182, 0.101, 0.373, 0.463, 0.063, 0.31, 0.0, 0.294, 0.028, 0.133]
== Status ==
Current time: 2024-01-07 00:26:28 (running for 00:33:37.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.269 |      0.092 |                   67 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.73  |      0.195 |                   41 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.114 |      0.252 |                   38 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 12.601 |      0.033 |                   14 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3111007462686567
[2m[36m(func pid=145070)[0m top5: 0.8493470149253731
[2m[36m(func pid=145070)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=145070)[0m f1_macro: 0.22494881815834064
[2m[36m(func pid=145070)[0m f1_weighted: 0.229699393013177
[2m[36m(func pid=145070)[0m f1_per_class: [0.0, 0.432, 0.193, 0.0, 0.165, 0.444, 0.231, 0.455, 0.154, 0.176]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2593 | Steps: 2 | Val loss: 2.2308 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=150330)[0m top1: 0.13432835820895522
[2m[36m(func pid=150330)[0m top5: 0.7336753731343284
[2m[36m(func pid=150330)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=150330)[0m f1_macro: 0.05740613002176516
[2m[36m(func pid=150330)[0m f1_weighted: 0.1370588624115367
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.0, 0.0, 0.43, 0.044, 0.0, 0.055, 0.0, 0.0, 0.046]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7298 | Steps: 2 | Val loss: 2.2714 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=138124)[0m top1: 0.11800373134328358
[2m[36m(func pid=138124)[0m top5: 0.5797574626865671
[2m[36m(func pid=138124)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=138124)[0m f1_macro: 0.09437833064147627
[2m[36m(func pid=138124)[0m f1_weighted: 0.08270344074436571
[2m[36m(func pid=138124)[0m f1_per_class: [0.124, 0.0, 0.253, 0.238, 0.09, 0.0, 0.0, 0.189, 0.0, 0.05]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7322 | Steps: 2 | Val loss: 3.6773 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 8.1567 | Steps: 2 | Val loss: 10.1739 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 00:26:33 (running for 00:33:42.50)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.259 |      0.094 |                   68 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.73  |      0.187 |                   42 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.982 |      0.225 |                   39 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 13.622 |      0.057 |                   15 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2248134328358209
[2m[36m(func pid=144068)[0m top5: 0.7182835820895522
[2m[36m(func pid=144068)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=144068)[0m f1_macro: 0.18736798546874098
[2m[36m(func pid=144068)[0m f1_weighted: 0.2030477473029597
[2m[36m(func pid=144068)[0m f1_per_class: [0.185, 0.126, 0.349, 0.458, 0.062, 0.255, 0.0, 0.269, 0.028, 0.142]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.29990671641791045
[2m[36m(func pid=145070)[0m top5: 0.855410447761194
[2m[36m(func pid=145070)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=145070)[0m f1_macro: 0.21648880727850078
[2m[36m(func pid=145070)[0m f1_weighted: 0.21556653166442682
[2m[36m(func pid=145070)[0m f1_per_class: [0.043, 0.438, 0.159, 0.0, 0.144, 0.429, 0.187, 0.446, 0.142, 0.176]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2966 | Steps: 2 | Val loss: 2.2347 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=150330)[0m top1: 0.2392723880597015
[2m[36m(func pid=150330)[0m top5: 0.7472014925373134
[2m[36m(func pid=150330)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=150330)[0m f1_macro: 0.1512747590409424
[2m[36m(func pid=150330)[0m f1_weighted: 0.22217253046219332
[2m[36m(func pid=150330)[0m f1_per_class: [0.167, 0.0, 0.0, 0.378, 0.154, 0.318, 0.212, 0.199, 0.0, 0.085]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6766 | Steps: 2 | Val loss: 2.2936 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=138124)[0m top1: 0.11287313432835822
[2m[36m(func pid=138124)[0m top5: 0.5834888059701493
[2m[36m(func pid=138124)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=138124)[0m f1_macro: 0.09494045027692322
[2m[36m(func pid=138124)[0m f1_weighted: 0.07236571772553184
[2m[36m(func pid=138124)[0m f1_per_class: [0.137, 0.0, 0.278, 0.199, 0.092, 0.0, 0.0, 0.191, 0.0, 0.053]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.0271 | Steps: 2 | Val loss: 3.5378 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 7.9795 | Steps: 2 | Val loss: 9.0115 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:26:38 (running for 00:33:47.79)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.297 |      0.095 |                   69 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.677 |      0.171 |                   43 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.732 |      0.216 |                   40 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  8.157 |      0.151 |                   16 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2103544776119403
[2m[36m(func pid=144068)[0m top5: 0.7196828358208955
[2m[36m(func pid=144068)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=144068)[0m f1_macro: 0.17121837974680595
[2m[36m(func pid=144068)[0m f1_weighted: 0.19110534490287054
[2m[36m(func pid=144068)[0m f1_per_class: [0.202, 0.142, 0.349, 0.454, 0.06, 0.152, 0.0, 0.253, 0.0, 0.099]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.34281716417910446
[2m[36m(func pid=145070)[0m top5: 0.8726679104477612
[2m[36m(func pid=145070)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=145070)[0m f1_macro: 0.2690231802319162
[2m[36m(func pid=145070)[0m f1_weighted: 0.33266998487419
[2m[36m(func pid=145070)[0m f1_per_class: [0.167, 0.511, 0.128, 0.272, 0.131, 0.391, 0.279, 0.494, 0.168, 0.15]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2538 | Steps: 2 | Val loss: 2.2339 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=150330)[0m top1: 0.17630597014925373
[2m[36m(func pid=150330)[0m top5: 0.7719216417910447
[2m[36m(func pid=150330)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=150330)[0m f1_macro: 0.14512306063967056
[2m[36m(func pid=150330)[0m f1_weighted: 0.11302111642573624
[2m[36m(func pid=150330)[0m f1_per_class: [0.205, 0.016, 0.0, 0.117, 0.0, 0.358, 0.0, 0.468, 0.1, 0.188]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.7086 | Steps: 2 | Val loss: 2.3042 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=138124)[0m top1: 0.11240671641791045
[2m[36m(func pid=138124)[0m top5: 0.5876865671641791
[2m[36m(func pid=138124)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=138124)[0m f1_macro: 0.0997926496228764
[2m[36m(func pid=138124)[0m f1_weighted: 0.07012680032394941
[2m[36m(func pid=138124)[0m f1_per_class: [0.138, 0.0, 0.324, 0.185, 0.086, 0.008, 0.0, 0.196, 0.0, 0.061]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.9264 | Steps: 2 | Val loss: 3.8091 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 5.4480 | Steps: 2 | Val loss: 9.0566 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 00:26:43 (running for 00:33:52.87)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.254 |      0.1   |                   70 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.709 |      0.169 |                   44 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.027 |      0.269 |                   41 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  7.98  |      0.145 |                   17 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.20615671641791045
[2m[36m(func pid=144068)[0m top5: 0.726679104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=144068)[0m f1_macro: 0.16901081550582336
[2m[36m(func pid=144068)[0m f1_weighted: 0.18630607705866709
[2m[36m(func pid=144068)[0m f1_per_class: [0.225, 0.196, 0.338, 0.436, 0.057, 0.055, 0.0, 0.268, 0.026, 0.088]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3316231343283582
[2m[36m(func pid=145070)[0m top5: 0.8791977611940298
[2m[36m(func pid=145070)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=145070)[0m f1_macro: 0.25005263103666675
[2m[36m(func pid=145070)[0m f1_weighted: 0.34641935499274623
[2m[36m(func pid=145070)[0m f1_per_class: [0.113, 0.271, 0.122, 0.488, 0.106, 0.342, 0.292, 0.502, 0.064, 0.2]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.1791044776119403
[2m[36m(func pid=150330)[0m top5: 0.9006529850746269
[2m[36m(func pid=150330)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=150330)[0m f1_macro: 0.14741861125555608
[2m[36m(func pid=150330)[0m f1_weighted: 0.14229297504926475
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.355, 0.0, 0.07, 0.2, 0.224, 0.018, 0.441, 0.074, 0.092]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2961 | Steps: 2 | Val loss: 2.2372 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.8207 | Steps: 2 | Val loss: 2.3022 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0098 | Steps: 2 | Val loss: 4.5086 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=138124)[0m top1: 0.10960820895522388
[2m[36m(func pid=138124)[0m top5: 0.5965485074626866
[2m[36m(func pid=138124)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=138124)[0m f1_macro: 0.10300414646780387
[2m[36m(func pid=138124)[0m f1_weighted: 0.06760229552057764
[2m[36m(func pid=138124)[0m f1_per_class: [0.126, 0.005, 0.379, 0.172, 0.078, 0.008, 0.0, 0.202, 0.0, 0.059]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 6.4659 | Steps: 2 | Val loss: 10.3111 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:26:49 (running for 00:33:58.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.296 |      0.103 |                   71 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.821 |      0.178 |                   45 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.926 |      0.25  |                   42 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  5.448 |      0.147 |                   18 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.20382462686567165
[2m[36m(func pid=144068)[0m top5: 0.7290111940298507
[2m[36m(func pid=144068)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=144068)[0m f1_macro: 0.17792609983089375
[2m[36m(func pid=144068)[0m f1_weighted: 0.1889703981220641
[2m[36m(func pid=144068)[0m f1_per_class: [0.236, 0.219, 0.338, 0.427, 0.054, 0.04, 0.0, 0.28, 0.097, 0.089]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.25513059701492535
[2m[36m(func pid=145070)[0m top5: 0.8577425373134329
[2m[36m(func pid=145070)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=145070)[0m f1_macro: 0.19824923278955195
[2m[36m(func pid=145070)[0m f1_weighted: 0.2683892888243922
[2m[36m(func pid=145070)[0m f1_per_class: [0.066, 0.027, 0.133, 0.413, 0.101, 0.288, 0.28, 0.429, 0.083, 0.163]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.2653917910447761
[2m[36m(func pid=150330)[0m top5: 0.9081156716417911
[2m[36m(func pid=150330)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=150330)[0m f1_macro: 0.17673416700855787
[2m[36m(func pid=150330)[0m f1_weighted: 0.26808499809331854
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.278, 0.312, 0.342, 0.35, 0.0, 0.395, 0.031, 0.0, 0.058]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2356 | Steps: 2 | Val loss: 2.2380 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6892 | Steps: 2 | Val loss: 2.3016 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4582 | Steps: 2 | Val loss: 4.8376 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138124)[0m top1: 0.10587686567164178
[2m[36m(func pid=138124)[0m top5: 0.6026119402985075
[2m[36m(func pid=138124)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=138124)[0m f1_macro: 0.10428797676801049
[2m[36m(func pid=138124)[0m f1_weighted: 0.06195221529769935
[2m[36m(func pid=138124)[0m f1_per_class: [0.121, 0.005, 0.415, 0.15, 0.073, 0.008, 0.0, 0.207, 0.0, 0.063]
[2m[36m(func pid=138124)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 9.6120 | Steps: 2 | Val loss: 11.9660 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 00:26:54 (running for 00:34:03.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.236 |      0.104 |                   72 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.689 |      0.181 |                   46 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.01  |      0.198 |                   43 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  6.466 |      0.177 |                   19 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.20335820895522388
[2m[36m(func pid=144068)[0m top5: 0.7458022388059702
[2m[36m(func pid=144068)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=144068)[0m f1_macro: 0.18129239852946763
[2m[36m(func pid=144068)[0m f1_weighted: 0.1888909871032519
[2m[36m(func pid=144068)[0m f1_per_class: [0.267, 0.217, 0.333, 0.421, 0.053, 0.032, 0.0, 0.318, 0.095, 0.075]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.22294776119402984
[2m[36m(func pid=145070)[0m top5: 0.835820895522388
[2m[36m(func pid=145070)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=145070)[0m f1_macro: 0.17387046404858458
[2m[36m(func pid=145070)[0m f1_weighted: 0.2429531522833388
[2m[36m(func pid=145070)[0m f1_per_class: [0.058, 0.011, 0.134, 0.385, 0.103, 0.266, 0.266, 0.295, 0.069, 0.151]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.21361940298507462
[2m[36m(func pid=150330)[0m top5: 0.7784514925373134
[2m[36m(func pid=150330)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=150330)[0m f1_macro: 0.1685456923412778
[2m[36m(func pid=150330)[0m f1_weighted: 0.2321021091704772
[2m[36m(func pid=150330)[0m f1_per_class: [0.039, 0.196, 0.289, 0.433, 0.08, 0.0, 0.164, 0.431, 0.0, 0.053]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2324 | Steps: 2 | Val loss: 2.2360 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.6243 | Steps: 2 | Val loss: 2.2837 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0527 | Steps: 2 | Val loss: 4.3514 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 4.0625 | Steps: 2 | Val loss: 21.5203 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=138124)[0m top1: 0.10261194029850747
[2m[36m(func pid=138124)[0m top5: 0.6124067164179104
[2m[36m(func pid=138124)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=138124)[0m f1_macro: 0.09851357469436463
[2m[36m(func pid=138124)[0m f1_weighted: 0.05857476865979965
[2m[36m(func pid=138124)[0m f1_per_class: [0.116, 0.005, 0.364, 0.134, 0.068, 0.016, 0.0, 0.218, 0.0, 0.064]
[2m[36m(func pid=138124)[0m 
== Status ==
Current time: 2024-01-07 00:26:59 (running for 00:34:08.28)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.232 |      0.099 |                   73 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.624 |      0.188 |                   47 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  2.458 |      0.174 |                   44 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  9.612 |      0.169 |                   20 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.19402985074626866
[2m[36m(func pid=144068)[0m top5: 0.7653917910447762
[2m[36m(func pid=144068)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=144068)[0m f1_macro: 0.18845252058868284
[2m[36m(func pid=144068)[0m f1_weighted: 0.18337186492492177
[2m[36m(func pid=144068)[0m f1_per_class: [0.291, 0.223, 0.328, 0.378, 0.053, 0.039, 0.0, 0.371, 0.139, 0.063]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.2332089552238806
[2m[36m(func pid=145070)[0m top5: 0.8442164179104478
[2m[36m(func pid=145070)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=145070)[0m f1_macro: 0.17868435046028758
[2m[36m(func pid=145070)[0m f1_weighted: 0.26005338183853544
[2m[36m(func pid=145070)[0m f1_per_class: [0.066, 0.058, 0.138, 0.409, 0.095, 0.288, 0.282, 0.19, 0.094, 0.167]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.05970149253731343
[2m[36m(func pid=150330)[0m top5: 0.5237873134328358
[2m[36m(func pid=150330)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=150330)[0m f1_macro: 0.10960684461236012
[2m[36m(func pid=150330)[0m f1_weighted: 0.04801880705725825
[2m[36m(func pid=150330)[0m f1_per_class: [0.126, 0.052, 0.244, 0.007, 0.024, 0.014, 0.0, 0.493, 0.062, 0.075]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2846 | Steps: 2 | Val loss: 2.2398 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.6990 | Steps: 2 | Val loss: 2.2642 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 8.0231 | Steps: 2 | Val loss: 26.4560 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8394 | Steps: 2 | Val loss: 3.7427 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=138124)[0m top1: 0.1021455223880597
[2m[36m(func pid=138124)[0m top5: 0.6259328358208955
[2m[36m(func pid=138124)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=138124)[0m f1_macro: 0.10051818837457789
[2m[36m(func pid=138124)[0m f1_weighted: 0.058852638952721054
[2m[36m(func pid=138124)[0m f1_per_class: [0.11, 0.011, 0.372, 0.125, 0.061, 0.024, 0.0, 0.235, 0.0, 0.067]
[2m[36m(func pid=138124)[0m 
== Status ==
Current time: 2024-01-07 00:27:04 (running for 00:34:13.55)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00016 | RUNNING    | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.285 |      0.101 |                   74 |
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.699 |      0.186 |                   48 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.053 |      0.179 |                   45 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  4.063 |      0.11  |                   21 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.18330223880597016
[2m[36m(func pid=144068)[0m top5: 0.7835820895522388
[2m[36m(func pid=144068)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=144068)[0m f1_macro: 0.1855013814044129
[2m[36m(func pid=144068)[0m f1_weighted: 0.17568659100284428
[2m[36m(func pid=144068)[0m f1_per_class: [0.282, 0.226, 0.324, 0.339, 0.051, 0.054, 0.0, 0.403, 0.118, 0.058]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m top1: 0.09095149253731344
[2m[36m(func pid=150330)[0m top5: 0.49533582089552236
[2m[36m(func pid=150330)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=150330)[0m f1_macro: 0.12032923391871413
[2m[36m(func pid=150330)[0m f1_weighted: 0.06032792095602534
[2m[36m(func pid=150330)[0m f1_per_class: [0.142, 0.0, 0.239, 0.0, 0.037, 0.281, 0.0, 0.344, 0.083, 0.077]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=145070)[0m top1: 0.2756529850746269
[2m[36m(func pid=145070)[0m top5: 0.8540111940298507
[2m[36m(func pid=145070)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=145070)[0m f1_macro: 0.22334430911803071
[2m[36m(func pid=145070)[0m f1_weighted: 0.3250662606103889
[2m[36m(func pid=145070)[0m f1_per_class: [0.082, 0.339, 0.15, 0.416, 0.107, 0.364, 0.304, 0.148, 0.122, 0.2]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=138124)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2257 | Steps: 2 | Val loss: 2.2389 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5373 | Steps: 2 | Val loss: 2.2257 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4127 | Steps: 2 | Val loss: 3.4390 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 7.3827 | Steps: 2 | Val loss: 20.7532 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=138124)[0m top1: 0.1044776119402985
[2m[36m(func pid=138124)[0m top5: 0.6422574626865671
[2m[36m(func pid=138124)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=138124)[0m f1_macro: 0.1076325552323022
[2m[36m(func pid=138124)[0m f1_weighted: 0.06352143183198111
[2m[36m(func pid=138124)[0m f1_per_class: [0.113, 0.027, 0.4, 0.123, 0.057, 0.038, 0.0, 0.246, 0.0, 0.072]
== Status ==
Current time: 2024-01-07 00:27:10 (running for 00:34:18.91)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 3 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.537 |      0.202 |                   49 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.839 |      0.223 |                   46 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  8.023 |      0.12  |                   22 |
| train_51d3e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.19029850746268656
[2m[36m(func pid=144068)[0m top5: 0.8106343283582089
[2m[36m(func pid=144068)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=144068)[0m f1_macro: 0.2015464024208336
[2m[36m(func pid=144068)[0m f1_weighted: 0.19332327170661062
[2m[36m(func pid=144068)[0m f1_per_class: [0.284, 0.235, 0.328, 0.317, 0.049, 0.126, 0.037, 0.454, 0.119, 0.067]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.29011194029850745
[2m[36m(func pid=145070)[0m top5: 0.8456156716417911
[2m[36m(func pid=145070)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=145070)[0m f1_macro: 0.22482158335950805
[2m[36m(func pid=145070)[0m f1_weighted: 0.2964582204006908
[2m[36m(func pid=145070)[0m f1_per_class: [0.134, 0.503, 0.162, 0.18, 0.144, 0.378, 0.327, 0.148, 0.114, 0.158]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.1730410447761194
[2m[36m(func pid=150330)[0m top5: 0.574160447761194
[2m[36m(func pid=150330)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=150330)[0m f1_macro: 0.1634546400299823
[2m[36m(func pid=150330)[0m f1_weighted: 0.1284960304045269
[2m[36m(func pid=150330)[0m f1_per_class: [0.131, 0.027, 0.216, 0.168, 0.093, 0.351, 0.0, 0.5, 0.082, 0.069]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.5413 | Steps: 2 | Val loss: 2.1908 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7014 | Steps: 2 | Val loss: 3.5767 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 5.1761 | Steps: 2 | Val loss: 13.3756 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=144068)[0m top1: 0.23414179104477612
[2m[36m(func pid=144068)[0m top5: 0.8339552238805971
[2m[36m(func pid=144068)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=144068)[0m f1_macro: 0.24153523361303017
[2m[36m(func pid=144068)[0m f1_weighted: 0.279042921077178
[2m[36m(func pid=144068)[0m f1_per_class: [0.294, 0.248, 0.333, 0.297, 0.049, 0.166, 0.305, 0.525, 0.116, 0.081]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3069029850746269
[2m[36m(func pid=145070)[0m top5: 0.8325559701492538
[2m[36m(func pid=145070)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=145070)[0m f1_macro: 0.2228068610743339
[2m[36m(func pid=145070)[0m f1_weighted: 0.2567626643995735
[2m[36m(func pid=145070)[0m f1_per_class: [0.105, 0.462, 0.232, 0.007, 0.172, 0.394, 0.353, 0.26, 0.109, 0.133]
[2m[36m(func pid=150330)[0m top1: 0.3512126865671642
[2m[36m(func pid=150330)[0m top5: 0.7691231343283582
[2m[36m(func pid=150330)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=150330)[0m f1_macro: 0.24187976044865628
[2m[36m(func pid=150330)[0m f1_weighted: 0.2780348413083322
[2m[36m(func pid=150330)[0m f1_per_class: [0.187, 0.106, 0.22, 0.521, 0.192, 0.474, 0.081, 0.477, 0.0, 0.161]
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6351 | Steps: 2 | Val loss: 2.1585 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 00:27:15 (running for 00:34:24.46)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.541 |      0.242 |                   50 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.413 |      0.225 |                   47 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  7.383 |      0.163 |                   23 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=155946)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=155946)[0m Configuration completed!
[2m[36m(func pid=155946)[0m New optimizer parameters:
[2m[36m(func pid=155946)[0m SGD (
[2m[36m(func pid=155946)[0m Parameter Group 0
[2m[36m(func pid=155946)[0m     dampening: 0
[2m[36m(func pid=155946)[0m     differentiable: False
[2m[36m(func pid=155946)[0m     foreach: None
[2m[36m(func pid=155946)[0m     lr: 0.0001
[2m[36m(func pid=155946)[0m     maximize: False
[2m[36m(func pid=155946)[0m     momentum: 0.9
[2m[36m(func pid=155946)[0m     nesterov: False
[2m[36m(func pid=155946)[0m     weight_decay: 1e-05
[2m[36m(func pid=155946)[0m )
[2m[36m(func pid=155946)[0m 
== Status ==
Current time: 2024-01-07 00:27:20 (running for 00:34:29.70)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.635 |      0.249 |                   51 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.701 |      0.223 |                   48 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  5.176 |      0.242 |                   24 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.26399253731343286
[2m[36m(func pid=144068)[0m top5: 0.8731343283582089
[2m[36m(func pid=144068)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=144068)[0m f1_macro: 0.2491479601444977
[2m[36m(func pid=144068)[0m f1_weighted: 0.3161275903406598
[2m[36m(func pid=144068)[0m f1_per_class: [0.25, 0.263, 0.344, 0.284, 0.052, 0.25, 0.428, 0.398, 0.106, 0.118]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.4871 | Steps: 2 | Val loss: 3.7410 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.8683 | Steps: 2 | Val loss: 12.8366 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0847 | Steps: 2 | Val loss: 2.3715 | Batch size: 32 | lr: 0.0001 | Duration: 4.77s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6214 | Steps: 2 | Val loss: 2.1108 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=145070)[0m top1: 0.3180970149253731
[2m[36m(func pid=145070)[0m top5: 0.8250932835820896
[2m[36m(func pid=145070)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=145070)[0m f1_macro: 0.23736484053638285
[2m[36m(func pid=145070)[0m f1_weighted: 0.2562335385716406
[2m[36m(func pid=145070)[0m f1_per_class: [0.111, 0.441, 0.333, 0.0, 0.159, 0.39, 0.357, 0.311, 0.126, 0.145]
[2m[36m(func pid=150330)[0m top1: 0.36847014925373134
[2m[36m(func pid=150330)[0m top5: 0.863339552238806
[2m[36m(func pid=150330)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=150330)[0m f1_macro: 0.19483714457321605
[2m[36m(func pid=150330)[0m f1_weighted: 0.3288303113294431
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.176, 0.299, 0.396, 0.227, 0.063, 0.567, 0.12, 0.0, 0.099]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=155946)[0m top1: 0.06669776119402986
[2m[36m(func pid=155946)[0m top5: 0.38572761194029853
[2m[36m(func pid=155946)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=155946)[0m f1_macro: 0.016861081384090097
[2m[36m(func pid=155946)[0m f1_weighted: 0.021043571984900443
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.0, 0.051, 0.0, 0.0, 0.0, 0.118, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
== Status ==
Current time: 2024-01-07 00:27:26 (running for 00:34:35.05)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.621 |      0.243 |                   52 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.487 |      0.237 |                   49 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  3.868 |      0.195 |                   25 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  3.085 |      0.017 |                    1 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.28777985074626866
[2m[36m(func pid=144068)[0m top5: 0.8880597014925373
[2m[36m(func pid=144068)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=144068)[0m f1_macro: 0.24306851985607208
[2m[36m(func pid=144068)[0m f1_weighted: 0.33071754990876745
[2m[36m(func pid=144068)[0m f1_per_class: [0.245, 0.268, 0.361, 0.31, 0.056, 0.296, 0.475, 0.176, 0.101, 0.143]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 9.5677 | Steps: 2 | Val loss: 15.6438 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7413 | Steps: 2 | Val loss: 3.4512 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0953 | Steps: 2 | Val loss: 2.3259 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.7092 | Steps: 2 | Val loss: 2.0884 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=150330)[0m top1: 0.28638059701492535
[2m[36m(func pid=150330)[0m top5: 0.8498134328358209
[2m[36m(func pid=150330)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=150330)[0m f1_macro: 0.210073784299676
[2m[36m(func pid=150330)[0m f1_weighted: 0.2726134239305173
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.297, 0.381, 0.166, 0.286, 0.0, 0.489, 0.416, 0.0, 0.065]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3125
[2m[36m(func pid=145070)[0m top5: 0.8395522388059702
[2m[36m(func pid=145070)[0m f1_micro: 0.3125
[2m[36m(func pid=145070)[0m f1_macro: 0.24435293659972
[2m[36m(func pid=145070)[0m f1_weighted: 0.25760803561208157
[2m[36m(func pid=145070)[0m f1_per_class: [0.108, 0.449, 0.303, 0.003, 0.151, 0.376, 0.337, 0.439, 0.105, 0.172]
[2m[36m(func pid=145070)[0m 
== Status ==
Current time: 2024-01-07 00:27:31 (running for 00:34:40.22)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.709 |      0.242 |                   53 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.741 |      0.244 |                   50 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  9.568 |      0.21  |                   26 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  3.085 |      0.017 |                    1 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.3064365671641791
[2m[36m(func pid=144068)[0m top5: 0.8908582089552238
[2m[36m(func pid=144068)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=144068)[0m f1_macro: 0.24249290792264994
[2m[36m(func pid=144068)[0m f1_weighted: 0.34129763753146525
[2m[36m(func pid=144068)[0m f1_per_class: [0.236, 0.263, 0.361, 0.344, 0.062, 0.317, 0.494, 0.062, 0.107, 0.179]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.14598880597014927
[2m[36m(func pid=155946)[0m top5: 0.5293843283582089
[2m[36m(func pid=155946)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=155946)[0m f1_macro: 0.043342142427674535
[2m[36m(func pid=155946)[0m f1_weighted: 0.08285336969249979
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.0, 0.261, 0.0, 0.0, 0.0, 0.172, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 5.5634 | Steps: 2 | Val loss: 15.1442 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7700 | Steps: 2 | Val loss: 3.0312 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5043 | Steps: 2 | Val loss: 2.0531 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0983 | Steps: 2 | Val loss: 2.3136 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=145070)[0m top1: 0.283115671641791
[2m[36m(func pid=145070)[0m top5: 0.875
[2m[36m(func pid=145070)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=145070)[0m f1_macro: 0.2505528265649381
[2m[36m(func pid=145070)[0m f1_weighted: 0.26293881087929405
[2m[36m(func pid=145070)[0m f1_per_class: [0.136, 0.43, 0.381, 0.066, 0.092, 0.349, 0.311, 0.492, 0.025, 0.222]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.18796641791044777
[2m[36m(func pid=150330)[0m top5: 0.7989738805970149
[2m[36m(func pid=150330)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=150330)[0m f1_macro: 0.1800050449460587
[2m[36m(func pid=150330)[0m f1_weighted: 0.16879204728697697
[2m[36m(func pid=150330)[0m f1_per_class: [0.118, 0.27, 0.286, 0.27, 0.248, 0.0, 0.048, 0.375, 0.133, 0.053]
[2m[36m(func pid=150330)[0m 
== Status ==
Current time: 2024-01-07 00:27:36 (running for 00:34:45.41)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.504 |      0.248 |                   54 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.77  |      0.251 |                   51 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  5.563 |      0.18  |                   27 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  3.095 |      0.043 |                    2 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.3269589552238806
[2m[36m(func pid=144068)[0m top5: 0.8927238805970149
[2m[36m(func pid=144068)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.24789128951304215
[2m[36m(func pid=144068)[0m f1_weighted: 0.3559671970562693
[2m[36m(func pid=144068)[0m f1_per_class: [0.23, 0.241, 0.367, 0.405, 0.067, 0.33, 0.499, 0.032, 0.109, 0.2]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.18889925373134328
[2m[36m(func pid=155946)[0m top5: 0.5718283582089553
[2m[36m(func pid=155946)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=155946)[0m f1_macro: 0.0487545753167527
[2m[36m(func pid=155946)[0m f1_weighted: 0.10061210839280837
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.0, 0.328, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8879 | Steps: 2 | Val loss: 17.3554 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.3101 | Steps: 2 | Val loss: 2.8656 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4372 | Steps: 2 | Val loss: 2.0292 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0318 | Steps: 2 | Val loss: 2.3129 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=150330)[0m top1: 0.15298507462686567
[2m[36m(func pid=150330)[0m top5: 0.7863805970149254
[2m[36m(func pid=150330)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=150330)[0m f1_macro: 0.2073441386605729
[2m[36m(func pid=150330)[0m f1_weighted: 0.16642603084054008
[2m[36m(func pid=150330)[0m f1_per_class: [0.341, 0.157, 0.27, 0.265, 0.168, 0.24, 0.003, 0.391, 0.087, 0.152]
[2m[36m(func pid=145070)[0m top1: 0.271455223880597
[2m[36m(func pid=145070)[0m top5: 0.8754664179104478
[2m[36m(func pid=145070)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=145070)[0m f1_macro: 0.21875966649963957
[2m[36m(func pid=145070)[0m f1_weighted: 0.29690073120692145
[2m[36m(func pid=145070)[0m f1_per_class: [0.147, 0.206, 0.211, 0.408, 0.061, 0.314, 0.27, 0.431, 0.0, 0.14]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=145070)[0m 
== Status ==
Current time: 2024-01-07 00:27:41 (running for 00:34:50.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.437 |      0.257 |                   55 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.31  |      0.219 |                   52 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.888 |      0.207 |                   28 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  3.098 |      0.049 |                    3 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.3474813432835821
[2m[36m(func pid=144068)[0m top5: 0.8978544776119403
[2m[36m(func pid=144068)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=144068)[0m f1_macro: 0.2570471168271402
[2m[36m(func pid=144068)[0m f1_weighted: 0.3687805242518777
[2m[36m(func pid=144068)[0m f1_per_class: [0.238, 0.228, 0.379, 0.448, 0.073, 0.344, 0.501, 0.032, 0.121, 0.207]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.19636194029850745
[2m[36m(func pid=155946)[0m top5: 0.5788246268656716
[2m[36m(func pid=155946)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=155946)[0m f1_macro: 0.041323534083695
[2m[36m(func pid=155946)[0m f1_weighted: 0.1020029190679926
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.005, 0.0, 0.35, 0.0, 0.0, 0.0, 0.057, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4451 | Steps: 2 | Val loss: 3.0395 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.0248 | Steps: 2 | Val loss: 16.4230 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1398 | Steps: 2 | Val loss: 2.0470 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=145070)[0m top1: 0.30550373134328357
[2m[36m(func pid=145070)[0m top5: 0.8610074626865671
[2m[36m(func pid=145070)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=145070)[0m f1_macro: 0.22141220341508103
[2m[36m(func pid=145070)[0m f1_weighted: 0.29755960115151914
[2m[36m(func pid=145070)[0m f1_per_class: [0.173, 0.032, 0.333, 0.515, 0.063, 0.219, 0.305, 0.425, 0.0, 0.148]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.27005597014925375
[2m[36m(func pid=150330)[0m top5: 0.7737873134328358
[2m[36m(func pid=150330)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=150330)[0m f1_macro: 0.22884949928897527
[2m[36m(func pid=150330)[0m f1_weighted: 0.23015768938860526
[2m[36m(func pid=150330)[0m f1_per_class: [0.187, 0.098, 0.333, 0.449, 0.149, 0.411, 0.03, 0.345, 0.083, 0.203]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0342 | Steps: 2 | Val loss: 2.3156 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:27:47 (running for 00:34:56.18)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  2.14  |      0.255 |                   56 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.445 |      0.221 |                   53 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  3.025 |      0.229 |                   29 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  3.032 |      0.041 |                    4 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.35261194029850745
[2m[36m(func pid=144068)[0m top5: 0.9039179104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=144068)[0m f1_macro: 0.25526841066745815
[2m[36m(func pid=144068)[0m f1_weighted: 0.3705806360792245
[2m[36m(func pid=144068)[0m f1_per_class: [0.216, 0.212, 0.349, 0.473, 0.078, 0.367, 0.485, 0.047, 0.099, 0.226]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.2042910447761194
[2m[36m(func pid=155946)[0m top5: 0.5797574626865671
[2m[36m(func pid=155946)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=155946)[0m f1_macro: 0.03958341060314964
[2m[36m(func pid=155946)[0m f1_weighted: 0.10257930706384698
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.0, 0.36, 0.0, 0.0, 0.0, 0.035, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8854 | Steps: 2 | Val loss: 3.1803 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.4215 | Steps: 2 | Val loss: 18.7777 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.8726 | Steps: 2 | Val loss: 2.0675 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=145070)[0m top1: 0.3824626865671642
[2m[36m(func pid=145070)[0m top5: 0.8610074626865671
[2m[36m(func pid=145070)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=145070)[0m f1_macro: 0.2551698611420093
[2m[36m(func pid=145070)[0m f1_weighted: 0.3576998362449485
[2m[36m(func pid=145070)[0m f1_per_class: [0.247, 0.011, 0.375, 0.558, 0.075, 0.174, 0.483, 0.458, 0.0, 0.171]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.25326492537313433
[2m[36m(func pid=150330)[0m top5: 0.8423507462686567
[2m[36m(func pid=150330)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=150330)[0m f1_macro: 0.17714423852106156
[2m[36m(func pid=150330)[0m f1_weighted: 0.2130673055421789
[2m[36m(func pid=150330)[0m f1_per_class: [0.143, 0.061, 0.37, 0.426, 0.133, 0.363, 0.105, 0.062, 0.0, 0.108]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9896 | Steps: 2 | Val loss: 2.3183 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 00:27:52 (running for 00:35:01.48)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.873 |      0.252 |                   57 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.885 |      0.255 |                   54 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  3.421 |      0.177 |                   30 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  3.034 |      0.04  |                    5 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.35774253731343286
[2m[36m(func pid=144068)[0m top5: 0.9034514925373134
[2m[36m(func pid=144068)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=144068)[0m f1_macro: 0.2515631153978525
[2m[36m(func pid=144068)[0m f1_weighted: 0.37064272784809216
[2m[36m(func pid=144068)[0m f1_per_class: [0.196, 0.173, 0.324, 0.495, 0.084, 0.379, 0.479, 0.078, 0.102, 0.207]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.21455223880597016
[2m[36m(func pid=155946)[0m top5: 0.582089552238806
[2m[36m(func pid=155946)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=155946)[0m f1_macro: 0.03986139351189431
[2m[36m(func pid=155946)[0m f1_weighted: 0.10554071663546365
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.0, 0.373, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7576 | Steps: 2 | Val loss: 3.1639 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 6.6250 | Steps: 2 | Val loss: 14.3877 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.5985 | Steps: 2 | Val loss: 2.0498 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=145070)[0m top1: 0.41324626865671643
[2m[36m(func pid=145070)[0m top5: 0.8502798507462687
[2m[36m(func pid=145070)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=145070)[0m f1_macro: 0.2700485469440973
[2m[36m(func pid=145070)[0m f1_weighted: 0.3804686176133874
[2m[36m(func pid=145070)[0m f1_per_class: [0.276, 0.005, 0.4, 0.568, 0.088, 0.208, 0.545, 0.404, 0.024, 0.182]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.3064365671641791
[2m[36m(func pid=150330)[0m top5: 0.8796641791044776
[2m[36m(func pid=150330)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=150330)[0m f1_macro: 0.2104664309550926
[2m[36m(func pid=150330)[0m f1_weighted: 0.31272853555509
[2m[36m(func pid=150330)[0m f1_per_class: [0.198, 0.108, 0.378, 0.485, 0.06, 0.354, 0.361, 0.032, 0.045, 0.085]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9497 | Steps: 2 | Val loss: 2.3221 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 00:27:57 (running for 00:35:06.75)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.599 |      0.276 |                   58 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.758 |      0.27  |                   55 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  6.625 |      0.21  |                   31 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.99  |      0.04  |                    6 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.37220149253731344
[2m[36m(func pid=144068)[0m top5: 0.8992537313432836
[2m[36m(func pid=144068)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=144068)[0m f1_macro: 0.2762314234521398
[2m[36m(func pid=144068)[0m f1_weighted: 0.38365892342518326
[2m[36m(func pid=144068)[0m f1_per_class: [0.189, 0.139, 0.349, 0.512, 0.088, 0.398, 0.481, 0.254, 0.119, 0.233]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.22201492537313433
[2m[36m(func pid=155946)[0m top5: 0.5792910447761194
[2m[36m(func pid=155946)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=155946)[0m f1_macro: 0.05047588689952144
[2m[36m(func pid=155946)[0m f1_weighted: 0.10912373596878094
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.093, 0.383, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8938 | Steps: 2 | Val loss: 3.1374 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0349 | Steps: 2 | Val loss: 13.9992 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4321 | Steps: 2 | Val loss: 2.0554 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=145070)[0m top1: 0.4076492537313433
[2m[36m(func pid=145070)[0m top5: 0.8451492537313433
[2m[36m(func pid=145070)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=145070)[0m f1_macro: 0.28409417578207574
[2m[36m(func pid=145070)[0m f1_weighted: 0.39019409867292926
[2m[36m(func pid=145070)[0m f1_per_class: [0.242, 0.0, 0.37, 0.53, 0.116, 0.352, 0.556, 0.392, 0.107, 0.176]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.2905783582089552
[2m[36m(func pid=150330)[0m top5: 0.9011194029850746
[2m[36m(func pid=150330)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=150330)[0m f1_macro: 0.22569772632722143
[2m[36m(func pid=150330)[0m f1_weighted: 0.32697458703776144
[2m[36m(func pid=150330)[0m f1_per_class: [0.128, 0.206, 0.377, 0.401, 0.042, 0.016, 0.497, 0.316, 0.146, 0.128]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9862 | Steps: 2 | Val loss: 2.3290 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 00:28:03 (running for 00:35:12.15)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.432 |      0.284 |                   59 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.894 |      0.284 |                   56 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.035 |      0.226 |                   32 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.95  |      0.05  |                    7 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.3670708955223881
[2m[36m(func pid=144068)[0m top5: 0.8913246268656716
[2m[36m(func pid=144068)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=144068)[0m f1_macro: 0.28351912919833655
[2m[36m(func pid=144068)[0m f1_weighted: 0.3747958132049174
[2m[36m(func pid=144068)[0m f1_per_class: [0.19, 0.115, 0.338, 0.511, 0.092, 0.392, 0.44, 0.413, 0.097, 0.246]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7763 | Steps: 2 | Val loss: 3.2457 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.9557 | Steps: 2 | Val loss: 16.7240 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=155946)[0m top1: 0.2224813432835821
[2m[36m(func pid=155946)[0m top5: 0.566231343283582
[2m[36m(func pid=155946)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=155946)[0m f1_macro: 0.047550682994653495
[2m[36m(func pid=155946)[0m f1_weighted: 0.11173228814512565
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.005, 0.049, 0.39, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5090 | Steps: 2 | Val loss: 2.0921 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=145070)[0m top1: 0.3423507462686567
[2m[36m(func pid=145070)[0m top5: 0.8563432835820896
[2m[36m(func pid=145070)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=145070)[0m f1_macro: 0.27137224432843243
[2m[36m(func pid=145070)[0m f1_weighted: 0.3575356736014115
[2m[36m(func pid=145070)[0m f1_per_class: [0.216, 0.011, 0.328, 0.389, 0.129, 0.434, 0.542, 0.406, 0.105, 0.154]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.21688432835820895
[2m[36m(func pid=150330)[0m top5: 0.8591417910447762
[2m[36m(func pid=150330)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=150330)[0m f1_macro: 0.1893573530060097
[2m[36m(func pid=150330)[0m f1_weighted: 0.217389794560113
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.473, 0.34, 0.163, 0.058, 0.0, 0.21, 0.328, 0.124, 0.197]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9297 | Steps: 2 | Val loss: 2.3340 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:28:08 (running for 00:35:17.32)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.509 |      0.275 |                   60 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.776 |      0.271 |                   57 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.956 |      0.189 |                   33 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.986 |      0.048 |                    8 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.35494402985074625
[2m[36m(func pid=144068)[0m top5: 0.8754664179104478
[2m[36m(func pid=144068)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=144068)[0m f1_macro: 0.2754036719689435
[2m[36m(func pid=144068)[0m f1_weighted: 0.35623879798382285
[2m[36m(func pid=144068)[0m f1_per_class: [0.19, 0.097, 0.319, 0.51, 0.092, 0.386, 0.381, 0.481, 0.082, 0.216]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6125 | Steps: 2 | Val loss: 3.4488 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.7885 | Steps: 2 | Val loss: 16.2130 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=155946)[0m top1: 0.2178171641791045
[2m[36m(func pid=155946)[0m top5: 0.5573694029850746
[2m[36m(func pid=155946)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=155946)[0m f1_macro: 0.046935537689902856
[2m[36m(func pid=155946)[0m f1_weighted: 0.11381988579069481
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.005, 0.032, 0.397, 0.0, 0.0, 0.0, 0.036, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3045708955223881
[2m[36m(func pid=145070)[0m top5: 0.8656716417910447
[2m[36m(func pid=145070)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=145070)[0m f1_macro: 0.2807765545084829
[2m[36m(func pid=145070)[0m f1_weighted: 0.32779327154399657
[2m[36m(func pid=145070)[0m f1_per_class: [0.268, 0.19, 0.272, 0.234, 0.179, 0.449, 0.466, 0.435, 0.105, 0.211]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.3516 | Steps: 2 | Val loss: 2.1173 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=150330)[0m top1: 0.2677238805970149
[2m[36m(func pid=150330)[0m top5: 0.8451492537313433
[2m[36m(func pid=150330)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=150330)[0m f1_macro: 0.18286225864698955
[2m[36m(func pid=150330)[0m f1_weighted: 0.2174147865729343
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.531, 0.349, 0.093, 0.071, 0.0, 0.249, 0.296, 0.144, 0.095]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8778 | Steps: 2 | Val loss: 2.3372 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:28:13 (running for 00:35:22.81)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.352 |      0.253 |                   61 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.613 |      0.281 |                   58 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  6.788 |      0.183 |                   34 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.93  |      0.047 |                    9 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.3292910447761194
[2m[36m(func pid=144068)[0m top5: 0.8572761194029851
[2m[36m(func pid=144068)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=144068)[0m f1_macro: 0.25303121452201494
[2m[36m(func pid=144068)[0m f1_weighted: 0.3063378271438973
[2m[36m(func pid=144068)[0m f1_per_class: [0.196, 0.093, 0.301, 0.511, 0.092, 0.396, 0.215, 0.476, 0.065, 0.184]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5900 | Steps: 2 | Val loss: 3.5678 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 6.0121 | Steps: 2 | Val loss: 13.7533 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=155946)[0m top1: 0.20289179104477612
[2m[36m(func pid=155946)[0m top5: 0.5475746268656716
[2m[36m(func pid=155946)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=155946)[0m f1_macro: 0.04612704215884029
[2m[36m(func pid=155946)[0m f1_weighted: 0.11333448162987385
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.029, 0.399, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3050373134328358
[2m[36m(func pid=145070)[0m top5: 0.8661380597014925
[2m[36m(func pid=145070)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=145070)[0m f1_macro: 0.2832747693434163
[2m[36m(func pid=145070)[0m f1_weighted: 0.31994587746842557
[2m[36m(func pid=145070)[0m f1_per_class: [0.2, 0.415, 0.227, 0.137, 0.237, 0.444, 0.403, 0.454, 0.106, 0.211]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.3959 | Steps: 2 | Val loss: 2.1340 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=150330)[0m top1: 0.37779850746268656
[2m[36m(func pid=150330)[0m top5: 0.8964552238805971
[2m[36m(func pid=150330)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=150330)[0m f1_macro: 0.22945168798002386
[2m[36m(func pid=150330)[0m f1_weighted: 0.36958053756435905
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.51, 0.286, 0.301, 0.0, 0.246, 0.484, 0.369, 0.059, 0.04]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8839 | Steps: 2 | Val loss: 2.3419 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:28:19 (running for 00:35:28.18)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.396 |      0.228 |                   62 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.59  |      0.283 |                   59 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  6.012 |      0.229 |                   35 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.878 |      0.046 |                   10 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.30363805970149255
[2m[36m(func pid=144068)[0m top5: 0.8521455223880597
[2m[36m(func pid=144068)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=144068)[0m f1_macro: 0.22757246920554297
[2m[36m(func pid=144068)[0m f1_weighted: 0.2580072970798237
[2m[36m(func pid=144068)[0m f1_per_class: [0.189, 0.102, 0.286, 0.517, 0.088, 0.399, 0.054, 0.42, 0.067, 0.155]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3869 | Steps: 2 | Val loss: 3.6557 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.9795 | Steps: 2 | Val loss: 12.9350 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=155946)[0m top1: 0.17537313432835822
[2m[36m(func pid=155946)[0m top5: 0.5433768656716418
[2m[36m(func pid=155946)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=155946)[0m f1_macro: 0.04316366929530081
[2m[36m(func pid=155946)[0m f1_weighted: 0.10804277351609576
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.029, 0.383, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m top1: 0.31716417910447764
[2m[36m(func pid=145070)[0m top5: 0.851679104477612
[2m[36m(func pid=145070)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=145070)[0m f1_macro: 0.28523615810856373
[2m[36m(func pid=145070)[0m f1_weighted: 0.3022540706923362
[2m[36m(func pid=145070)[0m f1_per_class: [0.137, 0.554, 0.195, 0.035, 0.298, 0.428, 0.367, 0.453, 0.085, 0.3]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.2910 | Steps: 2 | Val loss: 2.1823 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=150330)[0m top1: 0.4449626865671642
[2m[36m(func pid=150330)[0m top5: 0.9067164179104478
[2m[36m(func pid=150330)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=150330)[0m f1_macro: 0.2778321793101056
[2m[36m(func pid=150330)[0m f1_weighted: 0.4480944552584897
[2m[36m(func pid=150330)[0m f1_per_class: [0.182, 0.529, 0.229, 0.501, 0.0, 0.485, 0.465, 0.273, 0.043, 0.07]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8858 | Steps: 2 | Val loss: 2.3456 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:28:24 (running for 00:35:33.27)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.291 |      0.218 |                   63 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.387 |      0.285 |                   60 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.979 |      0.278 |                   36 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.884 |      0.043 |                   11 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2905783582089552
[2m[36m(func pid=144068)[0m top5: 0.8456156716417911
[2m[36m(func pid=144068)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=144068)[0m f1_macro: 0.21776718644134835
[2m[36m(func pid=144068)[0m f1_weighted: 0.24263154022001876
[2m[36m(func pid=144068)[0m f1_per_class: [0.189, 0.119, 0.262, 0.517, 0.089, 0.396, 0.0, 0.372, 0.101, 0.132]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3460 | Steps: 2 | Val loss: 3.8580 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.9321 | Steps: 2 | Val loss: 16.2911 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=155946)[0m top1: 0.13666044776119404
[2m[36m(func pid=155946)[0m top5: 0.5382462686567164
[2m[36m(func pid=155946)[0m f1_micro: 0.13666044776119404
[2m[36m(func pid=155946)[0m f1_macro: 0.03952540214429203
[2m[36m(func pid=155946)[0m f1_weighted: 0.09767977257297622
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.005, 0.022, 0.341, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3278917910447761
[2m[36m(func pid=145070)[0m top5: 0.8302238805970149
[2m[36m(func pid=145070)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=145070)[0m f1_macro: 0.23003182599173724
[2m[36m(func pid=145070)[0m f1_weighted: 0.28022341088324393
[2m[36m(func pid=145070)[0m f1_per_class: [0.108, 0.531, 0.18, 0.003, 0.0, 0.428, 0.351, 0.446, 0.107, 0.146]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.3787313432835821
[2m[36m(func pid=150330)[0m top5: 0.8959888059701493
[2m[36m(func pid=150330)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=150330)[0m f1_macro: 0.260888296268969
[2m[36m(func pid=150330)[0m f1_weighted: 0.37948271325709076
[2m[36m(func pid=150330)[0m f1_per_class: [0.169, 0.463, 0.222, 0.481, 0.2, 0.416, 0.346, 0.062, 0.087, 0.162]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3627 | Steps: 2 | Val loss: 2.2219 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8257 | Steps: 2 | Val loss: 2.3467 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3080 | Steps: 2 | Val loss: 4.3149 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 00:28:29 (running for 00:35:38.72)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.363 |      0.217 |                   64 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.346 |      0.23  |                   61 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.932 |      0.261 |                   37 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.886 |      0.04  |                   12 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2835820895522388
[2m[36m(func pid=144068)[0m top5: 0.8386194029850746
[2m[36m(func pid=144068)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=144068)[0m f1_macro: 0.21695320897570242
[2m[36m(func pid=144068)[0m f1_weighted: 0.24290258263427159
[2m[36m(func pid=144068)[0m f1_per_class: [0.183, 0.133, 0.253, 0.518, 0.087, 0.382, 0.0, 0.349, 0.125, 0.14]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3627 | Steps: 2 | Val loss: 22.1209 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=155946)[0m top1: 0.10587686567164178
[2m[36m(func pid=155946)[0m top5: 0.5401119402985075
[2m[36m(func pid=155946)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=155946)[0m f1_macro: 0.032717444759656995
[2m[36m(func pid=155946)[0m f1_weighted: 0.08228509163257325
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.02, 0.291, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m top1: 0.32975746268656714
[2m[36m(func pid=145070)[0m top5: 0.8353544776119403
[2m[36m(func pid=145070)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=145070)[0m f1_macro: 0.2205907024985835
[2m[36m(func pid=145070)[0m f1_weighted: 0.2748632945263967
[2m[36m(func pid=145070)[0m f1_per_class: [0.123, 0.48, 0.188, 0.0, 0.0, 0.421, 0.372, 0.462, 0.05, 0.11]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2792 | Steps: 2 | Val loss: 2.2540 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=150330)[0m top1: 0.28544776119402987
[2m[36m(func pid=150330)[0m top5: 0.8740671641791045
[2m[36m(func pid=150330)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=150330)[0m f1_macro: 0.24132544390586674
[2m[36m(func pid=150330)[0m f1_weighted: 0.30598381061271973
[2m[36m(func pid=150330)[0m f1_per_class: [0.078, 0.413, 0.25, 0.307, 0.224, 0.398, 0.303, 0.047, 0.071, 0.323]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8895 | Steps: 2 | Val loss: 2.3495 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6018 | Steps: 2 | Val loss: 4.5822 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:28:34 (running for 00:35:43.90)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.279 |      0.217 |                   65 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.308 |      0.221 |                   62 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.363 |      0.241 |                   38 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.826 |      0.033 |                   13 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2737873134328358
[2m[36m(func pid=144068)[0m top5: 0.8302238805970149
[2m[36m(func pid=144068)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=144068)[0m f1_macro: 0.2165268363900839
[2m[36m(func pid=144068)[0m f1_weighted: 0.24446742447588857
[2m[36m(func pid=144068)[0m f1_per_class: [0.185, 0.177, 0.253, 0.501, 0.089, 0.384, 0.0, 0.328, 0.123, 0.126]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8508 | Steps: 2 | Val loss: 29.4855 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=155946)[0m top1: 0.07835820895522388
[2m[36m(func pid=155946)[0m top5: 0.5354477611940298
[2m[36m(func pid=155946)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=155946)[0m f1_macro: 0.027992258243531975
[2m[36m(func pid=155946)[0m f1_weighted: 0.06985169079651218
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.017, 0.247, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3041044776119403
[2m[36m(func pid=145070)[0m top5: 0.8535447761194029
[2m[36m(func pid=145070)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=145070)[0m f1_macro: 0.195901448449604
[2m[36m(func pid=145070)[0m f1_weighted: 0.2571616557394853
[2m[36m(func pid=145070)[0m f1_per_class: [0.123, 0.452, 0.2, 0.0, 0.0, 0.278, 0.39, 0.47, 0.0, 0.046]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.4619 | Steps: 2 | Val loss: 2.2813 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=150330)[0m top1: 0.1525186567164179
[2m[36m(func pid=150330)[0m top5: 0.8418843283582089
[2m[36m(func pid=150330)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=150330)[0m f1_macro: 0.15811125868971426
[2m[36m(func pid=150330)[0m f1_weighted: 0.18280184579620212
[2m[36m(func pid=150330)[0m f1_per_class: [0.065, 0.199, 0.275, 0.059, 0.057, 0.297, 0.295, 0.047, 0.043, 0.245]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8299 | Steps: 2 | Val loss: 2.3516 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 00:28:40 (running for 00:35:49.29)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.462 |      0.219 |                   66 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.602 |      0.196 |                   63 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.851 |      0.158 |                   39 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.89  |      0.028 |                   14 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2630597014925373
[2m[36m(func pid=144068)[0m top5: 0.8274253731343284
[2m[36m(func pid=144068)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=144068)[0m f1_macro: 0.21884964295272996
[2m[36m(func pid=144068)[0m f1_weighted: 0.2438480911846507
[2m[36m(func pid=144068)[0m f1_per_class: [0.185, 0.229, 0.256, 0.47, 0.085, 0.376, 0.0, 0.321, 0.128, 0.139]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5035 | Steps: 2 | Val loss: 4.6200 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 5.1069 | Steps: 2 | Val loss: 34.0085 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=155946)[0m top1: 0.05783582089552239
[2m[36m(func pid=155946)[0m top5: 0.5354477611940298
[2m[36m(func pid=155946)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=155946)[0m f1_macro: 0.02335353454771085
[2m[36m(func pid=155946)[0m f1_weighted: 0.057315930031070536
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.0, 0.016, 0.202, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m top1: 0.2737873134328358
[2m[36m(func pid=145070)[0m top5: 0.8628731343283582
[2m[36m(func pid=145070)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=145070)[0m f1_macro: 0.17479141744072063
[2m[36m(func pid=145070)[0m f1_weighted: 0.2325355825412815
[2m[36m(func pid=145070)[0m f1_per_class: [0.115, 0.468, 0.195, 0.006, 0.026, 0.095, 0.371, 0.422, 0.0, 0.049]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.08768656716417911
[2m[36m(func pid=150330)[0m top5: 0.8069029850746269
[2m[36m(func pid=150330)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=150330)[0m f1_macro: 0.09634955691469665
[2m[36m(func pid=150330)[0m f1_weighted: 0.10369058961928006
[2m[36m(func pid=150330)[0m f1_per_class: [0.066, 0.11, 0.275, 0.032, 0.036, 0.008, 0.213, 0.091, 0.066, 0.067]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.6199 | Steps: 2 | Val loss: 2.2807 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8118 | Steps: 2 | Val loss: 2.3521 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7100 | Steps: 2 | Val loss: 4.3963 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:28:45 (running for 00:35:54.58)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.62  |      0.215 |                   67 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.504 |      0.175 |                   64 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  5.107 |      0.096 |                   40 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.83  |      0.023 |                   15 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.24253731343283583
[2m[36m(func pid=144068)[0m top5: 0.8278917910447762
[2m[36m(func pid=144068)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=144068)[0m f1_macro: 0.21468422679798443
[2m[36m(func pid=144068)[0m f1_weighted: 0.2294331068125295
[2m[36m(func pid=144068)[0m f1_per_class: [0.194, 0.263, 0.278, 0.4, 0.083, 0.377, 0.0, 0.313, 0.107, 0.131]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 5.7859 | Steps: 2 | Val loss: 26.5472 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=145070)[0m top1: 0.2798507462686567
[2m[36m(func pid=145070)[0m top5: 0.8591417910447762
[2m[36m(func pid=145070)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=145070)[0m f1_macro: 0.19480366853303888
[2m[36m(func pid=145070)[0m f1_weighted: 0.27646574294790693
[2m[36m(func pid=145070)[0m f1_per_class: [0.11, 0.52, 0.191, 0.176, 0.084, 0.016, 0.359, 0.42, 0.0, 0.073]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=155946)[0m top1: 0.05037313432835821
[2m[36m(func pid=155946)[0m top5: 0.5335820895522388
[2m[36m(func pid=155946)[0m f1_micro: 0.05037313432835821
[2m[36m(func pid=155946)[0m f1_macro: 0.020283208417834993
[2m[36m(func pid=155946)[0m f1_weighted: 0.05186969059780018
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.005, 0.015, 0.182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m top1: 0.18190298507462688
[2m[36m(func pid=150330)[0m top5: 0.8055037313432836
[2m[36m(func pid=150330)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=150330)[0m f1_macro: 0.1806235905491292
[2m[36m(func pid=150330)[0m f1_weighted: 0.2157863586352603
[2m[36m(func pid=150330)[0m f1_per_class: [0.127, 0.433, 0.234, 0.285, 0.043, 0.0, 0.121, 0.282, 0.088, 0.194]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.3087 | Steps: 2 | Val loss: 2.2998 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6826 | Steps: 2 | Val loss: 4.3429 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=144068)[0m top1: 0.22667910447761194
[2m[36m(func pid=144068)[0m top5: 0.8288246268656716
[2m[36m(func pid=144068)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=144068)[0m f1_macro: 0.2083327160189948
[2m[36m(func pid=144068)[0m f1_weighted: 0.21316614529281339
[2m[36m(func pid=144068)[0m f1_per_class: [0.183, 0.281, 0.278, 0.333, 0.082, 0.366, 0.0, 0.328, 0.106, 0.126]
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8095 | Steps: 2 | Val loss: 2.3525 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=144068)[0m 
== Status ==
Current time: 2024-01-07 00:28:50 (running for 00:35:59.86)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.309 |      0.208 |                   68 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.71  |      0.195 |                   65 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  5.786 |      0.181 |                   41 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.812 |      0.02  |                   16 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.1109 | Steps: 2 | Val loss: 21.4929 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=145070)[0m top1: 0.30736940298507465
[2m[36m(func pid=145070)[0m top5: 0.8703358208955224
[2m[36m(func pid=145070)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=145070)[0m f1_macro: 0.20895792601906416
[2m[36m(func pid=145070)[0m f1_weighted: 0.3300287829108704
[2m[36m(func pid=145070)[0m f1_per_class: [0.12, 0.527, 0.179, 0.424, 0.066, 0.0, 0.323, 0.339, 0.0, 0.112]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.32509328358208955
[2m[36m(func pid=150330)[0m top5: 0.8222947761194029
[2m[36m(func pid=150330)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=150330)[0m f1_macro: 0.2272078489330378
[2m[36m(func pid=150330)[0m f1_weighted: 0.32415322896900567
[2m[36m(func pid=150330)[0m f1_per_class: [0.214, 0.537, 0.202, 0.447, 0.03, 0.076, 0.233, 0.301, 0.134, 0.098]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m top1: 0.03731343283582089
[2m[36m(func pid=155946)[0m top5: 0.5331156716417911
[2m[36m(func pid=155946)[0m f1_micro: 0.03731343283582089
[2m[36m(func pid=155946)[0m f1_macro: 0.017389950163698004
[2m[36m(func pid=155946)[0m f1_weighted: 0.039846026948178105
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.011, 0.015, 0.133, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3529 | Steps: 2 | Val loss: 2.2987 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4103 | Steps: 2 | Val loss: 4.4921 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 00:28:56 (running for 00:36:05.03)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.353 |      0.205 |                   69 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.683 |      0.209 |                   66 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  3.111 |      0.227 |                   42 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.809 |      0.017 |                   17 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2150186567164179
[2m[36m(func pid=144068)[0m top5: 0.8306902985074627
[2m[36m(func pid=144068)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=144068)[0m f1_macro: 0.2050580698803953
[2m[36m(func pid=144068)[0m f1_weighted: 0.19823068759126097
[2m[36m(func pid=144068)[0m f1_per_class: [0.182, 0.293, 0.293, 0.265, 0.083, 0.369, 0.003, 0.347, 0.102, 0.115]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2006 | Steps: 2 | Val loss: 20.6110 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8240 | Steps: 2 | Val loss: 2.3542 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=145070)[0m top1: 0.30363805970149255
[2m[36m(func pid=145070)[0m top5: 0.8889925373134329
[2m[36m(func pid=145070)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=145070)[0m f1_macro: 0.19162958725622392
[2m[36m(func pid=145070)[0m f1_weighted: 0.31281893619110057
[2m[36m(func pid=145070)[0m f1_per_class: [0.153, 0.406, 0.171, 0.485, 0.054, 0.016, 0.296, 0.199, 0.0, 0.137]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.3805970149253731
[2m[36m(func pid=150330)[0m top5: 0.8684701492537313
[2m[36m(func pid=150330)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=150330)[0m f1_macro: 0.23360604540974167
[2m[36m(func pid=150330)[0m f1_weighted: 0.3552361030582053
[2m[36m(func pid=150330)[0m f1_per_class: [0.113, 0.462, 0.191, 0.456, 0.0, 0.252, 0.31, 0.333, 0.098, 0.12]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m top1: 0.03125
[2m[36m(func pid=155946)[0m top5: 0.5247201492537313
[2m[36m(func pid=155946)[0m f1_micro: 0.03125
[2m[36m(func pid=155946)[0m f1_macro: 0.01417046332808176
[2m[36m(func pid=155946)[0m f1_weighted: 0.03395168219262431
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.016, 0.014, 0.112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.3051 | Steps: 2 | Val loss: 2.2914 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3864 | Steps: 2 | Val loss: 4.6169 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:29:01 (running for 00:36:10.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.305 |      0.211 |                   70 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.41  |      0.192 |                   67 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.201 |      0.234 |                   43 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.824 |      0.014 |                   18 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.21921641791044777
[2m[36m(func pid=144068)[0m top5: 0.8344216417910447
[2m[36m(func pid=144068)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=144068)[0m f1_macro: 0.21067893066670568
[2m[36m(func pid=144068)[0m f1_weighted: 0.20565828133918823
[2m[36m(func pid=144068)[0m f1_per_class: [0.183, 0.322, 0.293, 0.252, 0.082, 0.367, 0.019, 0.373, 0.106, 0.11]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 4.4433 | Steps: 2 | Val loss: 19.2063 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8390 | Steps: 2 | Val loss: 2.3555 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=145070)[0m top1: 0.292910447761194
[2m[36m(func pid=145070)[0m top5: 0.8978544776119403
[2m[36m(func pid=145070)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=145070)[0m f1_macro: 0.1813204726270831
[2m[36m(func pid=145070)[0m f1_weighted: 0.28888452347569854
[2m[36m(func pid=145070)[0m f1_per_class: [0.162, 0.23, 0.168, 0.505, 0.048, 0.083, 0.283, 0.119, 0.026, 0.189]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.4281716417910448
[2m[36m(func pid=150330)[0m top5: 0.9057835820895522
[2m[36m(func pid=150330)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=150330)[0m f1_macro: 0.24692764170872664
[2m[36m(func pid=150330)[0m f1_weighted: 0.4145096158390187
[2m[36m(func pid=150330)[0m f1_per_class: [0.125, 0.493, 0.172, 0.46, 0.0, 0.375, 0.47, 0.203, 0.046, 0.125]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3487 | Steps: 2 | Val loss: 2.2676 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=155946)[0m top1: 0.028917910447761194
[2m[36m(func pid=155946)[0m top5: 0.5223880597014925
[2m[36m(func pid=155946)[0m f1_micro: 0.028917910447761194
[2m[36m(func pid=155946)[0m f1_macro: 0.01398340120220715
[2m[36m(func pid=155946)[0m f1_weighted: 0.03235291532199226
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.027, 0.014, 0.099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.8054 | Steps: 2 | Val loss: 4.6366 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:29:06 (running for 00:36:15.53)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.349 |      0.226 |                   71 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.386 |      0.181 |                   68 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  4.443 |      0.247 |                   44 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.839 |      0.014 |                   19 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.228544776119403
[2m[36m(func pid=144068)[0m top5: 0.84375
[2m[36m(func pid=144068)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=144068)[0m f1_macro: 0.22595247527856363
[2m[36m(func pid=144068)[0m f1_weighted: 0.22803632125803688
[2m[36m(func pid=144068)[0m f1_per_class: [0.182, 0.326, 0.314, 0.213, 0.08, 0.372, 0.114, 0.432, 0.107, 0.12]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3301 | Steps: 2 | Val loss: 18.3878 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7491 | Steps: 2 | Val loss: 2.3531 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=145070)[0m top1: 0.2887126865671642
[2m[36m(func pid=145070)[0m top5: 0.8889925373134329
[2m[36m(func pid=145070)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=145070)[0m f1_macro: 0.1895730474518193
[2m[36m(func pid=145070)[0m f1_weighted: 0.28758097978541747
[2m[36m(func pid=145070)[0m f1_per_class: [0.179, 0.161, 0.159, 0.518, 0.052, 0.212, 0.256, 0.062, 0.134, 0.162]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1586 | Steps: 2 | Val loss: 2.2494 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=150330)[0m top1: 0.4295708955223881
[2m[36m(func pid=150330)[0m top5: 0.8810634328358209
[2m[36m(func pid=150330)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=150330)[0m f1_macro: 0.23892474109183456
[2m[36m(func pid=150330)[0m f1_weighted: 0.42776083336085235
[2m[36m(func pid=150330)[0m f1_per_class: [0.044, 0.548, 0.165, 0.412, 0.0, 0.466, 0.517, 0.127, 0.027, 0.083]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m top1: 0.030783582089552237
[2m[36m(func pid=155946)[0m top5: 0.5261194029850746
[2m[36m(func pid=155946)[0m f1_micro: 0.030783582089552237
[2m[36m(func pid=155946)[0m f1_macro: 0.017367266296166518
[2m[36m(func pid=155946)[0m f1_weighted: 0.03607931872885891
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.047, 0.014, 0.097, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5026 | Steps: 2 | Val loss: 4.5996 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:29:12 (running for 00:36:20.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.159 |      0.251 |                   72 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.805 |      0.19  |                   69 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.33  |      0.239 |                   45 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.749 |      0.017 |                   20 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2593283582089552
[2m[36m(func pid=144068)[0m top5: 0.851679104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=144068)[0m f1_macro: 0.2508501785649288
[2m[36m(func pid=144068)[0m f1_weighted: 0.2831413369082801
[2m[36m(func pid=144068)[0m f1_per_class: [0.181, 0.35, 0.319, 0.184, 0.08, 0.374, 0.298, 0.497, 0.104, 0.122]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.2305 | Steps: 2 | Val loss: 21.1925 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7406 | Steps: 2 | Val loss: 2.3515 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=145070)[0m top1: 0.2523320895522388
[2m[36m(func pid=145070)[0m top5: 0.8871268656716418
[2m[36m(func pid=145070)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=145070)[0m f1_macro: 0.19094025204683823
[2m[36m(func pid=145070)[0m f1_weighted: 0.2691701037394647
[2m[36m(func pid=145070)[0m f1_per_class: [0.188, 0.183, 0.168, 0.454, 0.065, 0.279, 0.212, 0.092, 0.102, 0.167]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.33908582089552236
[2m[36m(func pid=150330)[0m top5: 0.8456156716417911
[2m[36m(func pid=150330)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=150330)[0m f1_macro: 0.22727189135703094
[2m[36m(func pid=150330)[0m f1_weighted: 0.3353838824763504
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.504, 0.165, 0.145, 0.286, 0.457, 0.479, 0.16, 0.0, 0.077]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.2347 | Steps: 2 | Val loss: 2.2377 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=155946)[0m top1: 0.029850746268656716
[2m[36m(func pid=155946)[0m top5: 0.5265858208955224
[2m[36m(func pid=155946)[0m f1_micro: 0.029850746268656716
[2m[36m(func pid=155946)[0m f1_macro: 0.018664379704415483
[2m[36m(func pid=155946)[0m f1_weighted: 0.03519071230806984
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.056, 0.014, 0.085, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7178 | Steps: 2 | Val loss: 4.9146 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 00:29:17 (running for 00:36:26.43)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.235 |      0.255 |                   73 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.503 |      0.191 |                   70 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  3.23  |      0.227 |                   46 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.741 |      0.019 |                   21 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.27098880597014924
[2m[36m(func pid=144068)[0m top5: 0.8572761194029851
[2m[36m(func pid=144068)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=144068)[0m f1_macro: 0.2551171902137358
[2m[36m(func pid=144068)[0m f1_weighted: 0.30332591764315536
[2m[36m(func pid=144068)[0m f1_per_class: [0.18, 0.351, 0.319, 0.177, 0.08, 0.374, 0.377, 0.475, 0.107, 0.112]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 10.4361 | Steps: 2 | Val loss: 22.7631 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7669 | Steps: 2 | Val loss: 2.3518 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=145070)[0m top1: 0.21875
[2m[36m(func pid=145070)[0m top5: 0.8871268656716418
[2m[36m(func pid=145070)[0m f1_micro: 0.21875
[2m[36m(func pid=145070)[0m f1_macro: 0.18888305881648593
[2m[36m(func pid=145070)[0m f1_weighted: 0.23166222831383806
[2m[36m(func pid=145070)[0m f1_per_class: [0.194, 0.292, 0.167, 0.281, 0.098, 0.31, 0.168, 0.105, 0.117, 0.158]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=150330)[0m top1: 0.2994402985074627
[2m[36m(func pid=150330)[0m top5: 0.7989738805970149
[2m[36m(func pid=150330)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=150330)[0m f1_macro: 0.21841246472783404
[2m[36m(func pid=150330)[0m f1_weighted: 0.2820525332887263
[2m[36m(func pid=150330)[0m f1_per_class: [0.0, 0.461, 0.165, 0.032, 0.242, 0.427, 0.431, 0.15, 0.08, 0.196]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2190 | Steps: 2 | Val loss: 2.2137 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=155946)[0m top1: 0.03311567164179104
[2m[36m(func pid=155946)[0m top5: 0.5237873134328358
[2m[36m(func pid=155946)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=155946)[0m f1_macro: 0.019652082201758868
[2m[36m(func pid=155946)[0m f1_weighted: 0.03957605830220712
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.073, 0.014, 0.093, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1147 | Steps: 2 | Val loss: 4.9548 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.2623 | Steps: 2 | Val loss: 22.1056 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 00:29:22 (running for 00:36:31.74)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.219 |      0.258 |                   74 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.718 |      0.189 |                   71 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 10.436 |      0.218 |                   47 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.767 |      0.02  |                   22 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.28218283582089554
[2m[36m(func pid=144068)[0m top5: 0.8703358208955224
[2m[36m(func pid=144068)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=144068)[0m f1_macro: 0.2581003080137737
[2m[36m(func pid=144068)[0m f1_weighted: 0.3157368562051021
[2m[36m(func pid=144068)[0m f1_per_class: [0.177, 0.357, 0.319, 0.187, 0.082, 0.38, 0.413, 0.42, 0.106, 0.14]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.228544776119403
[2m[36m(func pid=145070)[0m top5: 0.8773320895522388
[2m[36m(func pid=145070)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=145070)[0m f1_macro: 0.20908791252096362
[2m[36m(func pid=145070)[0m f1_weighted: 0.23399805905085969
[2m[36m(func pid=145070)[0m f1_per_class: [0.191, 0.399, 0.167, 0.143, 0.144, 0.332, 0.216, 0.194, 0.099, 0.205]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7463 | Steps: 2 | Val loss: 2.3512 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=150330)[0m top1: 0.2826492537313433
[2m[36m(func pid=150330)[0m top5: 0.7803171641791045
[2m[36m(func pid=150330)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=150330)[0m f1_macro: 0.229753592304142
[2m[36m(func pid=150330)[0m f1_weighted: 0.27289965499309554
[2m[36m(func pid=150330)[0m f1_per_class: [0.151, 0.442, 0.214, 0.007, 0.121, 0.411, 0.42, 0.196, 0.099, 0.237]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.6039 | Steps: 2 | Val loss: 2.1983 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=155946)[0m top1: 0.03125
[2m[36m(func pid=155946)[0m top5: 0.5181902985074627
[2m[36m(func pid=155946)[0m f1_micro: 0.03125
[2m[36m(func pid=155946)[0m f1_macro: 0.017228210850153143
[2m[36m(func pid=155946)[0m f1_weighted: 0.036424051289325576
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.072, 0.015, 0.086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7134 | Steps: 2 | Val loss: 4.7061 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4879 | Steps: 2 | Val loss: 23.1770 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 00:29:28 (running for 00:36:36.98)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.604 |      0.262 |                   75 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  1.115 |      0.209 |                   72 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  4.262 |      0.23  |                   48 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.746 |      0.017 |                   23 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2966417910447761
[2m[36m(func pid=144068)[0m top5: 0.886660447761194
[2m[36m(func pid=144068)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=144068)[0m f1_macro: 0.26183432296737075
[2m[36m(func pid=144068)[0m f1_weighted: 0.3312461561230702
[2m[36m(func pid=144068)[0m f1_per_class: [0.181, 0.381, 0.31, 0.216, 0.083, 0.378, 0.436, 0.357, 0.105, 0.172]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.28078358208955223
[2m[36m(func pid=145070)[0m top5: 0.8605410447761194
[2m[36m(func pid=145070)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=145070)[0m f1_macro: 0.24310145915885503
[2m[36m(func pid=145070)[0m f1_weighted: 0.28419728313143144
[2m[36m(func pid=145070)[0m f1_per_class: [0.125, 0.482, 0.172, 0.071, 0.194, 0.37, 0.368, 0.317, 0.093, 0.238]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7579 | Steps: 2 | Val loss: 2.3479 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=150330)[0m top1: 0.26119402985074625
[2m[36m(func pid=150330)[0m top5: 0.7873134328358209
[2m[36m(func pid=150330)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=150330)[0m f1_macro: 0.2470996555361272
[2m[36m(func pid=150330)[0m f1_weighted: 0.27648638516769675
[2m[36m(func pid=150330)[0m f1_per_class: [0.284, 0.37, 0.262, 0.007, 0.068, 0.343, 0.475, 0.271, 0.098, 0.294]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.1579 | Steps: 2 | Val loss: 2.1767 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3156 | Steps: 2 | Val loss: 4.4571 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=155946)[0m top1: 0.029384328358208957
[2m[36m(func pid=155946)[0m top5: 0.5214552238805971
[2m[36m(func pid=155946)[0m f1_micro: 0.029384328358208953
[2m[36m(func pid=155946)[0m f1_macro: 0.015969581875231266
[2m[36m(func pid=155946)[0m f1_weighted: 0.03325020826580081
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.068, 0.015, 0.077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1400 | Steps: 2 | Val loss: 25.3722 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 00:29:33 (running for 00:36:42.33)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.158 |      0.259 |                   76 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.713 |      0.243 |                   73 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.488 |      0.247 |                   49 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.758 |      0.016 |                   24 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.30550373134328357
[2m[36m(func pid=144068)[0m top5: 0.8950559701492538
[2m[36m(func pid=144068)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=144068)[0m f1_macro: 0.25945546426562216
[2m[36m(func pid=144068)[0m f1_weighted: 0.3395916845087692
[2m[36m(func pid=144068)[0m f1_per_class: [0.174, 0.379, 0.324, 0.253, 0.086, 0.399, 0.445, 0.241, 0.1, 0.195]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.3316231343283582
[2m[36m(func pid=145070)[0m top5: 0.8409514925373134
[2m[36m(func pid=145070)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=145070)[0m f1_macro: 0.2700542218197627
[2m[36m(func pid=145070)[0m f1_weighted: 0.3179824267209276
[2m[36m(func pid=145070)[0m f1_per_class: [0.109, 0.532, 0.176, 0.048, 0.237, 0.406, 0.449, 0.373, 0.092, 0.28]
[2m[36m(func pid=145070)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7414 | Steps: 2 | Val loss: 2.3506 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=150330)[0m top1: 0.20522388059701493
[2m[36m(func pid=150330)[0m top5: 0.8176305970149254
[2m[36m(func pid=150330)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=150330)[0m f1_macro: 0.21670972931129578
[2m[36m(func pid=150330)[0m f1_weighted: 0.2346492909508582
[2m[36m(func pid=150330)[0m f1_per_class: [0.145, 0.316, 0.377, 0.007, 0.045, 0.146, 0.427, 0.404, 0.094, 0.207]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.1692 | Steps: 2 | Val loss: 2.1399 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=145070)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2596 | Steps: 2 | Val loss: 4.5021 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=155946)[0m top1: 0.030783582089552237
[2m[36m(func pid=155946)[0m top5: 0.5163246268656716
[2m[36m(func pid=155946)[0m f1_micro: 0.030783582089552237
[2m[36m(func pid=155946)[0m f1_macro: 0.01771274567090621
[2m[36m(func pid=155946)[0m f1_weighted: 0.03591184756486293
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.088, 0.015, 0.074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 8.6417 | Steps: 2 | Val loss: 28.6184 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:29:38 (running for 00:36:47.53)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.169 |      0.258 |                   77 |
| train_51d3e_00018 | RUNNING    | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.316 |      0.27  |                   74 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.14  |      0.217 |                   50 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.741 |      0.018 |                   25 |
| train_51d3e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.31156716417910446
[2m[36m(func pid=144068)[0m top5: 0.9029850746268657
[2m[36m(func pid=144068)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=144068)[0m f1_macro: 0.25789861498531697
[2m[36m(func pid=144068)[0m f1_weighted: 0.34374527473151834
[2m[36m(func pid=144068)[0m f1_per_class: [0.178, 0.363, 0.355, 0.297, 0.086, 0.392, 0.447, 0.135, 0.106, 0.219]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=145070)[0m top1: 0.34281716417910446
[2m[36m(func pid=145070)[0m top5: 0.8376865671641791
[2m[36m(func pid=145070)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=145070)[0m f1_macro: 0.24666395550903308
[2m[36m(func pid=145070)[0m f1_weighted: 0.3081561126005677
[2m[36m(func pid=145070)[0m f1_per_class: [0.12, 0.55, 0.185, 0.007, 0.219, 0.423, 0.467, 0.284, 0.0, 0.212]
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7760 | Steps: 2 | Val loss: 2.3511 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=150330)[0m top1: 0.1646455223880597
[2m[36m(func pid=150330)[0m top5: 0.840018656716418
[2m[36m(func pid=150330)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=150330)[0m f1_macro: 0.18744077795268782
[2m[36m(func pid=150330)[0m f1_weighted: 0.19639478985984266
[2m[36m(func pid=150330)[0m f1_per_class: [0.095, 0.343, 0.4, 0.023, 0.039, 0.039, 0.31, 0.418, 0.13, 0.077]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.5305 | Steps: 2 | Val loss: 2.1167 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=155946)[0m top1: 0.03171641791044776
[2m[36m(func pid=155946)[0m top5: 0.5135261194029851
[2m[36m(func pid=155946)[0m f1_micro: 0.03171641791044776
[2m[36m(func pid=155946)[0m f1_macro: 0.017641418302182736
[2m[36m(func pid=155946)[0m f1_weighted: 0.03622338098591995
[2m[36m(func pid=155946)[0m f1_per_class: [0.0, 0.083, 0.015, 0.078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 9.4202 | Steps: 2 | Val loss: 22.7332 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=144068)[0m top1: 0.332089552238806
[2m[36m(func pid=144068)[0m top5: 0.9132462686567164
[2m[36m(func pid=144068)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.26194083416729697
[2m[36m(func pid=144068)[0m f1_weighted: 0.36581363639348635
[2m[36m(func pid=144068)[0m f1_per_class: [0.18, 0.35, 0.355, 0.381, 0.083, 0.403, 0.456, 0.078, 0.122, 0.212]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m top1: 0.2537313432835821
[2m[36m(func pid=150330)[0m top5: 0.8675373134328358
[2m[36m(func pid=150330)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=150330)[0m f1_macro: 0.25852461071834565
[2m[36m(func pid=150330)[0m f1_weighted: 0.302617102135283
[2m[36m(func pid=150330)[0m f1_per_class: [0.107, 0.524, 0.389, 0.233, 0.053, 0.186, 0.298, 0.433, 0.149, 0.214]
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7259 | Steps: 2 | Val loss: 2.3490 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.2086 | Steps: 2 | Val loss: 2.0718 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 00:29:43 (running for 00:36:52.65)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.531 |      0.262 |                   78 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  8.642 |      0.187 |                   51 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.776 |      0.018 |                   26 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=162129)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=162129)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=162129)[0m Configuration completed!
[2m[36m(func pid=162129)[0m New optimizer parameters:
[2m[36m(func pid=162129)[0m SGD (
[2m[36m(func pid=162129)[0m Parameter Group 0
[2m[36m(func pid=162129)[0m     dampening: 0
[2m[36m(func pid=162129)[0m     differentiable: False
[2m[36m(func pid=162129)[0m     foreach: None
[2m[36m(func pid=162129)[0m     lr: 0.001
[2m[36m(func pid=162129)[0m     maximize: False
[2m[36m(func pid=162129)[0m     momentum: 0.9
[2m[36m(func pid=162129)[0m     nesterov: False
[2m[36m(func pid=162129)[0m     weight_decay: 1e-05
[2m[36m(func pid=162129)[0m )
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.033582089552238806
[2m[36m(func pid=155946)[0m top5: 0.5097947761194029
[2m[36m(func pid=155946)[0m f1_micro: 0.033582089552238806
[2m[36m(func pid=155946)[0m f1_macro: 0.020875074515913035
[2m[36m(func pid=155946)[0m f1_weighted: 0.03773979696993167
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.088, 0.016, 0.078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=144068)[0m top1: 0.34794776119402987
[2m[36m(func pid=144068)[0m top5: 0.917910447761194
[2m[36m(func pid=144068)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=144068)[0m f1_macro: 0.27010233607759315
[2m[36m(func pid=144068)[0m f1_weighted: 0.37627488735644404
[2m[36m(func pid=144068)[0m f1_per_class: [0.185, 0.312, 0.4, 0.452, 0.082, 0.4, 0.449, 0.047, 0.144, 0.23]
== Status ==
Current time: 2024-01-07 00:29:49 (running for 00:36:58.11)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.209 |      0.27  |                   79 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  9.42  |      0.259 |                   52 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.726 |      0.021 |                   27 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3370 | Steps: 2 | Val loss: 17.9661 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7868 | Steps: 2 | Val loss: 2.3491 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0791 | Steps: 2 | Val loss: 2.3707 | Batch size: 32 | lr: 0.001 | Duration: 4.43s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.1185 | Steps: 2 | Val loss: 2.0364 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=150330)[0m top1: 0.39598880597014924
[2m[36m(func pid=150330)[0m top5: 0.8880597014925373
[2m[36m(func pid=150330)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=150330)[0m f1_macro: 0.3003420394397892
[2m[36m(func pid=150330)[0m f1_weighted: 0.38297404819992464
[2m[36m(func pid=150330)[0m f1_per_class: [0.242, 0.54, 0.378, 0.484, 0.131, 0.411, 0.257, 0.361, 0.0, 0.2]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m top1: 0.033582089552238806
[2m[36m(func pid=155946)[0m top5: 0.5065298507462687
[2m[36m(func pid=155946)[0m f1_micro: 0.033582089552238806
[2m[36m(func pid=155946)[0m f1_macro: 0.019975284591811145
[2m[36m(func pid=155946)[0m f1_weighted: 0.036471329571088465
[2m[36m(func pid=155946)[0m f1_per_class: [0.024, 0.081, 0.016, 0.079, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m top1: 0.0708955223880597
[2m[36m(func pid=162129)[0m top5: 0.32322761194029853
[2m[36m(func pid=162129)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=162129)[0m f1_macro: 0.01951542937028433
[2m[36m(func pid=162129)[0m f1_weighted: 0.027449568066004147
[2m[36m(func pid=162129)[0m f1_per_class: [0.0, 0.0, 0.0, 0.073, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:29:54 (running for 00:37:03.53)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.119 |      0.271 |                   80 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  0.337 |      0.3   |                   53 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.787 |      0.02  |                   28 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  3.079 |      0.02  |                    1 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.36100746268656714
[2m[36m(func pid=144068)[0m top5: 0.9193097014925373
[2m[36m(func pid=144068)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=144068)[0m f1_macro: 0.2709959126732203
[2m[36m(func pid=144068)[0m f1_weighted: 0.3784265330544002
[2m[36m(func pid=144068)[0m f1_per_class: [0.199, 0.253, 0.4, 0.499, 0.083, 0.398, 0.444, 0.062, 0.138, 0.233]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 10.0596 | Steps: 2 | Val loss: 18.0643 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7240 | Steps: 2 | Val loss: 2.3448 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9868 | Steps: 2 | Val loss: 2.3276 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.5965 | Steps: 2 | Val loss: 1.9834 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=150330)[0m top1: 0.4300373134328358
[2m[36m(func pid=150330)[0m top5: 0.9011194029850746
[2m[36m(func pid=150330)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=150330)[0m f1_macro: 0.3089972244540494
[2m[36m(func pid=150330)[0m f1_weighted: 0.3897507407016828
[2m[36m(func pid=150330)[0m f1_per_class: [0.358, 0.522, 0.368, 0.511, 0.174, 0.478, 0.258, 0.214, 0.0, 0.205]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=162129)[0m top1: 0.12779850746268656
[2m[36m(func pid=162129)[0m top5: 0.49533582089552236
[2m[36m(func pid=162129)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=162129)[0m f1_macro: 0.05651377435591844
[2m[36m(func pid=162129)[0m f1_weighted: 0.08424552271651596
[2m[36m(func pid=162129)[0m f1_per_class: [0.0, 0.0, 0.029, 0.24, 0.0, 0.0, 0.0, 0.296, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.039645522388059705
[2m[36m(func pid=155946)[0m top5: 0.503731343283582
[2m[36m(func pid=155946)[0m f1_micro: 0.039645522388059705
[2m[36m(func pid=155946)[0m f1_macro: 0.022879844090644374
[2m[36m(func pid=155946)[0m f1_weighted: 0.04200036590190696
[2m[36m(func pid=155946)[0m f1_per_class: [0.021, 0.11, 0.017, 0.081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
== Status ==
Current time: 2024-01-07 00:29:59 (running for 00:37:08.83)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.597 |      0.263 |                   81 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 10.06  |      0.309 |                   54 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.724 |      0.023 |                   29 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.987 |      0.057 |                    2 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.36007462686567165
[2m[36m(func pid=144068)[0m top5: 0.9183768656716418
[2m[36m(func pid=144068)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=144068)[0m f1_macro: 0.26262240622843
[2m[36m(func pid=144068)[0m f1_weighted: 0.36676894994669507
[2m[36m(func pid=144068)[0m f1_per_class: [0.23, 0.216, 0.385, 0.516, 0.078, 0.389, 0.414, 0.078, 0.091, 0.23]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 4.7811 | Steps: 2 | Val loss: 16.8269 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8903 | Steps: 2 | Val loss: 2.3179 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7584 | Steps: 2 | Val loss: 2.3464 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.2221 | Steps: 2 | Val loss: 1.9784 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=150330)[0m top1: 0.43283582089552236
[2m[36m(func pid=150330)[0m top5: 0.9085820895522388
[2m[36m(func pid=150330)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=150330)[0m f1_macro: 0.2895532811026923
[2m[36m(func pid=150330)[0m f1_weighted: 0.39677202291233343
[2m[36m(func pid=150330)[0m f1_per_class: [0.381, 0.532, 0.364, 0.503, 0.0, 0.464, 0.317, 0.077, 0.0, 0.258]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=162129)[0m top1: 0.025186567164179104
[2m[36m(func pid=162129)[0m top5: 0.570429104477612
[2m[36m(func pid=162129)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=162129)[0m f1_macro: 0.013925655274726731
[2m[36m(func pid=162129)[0m f1_weighted: 0.02812968671827511
[2m[36m(func pid=162129)[0m f1_per_class: [0.0, 0.0, 0.014, 0.094, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:05 (running for 00:37:14.07)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.597 |      0.263 |                   81 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  4.781 |      0.29  |                   55 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.758 |      0.026 |                   30 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.89  |      0.014 |                    3 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=155946)[0m top1: 0.048507462686567165
[2m[36m(func pid=155946)[0m top5: 0.5027985074626866
[2m[36m(func pid=155946)[0m f1_micro: 0.048507462686567165
[2m[36m(func pid=155946)[0m f1_macro: 0.026301500066402438
[2m[36m(func pid=155946)[0m f1_weighted: 0.049965084988413566
[2m[36m(func pid=155946)[0m f1_per_class: [0.019, 0.127, 0.018, 0.099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=144068)[0m top1: 0.36427238805970147
[2m[36m(func pid=144068)[0m top5: 0.9197761194029851
[2m[36m(func pid=144068)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=144068)[0m f1_macro: 0.2655001446737339
[2m[36m(func pid=144068)[0m f1_weighted: 0.3644866422201177
[2m[36m(func pid=144068)[0m f1_per_class: [0.23, 0.2, 0.386, 0.531, 0.08, 0.376, 0.395, 0.135, 0.096, 0.226]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.1003 | Steps: 2 | Val loss: 16.1624 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8014 | Steps: 2 | Val loss: 2.3157 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.0468 | Steps: 2 | Val loss: 1.9602 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7340 | Steps: 2 | Val loss: 2.3451 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=150330)[0m top1: 0.43050373134328357
[2m[36m(func pid=150330)[0m top5: 0.90625
[2m[36m(func pid=150330)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=150330)[0m f1_macro: 0.28187418184952917
[2m[36m(func pid=150330)[0m f1_weighted: 0.41140590931216164
[2m[36m(func pid=150330)[0m f1_per_class: [0.361, 0.517, 0.333, 0.506, 0.0, 0.45, 0.378, 0.105, 0.0, 0.169]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=162129)[0m top1: 0.010261194029850746
[2m[36m(func pid=162129)[0m top5: 0.5685634328358209
[2m[36m(func pid=162129)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=162129)[0m f1_macro: 0.0040154676818127165
[2m[36m(func pid=162129)[0m f1_weighted: 0.007834480745459286
[2m[36m(func pid=162129)[0m f1_per_class: [0.0, 0.0, 0.012, 0.028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=144068)[0m top1: 0.3726679104477612
[2m[36m(func pid=144068)[0m top5: 0.914179104477612
[2m[36m(func pid=144068)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=144068)[0m f1_macro: 0.2736200437819035
[2m[36m(func pid=144068)[0m f1_weighted: 0.37043020651511105
[2m[36m(func pid=144068)[0m f1_per_class: [0.263, 0.194, 0.367, 0.535, 0.081, 0.379, 0.399, 0.201, 0.099, 0.219]
== Status ==
Current time: 2024-01-07 00:30:10 (running for 00:37:19.49)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.047 |      0.274 |                   83 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.1   |      0.282 |                   56 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.758 |      0.026 |                   30 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.801 |      0.004 |                    4 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.055970149253731345
[2m[36m(func pid=155946)[0m top5: 0.4962686567164179
[2m[36m(func pid=155946)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=155946)[0m f1_macro: 0.030153899687727602
[2m[36m(func pid=155946)[0m f1_weighted: 0.056133771701218145
[2m[36m(func pid=155946)[0m f1_per_class: [0.015, 0.142, 0.02, 0.109, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.8370 | Steps: 2 | Val loss: 16.8044 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7843 | Steps: 2 | Val loss: 2.3109 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.2375 | Steps: 2 | Val loss: 1.9505 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7197 | Steps: 2 | Val loss: 2.3408 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=150330)[0m top1: 0.4006529850746269
[2m[36m(func pid=150330)[0m top5: 0.8903917910447762
[2m[36m(func pid=150330)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=150330)[0m f1_macro: 0.31601269289312234
[2m[36m(func pid=150330)[0m f1_weighted: 0.4191690168479939
[2m[36m(func pid=150330)[0m f1_per_class: [0.372, 0.476, 0.314, 0.501, 0.25, 0.415, 0.405, 0.231, 0.101, 0.094]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=162129)[0m top1: 0.00792910447761194
[2m[36m(func pid=162129)[0m top5: 0.5755597014925373
[2m[36m(func pid=162129)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=162129)[0m f1_macro: 0.0025126862379145297
[2m[36m(func pid=162129)[0m f1_weighted: 0.0036901764701183547
[2m[36m(func pid=162129)[0m f1_per_class: [0.0, 0.0, 0.012, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:15 (running for 00:37:24.74)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.237 |      0.282 |                   84 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  0.837 |      0.316 |                   57 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.734 |      0.03  |                   31 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.784 |      0.003 |                    5 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.37779850746268656
[2m[36m(func pid=144068)[0m top5: 0.9057835820895522
[2m[36m(func pid=144068)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=144068)[0m f1_macro: 0.2819880138237949
[2m[36m(func pid=144068)[0m f1_weighted: 0.37910806227951
[2m[36m(func pid=144068)[0m f1_per_class: [0.268, 0.205, 0.344, 0.541, 0.08, 0.371, 0.4, 0.293, 0.115, 0.203]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.0625
[2m[36m(func pid=155946)[0m top5: 0.49486940298507465
[2m[36m(func pid=155946)[0m f1_micro: 0.0625
[2m[36m(func pid=155946)[0m f1_macro: 0.03492734614057079
[2m[36m(func pid=155946)[0m f1_weighted: 0.06111508706736387
[2m[36m(func pid=155946)[0m f1_per_class: [0.025, 0.158, 0.022, 0.113, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.1831 | Steps: 2 | Val loss: 19.7851 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7395 | Steps: 2 | Val loss: 2.2994 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.0629 | Steps: 2 | Val loss: 1.9620 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=150330)[0m top1: 0.302705223880597
[2m[36m(func pid=150330)[0m top5: 0.8563432835820896
[2m[36m(func pid=150330)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=150330)[0m f1_macro: 0.2773717522138481
[2m[36m(func pid=150330)[0m f1_weighted: 0.36013061754529097
[2m[36m(func pid=150330)[0m f1_per_class: [0.222, 0.403, 0.227, 0.378, 0.326, 0.334, 0.393, 0.273, 0.161, 0.057]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.7632 | Steps: 2 | Val loss: 2.3399 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=162129)[0m top1: 0.01632462686567164
[2m[36m(func pid=162129)[0m top5: 0.5783582089552238
[2m[36m(func pid=162129)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=162129)[0m f1_macro: 0.007454910150933207
[2m[36m(func pid=162129)[0m f1_weighted: 0.017313817030261077
[2m[36m(func pid=162129)[0m f1_per_class: [0.0, 0.0, 0.013, 0.062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=144068)[0m top1: 0.37919776119402987
[2m[36m(func pid=144068)[0m top5: 0.8987873134328358
[2m[36m(func pid=144068)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=144068)[0m f1_macro: 0.29007893177915595
== Status ==
Current time: 2024-01-07 00:30:21 (running for 00:37:30.11)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.063 |      0.29  |                   85 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.183 |      0.277 |                   58 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.72  |      0.035 |                   32 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.739 |      0.007 |                    6 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=144068)[0m f1_weighted: 0.38595531687899215

[2m[36m(func pid=144068)[0m f1_per_class: [0.27, 0.219, 0.328, 0.537, 0.083, 0.381, 0.398, 0.391, 0.099, 0.195]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.06716417910447761
[2m[36m(func pid=155946)[0m top5: 0.49113805970149255
[2m[36m(func pid=155946)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=155946)[0m f1_macro: 0.03702845250444847
[2m[36m(func pid=155946)[0m f1_weighted: 0.06435346433804724
[2m[36m(func pid=155946)[0m f1_per_class: [0.021, 0.157, 0.023, 0.122, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.3897 | Steps: 2 | Val loss: 22.6655 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7697 | Steps: 2 | Val loss: 2.2822 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.0567 | Steps: 2 | Val loss: 1.9818 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=150330)[0m top1: 0.2392723880597015
[2m[36m(func pid=150330)[0m top5: 0.8236940298507462
[2m[36m(func pid=150330)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=150330)[0m f1_macro: 0.22947478614191805
[2m[36m(func pid=150330)[0m f1_weighted: 0.2752611100820159
[2m[36m(func pid=150330)[0m f1_per_class: [0.182, 0.392, 0.208, 0.18, 0.179, 0.28, 0.313, 0.357, 0.131, 0.072]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6889 | Steps: 2 | Val loss: 2.3343 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=162129)[0m top1: 0.06669776119402986
[2m[36m(func pid=162129)[0m top5: 0.5839552238805971
[2m[36m(func pid=162129)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=162129)[0m f1_macro: 0.03178335771681785
[2m[36m(func pid=162129)[0m f1_weighted: 0.06447068833152718
[2m[36m(func pid=162129)[0m f1_per_class: [0.07, 0.01, 0.018, 0.219, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:26 (running for 00:37:35.30)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.057 |      0.305 |                   86 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  3.39  |      0.229 |                   59 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.763 |      0.037 |                   33 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.77  |      0.032 |                    7 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.376865671641791
[2m[36m(func pid=144068)[0m top5: 0.8889925373134329
[2m[36m(func pid=144068)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=144068)[0m f1_macro: 0.3045719491751392
[2m[36m(func pid=144068)[0m f1_weighted: 0.39624977739188716
[2m[36m(func pid=144068)[0m f1_per_class: [0.271, 0.297, 0.324, 0.519, 0.083, 0.382, 0.386, 0.461, 0.138, 0.184]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m top1: 0.07555970149253731
[2m[36m(func pid=155946)[0m top5: 0.4944029850746269
[2m[36m(func pid=155946)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=155946)[0m f1_macro: 0.04074489166576099
[2m[36m(func pid=155946)[0m f1_weighted: 0.06992124191339703
[2m[36m(func pid=155946)[0m f1_per_class: [0.035, 0.161, 0.026, 0.139, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.4522 | Steps: 2 | Val loss: 27.0555 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7140 | Steps: 2 | Val loss: 2.2601 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.0229 | Steps: 2 | Val loss: 2.0088 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=150330)[0m top1: 0.19402985074626866
[2m[36m(func pid=150330)[0m top5: 0.8129664179104478
[2m[36m(func pid=150330)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=150330)[0m f1_macro: 0.18679296413761126
[2m[36m(func pid=150330)[0m f1_weighted: 0.19273649215131536
[2m[36m(func pid=150330)[0m f1_per_class: [0.118, 0.402, 0.177, 0.07, 0.09, 0.253, 0.157, 0.313, 0.125, 0.162]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7494 | Steps: 2 | Val loss: 2.3347 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=162129)[0m top1: 0.09934701492537314
[2m[36m(func pid=162129)[0m top5: 0.6058768656716418
[2m[36m(func pid=162129)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=162129)[0m f1_macro: 0.046509751367683365
[2m[36m(func pid=162129)[0m f1_weighted: 0.07106169359174605
[2m[36m(func pid=162129)[0m f1_per_class: [0.07, 0.065, 0.123, 0.207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:31 (running for 00:37:40.71)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.023 |      0.306 |                   87 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.452 |      0.187 |                   60 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.689 |      0.041 |                   34 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.714 |      0.047 |                    8 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.363339552238806
[2m[36m(func pid=144068)[0m top5: 0.8824626865671642
[2m[36m(func pid=144068)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.3061502943316428
[2m[36m(func pid=144068)[0m f1_weighted: 0.38773587801125764
[2m[36m(func pid=144068)[0m f1_per_class: [0.264, 0.358, 0.31, 0.485, 0.083, 0.379, 0.347, 0.5, 0.162, 0.174]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.0735 | Steps: 2 | Val loss: 31.0718 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=155946)[0m top1: 0.0732276119402985
[2m[36m(func pid=155946)[0m top5: 0.4906716417910448
[2m[36m(func pid=155946)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=155946)[0m f1_macro: 0.03901409598778943
[2m[36m(func pid=155946)[0m f1_weighted: 0.067053962072094
[2m[36m(func pid=155946)[0m f1_per_class: [0.029, 0.153, 0.028, 0.134, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6842 | Steps: 2 | Val loss: 2.2374 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.0126 | Steps: 2 | Val loss: 2.0355 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=150330)[0m top1: 0.15764925373134328
[2m[36m(func pid=150330)[0m top5: 0.8246268656716418
[2m[36m(func pid=150330)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=150330)[0m f1_macro: 0.15894593824996398
[2m[36m(func pid=150330)[0m f1_weighted: 0.14686955913589225
[2m[36m(func pid=150330)[0m f1_per_class: [0.113, 0.317, 0.154, 0.039, 0.056, 0.111, 0.142, 0.268, 0.126, 0.264]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7087 | Steps: 2 | Val loss: 2.3298 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=162129)[0m top1: 0.10074626865671642
[2m[36m(func pid=162129)[0m top5: 0.7159514925373134
[2m[36m(func pid=162129)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=162129)[0m f1_macro: 0.06639670293233399
[2m[36m(func pid=162129)[0m f1_weighted: 0.07176371075952838
[2m[36m(func pid=162129)[0m f1_per_class: [0.09, 0.079, 0.3, 0.195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=144068)[0m top1: 0.3358208955223881
[2m[36m(func pid=144068)[0m top5: 0.8754664179104478
[2m[36m(func pid=144068)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=144068)[0m f1_macro: 0.296425073575446
[2m[36m(func pid=144068)[0m f1_weighted: 0.3586936197833428
[2m[36m(func pid=144068)[0m f1_per_class: [0.273, 0.408, 0.324, 0.439, 0.077, 0.356, 0.277, 0.466, 0.176, 0.168]
== Status ==
Current time: 2024-01-07 00:30:37 (running for 00:37:45.98)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.013 |      0.296 |                   88 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.073 |      0.159 |                   61 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.749 |      0.039 |                   35 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.684 |      0.066 |                    9 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4834 | Steps: 2 | Val loss: 33.7922 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=155946)[0m top1: 0.07695895522388059
[2m[36m(func pid=155946)[0m top5: 0.4920708955223881
[2m[36m(func pid=155946)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=155946)[0m f1_macro: 0.042874004682269075
[2m[36m(func pid=155946)[0m f1_weighted: 0.07041134929418479
[2m[36m(func pid=155946)[0m f1_per_class: [0.025, 0.162, 0.03, 0.134, 0.0, 0.0, 0.0, 0.078, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6457 | Steps: 2 | Val loss: 2.2212 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.4260 | Steps: 2 | Val loss: 2.0987 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=150330)[0m top1: 0.1814365671641791
[2m[36m(func pid=150330)[0m top5: 0.8232276119402985
[2m[36m(func pid=150330)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=150330)[0m f1_macro: 0.17209318559212997
[2m[36m(func pid=150330)[0m f1_weighted: 0.1912774529162082
[2m[36m(func pid=150330)[0m f1_per_class: [0.254, 0.282, 0.133, 0.051, 0.047, 0.069, 0.31, 0.274, 0.114, 0.188]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.7191 | Steps: 2 | Val loss: 2.3265 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=162129)[0m top1: 0.11194029850746269
[2m[36m(func pid=162129)[0m top5: 0.8278917910447762
[2m[36m(func pid=162129)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=162129)[0m f1_macro: 0.07929610989304292
[2m[36m(func pid=162129)[0m f1_weighted: 0.08003026381070753
[2m[36m(func pid=162129)[0m f1_per_class: [0.096, 0.161, 0.364, 0.169, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:42 (running for 00:37:51.41)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.426 |      0.273 |                   89 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  0.483 |      0.172 |                   62 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.709 |      0.043 |                   36 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.646 |      0.079 |                   10 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2957089552238806
[2m[36m(func pid=144068)[0m top5: 0.8684701492537313
[2m[36m(func pid=144068)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.27325117168901436
[2m[36m(func pid=144068)[0m f1_weighted: 0.3029074114783992
[2m[36m(func pid=144068)[0m f1_per_class: [0.286, 0.418, 0.301, 0.326, 0.079, 0.36, 0.194, 0.433, 0.172, 0.163]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 3.0822 | Steps: 2 | Val loss: 33.5552 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=155946)[0m top1: 0.08022388059701492
[2m[36m(func pid=155946)[0m top5: 0.49533582089552236
[2m[36m(func pid=155946)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=155946)[0m f1_macro: 0.043832554988714224
[2m[36m(func pid=155946)[0m f1_weighted: 0.07254075143790534
[2m[36m(func pid=155946)[0m f1_per_class: [0.023, 0.166, 0.033, 0.139, 0.0, 0.0, 0.0, 0.078, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5913 | Steps: 2 | Val loss: 2.2063 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.9876 | Steps: 2 | Val loss: 2.1354 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=150330)[0m top1: 0.2490671641791045
[2m[36m(func pid=150330)[0m top5: 0.8339552238805971
[2m[36m(func pid=150330)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=150330)[0m f1_macro: 0.20101699403344003
[2m[36m(func pid=150330)[0m f1_weighted: 0.27258310565090166
[2m[36m(func pid=150330)[0m f1_per_class: [0.225, 0.328, 0.119, 0.118, 0.049, 0.148, 0.475, 0.226, 0.108, 0.214]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6758 | Steps: 2 | Val loss: 2.3223 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=162129)[0m top1: 0.13899253731343283
[2m[36m(func pid=162129)[0m top5: 0.8376865671641791
[2m[36m(func pid=162129)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=162129)[0m f1_macro: 0.08785160063098983
[2m[36m(func pid=162129)[0m f1_weighted: 0.10494025184207852
[2m[36m(func pid=162129)[0m f1_per_class: [0.103, 0.224, 0.333, 0.144, 0.0, 0.0, 0.074, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:47 (running for 00:37:56.88)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.988 |      0.25  |                   90 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  3.082 |      0.201 |                   63 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.719 |      0.044 |                   37 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.591 |      0.088 |                   11 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.26119402985074625
[2m[36m(func pid=144068)[0m top5: 0.8600746268656716
[2m[36m(func pid=144068)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=144068)[0m f1_macro: 0.24969870465252839
[2m[36m(func pid=144068)[0m f1_weighted: 0.2511411748137267
[2m[36m(func pid=144068)[0m f1_per_class: [0.263, 0.42, 0.328, 0.229, 0.08, 0.352, 0.124, 0.399, 0.147, 0.154]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.9601 | Steps: 2 | Val loss: 31.3300 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=155946)[0m top1: 0.0830223880597015
[2m[36m(func pid=155946)[0m top5: 0.5032649253731343
[2m[36m(func pid=155946)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=155946)[0m f1_macro: 0.04609352028739231
[2m[36m(func pid=155946)[0m f1_weighted: 0.0745432275132484
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.164, 0.036, 0.145, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6184 | Steps: 2 | Val loss: 2.1951 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=150330)[0m top1: 0.3376865671641791
[2m[36m(func pid=150330)[0m top5: 0.8628731343283582
[2m[36m(func pid=150330)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=150330)[0m f1_macro: 0.23677168290030198
[2m[36m(func pid=150330)[0m f1_weighted: 0.352487541648203
[2m[36m(func pid=150330)[0m f1_per_class: [0.21, 0.493, 0.119, 0.202, 0.069, 0.271, 0.531, 0.22, 0.038, 0.214]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.9743 | Steps: 2 | Val loss: 2.1612 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7314 | Steps: 2 | Val loss: 2.3230 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=162129)[0m top1: 0.18703358208955223
[2m[36m(func pid=162129)[0m top5: 0.8157649253731343
[2m[36m(func pid=162129)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=162129)[0m f1_macro: 0.10780735504131626
[2m[36m(func pid=162129)[0m f1_weighted: 0.14420294627350974
[2m[36m(func pid=162129)[0m f1_per_class: [0.122, 0.283, 0.343, 0.101, 0.0, 0.0, 0.208, 0.0, 0.022, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:53 (running for 00:38:01.98)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.974 |      0.233 |                   91 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.96  |      0.237 |                   64 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.676 |      0.046 |                   38 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.618 |      0.108 |                   12 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.240205223880597
[2m[36m(func pid=144068)[0m top5: 0.8577425373134329
[2m[36m(func pid=144068)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=144068)[0m f1_macro: 0.23287474618287646
[2m[36m(func pid=144068)[0m f1_weighted: 0.21533081510972496
[2m[36m(func pid=144068)[0m f1_per_class: [0.24, 0.418, 0.344, 0.165, 0.083, 0.362, 0.07, 0.375, 0.124, 0.148]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.5927 | Steps: 2 | Val loss: 30.8017 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=155946)[0m top1: 0.08022388059701492
[2m[36m(func pid=155946)[0m top5: 0.5051305970149254
[2m[36m(func pid=155946)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=155946)[0m f1_macro: 0.045163801901032155
[2m[36m(func pid=155946)[0m f1_weighted: 0.0720199751696014
[2m[36m(func pid=155946)[0m f1_per_class: [0.024, 0.164, 0.036, 0.135, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5513 | Steps: 2 | Val loss: 2.1859 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.0832 | Steps: 2 | Val loss: 2.1980 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=150330)[0m top1: 0.37033582089552236
[2m[36m(func pid=150330)[0m top5: 0.8889925373134329
[2m[36m(func pid=150330)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=150330)[0m f1_macro: 0.24219975460043827
[2m[36m(func pid=150330)[0m f1_weighted: 0.36356818491375603
[2m[36m(func pid=150330)[0m f1_per_class: [0.147, 0.528, 0.127, 0.187, 0.117, 0.374, 0.535, 0.193, 0.0, 0.214]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7266 | Steps: 2 | Val loss: 2.3223 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=162129)[0m top1: 0.22154850746268656
[2m[36m(func pid=162129)[0m top5: 0.8031716417910447
[2m[36m(func pid=162129)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=162129)[0m f1_macro: 0.13179653483630455
[2m[36m(func pid=162129)[0m f1_weighted: 0.18015344959567242
[2m[36m(func pid=162129)[0m f1_per_class: [0.176, 0.298, 0.378, 0.088, 0.0, 0.008, 0.322, 0.0, 0.047, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:30:58 (running for 00:38:07.37)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.083 |      0.226 |                   92 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.593 |      0.242 |                   65 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.731 |      0.045 |                   39 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.551 |      0.132 |                   13 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2332089552238806
[2m[36m(func pid=144068)[0m top5: 0.8563432835820896
[2m[36m(func pid=144068)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.22604105138434288
[2m[36m(func pid=144068)[0m f1_weighted: 0.20538555996200714
[2m[36m(func pid=144068)[0m f1_per_class: [0.218, 0.425, 0.355, 0.147, 0.086, 0.359, 0.056, 0.367, 0.103, 0.144]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 10.4107 | Steps: 2 | Val loss: 30.4625 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=155946)[0m top1: 0.08022388059701492
[2m[36m(func pid=155946)[0m top5: 0.5046641791044776
[2m[36m(func pid=155946)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=155946)[0m f1_macro: 0.045152059183073356
[2m[36m(func pid=155946)[0m f1_weighted: 0.07209967949720912
[2m[36m(func pid=155946)[0m f1_per_class: [0.023, 0.165, 0.037, 0.135, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5973 | Steps: 2 | Val loss: 2.1844 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.9247 | Steps: 2 | Val loss: 2.2036 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=150330)[0m top1: 0.3516791044776119
[2m[36m(func pid=150330)[0m top5: 0.9076492537313433
[2m[36m(func pid=150330)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=150330)[0m f1_macro: 0.241470264170423
[2m[36m(func pid=150330)[0m f1_weighted: 0.33581427486848586
[2m[36m(func pid=150330)[0m f1_per_class: [0.099, 0.537, 0.154, 0.124, 0.191, 0.407, 0.482, 0.207, 0.0, 0.214]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6876 | Steps: 2 | Val loss: 2.3192 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=162129)[0m top1: 0.24067164179104478
[2m[36m(func pid=162129)[0m top5: 0.7845149253731343
[2m[36m(func pid=162129)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=162129)[0m f1_macro: 0.1430633217700923
[2m[36m(func pid=162129)[0m f1_weighted: 0.21186716505660858
[2m[36m(func pid=162129)[0m f1_per_class: [0.138, 0.307, 0.333, 0.101, 0.024, 0.031, 0.402, 0.0, 0.07, 0.025]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:31:03 (running for 00:38:12.61)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.925 |      0.23  |                   93 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 10.411 |      0.241 |                   66 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.727 |      0.045 |                   40 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.597 |      0.143 |                   14 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.23600746268656717
[2m[36m(func pid=144068)[0m top5: 0.8568097014925373
[2m[36m(func pid=144068)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=144068)[0m f1_macro: 0.23046835974692229
[2m[36m(func pid=144068)[0m f1_weighted: 0.21054069632587133
[2m[36m(func pid=144068)[0m f1_per_class: [0.221, 0.425, 0.373, 0.147, 0.089, 0.367, 0.07, 0.364, 0.102, 0.147]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3738 | Steps: 2 | Val loss: 32.8049 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=155946)[0m top1: 0.08208955223880597
[2m[36m(func pid=155946)[0m top5: 0.5088619402985075
[2m[36m(func pid=155946)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=155946)[0m f1_macro: 0.04593535243766568
[2m[36m(func pid=155946)[0m f1_weighted: 0.07334594371679307
[2m[36m(func pid=155946)[0m f1_per_class: [0.023, 0.171, 0.038, 0.136, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5179 | Steps: 2 | Val loss: 2.1823 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.9122 | Steps: 2 | Val loss: 2.2082 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=150330)[0m top1: 0.31343283582089554
[2m[36m(func pid=150330)[0m top5: 0.9067164179104478
[2m[36m(func pid=150330)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=150330)[0m f1_macro: 0.23752366231062633
[2m[36m(func pid=150330)[0m f1_weighted: 0.29274934808085484
[2m[36m(func pid=150330)[0m f1_per_class: [0.084, 0.539, 0.162, 0.098, 0.244, 0.405, 0.345, 0.284, 0.0, 0.214]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.6980 | Steps: 2 | Val loss: 2.3157 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=162129)[0m top1: 0.22621268656716417
[2m[36m(func pid=162129)[0m top5: 0.7513992537313433
[2m[36m(func pid=162129)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=162129)[0m f1_macro: 0.14339901073636271
[2m[36m(func pid=162129)[0m f1_weighted: 0.19758845653257437
[2m[36m(func pid=162129)[0m f1_per_class: [0.154, 0.296, 0.341, 0.075, 0.032, 0.045, 0.373, 0.027, 0.065, 0.026]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:31:08 (running for 00:38:17.88)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.912 |      0.238 |                   94 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  0.374 |      0.238 |                   67 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.688 |      0.046 |                   41 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.518 |      0.143 |                   15 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.24253731343283583
[2m[36m(func pid=144068)[0m top5: 0.8582089552238806
[2m[36m(func pid=144068)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=144068)[0m f1_macro: 0.23792371080765923
[2m[36m(func pid=144068)[0m f1_weighted: 0.2271671264052001
[2m[36m(func pid=144068)[0m f1_per_class: [0.203, 0.419, 0.393, 0.154, 0.092, 0.371, 0.119, 0.375, 0.103, 0.15]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7414 | Steps: 2 | Val loss: 34.3125 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=155946)[0m top1: 0.08395522388059702
[2m[36m(func pid=155946)[0m top5: 0.5205223880597015
[2m[36m(func pid=155946)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=155946)[0m f1_macro: 0.049729690992961414
[2m[36m(func pid=155946)[0m f1_weighted: 0.07550529291450242
[2m[36m(func pid=155946)[0m f1_per_class: [0.023, 0.164, 0.039, 0.143, 0.0, 0.0, 0.003, 0.09, 0.0, 0.034]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4843 | Steps: 2 | Val loss: 2.1813 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.1156 | Steps: 2 | Val loss: 2.1874 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=150330)[0m top1: 0.28824626865671643
[2m[36m(func pid=150330)[0m top5: 0.882929104477612
[2m[36m(func pid=150330)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=150330)[0m f1_macro: 0.20520876487417644
[2m[36m(func pid=150330)[0m f1_weighted: 0.2567575861924017
[2m[36m(func pid=150330)[0m f1_per_class: [0.081, 0.538, 0.186, 0.11, 0.0, 0.405, 0.217, 0.301, 0.0, 0.214]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6971 | Steps: 2 | Val loss: 2.3124 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=162129)[0m top1: 0.208955223880597
[2m[36m(func pid=162129)[0m top5: 0.7257462686567164
[2m[36m(func pid=162129)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=162129)[0m f1_macro: 0.15386596749843312
[2m[36m(func pid=162129)[0m f1_weighted: 0.18381822030989248
[2m[36m(func pid=162129)[0m f1_per_class: [0.173, 0.288, 0.341, 0.071, 0.017, 0.03, 0.298, 0.261, 0.029, 0.031]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:31:14 (running for 00:38:23.11)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  1.116 |      0.25  |                   95 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.741 |      0.205 |                   68 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.698 |      0.05  |                   42 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.484 |      0.154 |                   16 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.25652985074626866
[2m[36m(func pid=144068)[0m top5: 0.8614738805970149
[2m[36m(func pid=144068)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=144068)[0m f1_macro: 0.24950276265215346
[2m[36m(func pid=144068)[0m f1_weighted: 0.25348315349452927
[2m[36m(func pid=144068)[0m f1_per_class: [0.199, 0.42, 0.4, 0.189, 0.096, 0.381, 0.169, 0.387, 0.1, 0.155]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3705 | Steps: 2 | Val loss: 33.8923 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=155946)[0m top1: 0.0853544776119403
[2m[36m(func pid=155946)[0m top5: 0.5284514925373134
[2m[36m(func pid=155946)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=155946)[0m f1_macro: 0.05025068247214894
[2m[36m(func pid=155946)[0m f1_weighted: 0.07624179367833518
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.163, 0.041, 0.147, 0.0, 0.0, 0.003, 0.09, 0.0, 0.032]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5020 | Steps: 2 | Val loss: 2.1847 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.9589 | Steps: 2 | Val loss: 2.1596 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=150330)[0m top1: 0.2994402985074627
[2m[36m(func pid=150330)[0m top5: 0.8591417910447762
[2m[36m(func pid=150330)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=150330)[0m f1_macro: 0.21738395259542234
[2m[36m(func pid=150330)[0m f1_weighted: 0.27083391730383755
[2m[36m(func pid=150330)[0m f1_per_class: [0.103, 0.536, 0.214, 0.192, 0.0, 0.417, 0.182, 0.273, 0.043, 0.214]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=162129)[0m top1: 0.18703358208955223
[2m[36m(func pid=162129)[0m top5: 0.695429104477612
[2m[36m(func pid=162129)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=162129)[0m f1_macro: 0.1532680826473863
[2m[36m(func pid=162129)[0m f1_weighted: 0.15885902355190912
[2m[36m(func pid=162129)[0m f1_per_class: [0.196, 0.298, 0.341, 0.061, 0.037, 0.078, 0.194, 0.272, 0.054, 0.0]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:31:19 (running for 00:38:28.33)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.959 |      0.26  |                   96 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.37  |      0.217 |                   69 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.697 |      0.05  |                   43 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.502 |      0.153 |                   17 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.2751865671641791
[2m[36m(func pid=144068)[0m top5: 0.8638059701492538
[2m[36m(func pid=144068)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=144068)[0m f1_macro: 0.25975501650100374
[2m[36m(func pid=144068)[0m f1_weighted: 0.2865078013734748
[2m[36m(func pid=144068)[0m f1_per_class: [0.189, 0.404, 0.377, 0.247, 0.099, 0.387, 0.228, 0.413, 0.097, 0.157]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7608 | Steps: 2 | Val loss: 2.3109 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 15.6589 | Steps: 2 | Val loss: 32.3556 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=155946)[0m top1: 0.08768656716417911
[2m[36m(func pid=155946)[0m top5: 0.5363805970149254
[2m[36m(func pid=155946)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=155946)[0m f1_macro: 0.05230923421019501
[2m[36m(func pid=155946)[0m f1_weighted: 0.07846176590695474
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.171, 0.042, 0.147, 0.0, 0.0, 0.003, 0.104, 0.0, 0.03]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5122 | Steps: 2 | Val loss: 2.1926 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.2212 | Steps: 2 | Val loss: 2.1432 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=150330)[0m top1: 0.3283582089552239
[2m[36m(func pid=150330)[0m top5: 0.8442164179104478
[2m[36m(func pid=150330)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=150330)[0m f1_macro: 0.2234992251620079
[2m[36m(func pid=150330)[0m f1_weighted: 0.2974340945043554
[2m[36m(func pid=150330)[0m f1_per_class: [0.139, 0.532, 0.22, 0.346, 0.0, 0.466, 0.124, 0.197, 0.057, 0.154]
[2m[36m(func pid=150330)[0m 
== Status ==
Current time: 2024-01-07 00:31:24 (running for 00:38:33.42)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.959 |      0.26  |                   96 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  | 15.659 |      0.223 |                   70 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.761 |      0.052 |                   44 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.512 |      0.147 |                   18 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=162129)[0m top1: 0.15578358208955223
[2m[36m(func pid=162129)[0m top5: 0.6693097014925373
[2m[36m(func pid=162129)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=162129)[0m f1_macro: 0.14717495954037357
[2m[36m(func pid=162129)[0m f1_weighted: 0.12075714513569172
[2m[36m(func pid=162129)[0m f1_per_class: [0.2, 0.313, 0.328, 0.058, 0.032, 0.11, 0.048, 0.249, 0.068, 0.066]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=144068)[0m top1: 0.2971082089552239
[2m[36m(func pid=144068)[0m top5: 0.8731343283582089
[2m[36m(func pid=144068)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=144068)[0m f1_macro: 0.27358004569607935
[2m[36m(func pid=144068)[0m f1_weighted: 0.31976269745419433
[2m[36m(func pid=144068)[0m f1_per_class: [0.181, 0.412, 0.37, 0.291, 0.101, 0.378, 0.291, 0.443, 0.102, 0.165]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6909 | Steps: 2 | Val loss: 2.3070 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1473 | Steps: 2 | Val loss: 29.2706 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4840 | Steps: 2 | Val loss: 2.2010 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.8847 | Steps: 2 | Val loss: 2.1048 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=155946)[0m top1: 0.09281716417910447
[2m[36m(func pid=155946)[0m top5: 0.5433768656716418
[2m[36m(func pid=155946)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=155946)[0m f1_macro: 0.057823566550036995
[2m[36m(func pid=155946)[0m f1_weighted: 0.08266582267767227
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.181, 0.043, 0.152, 0.0, 0.0, 0.003, 0.118, 0.0, 0.054]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m top1: 0.332089552238806
[2m[36m(func pid=150330)[0m top5: 0.8460820895522388
[2m[36m(func pid=150330)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=150330)[0m f1_macro: 0.2137974102333943
[2m[36m(func pid=150330)[0m f1_weighted: 0.2896328318839488
[2m[36m(func pid=150330)[0m f1_per_class: [0.236, 0.519, 0.25, 0.409, 0.0, 0.369, 0.103, 0.062, 0.062, 0.129]
[2m[36m(func pid=150330)[0m 
== Status ==
Current time: 2024-01-07 00:31:29 (running for 00:38:38.84)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.885 |      0.288 |                   98 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.147 |      0.214 |                   71 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.691 |      0.058 |                   45 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.512 |      0.147 |                   18 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.3204291044776119
[2m[36m(func pid=144068)[0m top5: 0.8805970149253731
[2m[36m(func pid=144068)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=144068)[0m f1_macro: 0.2884334698340563
[2m[36m(func pid=144068)[0m f1_weighted: 0.354299222771233
[2m[36m(func pid=144068)[0m f1_per_class: [0.181, 0.417, 0.377, 0.336, 0.104, 0.376, 0.359, 0.453, 0.11, 0.17]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=162129)[0m top1: 0.1226679104477612
[2m[36m(func pid=162129)[0m top5: 0.6511194029850746
[2m[36m(func pid=162129)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=162129)[0m f1_macro: 0.1273875445569428
[2m[36m(func pid=162129)[0m f1_weighted: 0.10064356360802393
[2m[36m(func pid=162129)[0m f1_per_class: [0.125, 0.229, 0.242, 0.072, 0.039, 0.131, 0.012, 0.25, 0.081, 0.094]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.6684 | Steps: 2 | Val loss: 2.3044 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.8589 | Steps: 2 | Val loss: 26.8053 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.8759 | Steps: 2 | Val loss: 2.0695 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4182 | Steps: 2 | Val loss: 2.2021 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=155946)[0m top1: 0.09421641791044776
[2m[36m(func pid=155946)[0m top5: 0.5503731343283582
[2m[36m(func pid=155946)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=155946)[0m f1_macro: 0.058052481702155226
[2m[36m(func pid=155946)[0m f1_weighted: 0.08262113778318525
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.189, 0.045, 0.147, 0.0, 0.0, 0.003, 0.117, 0.0, 0.053]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=150330)[0m top1: 0.32882462686567165
[2m[36m(func pid=150330)[0m top5: 0.8577425373134329
[2m[36m(func pid=150330)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=150330)[0m f1_macro: 0.21128856907161359
[2m[36m(func pid=150330)[0m f1_weighted: 0.2893562906484462
[2m[36m(func pid=150330)[0m f1_per_class: [0.278, 0.5, 0.282, 0.433, 0.0, 0.252, 0.137, 0.031, 0.058, 0.14]
[2m[36m(func pid=150330)[0m 
== Status ==
Current time: 2024-01-07 00:31:35 (running for 00:38:44.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00017 | RUNNING    | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.876 |      0.302 |                   99 |
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.859 |      0.211 |                   72 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.668 |      0.058 |                   46 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.484 |      0.127 |                   19 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=144068)[0m top1: 0.34701492537313433
[2m[36m(func pid=144068)[0m top5: 0.8917910447761194
[2m[36m(func pid=144068)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=144068)[0m f1_macro: 0.3022791691846116
[2m[36m(func pid=144068)[0m f1_weighted: 0.3862595585834035
[2m[36m(func pid=144068)[0m f1_per_class: [0.18, 0.411, 0.377, 0.361, 0.112, 0.395, 0.437, 0.463, 0.112, 0.173]
[2m[36m(func pid=144068)[0m 
[2m[36m(func pid=162129)[0m top1: 0.1296641791044776
[2m[36m(func pid=162129)[0m top5: 0.65625
[2m[36m(func pid=162129)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=162129)[0m f1_macro: 0.13380925225042378
[2m[36m(func pid=162129)[0m f1_weighted: 0.11497400126951997
[2m[36m(func pid=162129)[0m f1_per_class: [0.136, 0.214, 0.218, 0.128, 0.039, 0.166, 0.003, 0.254, 0.06, 0.119]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.3546 | Steps: 2 | Val loss: 24.2260 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.6814 | Steps: 2 | Val loss: 2.3041 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=144068)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.8482 | Steps: 2 | Val loss: 2.0272 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4104 | Steps: 2 | Val loss: 2.2044 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=150330)[0m top1: 0.3376865671641791
[2m[36m(func pid=150330)[0m top5: 0.8852611940298507
[2m[36m(func pid=150330)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=150330)[0m f1_macro: 0.23015518502268334
[2m[36m(func pid=150330)[0m f1_weighted: 0.321559235451519
[2m[36m(func pid=150330)[0m f1_per_class: [0.222, 0.491, 0.324, 0.425, 0.152, 0.182, 0.286, 0.0, 0.105, 0.116]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m top1: 0.09654850746268656
[2m[36m(func pid=155946)[0m top5: 0.5531716417910447
[2m[36m(func pid=155946)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=155946)[0m f1_macro: 0.05889588679774053
[2m[36m(func pid=155946)[0m f1_weighted: 0.08412224885554095
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.2, 0.045, 0.146, 0.0, 0.0, 0.003, 0.117, 0.0, 0.051]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=144068)[0m top1: 0.363339552238806
[2m[36m(func pid=144068)[0m top5: 0.8973880597014925
[2m[36m(func pid=144068)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=144068)[0m f1_macro: 0.30971298876967673
[2m[36m(func pid=144068)[0m f1_weighted: 0.402212553215345
[2m[36m(func pid=144068)[0m f1_per_class: [0.186, 0.396, 0.4, 0.4, 0.114, 0.386, 0.469, 0.446, 0.11, 0.189]
== Status ==
Current time: 2024-01-07 00:31:40 (running for 00:38:49.31)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 3 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  2.355 |      0.23  |                   73 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.681 |      0.059 |                   47 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.418 |      0.134 |                   20 |
| train_51d3e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=162129)[0m top1: 0.1287313432835821
[2m[36m(func pid=162129)[0m top5: 0.6548507462686567
[2m[36m(func pid=162129)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=162129)[0m f1_macro: 0.13333064131659828
[2m[36m(func pid=162129)[0m f1_weighted: 0.11670502292032336
[2m[36m(func pid=162129)[0m f1_per_class: [0.113, 0.196, 0.206, 0.148, 0.037, 0.175, 0.0, 0.247, 0.055, 0.157]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2269 | Steps: 2 | Val loss: 22.6534 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.6418 | Steps: 2 | Val loss: 2.3003 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4526 | Steps: 2 | Val loss: 2.2027 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=150330)[0m top1: 0.3516791044776119
[2m[36m(func pid=150330)[0m top5: 0.898320895522388
[2m[36m(func pid=150330)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=150330)[0m f1_macro: 0.2275362778963396
[2m[36m(func pid=150330)[0m f1_weighted: 0.35779438602260133
[2m[36m(func pid=150330)[0m f1_per_class: [0.08, 0.464, 0.353, 0.394, 0.141, 0.103, 0.487, 0.0, 0.157, 0.097]
[2m[36m(func pid=150330)[0m 
[2m[36m(func pid=155946)[0m top1: 0.09888059701492537
[2m[36m(func pid=155946)[0m top5: 0.5620335820895522
[2m[36m(func pid=155946)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=155946)[0m f1_macro: 0.0595725781070162
[2m[36m(func pid=155946)[0m f1_weighted: 0.08509069274039159
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.206, 0.049, 0.145, 0.0, 0.0, 0.003, 0.116, 0.0, 0.051]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m top1: 0.1287313432835821
[2m[36m(func pid=162129)[0m top5: 0.6749067164179104
[2m[36m(func pid=162129)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=162129)[0m f1_macro: 0.12845500514957386
[2m[36m(func pid=162129)[0m f1_weighted: 0.12293060098284578
[2m[36m(func pid=162129)[0m f1_per_class: [0.115, 0.169, 0.216, 0.204, 0.035, 0.125, 0.0, 0.273, 0.054, 0.092]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=150330)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.8188 | Steps: 2 | Val loss: 23.8855 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6478 | Steps: 2 | Val loss: 2.2978 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.3804 | Steps: 2 | Val loss: 2.2040 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 00:31:46 (running for 00:38:55.22)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00019 | RUNNING    | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.227 |      0.228 |                   74 |
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.642 |      0.06  |                   48 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.453 |      0.128 |                   22 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150330)[0m top1: 0.3041044776119403
[2m[36m(func pid=150330)[0m top5: 0.8843283582089553
[2m[36m(func pid=150330)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=150330)[0m f1_macro: 0.19121481908332105
[2m[36m(func pid=150330)[0m f1_weighted: 0.30886281077944383
[2m[36m(func pid=150330)[0m f1_per_class: [0.044, 0.405, 0.343, 0.231, 0.123, 0.039, 0.54, 0.031, 0.094, 0.062]
[2m[36m(func pid=167216)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=167216)[0m Configuration completed!
[2m[36m(func pid=167216)[0m New optimizer parameters:
[2m[36m(func pid=167216)[0m SGD (
[2m[36m(func pid=167216)[0m Parameter Group 0
[2m[36m(func pid=167216)[0m     dampening: 0
[2m[36m(func pid=167216)[0m     differentiable: False
[2m[36m(func pid=167216)[0m     foreach: None
[2m[36m(func pid=167216)[0m     lr: 0.01
[2m[36m(func pid=167216)[0m     maximize: False
[2m[36m(func pid=167216)[0m     momentum: 0.9
[2m[36m(func pid=167216)[0m     nesterov: False
[2m[36m(func pid=167216)[0m     weight_decay: 1e-05
[2m[36m(func pid=167216)[0m )
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=155946)[0m top1: 0.09934701492537314
[2m[36m(func pid=155946)[0m top5: 0.5643656716417911
[2m[36m(func pid=155946)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=155946)[0m f1_macro: 0.05952341700210619
[2m[36m(func pid=155946)[0m f1_weighted: 0.08488317181928966
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.21, 0.05, 0.143, 0.0, 0.0, 0.003, 0.115, 0.0, 0.049]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m top1: 0.13292910447761194
[2m[36m(func pid=162129)[0m top5: 0.6893656716417911
[2m[36m(func pid=162129)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=162129)[0m f1_macro: 0.12763228298897097
[2m[36m(func pid=162129)[0m f1_weighted: 0.12752853629739774
[2m[36m(func pid=162129)[0m f1_per_class: [0.091, 0.166, 0.204, 0.22, 0.037, 0.132, 0.0, 0.299, 0.019, 0.11]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.6533 | Steps: 2 | Val loss: 2.2945 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1354 | Steps: 2 | Val loss: 2.3926 | Batch size: 32 | lr: 0.01 | Duration: 4.41s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4253 | Steps: 2 | Val loss: 2.2019 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=155946)[0m top1: 0.10307835820895522
[2m[36m(func pid=155946)[0m top5: 0.5769589552238806
[2m[36m(func pid=155946)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=155946)[0m f1_macro: 0.06085776567400668
[2m[36m(func pid=155946)[0m f1_weighted: 0.08692382817568799
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.225, 0.051, 0.141, 0.0, 0.0, 0.003, 0.114, 0.0, 0.048]
[2m[36m(func pid=167216)[0m top1: 0.006063432835820896
[2m[36m(func pid=167216)[0m top5: 0.3969216417910448
[2m[36m(func pid=167216)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=167216)[0m f1_macro: 0.00124282982791587
[2m[36m(func pid=167216)[0m f1_weighted: 7.535815187922719e-05
[2m[36m(func pid=167216)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162129)[0m top1: 0.13712686567164178
[2m[36m(func pid=162129)[0m top5: 0.7154850746268657
[2m[36m(func pid=162129)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=162129)[0m f1_macro: 0.12759505768198579
[2m[36m(func pid=162129)[0m f1_weighted: 0.13039891282748586
[2m[36m(func pid=162129)[0m f1_per_class: [0.086, 0.188, 0.196, 0.209, 0.04, 0.139, 0.003, 0.325, 0.0, 0.09]
== Status ==
Current time: 2024-01-07 00:31:51 (running for 00:39:00.58)
Memory usage on this node: 21.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.648 |      0.06  |                   49 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.38  |      0.128 |                   23 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 00:31:58 (running for 00:39:07.14)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.653 |      0.061 |                   50 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.38  |      0.128 |                   23 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167836)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=167836)[0m Configuration completed!
[2m[36m(func pid=167836)[0m New optimizer parameters:
[2m[36m(func pid=167836)[0m SGD (
[2m[36m(func pid=167836)[0m Parameter Group 0
[2m[36m(func pid=167836)[0m     dampening: 0
[2m[36m(func pid=167836)[0m     differentiable: False
[2m[36m(func pid=167836)[0m     foreach: None
[2m[36m(func pid=167836)[0m     lr: 0.1
[2m[36m(func pid=167836)[0m     maximize: False
[2m[36m(func pid=167836)[0m     momentum: 0.9
[2m[36m(func pid=167836)[0m     nesterov: False
[2m[36m(func pid=167836)[0m     weight_decay: 1e-05
[2m[36m(func pid=167836)[0m )
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7787 | Steps: 2 | Val loss: 2.3281 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6901 | Steps: 2 | Val loss: 2.2949 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4149 | Steps: 2 | Val loss: 2.1909 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.9345 | Steps: 2 | Val loss: 2.5003 | Batch size: 32 | lr: 0.1 | Duration: 4.58s
== Status ==
Current time: 2024-01-07 00:32:03 (running for 00:39:12.18)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.653 |      0.061 |                   50 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.425 |      0.128 |                   24 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  3.135 |      0.001 |                    1 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=162129)[0m top1: 0.1478544776119403
[2m[36m(func pid=162129)[0m top5: 0.7555970149253731
[2m[36m(func pid=162129)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=162129)[0m f1_macro: 0.13917489596297766
[2m[36m(func pid=162129)[0m f1_weighted: 0.14480933909323496
[2m[36m(func pid=162129)[0m f1_per_class: [0.081, 0.228, 0.222, 0.212, 0.041, 0.165, 0.012, 0.336, 0.0, 0.093]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m top1: 0.013059701492537313
[2m[36m(func pid=167216)[0m top5: 0.3726679104477612
[2m[36m(func pid=167216)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=167216)[0m f1_macro: 0.006361956482582652
[2m[36m(func pid=167216)[0m f1_weighted: 0.00244959334411825
[2m[36m(func pid=167216)[0m f1_per_class: [0.02, 0.005, 0.03, 0.0, 0.0, 0.008, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10121268656716417
[2m[36m(func pid=155946)[0m top5: 0.5811567164179104
[2m[36m(func pid=155946)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=155946)[0m f1_macro: 0.059991265695730336
[2m[36m(func pid=155946)[0m f1_weighted: 0.08562147720422546
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.22, 0.051, 0.139, 0.0, 0.0, 0.003, 0.113, 0.0, 0.047]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.024253731343283583
[2m[36m(func pid=167836)[0m top5: 0.6072761194029851
[2m[36m(func pid=167836)[0m f1_micro: 0.024253731343283583
[2m[36m(func pid=167836)[0m f1_macro: 0.00746736230495422
[2m[36m(func pid=167836)[0m f1_weighted: 0.007443183711703316
[2m[36m(func pid=167836)[0m f1_per_class: [0.043, 0.0, 0.0, 0.0, 0.0, 0.016, 0.016, 0.0, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5857 | Steps: 2 | Val loss: 2.1729 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7953 | Steps: 2 | Val loss: 2.2474 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6666 | Steps: 2 | Val loss: 2.2912 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 4.4653 | Steps: 2 | Val loss: 3.7207 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 00:32:09 (running for 00:39:18.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.69  |      0.06  |                   51 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.415 |      0.139 |                   25 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.795 |      0.105 |                    3 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  3.934 |      0.007 |                    1 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.17117537313432835
[2m[36m(func pid=167216)[0m top5: 0.6515858208955224
[2m[36m(func pid=167216)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=167216)[0m f1_macro: 0.1048964821402442
[2m[36m(func pid=167216)[0m f1_weighted: 0.1455771117800794
[2m[36m(func pid=167216)[0m f1_per_class: [0.064, 0.259, 0.112, 0.0, 0.0, 0.374, 0.182, 0.0, 0.058, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.15811567164179105
[2m[36m(func pid=162129)[0m top5: 0.804570895522388
[2m[36m(func pid=162129)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=162129)[0m f1_macro: 0.15004969758727918
[2m[36m(func pid=162129)[0m f1_weighted: 0.1709115108460578
[2m[36m(func pid=162129)[0m f1_per_class: [0.079, 0.204, 0.229, 0.239, 0.042, 0.181, 0.079, 0.357, 0.0, 0.091]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10914179104477612
[2m[36m(func pid=155946)[0m top5: 0.5890858208955224
[2m[36m(func pid=155946)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=155946)[0m f1_macro: 0.062306796058579195
[2m[36m(func pid=155946)[0m f1_weighted: 0.09141783928069824
[2m[36m(func pid=155946)[0m f1_per_class: [0.033, 0.223, 0.052, 0.159, 0.0, 0.0, 0.003, 0.11, 0.0, 0.043]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.11380597014925373
[2m[36m(func pid=167836)[0m top5: 0.5536380597014925
[2m[36m(func pid=167836)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=167836)[0m f1_macro: 0.046406847765322855
[2m[36m(func pid=167836)[0m f1_weighted: 0.030106455904938174
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.005, 0.05, 0.0, 0.167, 0.242, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6027 | Steps: 2 | Val loss: 2.1587 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3314 | Steps: 2 | Val loss: 2.1420 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6252 | Steps: 2 | Val loss: 2.2909 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 6.7088 | Steps: 2 | Val loss: 4.9830 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:32:14 (running for 00:39:23.66)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.667 |      0.062 |                   52 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.586 |      0.15  |                   26 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.603 |      0.126 |                    4 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  4.465 |      0.046 |                    2 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.2019589552238806
[2m[36m(func pid=167216)[0m top5: 0.8899253731343284
[2m[36m(func pid=167216)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=167216)[0m f1_macro: 0.12635670729561083
[2m[36m(func pid=167216)[0m f1_weighted: 0.19843948282886884
[2m[36m(func pid=167216)[0m f1_per_class: [0.123, 0.072, 0.246, 0.125, 0.037, 0.273, 0.388, 0.0, 0.0, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.20848880597014927
[2m[36m(func pid=162129)[0m top5: 0.8386194029850746
[2m[36m(func pid=162129)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=162129)[0m f1_macro: 0.17981666693669202
[2m[36m(func pid=162129)[0m f1_weighted: 0.25027773791843544
[2m[36m(func pid=162129)[0m f1_per_class: [0.092, 0.197, 0.262, 0.288, 0.048, 0.181, 0.308, 0.323, 0.0, 0.1]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.11007462686567164
[2m[36m(func pid=155946)[0m top5: 0.5918843283582089
[2m[36m(func pid=155946)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=155946)[0m f1_macro: 0.06259016958717878
[2m[36m(func pid=155946)[0m f1_weighted: 0.09192820043602981
[2m[36m(func pid=155946)[0m f1_per_class: [0.033, 0.228, 0.052, 0.158, 0.0, 0.0, 0.003, 0.109, 0.0, 0.043]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.06529850746268656
[2m[36m(func pid=167836)[0m top5: 0.46781716417910446
[2m[36m(func pid=167836)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=167836)[0m f1_macro: 0.04691466492845698
[2m[36m(func pid=167836)[0m f1_weighted: 0.01694539567108159
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.041, 0.264, 0.0, 0.0, 0.0, 0.0, 0.116, 0.048, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.5717 | Steps: 2 | Val loss: 2.1994 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3360 | Steps: 2 | Val loss: 2.1243 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.6559 | Steps: 2 | Val loss: 2.2902 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 9.4156 | Steps: 2 | Val loss: 6.3313 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:32:19 (running for 00:39:28.88)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.625 |      0.063 |                   53 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.331 |      0.18  |                   27 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.572 |      0.065 |                    5 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  6.709 |      0.047 |                    3 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.054104477611940295
[2m[36m(func pid=167216)[0m top5: 0.5265858208955224
[2m[36m(func pid=167216)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=167216)[0m f1_macro: 0.06513356112228767
[2m[36m(func pid=167216)[0m f1_weighted: 0.053834983623755134
[2m[36m(func pid=167216)[0m f1_per_class: [0.107, 0.067, 0.174, 0.061, 0.022, 0.163, 0.0, 0.059, 0.0, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.25046641791044777
[2m[36m(func pid=162129)[0m top5: 0.8642723880597015
[2m[36m(func pid=162129)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=162129)[0m f1_macro: 0.18873394188409848
[2m[36m(func pid=162129)[0m f1_weighted: 0.2916423630310229
[2m[36m(func pid=162129)[0m f1_per_class: [0.104, 0.173, 0.275, 0.353, 0.052, 0.157, 0.425, 0.218, 0.023, 0.107]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10914179104477612
[2m[36m(func pid=155946)[0m top5: 0.5956156716417911
[2m[36m(func pid=155946)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=155946)[0m f1_macro: 0.061406669844051395
[2m[36m(func pid=155946)[0m f1_weighted: 0.09019842692780708
[2m[36m(func pid=155946)[0m f1_per_class: [0.029, 0.229, 0.052, 0.151, 0.0, 0.0, 0.003, 0.109, 0.0, 0.04]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.024720149253731342
[2m[36m(func pid=167836)[0m top5: 0.47201492537313433
[2m[36m(func pid=167836)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=167836)[0m f1_macro: 0.01167145784658893
[2m[36m(func pid=167836)[0m f1_weighted: 0.002100543643179038
[2m[36m(func pid=167836)[0m f1_per_class: [0.094, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.4147 | Steps: 2 | Val loss: 2.2947 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3156 | Steps: 2 | Val loss: 2.1120 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6448 | Steps: 2 | Val loss: 2.2904 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 9.8732 | Steps: 2 | Val loss: 7.2660 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:32:25 (running for 00:39:34.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.656 |      0.061 |                   54 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.336 |      0.189 |                   28 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.415 |      0.023 |                    6 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  9.416 |      0.012 |                    4 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.022388059701492536
[2m[36m(func pid=167216)[0m top5: 0.5265858208955224
[2m[36m(func pid=167216)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=167216)[0m f1_macro: 0.023486505996788615
[2m[36m(func pid=167216)[0m f1_weighted: 0.01200491089500278
[2m[36m(func pid=167216)[0m f1_per_class: [0.066, 0.0, 0.0, 0.0, 0.057, 0.087, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.27238805970149255
[2m[36m(func pid=162129)[0m top5: 0.8703358208955224
[2m[36m(func pid=162129)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=162129)[0m f1_macro: 0.18295470611855677
[2m[36m(func pid=162129)[0m f1_weighted: 0.2948661694611767
[2m[36m(func pid=162129)[0m f1_per_class: [0.105, 0.121, 0.268, 0.366, 0.06, 0.122, 0.484, 0.121, 0.022, 0.161]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10680970149253731
[2m[36m(func pid=155946)[0m top5: 0.5984141791044776
[2m[36m(func pid=155946)[0m f1_micro: 0.10680970149253732
[2m[36m(func pid=155946)[0m f1_macro: 0.06112360299620312
[2m[36m(func pid=155946)[0m f1_weighted: 0.08793917464642877
[2m[36m(func pid=155946)[0m f1_per_class: [0.032, 0.232, 0.053, 0.134, 0.0, 0.0, 0.009, 0.11, 0.0, 0.041]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.012593283582089552
[2m[36m(func pid=167836)[0m top5: 0.4925373134328358
[2m[36m(func pid=167836)[0m f1_micro: 0.012593283582089552
[2m[36m(func pid=167836)[0m f1_macro: 0.0033776155075587616
[2m[36m(func pid=167836)[0m f1_weighted: 0.0020783360230037947
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4087 | Steps: 2 | Val loss: 2.3198 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3808 | Steps: 2 | Val loss: 2.1050 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6430 | Steps: 2 | Val loss: 2.2888 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 11.6515 | Steps: 2 | Val loss: 4.0138 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:32:30 (running for 00:39:39.63)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.645 |      0.061 |                   55 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.316 |      0.183 |                   29 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.409 |      0.015 |                    7 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  9.873 |      0.003 |                    5 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.017723880597014924
[2m[36m(func pid=167216)[0m top5: 0.5317164179104478
[2m[36m(func pid=167216)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=167216)[0m f1_macro: 0.015072894311220917
[2m[36m(func pid=167216)[0m f1_weighted: 0.002629641682764558
[2m[36m(func pid=167216)[0m f1_per_class: [0.039, 0.005, 0.0, 0.0, 0.081, 0.0, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.29757462686567165
[2m[36m(func pid=162129)[0m top5: 0.8726679104477612
[2m[36m(func pid=162129)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=162129)[0m f1_macro: 0.1670525037467406
[2m[36m(func pid=162129)[0m f1_weighted: 0.30021551663207563
[2m[36m(func pid=162129)[0m f1_per_class: [0.104, 0.081, 0.224, 0.404, 0.068, 0.092, 0.52, 0.045, 0.0, 0.133]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10494402985074627
[2m[36m(func pid=155946)[0m top5: 0.6030783582089553
[2m[36m(func pid=155946)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=155946)[0m f1_macro: 0.06033392065225054
[2m[36m(func pid=155946)[0m f1_weighted: 0.08636580261014487
[2m[36m(func pid=155946)[0m f1_per_class: [0.03, 0.233, 0.055, 0.125, 0.0, 0.0, 0.012, 0.11, 0.0, 0.038]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.045242537313432835
[2m[36m(func pid=167836)[0m top5: 0.8078358208955224
[2m[36m(func pid=167836)[0m f1_micro: 0.045242537313432835
[2m[36m(func pid=167836)[0m f1_macro: 0.02616252506280754
[2m[36m(func pid=167836)[0m f1_weighted: 0.05055992439750188
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.177, 0.0, 0.0, 0.018, 0.0, 0.067, 0.0, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2466 | Steps: 2 | Val loss: 2.2852 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3098 | Steps: 2 | Val loss: 2.0950 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6425 | Steps: 2 | Val loss: 2.2876 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 9.7388 | Steps: 2 | Val loss: 3.9724 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 00:32:35 (running for 00:39:44.86)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.643 |      0.06  |                   56 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.381 |      0.167 |                   30 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.247 |      0.009 |                    8 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  | 11.651 |      0.026 |                    6 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.013059701492537313
[2m[36m(func pid=167216)[0m top5: 0.5013992537313433
[2m[36m(func pid=167216)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=167216)[0m f1_macro: 0.009136740715086206
[2m[36m(func pid=167216)[0m f1_weighted: 0.002593870054291684
[2m[36m(func pid=167216)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.075, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.32322761194029853
[2m[36m(func pid=162129)[0m top5: 0.8745335820895522
[2m[36m(func pid=162129)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=162129)[0m f1_macro: 0.16732017984425646
[2m[36m(func pid=162129)[0m f1_weighted: 0.3169212267986798
[2m[36m(func pid=162129)[0m f1_per_class: [0.1, 0.086, 0.179, 0.457, 0.064, 0.08, 0.527, 0.046, 0.039, 0.096]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10261194029850747
[2m[36m(func pid=155946)[0m top5: 0.6077425373134329
[2m[36m(func pid=155946)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=155946)[0m f1_macro: 0.06023497306864693
[2m[36m(func pid=155946)[0m f1_weighted: 0.08590610836938993
[2m[36m(func pid=155946)[0m f1_per_class: [0.028, 0.228, 0.058, 0.123, 0.0, 0.0, 0.015, 0.11, 0.0, 0.041]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.16744402985074627
[2m[36m(func pid=167836)[0m top5: 0.5970149253731343
[2m[36m(func pid=167836)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=167836)[0m f1_macro: 0.06690966456460594
[2m[36m(func pid=167836)[0m f1_weighted: 0.1346040430111282
[2m[36m(func pid=167836)[0m f1_per_class: [0.173, 0.0, 0.0, 0.469, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2289 | Steps: 2 | Val loss: 2.1482 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.4832 | Steps: 2 | Val loss: 2.0808 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6199 | Steps: 2 | Val loss: 2.2853 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 9.4501 | Steps: 2 | Val loss: 5.8071 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 00:32:41 (running for 00:39:50.24)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.642 |      0.06  |                   57 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.31  |      0.167 |                   31 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.229 |      0.026 |                    9 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  9.739 |      0.067 |                    7 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.02332089552238806
[2m[36m(func pid=167216)[0m top5: 0.7341417910447762
[2m[36m(func pid=167216)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=167216)[0m f1_macro: 0.025996435497591064
[2m[36m(func pid=167216)[0m f1_weighted: 0.024599168927539508
[2m[36m(func pid=167216)[0m f1_per_class: [0.083, 0.086, 0.0, 0.0, 0.017, 0.0, 0.021, 0.0, 0.053, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.3362873134328358
[2m[36m(func pid=162129)[0m top5: 0.8768656716417911
[2m[36m(func pid=162129)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=162129)[0m f1_macro: 0.16788940014334136
[2m[36m(func pid=162129)[0m f1_weighted: 0.32351508089877473
[2m[36m(func pid=162129)[0m f1_per_class: [0.096, 0.114, 0.179, 0.472, 0.065, 0.06, 0.53, 0.031, 0.02, 0.111]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.1044776119402985
[2m[36m(func pid=155946)[0m top5: 0.6175373134328358
[2m[36m(func pid=155946)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=155946)[0m f1_macro: 0.06124170472845044
[2m[36m(func pid=155946)[0m f1_weighted: 0.08665503214001115
[2m[36m(func pid=155946)[0m f1_per_class: [0.031, 0.231, 0.06, 0.124, 0.0, 0.0, 0.015, 0.11, 0.0, 0.042]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.11893656716417911
[2m[36m(func pid=167836)[0m top5: 0.5774253731343284
[2m[36m(func pid=167836)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=167836)[0m f1_macro: 0.03514020726528061
[2m[36m(func pid=167836)[0m f1_weighted: 0.034679885951659675
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.021, 0.0, 0.0, 0.0, 0.248, 0.0, 0.0, 0.082, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1017 | Steps: 2 | Val loss: 1.9819 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2955 | Steps: 2 | Val loss: 2.0855 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6454 | Steps: 2 | Val loss: 2.2859 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 6.9277 | Steps: 2 | Val loss: 3.6232 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 00:32:46 (running for 00:39:55.57)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.62  |      0.061 |                   58 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.483 |      0.168 |                   32 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.102 |      0.109 |                   10 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  9.45  |      0.035 |                    8 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.19682835820895522
[2m[36m(func pid=167216)[0m top5: 0.9123134328358209
[2m[36m(func pid=167216)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=167216)[0m f1_macro: 0.10866692458094987
[2m[36m(func pid=167216)[0m f1_weighted: 0.21623209364153947
[2m[36m(func pid=167216)[0m f1_per_class: [0.073, 0.173, 0.0, 0.227, 0.037, 0.275, 0.302, 0.0, 0.0, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.332089552238806
[2m[36m(func pid=162129)[0m top5: 0.8698694029850746
[2m[36m(func pid=162129)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=162129)[0m f1_macro: 0.16888101439141243
[2m[36m(func pid=162129)[0m f1_weighted: 0.328241699093774
[2m[36m(func pid=162129)[0m f1_per_class: [0.111, 0.12, 0.163, 0.475, 0.074, 0.092, 0.53, 0.031, 0.0, 0.093]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=155946)[0m top1: 0.1044776119402985
[2m[36m(func pid=155946)[0m top5: 0.6180037313432836
[2m[36m(func pid=155946)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=155946)[0m f1_macro: 0.061778523534280164
[2m[36m(func pid=155946)[0m f1_weighted: 0.08826180815680787
[2m[36m(func pid=155946)[0m f1_per_class: [0.031, 0.228, 0.06, 0.125, 0.0, 0.0, 0.022, 0.11, 0.0, 0.043]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167836)[0m top1: 0.07882462686567164
[2m[36m(func pid=167836)[0m top5: 0.7555970149253731
[2m[36m(func pid=167836)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=167836)[0m f1_macro: 0.025731944140942282
[2m[36m(func pid=167836)[0m f1_weighted: 0.07047617530413482
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.021, 0.0, 0.236, 0.0, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0756 | Steps: 2 | Val loss: 1.9593 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2983 | Steps: 2 | Val loss: 2.0978 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6292 | Steps: 2 | Val loss: 2.2825 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 8.1299 | Steps: 2 | Val loss: 7.6559 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:32:51 (running for 00:40:00.88)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.645 |      0.062 |                   59 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.296 |      0.169 |                   33 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.076 |      0.127 |                   11 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  6.928 |      0.026 |                    9 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.27798507462686567
[2m[36m(func pid=167216)[0m top5: 0.8885261194029851
[2m[36m(func pid=167216)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=167216)[0m f1_macro: 0.12660194712601444
[2m[36m(func pid=167216)[0m f1_weighted: 0.24226406205337842
[2m[36m(func pid=167216)[0m f1_per_class: [0.154, 0.047, 0.0, 0.515, 0.063, 0.318, 0.17, 0.0, 0.0, 0.0]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.30223880597014924
[2m[36m(func pid=162129)[0m top5: 0.8684701492537313
[2m[36m(func pid=162129)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=162129)[0m f1_macro: 0.1658598239850904
[2m[36m(func pid=162129)[0m f1_weighted: 0.31981998903377284
[2m[36m(func pid=162129)[0m f1_per_class: [0.124, 0.176, 0.152, 0.438, 0.072, 0.079, 0.508, 0.031, 0.023, 0.057]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.07695895522388059
[2m[36m(func pid=167836)[0m top5: 0.7765858208955224
[2m[36m(func pid=167836)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=167836)[0m f1_macro: 0.0344169803760674
[2m[36m(func pid=167836)[0m f1_weighted: 0.08970334703158624
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.0, 0.0, 0.321, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10867537313432836
[2m[36m(func pid=155946)[0m top5: 0.6259328358208955
[2m[36m(func pid=155946)[0m f1_micro: 0.10867537313432836
[2m[36m(func pid=155946)[0m f1_macro: 0.06728553232505101
[2m[36m(func pid=155946)[0m f1_weighted: 0.09546711937316538
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.233, 0.064, 0.124, 0.0, 0.0, 0.036, 0.148, 0.0, 0.042]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0432 | Steps: 2 | Val loss: 2.0158 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2941 | Steps: 2 | Val loss: 2.1162 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 7.6224 | Steps: 2 | Val loss: 5.6731 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6484 | Steps: 2 | Val loss: 2.2836 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 00:32:57 (running for 00:40:06.11)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.629 |      0.067 |                   60 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.298 |      0.166 |                   34 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.043 |      0.14  |                   12 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  8.13  |      0.034 |                   10 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.25513059701492535
[2m[36m(func pid=167216)[0m top5: 0.8418843283582089
[2m[36m(func pid=167216)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=167216)[0m f1_macro: 0.14009143499490348
[2m[36m(func pid=167216)[0m f1_weighted: 0.19287741956995438
[2m[36m(func pid=167216)[0m f1_per_class: [0.171, 0.05, 0.0, 0.507, 0.071, 0.286, 0.006, 0.016, 0.0, 0.294]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.26026119402985076
[2m[36m(func pid=162129)[0m top5: 0.8624067164179104
[2m[36m(func pid=162129)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=162129)[0m f1_macro: 0.16065511894412654
[2m[36m(func pid=162129)[0m f1_weighted: 0.29715084423666677
[2m[36m(func pid=162129)[0m f1_per_class: [0.114, 0.221, 0.153, 0.378, 0.073, 0.046, 0.463, 0.11, 0.0, 0.05]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.1646455223880597
[2m[36m(func pid=167836)[0m top5: 0.7868470149253731
[2m[36m(func pid=167836)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=167836)[0m f1_macro: 0.040763816652145915
[2m[36m(func pid=167836)[0m f1_weighted: 0.06968859641426794
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.399, 0.0, 0.0, 0.005, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10541044776119403
[2m[36m(func pid=155946)[0m top5: 0.628731343283582
[2m[36m(func pid=155946)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=155946)[0m f1_macro: 0.06542772573444218
[2m[36m(func pid=155946)[0m f1_weighted: 0.09171916165823876
[2m[36m(func pid=155946)[0m f1_per_class: [0.028, 0.231, 0.065, 0.117, 0.0, 0.0, 0.033, 0.136, 0.0, 0.044]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0235 | Steps: 2 | Val loss: 2.2094 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2961 | Steps: 2 | Val loss: 2.1289 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 5.1928 | Steps: 2 | Val loss: 5.8871 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6123 | Steps: 2 | Val loss: 2.2820 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:33:02 (running for 00:40:11.35)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.648 |      0.065 |                   61 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.294 |      0.161 |                   35 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.023 |      0.093 |                   13 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  7.622 |      0.041 |                   11 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.054104477611940295
[2m[36m(func pid=167216)[0m top5: 0.7700559701492538
[2m[36m(func pid=167216)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=167216)[0m f1_macro: 0.09272842393596123
[2m[36m(func pid=167216)[0m f1_weighted: 0.05063916305357861
[2m[36m(func pid=167216)[0m f1_per_class: [0.14, 0.149, 0.25, 0.01, 0.103, 0.068, 0.0, 0.127, 0.05, 0.03]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.20662313432835822
[2m[36m(func pid=162129)[0m top5: 0.8610074626865671
[2m[36m(func pid=162129)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=162129)[0m f1_macro: 0.15477069375151137
[2m[36m(func pid=162129)[0m f1_weighted: 0.2493408134305952
[2m[36m(func pid=162129)[0m f1_per_class: [0.096, 0.233, 0.168, 0.263, 0.071, 0.016, 0.383, 0.276, 0.0, 0.041]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.12220149253731344
[2m[36m(func pid=167836)[0m top5: 0.6497201492537313
[2m[36m(func pid=167836)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=167836)[0m f1_macro: 0.06355307213496265
[2m[36m(func pid=167836)[0m f1_weighted: 0.035208987448715846
[2m[36m(func pid=167836)[0m f1_per_class: [0.177, 0.0, 0.0, 0.0, 0.125, 0.241, 0.0, 0.0, 0.092, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.10774253731343283
[2m[36m(func pid=155946)[0m top5: 0.6338619402985075
[2m[36m(func pid=155946)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=155946)[0m f1_macro: 0.06758746605013415
[2m[36m(func pid=155946)[0m f1_weighted: 0.09387378401567377
[2m[36m(func pid=155946)[0m f1_per_class: [0.028, 0.236, 0.066, 0.116, 0.0, 0.0, 0.036, 0.148, 0.0, 0.046]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9545 | Steps: 2 | Val loss: 2.4220 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2464 | Steps: 2 | Val loss: 2.1442 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 6.7040 | Steps: 2 | Val loss: 3.4900 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=167216)[0m top1: 0.03264925373134328
[2m[36m(func pid=167216)[0m top5: 0.5200559701492538
[2m[36m(func pid=167216)[0m f1_micro: 0.03264925373134328
[2m[36m(func pid=167216)[0m f1_macro: 0.06895950361568108
[2m[36m(func pid=167216)[0m f1_weighted: 0.014612372577165019
[2m[36m(func pid=167216)[0m f1_per_class: [0.082, 0.005, 0.308, 0.0, 0.066, 0.006, 0.0, 0.091, 0.099, 0.032]
== Status ==
Current time: 2024-01-07 00:33:07 (running for 00:40:16.63)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.612 |      0.068 |                   62 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.296 |      0.155 |                   36 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.955 |      0.069 |                   14 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  5.193 |      0.064 |                   12 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6106 | Steps: 2 | Val loss: 2.2809 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=162129)[0m top1: 0.15951492537313433
[2m[36m(func pid=162129)[0m top5: 0.8390858208955224
[2m[36m(func pid=162129)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=162129)[0m f1_macro: 0.1415224539309825
[2m[36m(func pid=162129)[0m f1_weighted: 0.1908847457874809
[2m[36m(func pid=162129)[0m f1_per_class: [0.078, 0.246, 0.196, 0.15, 0.071, 0.016, 0.274, 0.324, 0.027, 0.034]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.11147388059701492
[2m[36m(func pid=167836)[0m top5: 0.7532649253731343
[2m[36m(func pid=167836)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=167836)[0m f1_macro: 0.08400577658559741
[2m[36m(func pid=167836)[0m f1_weighted: 0.09302156105929714
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.42, 0.0, 0.0, 0.028, 0.0, 0.0, 0.305, 0.087, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.11240671641791045
[2m[36m(func pid=155946)[0m top5: 0.6394589552238806
[2m[36m(func pid=155946)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=155946)[0m f1_macro: 0.07421839041031017
[2m[36m(func pid=155946)[0m f1_weighted: 0.1002003888107014
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.241, 0.068, 0.112, 0.0, 0.016, 0.045, 0.184, 0.0, 0.049]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.1449 | Steps: 2 | Val loss: 2.4171 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.2277 | Steps: 2 | Val loss: 2.1616 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:33:12 (running for 00:40:21.87)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.611 |      0.074 |                   63 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.246 |      0.142 |                   37 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.145 |      0.07  |                   15 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  6.704 |      0.084 |                   13 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.049440298507462684
[2m[36m(func pid=167216)[0m top5: 0.46828358208955223
[2m[36m(func pid=167216)[0m f1_micro: 0.049440298507462684
[2m[36m(func pid=167216)[0m f1_macro: 0.06986387468231518
[2m[36m(func pid=167216)[0m f1_weighted: 0.04244754014646104
[2m[36m(func pid=167216)[0m f1_per_class: [0.107, 0.113, 0.0, 0.0, 0.028, 0.0, 0.0, 0.277, 0.119, 0.055]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 5.6170 | Steps: 2 | Val loss: 5.2540 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6126 | Steps: 2 | Val loss: 2.2826 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=162129)[0m top1: 0.14505597014925373
[2m[36m(func pid=162129)[0m top5: 0.8138992537313433
[2m[36m(func pid=162129)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=162129)[0m f1_macro: 0.1453097433640914
[2m[36m(func pid=162129)[0m f1_weighted: 0.16183605212636176
[2m[36m(func pid=162129)[0m f1_per_class: [0.097, 0.249, 0.265, 0.094, 0.062, 0.023, 0.206, 0.42, 0.0, 0.038]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.18516791044776118
[2m[36m(func pid=167836)[0m top5: 0.820429104477612
[2m[36m(func pid=167836)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=167836)[0m f1_macro: 0.07334146994317883
[2m[36m(func pid=167836)[0m f1_weighted: 0.07547032451142967
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.302, 0.296, 0.003, 0.0, 0.0, 0.055, 0.077, 0.0, 0.0]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9523 | Steps: 2 | Val loss: 2.2764 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=155946)[0m top1: 0.11147388059701492
[2m[36m(func pid=155946)[0m top5: 0.6352611940298507
[2m[36m(func pid=155946)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=155946)[0m f1_macro: 0.07291664120536989
[2m[36m(func pid=155946)[0m f1_weighted: 0.09895921067540554
[2m[36m(func pid=155946)[0m f1_per_class: [0.026, 0.242, 0.068, 0.113, 0.0, 0.016, 0.042, 0.173, 0.0, 0.049]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2614 | Steps: 2 | Val loss: 2.1781 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:33:18 (running for 00:40:27.19)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.613 |      0.073 |                   64 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.228 |      0.145 |                   38 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.952 |      0.092 |                   16 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  5.617 |      0.073 |                   14 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.06763059701492537
[2m[36m(func pid=167216)[0m top5: 0.6972947761194029
[2m[36m(func pid=167216)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=167216)[0m f1_macro: 0.09215170177945584
[2m[36m(func pid=167216)[0m f1_weighted: 0.05925654680726006
[2m[36m(func pid=167216)[0m f1_per_class: [0.043, 0.176, 0.0, 0.0, 0.021, 0.007, 0.0, 0.407, 0.025, 0.242]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 7.3602 | Steps: 2 | Val loss: 5.7697 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6307 | Steps: 2 | Val loss: 2.2825 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=162129)[0m top1: 0.12453358208955224
[2m[36m(func pid=162129)[0m top5: 0.8031716417910447
[2m[36m(func pid=162129)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=162129)[0m f1_macro: 0.1369703945039086
[2m[36m(func pid=162129)[0m f1_weighted: 0.12689125909241897
[2m[36m(func pid=162129)[0m f1_per_class: [0.09, 0.233, 0.314, 0.062, 0.051, 0.031, 0.125, 0.421, 0.0, 0.043]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.04337686567164179
[2m[36m(func pid=167836)[0m top5: 0.8689365671641791
[2m[36m(func pid=167836)[0m f1_micro: 0.04337686567164179
[2m[36m(func pid=167836)[0m f1_macro: 0.08591230426520328
[2m[36m(func pid=167836)[0m f1_weighted: 0.047457754227519966
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.158, 0.312, 0.007, 0.017, 0.0, 0.006, 0.225, 0.0, 0.133]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.0573 | Steps: 2 | Val loss: 2.0020 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=155946)[0m top1: 0.11194029850746269
[2m[36m(func pid=155946)[0m top5: 0.6413246268656716
[2m[36m(func pid=155946)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=155946)[0m f1_macro: 0.0721887371800595
[2m[36m(func pid=155946)[0m f1_weighted: 0.09836899126934623
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.245, 0.068, 0.108, 0.0, 0.016, 0.045, 0.159, 0.0, 0.054]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2408 | Steps: 2 | Val loss: 2.1914 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:33:23 (running for 00:40:32.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.631 |      0.072 |                   65 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.261 |      0.137 |                   39 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  2.057 |      0.179 |                   17 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  7.36  |      0.086 |                   15 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.20102611940298507
[2m[36m(func pid=167216)[0m top5: 0.8166977611940298
[2m[36m(func pid=167216)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=167216)[0m f1_macro: 0.17937475045447326
[2m[36m(func pid=167216)[0m f1_weighted: 0.22755044280866957
[2m[36m(func pid=167216)[0m f1_per_class: [0.085, 0.056, 0.308, 0.479, 0.027, 0.034, 0.172, 0.391, 0.0, 0.242]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6354 | Steps: 2 | Val loss: 9.0631 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.5965 | Steps: 2 | Val loss: 2.2798 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=162129)[0m top1: 0.11333955223880597
[2m[36m(func pid=162129)[0m top5: 0.7896455223880597
[2m[36m(func pid=162129)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=162129)[0m f1_macro: 0.13461033124116645
[2m[36m(func pid=162129)[0m f1_weighted: 0.10256756579256618
[2m[36m(func pid=162129)[0m f1_per_class: [0.085, 0.221, 0.373, 0.059, 0.047, 0.045, 0.045, 0.423, 0.0, 0.047]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.02751865671641791
[2m[36m(func pid=167836)[0m top5: 0.5755597014925373
[2m[36m(func pid=167836)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=167836)[0m f1_macro: 0.073823516429304
[2m[36m(func pid=167836)[0m f1_weighted: 0.015401487683423275
[2m[36m(func pid=167836)[0m f1_per_class: [0.22, 0.0, 0.348, 0.0, 0.0, 0.0, 0.0, 0.147, 0.0, 0.023]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7308 | Steps: 2 | Val loss: 1.8934 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=155946)[0m top1: 0.11707089552238806
[2m[36m(func pid=155946)[0m top5: 0.6469216417910447
[2m[36m(func pid=155946)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=155946)[0m f1_macro: 0.0729320023844641
[2m[36m(func pid=155946)[0m f1_weighted: 0.1042020291307992
[2m[36m(func pid=155946)[0m f1_per_class: [0.031, 0.249, 0.07, 0.116, 0.0, 0.016, 0.06, 0.136, 0.0, 0.053]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3146 | Steps: 2 | Val loss: 2.1966 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:33:29 (running for 00:40:37.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.597 |      0.073 |                   66 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.241 |      0.135 |                   40 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.731 |      0.179 |                   18 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.635 |      0.074 |                   16 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.34048507462686567
[2m[36m(func pid=167216)[0m top5: 0.8157649253731343
[2m[36m(func pid=167216)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=167216)[0m f1_macro: 0.17929127798157032
[2m[36m(func pid=167216)[0m f1_weighted: 0.3339007680786691
[2m[36m(func pid=167216)[0m f1_per_class: [0.0, 0.005, 0.348, 0.522, 0.048, 0.257, 0.518, 0.0, 0.0, 0.094]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 7.1527 | Steps: 2 | Val loss: 5.7499 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.7184 | Steps: 2 | Val loss: 2.2822 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=162129)[0m top1: 0.11333955223880597
[2m[36m(func pid=162129)[0m top5: 0.78125
[2m[36m(func pid=162129)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=162129)[0m f1_macro: 0.13293903979292146
[2m[36m(func pid=162129)[0m f1_weighted: 0.0975954425783463
[2m[36m(func pid=162129)[0m f1_per_class: [0.088, 0.238, 0.373, 0.059, 0.044, 0.031, 0.025, 0.42, 0.0, 0.052]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.8374 | Steps: 2 | Val loss: 1.9724 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=167836)[0m top1: 0.15391791044776118
[2m[36m(func pid=167836)[0m top5: 0.6823694029850746
[2m[36m(func pid=167836)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=167836)[0m f1_macro: 0.15208756090383646
[2m[36m(func pid=167836)[0m f1_weighted: 0.08901464243640009
[2m[36m(func pid=167836)[0m f1_per_class: [0.163, 0.177, 0.348, 0.0, 0.0, 0.229, 0.0, 0.429, 0.0, 0.175]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.11287313432835822
[2m[36m(func pid=155946)[0m top5: 0.644589552238806
[2m[36m(func pid=155946)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=155946)[0m f1_macro: 0.07002345282310035
[2m[36m(func pid=155946)[0m f1_weighted: 0.10118851829664724
[2m[36m(func pid=155946)[0m f1_per_class: [0.027, 0.241, 0.069, 0.122, 0.0, 0.008, 0.054, 0.125, 0.0, 0.056]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.2388 | Steps: 2 | Val loss: 2.1846 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:33:34 (running for 00:40:43.23)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.718 |      0.07  |                   67 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.315 |      0.133 |                   41 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.837 |      0.2   |                   19 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  7.153 |      0.152 |                   17 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.27798507462686567
[2m[36m(func pid=167216)[0m top5: 0.8255597014925373
[2m[36m(func pid=167216)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=167216)[0m f1_macro: 0.2003880050446598
[2m[36m(func pid=167216)[0m f1_weighted: 0.2984356554033573
[2m[36m(func pid=167216)[0m f1_per_class: [0.338, 0.041, 0.333, 0.434, 0.099, 0.273, 0.433, 0.0, 0.0, 0.054]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.7800 | Steps: 2 | Val loss: 4.0403 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6149 | Steps: 2 | Val loss: 2.2801 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=162129)[0m top1: 0.11986940298507463
[2m[36m(func pid=162129)[0m top5: 0.7789179104477612
[2m[36m(func pid=162129)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=162129)[0m f1_macro: 0.13415042038410063
[2m[36m(func pid=162129)[0m f1_weighted: 0.10275775809748752
[2m[36m(func pid=162129)[0m f1_per_class: [0.099, 0.245, 0.373, 0.098, 0.047, 0.024, 0.006, 0.406, 0.0, 0.044]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7771 | Steps: 2 | Val loss: 2.1688 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=167836)[0m top1: 0.2224813432835821
[2m[36m(func pid=167836)[0m top5: 0.8250932835820896
[2m[36m(func pid=167836)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=167836)[0m f1_macro: 0.12943322760119502
[2m[36m(func pid=167836)[0m f1_weighted: 0.11152453403978349
[2m[36m(func pid=167836)[0m f1_per_class: [0.078, 0.424, 0.313, 0.0, 0.057, 0.0, 0.054, 0.309, 0.0, 0.059]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.11707089552238806
[2m[36m(func pid=155946)[0m top5: 0.6459888059701493
[2m[36m(func pid=155946)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=155946)[0m f1_macro: 0.07374211099014435
[2m[36m(func pid=155946)[0m f1_weighted: 0.10649087236834802
[2m[36m(func pid=155946)[0m f1_per_class: [0.031, 0.243, 0.069, 0.131, 0.0, 0.016, 0.057, 0.136, 0.0, 0.056]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2165 | Steps: 2 | Val loss: 2.1706 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=167216)[0m top1: 0.09468283582089553
[2m[36m(func pid=167216)[0m top5: 0.7933768656716418
[2m[36m(func pid=167216)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=167216)[0m f1_macro: 0.1618323910006194
[2m[36m(func pid=167216)[0m f1_weighted: 0.09983532153632893
[2m[36m(func pid=167216)[0m f1_per_class: [0.308, 0.209, 0.4, 0.0, 0.22, 0.015, 0.121, 0.199, 0.117, 0.03]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:33:39 (running for 00:40:48.63)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.615 |      0.074 |                   68 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.239 |      0.134 |                   42 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.777 |      0.162 |                   20 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  3.78  |      0.129 |                   18 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.5768 | Steps: 2 | Val loss: 7.5127 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6288 | Steps: 2 | Val loss: 2.2774 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=162129)[0m top1: 0.13712686567164178
[2m[36m(func pid=162129)[0m top5: 0.7756529850746269
[2m[36m(func pid=162129)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=162129)[0m f1_macro: 0.1367841238929053
[2m[36m(func pid=162129)[0m f1_weighted: 0.12652088377537
[2m[36m(func pid=162129)[0m f1_per_class: [0.112, 0.229, 0.286, 0.198, 0.05, 0.008, 0.003, 0.417, 0.027, 0.038]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.7222 | Steps: 2 | Val loss: 2.2837 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=167836)[0m top1: 0.11753731343283583
[2m[36m(func pid=167836)[0m top5: 0.8624067164179104
[2m[36m(func pid=167836)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=167836)[0m f1_macro: 0.12224719618539359
[2m[36m(func pid=167836)[0m f1_weighted: 0.14407661114432624
[2m[36m(func pid=167836)[0m f1_per_class: [0.044, 0.103, 0.144, 0.34, 0.021, 0.0, 0.012, 0.418, 0.0, 0.14]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.11613805970149253
[2m[36m(func pid=155946)[0m top5: 0.6478544776119403
[2m[36m(func pid=155946)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=155946)[0m f1_macro: 0.07402338886148203
[2m[36m(func pid=155946)[0m f1_weighted: 0.10606704882747102
[2m[36m(func pid=155946)[0m f1_per_class: [0.033, 0.236, 0.073, 0.136, 0.0, 0.016, 0.054, 0.138, 0.0, 0.055]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.1941 | Steps: 2 | Val loss: 2.1576 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 00:33:44 (running for 00:40:53.87)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.629 |      0.074 |                   69 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.217 |      0.137 |                   43 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.722 |      0.182 |                   21 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  4.577 |      0.122 |                   19 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.14878731343283583
[2m[36m(func pid=167216)[0m top5: 0.7159514925373134
[2m[36m(func pid=167216)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=167216)[0m f1_macro: 0.18207865320949163
[2m[36m(func pid=167216)[0m f1_weighted: 0.1151352760942725
[2m[36m(func pid=167216)[0m f1_per_class: [0.242, 0.284, 0.348, 0.0, 0.077, 0.211, 0.006, 0.495, 0.096, 0.061]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 6.1172 | Steps: 2 | Val loss: 5.5154 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6107 | Steps: 2 | Val loss: 2.2736 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=162129)[0m top1: 0.15578358208955223
[2m[36m(func pid=162129)[0m top5: 0.7826492537313433
[2m[36m(func pid=162129)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=162129)[0m f1_macro: 0.13850579774031974
[2m[36m(func pid=162129)[0m f1_weighted: 0.14941400329650817
[2m[36m(func pid=162129)[0m f1_per_class: [0.112, 0.229, 0.227, 0.28, 0.049, 0.008, 0.006, 0.409, 0.025, 0.04]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.6522 | Steps: 2 | Val loss: 2.2046 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=167836)[0m top1: 0.1357276119402985
[2m[36m(func pid=167836)[0m top5: 0.8684701492537313
[2m[36m(func pid=167836)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=167836)[0m f1_macro: 0.10314966146546953
[2m[36m(func pid=167836)[0m f1_weighted: 0.1288217045030646
[2m[36m(func pid=167836)[0m f1_per_class: [0.04, 0.255, 0.232, 0.007, 0.197, 0.0, 0.265, 0.0, 0.0, 0.036]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.11893656716417911
[2m[36m(func pid=155946)[0m top5: 0.6511194029850746
[2m[36m(func pid=155946)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=155946)[0m f1_macro: 0.07565222455791103
[2m[36m(func pid=155946)[0m f1_weighted: 0.10845095768552301
[2m[36m(func pid=155946)[0m f1_per_class: [0.033, 0.241, 0.076, 0.138, 0.0, 0.016, 0.057, 0.138, 0.0, 0.058]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2712 | Steps: 2 | Val loss: 2.1532 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:33:50 (running for 00:40:59.05)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.611 |      0.076 |                   70 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.194 |      0.139 |                   44 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.652 |      0.207 |                   22 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  6.117 |      0.103 |                   20 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.18190298507462688
[2m[36m(func pid=167216)[0m top5: 0.730410447761194
[2m[36m(func pid=167216)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=167216)[0m f1_macro: 0.20724799317557677
[2m[36m(func pid=167216)[0m f1_weighted: 0.13127486316233059
[2m[36m(func pid=167216)[0m f1_per_class: [0.24, 0.323, 0.348, 0.0, 0.05, 0.277, 0.0, 0.53, 0.117, 0.189]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5956 | Steps: 2 | Val loss: 5.7868 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.5932 | Steps: 2 | Val loss: 2.2700 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=162129)[0m top1: 0.16791044776119404
[2m[36m(func pid=162129)[0m top5: 0.7919776119402985
[2m[36m(func pid=162129)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=162129)[0m f1_macro: 0.14312859633076344
[2m[36m(func pid=162129)[0m f1_weighted: 0.16553505250724568
[2m[36m(func pid=162129)[0m f1_per_class: [0.106, 0.228, 0.185, 0.326, 0.049, 0.008, 0.015, 0.417, 0.048, 0.05]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.6827 | Steps: 2 | Val loss: 2.0587 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=167836)[0m top1: 0.13712686567164178
[2m[36m(func pid=167836)[0m top5: 0.6888992537313433
[2m[36m(func pid=167836)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=167836)[0m f1_macro: 0.1668550460473545
[2m[36m(func pid=167836)[0m f1_weighted: 0.15202096732594786
[2m[36m(func pid=167836)[0m f1_per_class: [0.144, 0.074, 0.364, 0.303, 0.0, 0.199, 0.0, 0.409, 0.055, 0.12]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.11893656716417911
[2m[36m(func pid=155946)[0m top5: 0.6557835820895522
[2m[36m(func pid=155946)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=155946)[0m f1_macro: 0.07454399459680087
[2m[36m(func pid=155946)[0m f1_weighted: 0.10745417111255527
[2m[36m(func pid=155946)[0m f1_per_class: [0.034, 0.235, 0.073, 0.148, 0.0, 0.008, 0.051, 0.138, 0.0, 0.06]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3138 | Steps: 2 | Val loss: 2.1477 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:33:55 (running for 00:41:04.32)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.593 |      0.075 |                   71 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.271 |      0.143 |                   45 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.683 |      0.235 |                   23 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.596 |      0.167 |                   21 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.2234141791044776
[2m[36m(func pid=167216)[0m top5: 0.7868470149253731
[2m[36m(func pid=167216)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=167216)[0m f1_macro: 0.23534478796548833
[2m[36m(func pid=167216)[0m f1_weighted: 0.2186016304391316
[2m[36m(func pid=167216)[0m f1_per_class: [0.303, 0.287, 0.381, 0.394, 0.035, 0.154, 0.0, 0.419, 0.199, 0.182]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.9690 | Steps: 2 | Val loss: 4.9787 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.5830 | Steps: 2 | Val loss: 2.2675 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=162129)[0m top1: 0.1884328358208955
[2m[36m(func pid=162129)[0m top5: 0.8013059701492538
[2m[36m(func pid=162129)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=162129)[0m f1_macro: 0.15661152883331983
[2m[36m(func pid=162129)[0m f1_weighted: 0.1913493239239185
[2m[36m(func pid=162129)[0m f1_per_class: [0.124, 0.203, 0.18, 0.398, 0.06, 0.032, 0.034, 0.439, 0.051, 0.045]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8610 | Steps: 2 | Val loss: 1.9775 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=167836)[0m top1: 0.33488805970149255
[2m[36m(func pid=167836)[0m top5: 0.6567164179104478
[2m[36m(func pid=167836)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=167836)[0m f1_macro: 0.22912797164069493
[2m[36m(func pid=167836)[0m f1_weighted: 0.23869887684924965
[2m[36m(func pid=167836)[0m f1_per_class: [0.272, 0.021, 0.4, 0.551, 0.0, 0.35, 0.0, 0.503, 0.098, 0.097]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.12360074626865672
[2m[36m(func pid=155946)[0m top5: 0.6585820895522388
[2m[36m(func pid=155946)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=155946)[0m f1_macro: 0.07792006236110169
[2m[36m(func pid=155946)[0m f1_weighted: 0.11147958213437337
[2m[36m(func pid=155946)[0m f1_per_class: [0.034, 0.244, 0.076, 0.151, 0.0, 0.016, 0.051, 0.148, 0.0, 0.06]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167216)[0m top1: 0.27098880597014924
[2m[36m(func pid=167216)[0m top5: 0.8330223880597015
[2m[36m(func pid=167216)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=167216)[0m f1_macro: 0.19564113851323736
[2m[36m(func pid=167216)[0m f1_weighted: 0.19350842906230933
[2m[36m(func pid=167216)[0m f1_per_class: [0.333, 0.016, 0.386, 0.552, 0.038, 0.0, 0.0, 0.431, 0.0, 0.2]
== Status ==
Current time: 2024-01-07 00:34:00 (running for 00:41:09.72)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.583 |      0.078 |                   72 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.314 |      0.157 |                   46 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.861 |      0.196 |                   24 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.969 |      0.229 |                   22 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1660 | Steps: 2 | Val loss: 2.1317 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.3220 | Steps: 2 | Val loss: 3.6543 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6314 | Steps: 2 | Val loss: 2.2671 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=162129)[0m top1: 0.19263059701492538
[2m[36m(func pid=162129)[0m top5: 0.8036380597014925
[2m[36m(func pid=162129)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=162129)[0m f1_macro: 0.1562976972981506
[2m[36m(func pid=162129)[0m f1_weighted: 0.1990496683624881
[2m[36m(func pid=162129)[0m f1_per_class: [0.117, 0.146, 0.182, 0.422, 0.06, 0.046, 0.068, 0.428, 0.047, 0.048]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.6543 | Steps: 2 | Val loss: 1.9205 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=167836)[0m top1: 0.28218283582089554
[2m[36m(func pid=167836)[0m top5: 0.8138992537313433
[2m[36m(func pid=167836)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=167836)[0m f1_macro: 0.18858356582375824
[2m[36m(func pid=167836)[0m f1_weighted: 0.2647861542012925
[2m[36m(func pid=167836)[0m f1_per_class: [0.083, 0.086, 0.37, 0.231, 0.255, 0.061, 0.536, 0.216, 0.0, 0.048]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=155946)[0m top1: 0.12360074626865672
[2m[36m(func pid=155946)[0m top5: 0.6557835820895522
[2m[36m(func pid=155946)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=155946)[0m f1_macro: 0.07762531750808663
[2m[36m(func pid=155946)[0m f1_weighted: 0.10968345882800715
[2m[36m(func pid=155946)[0m f1_per_class: [0.034, 0.249, 0.075, 0.149, 0.0, 0.0, 0.048, 0.16, 0.0, 0.062]
[2m[36m(func pid=155946)[0m 
== Status ==
Current time: 2024-01-07 00:34:06 (running for 00:41:15.09)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.631 |      0.078 |                   73 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.166 |      0.156 |                   47 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.654 |      0.214 |                   25 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.322 |      0.189 |                   23 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.2751865671641791
[2m[36m(func pid=167216)[0m top5: 0.8689365671641791
[2m[36m(func pid=167216)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=167216)[0m f1_macro: 0.2136376196239847
[2m[36m(func pid=167216)[0m f1_weighted: 0.2359021463410038
[2m[36m(func pid=167216)[0m f1_per_class: [0.279, 0.011, 0.424, 0.533, 0.034, 0.0, 0.152, 0.503, 0.0, 0.2]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1938 | Steps: 2 | Val loss: 2.1223 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.2578 | Steps: 2 | Val loss: 6.3676 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.5822 | Steps: 2 | Val loss: 2.2631 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=162129)[0m top1: 0.20242537313432835
[2m[36m(func pid=162129)[0m top5: 0.8129664179104478
[2m[36m(func pid=162129)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=162129)[0m f1_macro: 0.17170013869269068
[2m[36m(func pid=162129)[0m f1_weighted: 0.22668853691397398
[2m[36m(func pid=162129)[0m f1_per_class: [0.111, 0.095, 0.196, 0.412, 0.065, 0.117, 0.17, 0.412, 0.096, 0.042]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.6067 | Steps: 2 | Val loss: 2.0431 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=167836)[0m top1: 0.08722014925373134
[2m[36m(func pid=167836)[0m top5: 0.715018656716418
[2m[36m(func pid=167836)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=167836)[0m f1_macro: 0.12076841540886178
[2m[36m(func pid=167836)[0m f1_weighted: 0.07544854987215208
[2m[36m(func pid=167836)[0m f1_per_class: [0.044, 0.021, 0.301, 0.095, 0.024, 0.0, 0.031, 0.476, 0.151, 0.064]
[2m[36m(func pid=167836)[0m 
== Status ==
Current time: 2024-01-07 00:34:11 (running for 00:41:20.10)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.582 |      0.079 |                   74 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.194 |      0.172 |                   48 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.654 |      0.214 |                   25 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  3.258 |      0.121 |                   24 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=155946)[0m top1: 0.12639925373134328
[2m[36m(func pid=155946)[0m top5: 0.6637126865671642
[2m[36m(func pid=155946)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=155946)[0m f1_macro: 0.07918614466857786
[2m[36m(func pid=155946)[0m f1_weighted: 0.11215392191630678
[2m[36m(func pid=155946)[0m f1_per_class: [0.034, 0.247, 0.078, 0.159, 0.0, 0.0, 0.048, 0.159, 0.0, 0.067]
[2m[36m(func pid=155946)[0m 
[2m[36m(func pid=167216)[0m top1: 0.25093283582089554
[2m[36m(func pid=167216)[0m top5: 0.8624067164179104
[2m[36m(func pid=167216)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=167216)[0m f1_macro: 0.24321222409946758
[2m[36m(func pid=167216)[0m f1_weighted: 0.30778676263611315
[2m[36m(func pid=167216)[0m f1_per_class: [0.18, 0.136, 0.37, 0.374, 0.037, 0.182, 0.414, 0.404, 0.105, 0.23]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1942 | Steps: 2 | Val loss: 2.1151 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5738 | Steps: 2 | Val loss: 5.6534 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.4611 | Steps: 2 | Val loss: 2.2040 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=155946)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6059 | Steps: 2 | Val loss: 2.2580 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=162129)[0m top1: 0.20569029850746268
[2m[36m(func pid=162129)[0m top5: 0.8208955223880597
[2m[36m(func pid=162129)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=162129)[0m f1_macro: 0.18485055602003073
[2m[36m(func pid=162129)[0m f1_weighted: 0.24334566293373672
[2m[36m(func pid=162129)[0m f1_per_class: [0.129, 0.128, 0.227, 0.343, 0.065, 0.141, 0.26, 0.414, 0.093, 0.049]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.19636194029850745
[2m[36m(func pid=167836)[0m top5: 0.7863805970149254
[2m[36m(func pid=167836)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=167836)[0m f1_macro: 0.1566975719477284
[2m[36m(func pid=167836)[0m f1_weighted: 0.22880712033116343
[2m[36m(func pid=167836)[0m f1_per_class: [0.043, 0.0, 0.173, 0.301, 0.039, 0.0, 0.403, 0.272, 0.135, 0.2]
[2m[36m(func pid=167836)[0m 
== Status ==
Current time: 2024-01-07 00:34:16 (running for 00:41:25.23)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.251
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00020 | RUNNING    | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.582 |      0.079 |                   74 |
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.194 |      0.185 |                   49 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.461 |      0.226 |                   27 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.574 |      0.157 |                   25 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.19916044776119404
[2m[36m(func pid=167216)[0m top5: 0.7779850746268657
[2m[36m(func pid=167216)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=167216)[0m f1_macro: 0.22619393872954946
[2m[36m(func pid=167216)[0m f1_weighted: 0.21621602439387455
[2m[36m(func pid=167216)[0m f1_per_class: [0.268, 0.254, 0.432, 0.0, 0.052, 0.291, 0.34, 0.423, 0.123, 0.079]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=155946)[0m top1: 0.134794776119403
[2m[36m(func pid=155946)[0m top5: 0.6702425373134329
[2m[36m(func pid=155946)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=155946)[0m f1_macro: 0.08211669576614788
[2m[36m(func pid=155946)[0m f1_weighted: 0.12246351449071198
[2m[36m(func pid=155946)[0m f1_per_class: [0.032, 0.254, 0.082, 0.172, 0.0, 0.0, 0.062, 0.183, 0.0, 0.036]
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.1732 | Steps: 2 | Val loss: 2.1074 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0613 | Steps: 2 | Val loss: 3.8985 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.4966 | Steps: 2 | Val loss: 2.2680 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=162129)[0m top1: 0.22901119402985073
[2m[36m(func pid=162129)[0m top5: 0.8194962686567164
[2m[36m(func pid=162129)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=162129)[0m f1_macro: 0.20425327007258648
[2m[36m(func pid=162129)[0m f1_weighted: 0.2768022998102136
[2m[36m(func pid=162129)[0m f1_per_class: [0.136, 0.182, 0.244, 0.282, 0.066, 0.196, 0.377, 0.404, 0.096, 0.059]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.40111940298507465
[2m[36m(func pid=167836)[0m top5: 0.9193097014925373
[2m[36m(func pid=167836)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=167836)[0m f1_macro: 0.2535242658876197
[2m[36m(func pid=167836)[0m f1_weighted: 0.4006190390992687
[2m[36m(func pid=167836)[0m f1_per_class: [0.12, 0.296, 0.177, 0.522, 0.0, 0.454, 0.43, 0.306, 0.0, 0.23]
[2m[36m(func pid=167836)[0m 
== Status ==
Current time: 2024-01-07 00:34:21 (running for 00:41:30.48)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.173 |      0.204 |                   50 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.497 |      0.22  |                   28 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.061 |      0.254 |                   26 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.1912313432835821
[2m[36m(func pid=167216)[0m top5: 0.7551305970149254
[2m[36m(func pid=167216)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=167216)[0m f1_macro: 0.21956077679491304
[2m[36m(func pid=167216)[0m f1_weighted: 0.17368770819324555
[2m[36m(func pid=167216)[0m f1_per_class: [0.294, 0.258, 0.333, 0.0, 0.068, 0.342, 0.155, 0.516, 0.137, 0.092]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1670 | Steps: 2 | Val loss: 2.1004 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.5919 | Steps: 2 | Val loss: 5.5971 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.5215 | Steps: 2 | Val loss: 2.1604 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=162129)[0m top1: 0.2416044776119403
[2m[36m(func pid=162129)[0m top5: 0.8264925373134329
[2m[36m(func pid=162129)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=162129)[0m f1_macro: 0.2063434082411093
[2m[36m(func pid=162129)[0m f1_weighted: 0.2798136686835588
[2m[36m(func pid=162129)[0m f1_per_class: [0.138, 0.206, 0.268, 0.186, 0.06, 0.228, 0.463, 0.335, 0.096, 0.084]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m top1: 0.18470149253731344
[2m[36m(func pid=167216)[0m top5: 0.8138992537313433
[2m[36m(func pid=167216)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=167216)[0m f1_macro: 0.20026235581204133
[2m[36m(func pid=167216)[0m f1_weighted: 0.14142732254572424
[2m[36m(func pid=167216)[0m f1_per_class: [0.286, 0.269, 0.247, 0.016, 0.064, 0.327, 0.037, 0.492, 0.139, 0.126]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:34:26 (running for 00:41:35.61)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.167 |      0.206 |                   51 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.521 |      0.2   |                   29 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.592 |      0.132 |                   27 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.2150186567164179
[2m[36m(func pid=167836)[0m top5: 0.8647388059701493
[2m[36m(func pid=167836)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=167836)[0m f1_macro: 0.1320847693235104
[2m[36m(func pid=167836)[0m f1_weighted: 0.10226698020531737
[2m[36m(func pid=167836)[0m f1_per_class: [0.116, 0.372, 0.25, 0.003, 0.0, 0.031, 0.006, 0.467, 0.0, 0.074]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1397 | Steps: 2 | Val loss: 2.0956 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4408 | Steps: 2 | Val loss: 2.0586 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5668 | Steps: 2 | Val loss: 4.6153 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=162129)[0m top1: 0.2513992537313433
[2m[36m(func pid=162129)[0m top5: 0.8330223880597015
[2m[36m(func pid=162129)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=162129)[0m f1_macro: 0.20828946221303438
[2m[36m(func pid=162129)[0m f1_weighted: 0.28420107733004635
[2m[36m(func pid=162129)[0m f1_per_class: [0.133, 0.234, 0.268, 0.17, 0.058, 0.263, 0.47, 0.303, 0.087, 0.097]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m top1: 0.2905783582089552
[2m[36m(func pid=167216)[0m top5: 0.8460820895522388
[2m[36m(func pid=167216)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=167216)[0m f1_macro: 0.2378309561186786
[2m[36m(func pid=167216)[0m f1_weighted: 0.24306923537652106
[2m[36m(func pid=167216)[0m f1_per_class: [0.211, 0.121, 0.272, 0.491, 0.086, 0.373, 0.003, 0.488, 0.13, 0.203]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:34:32 (running for 00:41:41.22)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.14  |      0.208 |                   52 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.441 |      0.238 |                   30 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.567 |      0.167 |                   28 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.22154850746268656
[2m[36m(func pid=167836)[0m top5: 0.9043843283582089
[2m[36m(func pid=167836)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=167836)[0m f1_macro: 0.16685263805286787
[2m[36m(func pid=167836)[0m f1_weighted: 0.24658172904617265
[2m[36m(func pid=167836)[0m f1_per_class: [0.087, 0.375, 0.212, 0.415, 0.0, 0.008, 0.158, 0.176, 0.097, 0.14]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1532 | Steps: 2 | Val loss: 2.0954 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.5354 | Steps: 2 | Val loss: 2.0465 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.8045 | Steps: 2 | Val loss: 4.8284 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=162129)[0m top1: 0.25886194029850745
[2m[36m(func pid=162129)[0m top5: 0.8376865671641791
[2m[36m(func pid=162129)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=162129)[0m f1_macro: 0.2046690119357843
[2m[36m(func pid=162129)[0m f1_weighted: 0.28530927202554823
[2m[36m(func pid=162129)[0m f1_per_class: [0.121, 0.244, 0.239, 0.156, 0.061, 0.284, 0.48, 0.292, 0.056, 0.114]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m top1: 0.27611940298507465
[2m[36m(func pid=167216)[0m top5: 0.8558768656716418
[2m[36m(func pid=167216)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=167216)[0m f1_macro: 0.22270427178138763
[2m[36m(func pid=167216)[0m f1_weighted: 0.22369638604620393
[2m[36m(func pid=167216)[0m f1_per_class: [0.159, 0.042, 0.392, 0.492, 0.075, 0.366, 0.003, 0.48, 0.0, 0.218]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:34:37 (running for 00:41:46.81)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.153 |      0.205 |                   53 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.535 |      0.223 |                   31 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.804 |      0.173 |                   29 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.3460820895522388
[2m[36m(func pid=167836)[0m top5: 0.8488805970149254
[2m[36m(func pid=167836)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=167836)[0m f1_macro: 0.17250147489762802
[2m[36m(func pid=167836)[0m f1_weighted: 0.3211810310214512
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.0, 0.216, 0.566, 0.062, 0.129, 0.471, 0.0, 0.142, 0.139]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1498 | Steps: 2 | Val loss: 2.0907 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.8257 | Steps: 2 | Val loss: 2.0591 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6389 | Steps: 2 | Val loss: 6.1705 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=162129)[0m top1: 0.2635261194029851
[2m[36m(func pid=162129)[0m top5: 0.8353544776119403
[2m[36m(func pid=162129)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=162129)[0m f1_macro: 0.20494621083212855
[2m[36m(func pid=162129)[0m f1_weighted: 0.287660838684765
[2m[36m(func pid=162129)[0m f1_per_class: [0.112, 0.257, 0.229, 0.157, 0.064, 0.298, 0.475, 0.305, 0.02, 0.132]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m top1: 0.27052238805970147
[2m[36m(func pid=167216)[0m top5: 0.8530783582089553
[2m[36m(func pid=167216)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=167216)[0m f1_macro: 0.214432164992748
[2m[36m(func pid=167216)[0m f1_weighted: 0.21407944689570121
[2m[36m(func pid=167216)[0m f1_per_class: [0.177, 0.0, 0.424, 0.504, 0.063, 0.328, 0.0, 0.469, 0.0, 0.179]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:34:43 (running for 00:41:52.45)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.15  |      0.205 |                   54 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.826 |      0.214 |                   32 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.639 |      0.139 |                   30 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.11194029850746269
[2m[36m(func pid=167836)[0m top5: 0.7588619402985075
[2m[36m(func pid=167836)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=167836)[0m f1_macro: 0.13948445733857187
[2m[36m(func pid=167836)[0m f1_weighted: 0.0752681707233644
[2m[36m(func pid=167836)[0m f1_per_class: [0.357, 0.052, 0.31, 0.066, 0.033, 0.173, 0.0, 0.301, 0.0, 0.103]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.5837 | Steps: 2 | Val loss: 1.9291 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.1555 | Steps: 2 | Val loss: 2.0926 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8879 | Steps: 2 | Val loss: 4.3444 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=167216)[0m top1: 0.310634328358209
[2m[36m(func pid=167216)[0m top5: 0.8600746268656716
[2m[36m(func pid=167216)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=167216)[0m f1_macro: 0.24642116634893538
[2m[36m(func pid=167216)[0m f1_weighted: 0.2540276240757316
[2m[36m(func pid=167216)[0m f1_per_class: [0.297, 0.021, 0.417, 0.545, 0.054, 0.283, 0.079, 0.493, 0.069, 0.206]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.26072761194029853
[2m[36m(func pid=162129)[0m top5: 0.8409514925373134
[2m[36m(func pid=162129)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=162129)[0m f1_macro: 0.20105052639699966
[2m[36m(func pid=162129)[0m f1_weighted: 0.2793559768698331
[2m[36m(func pid=162129)[0m f1_per_class: [0.11, 0.265, 0.234, 0.126, 0.07, 0.305, 0.47, 0.309, 0.0, 0.12]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:34:48 (running for 00:41:57.85)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.156 |      0.201 |                   55 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.584 |      0.246 |                   33 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.888 |      0.258 |                   31 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.3306902985074627
[2m[36m(func pid=167836)[0m top5: 0.8302238805970149
[2m[36m(func pid=167836)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=167836)[0m f1_macro: 0.25817394638966834
[2m[36m(func pid=167836)[0m f1_weighted: 0.2658882570368299
[2m[36m(func pid=167836)[0m f1_per_class: [0.294, 0.455, 0.377, 0.0, 0.118, 0.378, 0.36, 0.455, 0.0, 0.145]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1195 | Steps: 2 | Val loss: 2.0949 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5429 | Steps: 2 | Val loss: 1.9179 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.9875 | Steps: 2 | Val loss: 3.7893 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=167216)[0m top1: 0.31296641791044777
[2m[36m(func pid=167216)[0m top5: 0.867070895522388
[2m[36m(func pid=167216)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=167216)[0m f1_macro: 0.2562154736912736
[2m[36m(func pid=167216)[0m f1_weighted: 0.32611721220139095
[2m[36m(func pid=167216)[0m f1_per_class: [0.125, 0.307, 0.275, 0.477, 0.065, 0.344, 0.208, 0.467, 0.168, 0.126]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.25326492537313433
[2m[36m(func pid=162129)[0m top5: 0.8446828358208955
[2m[36m(func pid=162129)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=162129)[0m f1_macro: 0.20407021215291735
[2m[36m(func pid=162129)[0m f1_weighted: 0.2685273318006537
[2m[36m(func pid=162129)[0m f1_per_class: [0.124, 0.271, 0.256, 0.097, 0.066, 0.292, 0.459, 0.319, 0.0, 0.158]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:34:54 (running for 00:42:03.32)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.12  |      0.204 |                   56 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.543 |      0.256 |                   34 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.987 |      0.186 |                   32 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.3824626865671642
[2m[36m(func pid=167836)[0m top5: 0.9510261194029851
[2m[36m(func pid=167836)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=167836)[0m f1_macro: 0.18597939211006514
[2m[36m(func pid=167836)[0m f1_weighted: 0.31537460016929314
[2m[36m(func pid=167836)[0m f1_per_class: [0.23, 0.319, 0.25, 0.509, 0.0, 0.063, 0.345, 0.016, 0.0, 0.129]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.5046 | Steps: 2 | Val loss: 1.9836 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1827 | Steps: 2 | Val loss: 2.0967 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.5441 | Steps: 2 | Val loss: 5.9503 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=167216)[0m top1: 0.26725746268656714
[2m[36m(func pid=167216)[0m top5: 0.8600746268656716
[2m[36m(func pid=167216)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=167216)[0m f1_macro: 0.22721297347484687
[2m[36m(func pid=167216)[0m f1_weighted: 0.25725347188023273
[2m[36m(func pid=167216)[0m f1_per_class: [0.152, 0.315, 0.234, 0.01, 0.069, 0.347, 0.412, 0.47, 0.128, 0.137]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.24720149253731344
[2m[36m(func pid=162129)[0m top5: 0.835820895522388
[2m[36m(func pid=162129)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=162129)[0m f1_macro: 0.20683060706275533
[2m[36m(func pid=162129)[0m f1_weighted: 0.26199619762686116
[2m[36m(func pid=162129)[0m f1_per_class: [0.121, 0.266, 0.293, 0.071, 0.062, 0.304, 0.452, 0.357, 0.0, 0.141]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:34:59 (running for 00:42:08.88)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.183 |      0.207 |                   57 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.505 |      0.227 |                   35 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.544 |      0.224 |                   33 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.1912313432835821
[2m[36m(func pid=167836)[0m top5: 0.894589552238806
[2m[36m(func pid=167836)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=167836)[0m f1_macro: 0.22409809667169683
[2m[36m(func pid=167836)[0m f1_weighted: 0.22443145445812532
[2m[36m(func pid=167836)[0m f1_per_class: [0.044, 0.251, 0.2, 0.188, 0.323, 0.453, 0.162, 0.327, 0.091, 0.202]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.4838 | Steps: 2 | Val loss: 2.0530 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2514 | Steps: 2 | Val loss: 2.1026 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.1577 | Steps: 2 | Val loss: 7.0249 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=167216)[0m top1: 0.26026119402985076
[2m[36m(func pid=167216)[0m top5: 0.8507462686567164
[2m[36m(func pid=167216)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=167216)[0m f1_macro: 0.22850211928333888
[2m[36m(func pid=167216)[0m f1_weighted: 0.24478527953937904
[2m[36m(func pid=167216)[0m f1_per_class: [0.172, 0.307, 0.253, 0.003, 0.075, 0.372, 0.376, 0.449, 0.075, 0.203]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.24067164179104478
[2m[36m(func pid=162129)[0m top5: 0.8395522388059702
[2m[36m(func pid=162129)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=162129)[0m f1_macro: 0.20317236988814158
[2m[36m(func pid=162129)[0m f1_weighted: 0.25983485318553495
[2m[36m(func pid=162129)[0m f1_per_class: [0.117, 0.259, 0.297, 0.088, 0.063, 0.3, 0.439, 0.321, 0.023, 0.124]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:35:05 (running for 00:42:14.28)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.251 |      0.203 |                   58 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.484 |      0.229 |                   36 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.158 |      0.178 |                   34 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.18190298507462688
[2m[36m(func pid=167836)[0m top5: 0.6907649253731343
[2m[36m(func pid=167836)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=167836)[0m f1_macro: 0.17848952739932547
[2m[36m(func pid=167836)[0m f1_weighted: 0.16773861998158202
[2m[36m(func pid=167836)[0m f1_per_class: [0.053, 0.323, 0.154, 0.0, 0.081, 0.38, 0.126, 0.451, 0.0, 0.217]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.4744 | Steps: 2 | Val loss: 2.0962 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.1529 | Steps: 2 | Val loss: 2.1100 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.0004 | Steps: 2 | Val loss: 6.0168 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=167216)[0m top1: 0.24067164179104478
[2m[36m(func pid=167216)[0m top5: 0.8652052238805971
[2m[36m(func pid=167216)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=167216)[0m f1_macro: 0.22246862551869717
[2m[36m(func pid=167216)[0m f1_weighted: 0.23911202403025533
[2m[36m(func pid=167216)[0m f1_per_class: [0.111, 0.3, 0.265, 0.048, 0.077, 0.372, 0.32, 0.491, 0.026, 0.215]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.24067164179104478
[2m[36m(func pid=162129)[0m top5: 0.8334888059701493
[2m[36m(func pid=162129)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=162129)[0m f1_macro: 0.21475776209911665
[2m[36m(func pid=162129)[0m f1_weighted: 0.2663347921925638
[2m[36m(func pid=162129)[0m f1_per_class: [0.109, 0.266, 0.344, 0.119, 0.067, 0.303, 0.415, 0.373, 0.047, 0.105]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:35:10 (running for 00:42:19.85)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.153 |      0.215 |                   59 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.474 |      0.222 |                   37 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1     |      0.194 |                   35 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.30363805970149255
[2m[36m(func pid=167836)[0m top5: 0.7714552238805971
[2m[36m(func pid=167836)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=167836)[0m f1_macro: 0.19368779519534396
[2m[36m(func pid=167836)[0m f1_weighted: 0.27282289815565175
[2m[36m(func pid=167836)[0m f1_per_class: [0.102, 0.506, 0.188, 0.0, 0.061, 0.016, 0.525, 0.374, 0.0, 0.163]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.3229 | Steps: 2 | Val loss: 2.0534 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.1425 | Steps: 2 | Val loss: 2.1045 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9662 | Steps: 2 | Val loss: 3.9811 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=167216)[0m top1: 0.28171641791044777
[2m[36m(func pid=167216)[0m top5: 0.8782649253731343
[2m[36m(func pid=167216)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=167216)[0m f1_macro: 0.2576093384179655
[2m[36m(func pid=167216)[0m f1_weighted: 0.31529137949833214
[2m[36m(func pid=167216)[0m f1_per_class: [0.107, 0.338, 0.193, 0.349, 0.067, 0.363, 0.26, 0.5, 0.158, 0.241]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m top1: 0.23087686567164178
[2m[36m(func pid=162129)[0m top5: 0.8330223880597015
[2m[36m(func pid=162129)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=162129)[0m f1_macro: 0.21682793118626278
[2m[36m(func pid=162129)[0m f1_weighted: 0.25764591842224643
[2m[36m(func pid=162129)[0m f1_per_class: [0.111, 0.271, 0.367, 0.135, 0.062, 0.302, 0.365, 0.371, 0.061, 0.122]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167836)[0m top1: 0.373134328358209
[2m[36m(func pid=167836)[0m top5: 0.894589552238806
[2m[36m(func pid=167836)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=167836)[0m f1_macro: 0.250232217317428
[2m[36m(func pid=167836)[0m f1_weighted: 0.3301681096050144
[2m[36m(func pid=167836)[0m f1_per_class: [0.078, 0.524, 0.196, 0.487, 0.109, 0.327, 0.108, 0.502, 0.0, 0.17]
== Status ==
Current time: 2024-01-07 00:35:16 (running for 00:42:25.39)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.142 |      0.217 |                   60 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.323 |      0.258 |                   38 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.966 |      0.25  |                   36 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2772 | Steps: 2 | Val loss: 2.0074 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.0903 | Steps: 2 | Val loss: 2.1025 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=167216)[0m top1: 0.32136194029850745
[2m[36m(func pid=167216)[0m top5: 0.8731343283582089
[2m[36m(func pid=167216)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=167216)[0m f1_macro: 0.2619063590943509
[2m[36m(func pid=167216)[0m f1_weighted: 0.3529481856760048
[2m[36m(func pid=167216)[0m f1_per_class: [0.17, 0.305, 0.164, 0.495, 0.059, 0.276, 0.302, 0.512, 0.12, 0.215]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2897 | Steps: 2 | Val loss: 3.7916 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=162129)[0m top1: 0.23507462686567165
[2m[36m(func pid=162129)[0m top5: 0.8334888059701493
[2m[36m(func pid=162129)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=162129)[0m f1_macro: 0.2132130393084028
[2m[36m(func pid=162129)[0m f1_weighted: 0.2649897518398204
[2m[36m(func pid=162129)[0m f1_per_class: [0.124, 0.263, 0.328, 0.176, 0.063, 0.284, 0.366, 0.364, 0.048, 0.115]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:35:21 (running for 00:42:30.81)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.09  |      0.213 |                   61 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.277 |      0.262 |                   39 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.29  |      0.282 |                   37 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.38992537313432835
[2m[36m(func pid=167836)[0m top5: 0.914179104477612
[2m[36m(func pid=167836)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=167836)[0m f1_macro: 0.28209441847247285
[2m[36m(func pid=167836)[0m f1_weighted: 0.3483128626815842
[2m[36m(func pid=167836)[0m f1_per_class: [0.044, 0.368, 0.224, 0.546, 0.224, 0.457, 0.15, 0.4, 0.184, 0.222]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.2881 | Steps: 2 | Val loss: 1.9789 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.0899 | Steps: 2 | Val loss: 2.0948 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=167216)[0m top1: 0.3358208955223881
[2m[36m(func pid=167216)[0m top5: 0.8768656716417911
[2m[36m(func pid=167216)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=167216)[0m f1_macro: 0.25540914641993007
[2m[36m(func pid=167216)[0m f1_weighted: 0.35468566793815565
[2m[36m(func pid=167216)[0m f1_per_class: [0.194, 0.368, 0.216, 0.506, 0.059, 0.055, 0.344, 0.507, 0.128, 0.178]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3800 | Steps: 2 | Val loss: 3.7488 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=162129)[0m top1: 0.2453358208955224
[2m[36m(func pid=162129)[0m top5: 0.8316231343283582
[2m[36m(func pid=162129)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=162129)[0m f1_macro: 0.2146535997998043
[2m[36m(func pid=162129)[0m f1_weighted: 0.2847035542936381
[2m[36m(func pid=162129)[0m f1_per_class: [0.127, 0.244, 0.272, 0.241, 0.063, 0.28, 0.381, 0.371, 0.084, 0.084]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:35:27 (running for 00:42:36.35)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.09  |      0.215 |                   62 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.288 |      0.255 |                   40 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.38  |      0.278 |                   38 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.29524253731343286
[2m[36m(func pid=167836)[0m top5: 0.8927238805970149
[2m[36m(func pid=167836)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=167836)[0m f1_macro: 0.278116494217266
[2m[36m(func pid=167836)[0m f1_weighted: 0.2967976297039533
[2m[36m(func pid=167836)[0m f1_per_class: [0.273, 0.507, 0.256, 0.083, 0.188, 0.423, 0.353, 0.291, 0.122, 0.286]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.2189 | Steps: 2 | Val loss: 1.9433 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1245 | Steps: 2 | Val loss: 2.0976 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=167216)[0m top1: 0.33675373134328357
[2m[36m(func pid=167216)[0m top5: 0.871268656716418
[2m[36m(func pid=167216)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=167216)[0m f1_macro: 0.26471245591697345
[2m[36m(func pid=167216)[0m f1_weighted: 0.3410016476892341
[2m[36m(func pid=167216)[0m f1_per_class: [0.209, 0.387, 0.286, 0.494, 0.074, 0.223, 0.242, 0.477, 0.084, 0.171]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3269 | Steps: 2 | Val loss: 4.1295 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=162129)[0m top1: 0.23833955223880596
[2m[36m(func pid=162129)[0m top5: 0.8269589552238806
[2m[36m(func pid=162129)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=162129)[0m f1_macro: 0.20621770074490434
[2m[36m(func pid=162129)[0m f1_weighted: 0.2818249352567303
[2m[36m(func pid=162129)[0m f1_per_class: [0.118, 0.225, 0.22, 0.27, 0.06, 0.247, 0.368, 0.376, 0.09, 0.089]
[2m[36m(func pid=162129)[0m 
== Status ==
Current time: 2024-01-07 00:35:32 (running for 00:42:41.72)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.125 |      0.206 |                   63 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.219 |      0.265 |                   41 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.327 |      0.277 |                   39 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.1752 | Steps: 2 | Val loss: 1.9077 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=167836)[0m top1: 0.3516791044776119
[2m[36m(func pid=167836)[0m top5: 0.7845149253731343
[2m[36m(func pid=167836)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=167836)[0m f1_macro: 0.2766095230568092
[2m[36m(func pid=167836)[0m f1_weighted: 0.3272102248742173
[2m[36m(func pid=167836)[0m f1_per_class: [0.157, 0.497, 0.333, 0.0, 0.115, 0.386, 0.553, 0.345, 0.104, 0.276]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1229 | Steps: 2 | Val loss: 2.1152 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=167216)[0m top1: 0.34328358208955223
[2m[36m(func pid=167216)[0m top5: 0.8815298507462687
[2m[36m(func pid=167216)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=167216)[0m f1_macro: 0.28216957923722563
[2m[36m(func pid=167216)[0m f1_weighted: 0.3493704345131868
[2m[36m(func pid=167216)[0m f1_per_class: [0.185, 0.371, 0.349, 0.484, 0.094, 0.382, 0.229, 0.477, 0.072, 0.178]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4258 | Steps: 2 | Val loss: 4.2969 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=162129)[0m top1: 0.2150186567164179
[2m[36m(func pid=162129)[0m top5: 0.8176305970149254
[2m[36m(func pid=162129)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=162129)[0m f1_macro: 0.18656497404172015
[2m[36m(func pid=162129)[0m f1_weighted: 0.25611043017307955
[2m[36m(func pid=162129)[0m f1_per_class: [0.118, 0.219, 0.171, 0.285, 0.054, 0.202, 0.295, 0.344, 0.097, 0.081]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.2963 | Steps: 2 | Val loss: 1.8842 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:35:38 (running for 00:42:47.21)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.123 |      0.187 |                   64 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.175 |      0.282 |                   42 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.426 |      0.249 |                   40 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.2891791044776119
[2m[36m(func pid=167836)[0m top5: 0.8544776119402985
[2m[36m(func pid=167836)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=167836)[0m f1_macro: 0.24910596541303417
[2m[36m(func pid=167836)[0m f1_weighted: 0.29888065058203617
[2m[36m(func pid=167836)[0m f1_per_class: [0.11, 0.507, 0.4, 0.042, 0.078, 0.249, 0.462, 0.445, 0.024, 0.174]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.1156 | Steps: 2 | Val loss: 2.1181 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=167216)[0m top1: 0.3694029850746269
[2m[36m(func pid=167216)[0m top5: 0.8880597014925373
[2m[36m(func pid=167216)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=167216)[0m f1_macro: 0.3111964099435961
[2m[36m(func pid=167216)[0m f1_weighted: 0.39703182370552736
[2m[36m(func pid=167216)[0m f1_per_class: [0.192, 0.384, 0.361, 0.46, 0.109, 0.4, 0.385, 0.494, 0.128, 0.198]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1861 | Steps: 2 | Val loss: 3.6637 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=162129)[0m top1: 0.19916044776119404
[2m[36m(func pid=162129)[0m top5: 0.8138992537313433
[2m[36m(func pid=162129)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=162129)[0m f1_macro: 0.17290578337199236
[2m[36m(func pid=162129)[0m f1_weighted: 0.233226271184737
[2m[36m(func pid=162129)[0m f1_per_class: [0.116, 0.225, 0.159, 0.311, 0.048, 0.147, 0.215, 0.338, 0.086, 0.084]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1957 | Steps: 2 | Val loss: 1.8663 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 00:35:43 (running for 00:42:52.82)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.116 |      0.173 |                   65 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.296 |      0.311 |                   43 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.186 |      0.296 |                   41 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.363339552238806
[2m[36m(func pid=167836)[0m top5: 0.8973880597014925
[2m[36m(func pid=167836)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=167836)[0m f1_macro: 0.2957226001036141
[2m[36m(func pid=167836)[0m f1_weighted: 0.3797716564850129
[2m[36m(func pid=167836)[0m f1_per_class: [0.168, 0.497, 0.407, 0.488, 0.096, 0.36, 0.271, 0.463, 0.022, 0.185]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.1114 | Steps: 2 | Val loss: 2.1292 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=167216)[0m top1: 0.363339552238806
[2m[36m(func pid=167216)[0m top5: 0.898320895522388
[2m[36m(func pid=167216)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=167216)[0m f1_macro: 0.30399186505126496
[2m[36m(func pid=167216)[0m f1_weighted: 0.3957989764897742
[2m[36m(func pid=167216)[0m f1_per_class: [0.187, 0.334, 0.4, 0.39, 0.095, 0.417, 0.488, 0.387, 0.147, 0.195]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1792 | Steps: 2 | Val loss: 3.6640 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=162129)[0m top1: 0.19449626865671643
[2m[36m(func pid=162129)[0m top5: 0.8138992537313433
[2m[36m(func pid=162129)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=162129)[0m f1_macro: 0.16493352762404043
[2m[36m(func pid=162129)[0m f1_weighted: 0.22068281364988565
[2m[36m(func pid=162129)[0m f1_per_class: [0.137, 0.227, 0.145, 0.337, 0.046, 0.087, 0.168, 0.349, 0.07, 0.083]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1542 | Steps: 2 | Val loss: 1.8837 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 00:35:49 (running for 00:42:58.30)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.111 |      0.165 |                   66 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.196 |      0.304 |                   44 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.179 |      0.287 |                   42 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.3903917910447761
[2m[36m(func pid=167836)[0m top5: 0.8997201492537313
[2m[36m(func pid=167836)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=167836)[0m f1_macro: 0.28701450667781764
[2m[36m(func pid=167836)[0m f1_weighted: 0.369867463195112
[2m[36m(func pid=167836)[0m f1_per_class: [0.252, 0.499, 0.415, 0.521, 0.135, 0.383, 0.239, 0.188, 0.061, 0.176]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.0803 | Steps: 2 | Val loss: 2.1292 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=167216)[0m top1: 0.3530783582089552
[2m[36m(func pid=167216)[0m top5: 0.9006529850746269
[2m[36m(func pid=167216)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=167216)[0m f1_macro: 0.29588395134463275
[2m[36m(func pid=167216)[0m f1_weighted: 0.3845505798100821
[2m[36m(func pid=167216)[0m f1_per_class: [0.178, 0.265, 0.415, 0.392, 0.085, 0.414, 0.49, 0.394, 0.13, 0.196]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2084 | Steps: 2 | Val loss: 3.4470 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=162129)[0m top1: 0.19076492537313433
[2m[36m(func pid=162129)[0m top5: 0.8134328358208955
[2m[36m(func pid=162129)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=162129)[0m f1_macro: 0.15844997484589585
[2m[36m(func pid=162129)[0m f1_weighted: 0.2161391202932948
[2m[36m(func pid=162129)[0m f1_per_class: [0.13, 0.21, 0.147, 0.339, 0.043, 0.024, 0.186, 0.346, 0.07, 0.088]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.2967 | Steps: 2 | Val loss: 1.9367 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=167836)[0m top1: 0.36380597014925375
[2m[36m(func pid=167836)[0m top5: 0.9053171641791045
[2m[36m(func pid=167836)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=167836)[0m f1_macro: 0.292075803614307
[2m[36m(func pid=167836)[0m f1_weighted: 0.37809355569052144
[2m[36m(func pid=167836)[0m f1_per_class: [0.242, 0.542, 0.415, 0.437, 0.103, 0.362, 0.321, 0.188, 0.149, 0.162]
== Status ==
Current time: 2024-01-07 00:35:54 (running for 00:43:03.74)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.08  |      0.158 |                   67 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.154 |      0.296 |                   45 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.208 |      0.292 |                   43 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1358 | Steps: 2 | Val loss: 2.1265 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=167216)[0m top1: 0.292910447761194
[2m[36m(func pid=167216)[0m top5: 0.8889925373134329
[2m[36m(func pid=167216)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=167216)[0m f1_macro: 0.2809420975083322
[2m[36m(func pid=167216)[0m f1_weighted: 0.33013875769818796
[2m[36m(func pid=167216)[0m f1_per_class: [0.269, 0.267, 0.361, 0.303, 0.069, 0.382, 0.386, 0.466, 0.105, 0.203]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0940 | Steps: 2 | Val loss: 3.6369 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=162129)[0m top1: 0.19962686567164178
[2m[36m(func pid=162129)[0m top5: 0.8208955223880597
[2m[36m(func pid=162129)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=162129)[0m f1_macro: 0.16470056899029564
[2m[36m(func pid=162129)[0m f1_weighted: 0.2247781740267167
[2m[36m(func pid=162129)[0m f1_per_class: [0.129, 0.205, 0.151, 0.354, 0.042, 0.008, 0.205, 0.355, 0.096, 0.102]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1587 | Steps: 2 | Val loss: 1.9451 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:36:00 (running for 00:43:09.37)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.136 |      0.165 |                   68 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.297 |      0.281 |                   46 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.094 |      0.277 |                   44 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.30130597014925375
[2m[36m(func pid=167836)[0m top5: 0.8656716417910447
[2m[36m(func pid=167836)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=167836)[0m f1_macro: 0.2774835526558053
[2m[36m(func pid=167836)[0m f1_weighted: 0.289334056238642
[2m[36m(func pid=167836)[0m f1_per_class: [0.234, 0.504, 0.386, 0.08, 0.09, 0.353, 0.329, 0.475, 0.127, 0.196]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1247 | Steps: 2 | Val loss: 2.1152 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=167216)[0m top1: 0.25419776119402987
[2m[36m(func pid=167216)[0m top5: 0.8875932835820896
[2m[36m(func pid=167216)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=167216)[0m f1_macro: 0.2843963122601548
[2m[36m(func pid=167216)[0m f1_weighted: 0.28697348054113836
[2m[36m(func pid=167216)[0m f1_per_class: [0.342, 0.315, 0.4, 0.201, 0.066, 0.37, 0.295, 0.515, 0.124, 0.215]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1259 | Steps: 2 | Val loss: 3.7019 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=162129)[0m top1: 0.19636194029850745
[2m[36m(func pid=162129)[0m top5: 0.8269589552238806
[2m[36m(func pid=162129)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=162129)[0m f1_macro: 0.16422443191729252
[2m[36m(func pid=162129)[0m f1_weighted: 0.2207723059026646
[2m[36m(func pid=162129)[0m f1_per_class: [0.109, 0.217, 0.185, 0.349, 0.041, 0.008, 0.189, 0.364, 0.093, 0.087]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2120 | Steps: 2 | Val loss: 1.8890 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 00:36:05 (running for 00:43:14.88)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.125 |      0.164 |                   69 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.159 |      0.284 |                   47 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.126 |      0.259 |                   45 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.28777985074626866
[2m[36m(func pid=167836)[0m top5: 0.820429104477612
[2m[36m(func pid=167836)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=167836)[0m f1_macro: 0.25864991607033777
[2m[36m(func pid=167836)[0m f1_weighted: 0.24487781073821493
[2m[36m(func pid=167836)[0m f1_per_class: [0.282, 0.491, 0.367, 0.035, 0.099, 0.396, 0.232, 0.368, 0.112, 0.204]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.0654 | Steps: 2 | Val loss: 2.1123 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=167216)[0m top1: 0.3208955223880597
[2m[36m(func pid=167216)[0m top5: 0.8880597014925373
[2m[36m(func pid=167216)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=167216)[0m f1_macro: 0.30554790280897065
[2m[36m(func pid=167216)[0m f1_weighted: 0.33936940328925796
[2m[36m(func pid=167216)[0m f1_per_class: [0.312, 0.344, 0.4, 0.45, 0.067, 0.347, 0.227, 0.513, 0.168, 0.226]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4467 | Steps: 2 | Val loss: 3.2106 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=162129)[0m top1: 0.1875
[2m[36m(func pid=162129)[0m top5: 0.8264925373134329
[2m[36m(func pid=162129)[0m f1_micro: 0.1875
[2m[36m(func pid=162129)[0m f1_macro: 0.15837526786212472
[2m[36m(func pid=162129)[0m f1_weighted: 0.206407832982833
[2m[36m(func pid=162129)[0m f1_per_class: [0.113, 0.219, 0.224, 0.319, 0.042, 0.016, 0.172, 0.363, 0.023, 0.093]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.2483 | Steps: 2 | Val loss: 1.8258 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:36:11 (running for 00:43:20.54)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.065 |      0.158 |                   70 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.212 |      0.306 |                   48 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.447 |      0.312 |                   46 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.35261194029850745
[2m[36m(func pid=167836)[0m top5: 0.8731343283582089
[2m[36m(func pid=167836)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=167836)[0m f1_macro: 0.31245049244894596
[2m[36m(func pid=167836)[0m f1_weighted: 0.37114261633983847
[2m[36m(func pid=167836)[0m f1_per_class: [0.378, 0.507, 0.301, 0.261, 0.102, 0.37, 0.431, 0.387, 0.155, 0.234]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0360 | Steps: 2 | Val loss: 2.1060 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=167216)[0m top1: 0.3596082089552239
[2m[36m(func pid=167216)[0m top5: 0.8922574626865671
[2m[36m(func pid=167216)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=167216)[0m f1_macro: 0.2947204931701525
[2m[36m(func pid=167216)[0m f1_weighted: 0.3513942566670863
[2m[36m(func pid=167216)[0m f1_per_class: [0.272, 0.341, 0.4, 0.509, 0.086, 0.37, 0.225, 0.498, 0.05, 0.197]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1681 | Steps: 2 | Val loss: 3.3971 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=162129)[0m top1: 0.1884328358208955
[2m[36m(func pid=162129)[0m top5: 0.8297574626865671
[2m[36m(func pid=162129)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=162129)[0m f1_macro: 0.16659634097661324
[2m[36m(func pid=162129)[0m f1_weighted: 0.20774177965146476
[2m[36m(func pid=162129)[0m f1_per_class: [0.129, 0.236, 0.256, 0.295, 0.04, 0.008, 0.188, 0.361, 0.044, 0.109]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1523 | Steps: 2 | Val loss: 1.8123 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 00:36:17 (running for 00:43:26.10)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.036 |      0.167 |                   71 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.248 |      0.295 |                   49 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.168 |      0.287 |                   47 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.38386194029850745
[2m[36m(func pid=167836)[0m top5: 0.9188432835820896
[2m[36m(func pid=167836)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=167836)[0m f1_macro: 0.2868445973196733
[2m[36m(func pid=167836)[0m f1_weighted: 0.420093316748094
[2m[36m(func pid=167836)[0m f1_per_class: [0.187, 0.358, 0.208, 0.498, 0.108, 0.358, 0.491, 0.324, 0.165, 0.17]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m top1: 0.3885261194029851
[2m[36m(func pid=167216)[0m top5: 0.9183768656716418
[2m[36m(func pid=167216)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=167216)[0m f1_macro: 0.2961561971178641
[2m[36m(func pid=167216)[0m f1_weighted: 0.40633923972673613
[2m[36m(func pid=167216)[0m f1_per_class: [0.217, 0.369, 0.355, 0.5, 0.102, 0.397, 0.429, 0.321, 0.049, 0.222]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0990 | Steps: 2 | Val loss: 2.1118 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3167 | Steps: 2 | Val loss: 3.7052 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=162129)[0m top1: 0.17817164179104478
[2m[36m(func pid=162129)[0m top5: 0.8274253731343284
[2m[36m(func pid=162129)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=162129)[0m f1_macro: 0.1673597257844009
[2m[36m(func pid=162129)[0m f1_weighted: 0.1944901684947928
[2m[36m(func pid=162129)[0m f1_per_class: [0.124, 0.256, 0.275, 0.237, 0.04, 0.016, 0.178, 0.375, 0.059, 0.114]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0728 | Steps: 2 | Val loss: 1.9026 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 00:36:22 (running for 00:43:31.51)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.099 |      0.167 |                   72 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.152 |      0.296 |                   50 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.317 |      0.313 |                   48 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.425839552238806
[2m[36m(func pid=167836)[0m top5: 0.9146455223880597
[2m[36m(func pid=167836)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=167836)[0m f1_macro: 0.3126434078091854
[2m[36m(func pid=167836)[0m f1_weighted: 0.43536161096863496
[2m[36m(func pid=167836)[0m f1_per_class: [0.196, 0.447, 0.176, 0.55, 0.146, 0.408, 0.404, 0.404, 0.186, 0.211]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m top1: 0.3596082089552239
[2m[36m(func pid=167216)[0m top5: 0.9183768656716418
[2m[36m(func pid=167216)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=167216)[0m f1_macro: 0.2870671865822242
[2m[36m(func pid=167216)[0m f1_weighted: 0.3952076320077555
[2m[36m(func pid=167216)[0m f1_per_class: [0.175, 0.43, 0.275, 0.363, 0.102, 0.395, 0.483, 0.318, 0.109, 0.22]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1047 | Steps: 2 | Val loss: 2.1072 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1986 | Steps: 2 | Val loss: 5.2082 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=162129)[0m top1: 0.1837686567164179
[2m[36m(func pid=162129)[0m top5: 0.8306902985074627
[2m[36m(func pid=162129)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=162129)[0m f1_macro: 0.17269069064413672
[2m[36m(func pid=162129)[0m f1_weighted: 0.19812181524324485
[2m[36m(func pid=162129)[0m f1_per_class: [0.137, 0.286, 0.278, 0.183, 0.041, 0.008, 0.222, 0.382, 0.075, 0.115]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1198 | Steps: 2 | Val loss: 1.9609 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:36:28 (running for 00:43:37.11)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.105 |      0.173 |                   73 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.073 |      0.287 |                   51 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.199 |      0.262 |                   49 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.27798507462686567
[2m[36m(func pid=167836)[0m top5: 0.8927238805970149
[2m[36m(func pid=167836)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=167836)[0m f1_macro: 0.26179848553201224
[2m[36m(func pid=167836)[0m f1_weighted: 0.3038766817752241
[2m[36m(func pid=167836)[0m f1_per_class: [0.074, 0.547, 0.324, 0.177, 0.14, 0.417, 0.261, 0.482, 0.026, 0.171]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m top1: 0.32649253731343286
[2m[36m(func pid=167216)[0m top5: 0.9095149253731343
[2m[36m(func pid=167216)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=167216)[0m f1_macro: 0.2661884460767099
[2m[36m(func pid=167216)[0m f1_weighted: 0.36769032540056495
[2m[36m(func pid=167216)[0m f1_per_class: [0.161, 0.414, 0.218, 0.294, 0.096, 0.401, 0.467, 0.325, 0.097, 0.189]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2125 | Steps: 2 | Val loss: 2.0960 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.4545 | Steps: 2 | Val loss: 4.5704 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=162129)[0m top1: 0.20755597014925373
[2m[36m(func pid=162129)[0m top5: 0.8330223880597015
[2m[36m(func pid=162129)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=162129)[0m f1_macro: 0.18838451397315786
[2m[36m(func pid=162129)[0m f1_weighted: 0.23251021644035547
[2m[36m(func pid=162129)[0m f1_per_class: [0.144, 0.284, 0.289, 0.228, 0.043, 0.024, 0.288, 0.381, 0.088, 0.114]
[2m[36m(func pid=162129)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.3790 | Steps: 2 | Val loss: 2.0341 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 00:36:33 (running for 00:43:42.71)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.251
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00021 | RUNNING    | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.213 |      0.188 |                   74 |
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.379 |      0.277 |                   53 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.199 |      0.262 |                   49 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3376865671641791
[2m[36m(func pid=167216)[0m top5: 0.8913246268656716
[2m[36m(func pid=167216)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=167216)[0m f1_macro: 0.2766456272271315
[2m[36m(func pid=167216)[0m f1_weighted: 0.3857351818007491
[2m[36m(func pid=167216)[0m f1_per_class: [0.163, 0.385, 0.149, 0.413, 0.088, 0.376, 0.411, 0.491, 0.106, 0.185]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.2905783582089552
[2m[36m(func pid=167836)[0m top5: 0.8894589552238806
[2m[36m(func pid=167836)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=167836)[0m f1_macro: 0.279966552542979
[2m[36m(func pid=167836)[0m f1_weighted: 0.31321926848561993
[2m[36m(func pid=167836)[0m f1_per_class: [0.105, 0.498, 0.41, 0.325, 0.119, 0.4, 0.181, 0.466, 0.065, 0.231]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=162129)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0688 | Steps: 2 | Val loss: 2.0876 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.3328 | Steps: 2 | Val loss: 2.0031 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0436 | Steps: 2 | Val loss: 3.3341 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=162129)[0m top1: 0.2140858208955224
[2m[36m(func pid=162129)[0m top5: 0.8316231343283582
[2m[36m(func pid=162129)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=162129)[0m f1_macro: 0.19913388662118947
[2m[36m(func pid=162129)[0m f1_weighted: 0.24648949799588127
[2m[36m(func pid=162129)[0m f1_per_class: [0.141, 0.261, 0.349, 0.26, 0.046, 0.081, 0.298, 0.38, 0.083, 0.093]
== Status ==
Current time: 2024-01-07 00:36:39 (running for 00:43:48.17)
Memory usage on this node: 18.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.333 |      0.249 |                   54 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.455 |      0.28  |                   50 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3148320895522388
[2m[36m(func pid=167216)[0m top5: 0.8773320895522388
[2m[36m(func pid=167216)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=167216)[0m f1_macro: 0.24899210974104152
[2m[36m(func pid=167216)[0m f1_weighted: 0.32275267404804814
[2m[36m(func pid=167216)[0m f1_per_class: [0.193, 0.241, 0.159, 0.49, 0.072, 0.381, 0.213, 0.453, 0.122, 0.165]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.36613805970149255
[2m[36m(func pid=167836)[0m top5: 0.9001865671641791
[2m[36m(func pid=167836)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=167836)[0m f1_macro: 0.28168732239801814
[2m[36m(func pid=167836)[0m f1_weighted: 0.3651790367516744
[2m[36m(func pid=167836)[0m f1_per_class: [0.128, 0.524, 0.278, 0.254, 0.137, 0.392, 0.425, 0.342, 0.178, 0.158]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.1472 | Steps: 2 | Val loss: 2.0198 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3375 | Steps: 2 | Val loss: 4.8434 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:36:44 (running for 00:43:53.23)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.333 |      0.249 |                   54 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.044 |      0.282 |                   51 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.27798507462686567
[2m[36m(func pid=167216)[0m top5: 0.8698694029850746
[2m[36m(func pid=167216)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=167216)[0m f1_macro: 0.22811238424842073
[2m[36m(func pid=167216)[0m f1_weighted: 0.2683801337810537
[2m[36m(func pid=167216)[0m f1_per_class: [0.156, 0.378, 0.21, 0.387, 0.077, 0.366, 0.071, 0.417, 0.027, 0.192]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.36100746268656714
[2m[36m(func pid=167836)[0m top5: 0.8652052238805971
[2m[36m(func pid=167836)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=167836)[0m f1_macro: 0.21228188221813943
[2m[36m(func pid=167836)[0m f1_weighted: 0.29634680658257145
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.458, 0.224, 0.003, 0.113, 0.401, 0.516, 0.14, 0.169, 0.099]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2486 | Steps: 2 | Val loss: 2.0537 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8995 | Steps: 2 | Val loss: 4.5221 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 00:36:49 (running for 00:43:58.62)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.147 |      0.228 |                   55 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.338 |      0.212 |                   52 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.24580223880597016
[2m[36m(func pid=167216)[0m top5: 0.8572761194029851
[2m[36m(func pid=167216)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=167216)[0m f1_macro: 0.22854453952300666
[2m[36m(func pid=167216)[0m f1_weighted: 0.20614253676124417
[2m[36m(func pid=167216)[0m f1_per_class: [0.148, 0.451, 0.429, 0.137, 0.085, 0.362, 0.054, 0.42, 0.0, 0.2]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.3400186567164179
[2m[36m(func pid=167836)[0m top5: 0.8852611940298507
[2m[36m(func pid=167836)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=167836)[0m f1_macro: 0.23091420366984985
[2m[36m(func pid=167836)[0m f1_weighted: 0.3472498658139369
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.513, 0.15, 0.273, 0.104, 0.353, 0.423, 0.104, 0.189, 0.198]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.3771 | Steps: 2 | Val loss: 1.9726 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.2901 | Steps: 2 | Val loss: 5.6051 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 00:36:55 (running for 00:44:04.15)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.377 |      0.272 |                   57 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.899 |      0.231 |                   53 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.27845149253731344
[2m[36m(func pid=167216)[0m top5: 0.8777985074626866
[2m[36m(func pid=167216)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=167216)[0m f1_macro: 0.27167014356001185
[2m[36m(func pid=167216)[0m f1_weighted: 0.2682267473797528
[2m[36m(func pid=167216)[0m f1_per_class: [0.165, 0.462, 0.4, 0.173, 0.079, 0.353, 0.197, 0.454, 0.169, 0.264]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.22574626865671643
[2m[36m(func pid=167836)[0m top5: 0.8414179104477612
[2m[36m(func pid=167836)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=167836)[0m f1_macro: 0.1773099802743608
[2m[36m(func pid=167836)[0m f1_weighted: 0.23679265879647876
[2m[36m(func pid=167836)[0m f1_per_class: [0.129, 0.016, 0.195, 0.445, 0.051, 0.298, 0.194, 0.121, 0.13, 0.193]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.1135 | Steps: 2 | Val loss: 1.9088 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5704 | Steps: 2 | Val loss: 10.4989 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:37:00 (running for 00:44:09.45)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.114 |      0.294 |                   58 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.29  |      0.177 |                   54 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.27238805970149255
[2m[36m(func pid=167216)[0m top5: 0.8936567164179104
[2m[36m(func pid=167216)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=167216)[0m f1_macro: 0.2935417350556961
[2m[36m(func pid=167216)[0m f1_weighted: 0.31375272623703415
[2m[36m(func pid=167216)[0m f1_per_class: [0.288, 0.286, 0.421, 0.319, 0.072, 0.368, 0.293, 0.519, 0.123, 0.246]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.08255597014925373
[2m[36m(func pid=167836)[0m top5: 0.8106343283582089
[2m[36m(func pid=167836)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=167836)[0m f1_macro: 0.15333983958556224
[2m[36m(func pid=167836)[0m f1_weighted: 0.08789128953364864
[2m[36m(func pid=167836)[0m f1_per_class: [0.062, 0.175, 0.455, 0.016, 0.08, 0.184, 0.033, 0.228, 0.048, 0.253]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0863 | Steps: 2 | Val loss: 1.8590 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.8882 | Steps: 2 | Val loss: 3.7476 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 00:37:06 (running for 00:44:14.96)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.086 |      0.287 |                   59 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.57  |      0.153 |                   55 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.2905783582089552
[2m[36m(func pid=167216)[0m top5: 0.90625
[2m[36m(func pid=167216)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=167216)[0m f1_macro: 0.28734257125363805
[2m[36m(func pid=167216)[0m f1_weighted: 0.33026214596447884
[2m[36m(func pid=167216)[0m f1_per_class: [0.347, 0.086, 0.417, 0.363, 0.08, 0.379, 0.435, 0.416, 0.124, 0.225]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.33675373134328357
[2m[36m(func pid=167836)[0m top5: 0.902518656716418
[2m[36m(func pid=167836)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=167836)[0m f1_macro: 0.2587711222038148
[2m[36m(func pid=167836)[0m f1_weighted: 0.3297758806259148
[2m[36m(func pid=167836)[0m f1_per_class: [0.321, 0.496, 0.4, 0.489, 0.0, 0.106, 0.208, 0.339, 0.12, 0.109]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.0025 | Steps: 2 | Val loss: 1.7759 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.1123 | Steps: 2 | Val loss: 3.7107 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:37:11 (running for 00:44:20.27)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.002 |      0.302 |                   60 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.888 |      0.259 |                   56 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.39925373134328357
[2m[36m(func pid=167216)[0m top5: 0.9076492537313433
[2m[36m(func pid=167216)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=167216)[0m f1_macro: 0.30217562335339904
[2m[36m(func pid=167216)[0m f1_weighted: 0.39717513225247336
[2m[36m(func pid=167216)[0m f1_per_class: [0.28, 0.102, 0.379, 0.562, 0.099, 0.402, 0.465, 0.408, 0.114, 0.212]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.42490671641791045
[2m[36m(func pid=167836)[0m top5: 0.9347014925373134
[2m[36m(func pid=167836)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=167836)[0m f1_macro: 0.25595685009123376
[2m[36m(func pid=167836)[0m f1_weighted: 0.37777400472649564
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.493, 0.242, 0.159, 0.0, 0.432, 0.572, 0.368, 0.111, 0.182]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.0364 | Steps: 2 | Val loss: 1.7877 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3885 | Steps: 2 | Val loss: 4.8343 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 00:37:16 (running for 00:44:25.39)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.036 |      0.297 |                   61 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.112 |      0.256 |                   57 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3908582089552239
[2m[36m(func pid=167216)[0m top5: 0.909981343283582
[2m[36m(func pid=167216)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=167216)[0m f1_macro: 0.29716749473583515
[2m[36m(func pid=167216)[0m f1_weighted: 0.4097669904654481
[2m[36m(func pid=167216)[0m f1_per_class: [0.22, 0.309, 0.338, 0.504, 0.119, 0.41, 0.452, 0.429, 0.0, 0.191]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.29757462686567165
[2m[36m(func pid=167836)[0m top5: 0.820429104477612
[2m[36m(func pid=167836)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=167836)[0m f1_macro: 0.2224666760638378
[2m[36m(func pid=167836)[0m f1_weighted: 0.27476174748409865
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.381, 0.155, 0.097, 0.116, 0.368, 0.369, 0.379, 0.102, 0.258]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.0230 | Steps: 2 | Val loss: 1.8410 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9369 | Steps: 2 | Val loss: 5.9536 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 00:37:22 (running for 00:44:30.93)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.023 |      0.301 |                   62 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.388 |      0.222 |                   58 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3810634328358209
[2m[36m(func pid=167216)[0m top5: 0.9029850746268657
[2m[36m(func pid=167216)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=167216)[0m f1_macro: 0.30105539697434514
[2m[36m(func pid=167216)[0m f1_weighted: 0.40373990884340616
[2m[36m(func pid=167216)[0m f1_per_class: [0.189, 0.483, 0.328, 0.378, 0.139, 0.403, 0.446, 0.467, 0.0, 0.176]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.2178171641791045
[2m[36m(func pid=167836)[0m top5: 0.8017723880597015
[2m[36m(func pid=167836)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=167836)[0m f1_macro: 0.19019322844939987
[2m[36m(func pid=167836)[0m f1_weighted: 0.24032589422701894
[2m[36m(func pid=167836)[0m f1_per_class: [0.145, 0.095, 0.143, 0.081, 0.038, 0.231, 0.472, 0.417, 0.127, 0.152]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0065 | Steps: 2 | Val loss: 1.9265 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.2470 | Steps: 2 | Val loss: 7.5916 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:37:27 (running for 00:44:36.20)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.006 |      0.276 |                   63 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.937 |      0.19  |                   59 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.34421641791044777
[2m[36m(func pid=167216)[0m top5: 0.8857276119402985
[2m[36m(func pid=167216)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=167216)[0m f1_macro: 0.27648310060882186
[2m[36m(func pid=167216)[0m f1_weighted: 0.34920596760899547
[2m[36m(func pid=167216)[0m f1_per_class: [0.188, 0.47, 0.293, 0.216, 0.121, 0.374, 0.431, 0.471, 0.027, 0.173]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.1310634328358209
[2m[36m(func pid=167836)[0m top5: 0.8535447761194029
[2m[36m(func pid=167836)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=167836)[0m f1_macro: 0.16389912098064943
[2m[36m(func pid=167836)[0m f1_weighted: 0.14539157351394408
[2m[36m(func pid=167836)[0m f1_per_class: [0.11, 0.109, 0.4, 0.019, 0.044, 0.008, 0.304, 0.33, 0.099, 0.216]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.0684 | Steps: 2 | Val loss: 1.9477 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.7064 | Steps: 2 | Val loss: 5.4490 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 00:37:32 (running for 00:44:41.44)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.068 |      0.266 |                   64 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.247 |      0.164 |                   60 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.33722014925373134
[2m[36m(func pid=167216)[0m top5: 0.8955223880597015
[2m[36m(func pid=167216)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=167216)[0m f1_macro: 0.26607224858719
[2m[36m(func pid=167216)[0m f1_weighted: 0.3546793137985779
[2m[36m(func pid=167216)[0m f1_per_class: [0.191, 0.464, 0.224, 0.221, 0.077, 0.288, 0.486, 0.402, 0.125, 0.182]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.31902985074626866
[2m[36m(func pid=167836)[0m top5: 0.9319029850746269
[2m[36m(func pid=167836)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=167836)[0m f1_macro: 0.20686313915457966
[2m[36m(func pid=167836)[0m f1_weighted: 0.2842544941129888
[2m[36m(func pid=167836)[0m f1_per_class: [0.17, 0.459, 0.4, 0.425, 0.0, 0.063, 0.214, 0.092, 0.08, 0.167]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.0341 | Steps: 2 | Val loss: 1.9332 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2306 | Steps: 2 | Val loss: 5.1300 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 00:37:37 (running for 00:44:46.78)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.034 |      0.263 |                   65 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.706 |      0.207 |                   61 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.31669776119402987
[2m[36m(func pid=167216)[0m top5: 0.9029850746268657
[2m[36m(func pid=167216)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=167216)[0m f1_macro: 0.26348439237148613
[2m[36m(func pid=167216)[0m f1_weighted: 0.36476023662321133
[2m[36m(func pid=167216)[0m f1_per_class: [0.224, 0.426, 0.191, 0.384, 0.054, 0.205, 0.43, 0.321, 0.16, 0.24]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.3787313432835821
[2m[36m(func pid=167836)[0m top5: 0.9407649253731343
[2m[36m(func pid=167836)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=167836)[0m f1_macro: 0.2139812926865945
[2m[36m(func pid=167836)[0m f1_weighted: 0.2974669684222658
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.429, 0.392, 0.0, 0.0, 0.348, 0.573, 0.078, 0.113, 0.207]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.9743 | Steps: 2 | Val loss: 1.9177 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1439 | Steps: 2 | Val loss: 7.7089 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=167216)[0m top1: 0.3180970149253731
[2m[36m(func pid=167216)[0m top5: 0.9043843283582089
[2m[36m(func pid=167216)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=167216)[0m f1_macro: 0.2691167814585615
[2m[36m(func pid=167216)[0m f1_weighted: 0.3658034891156542
[2m[36m(func pid=167216)[0m f1_per_class: [0.222, 0.31, 0.21, 0.474, 0.055, 0.264, 0.38, 0.394, 0.156, 0.227]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:37:43 (running for 00:44:52.29)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.974 |      0.269 |                   66 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.231 |      0.214 |                   62 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.2355410447761194
[2m[36m(func pid=167836)[0m top5: 0.6431902985074627
[2m[36m(func pid=167836)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=167836)[0m f1_macro: 0.2005713155746288
[2m[36m(func pid=167836)[0m f1_weighted: 0.17281908539910198
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.397, 0.177, 0.0, 0.24, 0.389, 0.103, 0.328, 0.14, 0.231]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.0395 | Steps: 2 | Val loss: 1.8735 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.5073 | Steps: 2 | Val loss: 9.2348 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 00:37:48 (running for 00:44:57.38)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.039 |      0.305 |                   67 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.144 |      0.201 |                   63 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.35447761194029853
[2m[36m(func pid=167216)[0m top5: 0.9081156716417911
[2m[36m(func pid=167216)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=167216)[0m f1_macro: 0.3045846685396134
[2m[36m(func pid=167216)[0m f1_weighted: 0.3992493157821548
[2m[36m(func pid=167216)[0m f1_per_class: [0.235, 0.404, 0.268, 0.449, 0.08, 0.361, 0.41, 0.473, 0.12, 0.246]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.14412313432835822
[2m[36m(func pid=167836)[0m top5: 0.5517723880597015
[2m[36m(func pid=167836)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=167836)[0m f1_macro: 0.11614784563975529
[2m[36m(func pid=167836)[0m f1_weighted: 0.1305505971227467
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.005, 0.146, 0.0, 0.046, 0.278, 0.255, 0.312, 0.057, 0.062]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.3228 | Steps: 2 | Val loss: 1.9241 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 3.0750 | Steps: 2 | Val loss: 7.3062 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=167216)[0m top1: 0.333955223880597
[2m[36m(func pid=167216)[0m top5: 0.9039179104477612
[2m[36m(func pid=167216)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=167216)[0m f1_macro: 0.3065313415127037
[2m[36m(func pid=167216)[0m f1_weighted: 0.3695919823317816
[2m[36m(func pid=167216)[0m f1_per_class: [0.153, 0.508, 0.432, 0.243, 0.097, 0.374, 0.442, 0.453, 0.124, 0.237]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:37:53 (running for 00:45:02.58)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.323 |      0.307 |                   68 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.507 |      0.116 |                   64 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m top1: 0.29524253731343286
[2m[36m(func pid=167836)[0m top5: 0.8050373134328358
[2m[36m(func pid=167836)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=167836)[0m f1_macro: 0.1991830173665694
[2m[36m(func pid=167836)[0m f1_weighted: 0.3259506233759411
[2m[36m(func pid=167836)[0m f1_per_class: [0.154, 0.0, 0.121, 0.481, 0.037, 0.131, 0.5, 0.304, 0.13, 0.133]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1210 | Steps: 2 | Val loss: 1.9719 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.7007 | Steps: 2 | Val loss: 5.8057 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:37:58 (running for 00:45:07.65)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.121 |      0.286 |                   69 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  3.075 |      0.199 |                   65 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.30736940298507465
[2m[36m(func pid=167216)[0m top5: 0.9029850746268657
[2m[36m(func pid=167216)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=167216)[0m f1_macro: 0.2859146406485331
[2m[36m(func pid=167216)[0m f1_weighted: 0.34155242421213783
[2m[36m(func pid=167216)[0m f1_per_class: [0.14, 0.412, 0.435, 0.179, 0.085, 0.379, 0.471, 0.42, 0.12, 0.22]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.3353544776119403
[2m[36m(func pid=167836)[0m top5: 0.9076492537313433
[2m[36m(func pid=167836)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=167836)[0m f1_macro: 0.27075746032115766
[2m[36m(func pid=167836)[0m f1_weighted: 0.3146673806441468
[2m[36m(func pid=167836)[0m f1_per_class: [0.14, 0.245, 0.432, 0.482, 0.243, 0.466, 0.204, 0.204, 0.063, 0.227]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.0982 | Steps: 2 | Val loss: 1.8478 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.1898 | Steps: 2 | Val loss: 7.6206 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 00:38:03 (running for 00:45:12.84)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.098 |      0.308 |                   70 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.701 |      0.271 |                   66 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.34794776119402987
[2m[36m(func pid=167216)[0m top5: 0.9015858208955224
[2m[36m(func pid=167216)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=167216)[0m f1_macro: 0.30798418066417266
[2m[36m(func pid=167216)[0m f1_weighted: 0.3818833977039492
[2m[36m(func pid=167216)[0m f1_per_class: [0.271, 0.397, 0.379, 0.385, 0.083, 0.392, 0.402, 0.472, 0.109, 0.19]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.25419776119402987
[2m[36m(func pid=167836)[0m top5: 0.914179104477612
[2m[36m(func pid=167836)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=167836)[0m f1_macro: 0.21049798996000862
[2m[36m(func pid=167836)[0m f1_weighted: 0.23017004420527953
[2m[36m(func pid=167836)[0m f1_per_class: [0.098, 0.493, 0.421, 0.042, 0.0, 0.389, 0.245, 0.092, 0.11, 0.214]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.9379 | Steps: 2 | Val loss: 1.8855 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0629 | Steps: 2 | Val loss: 4.1213 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:38:09 (running for 00:45:18.11)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.938 |      0.272 |                   71 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.19  |      0.21  |                   67 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.32882462686567165
[2m[36m(func pid=167216)[0m top5: 0.8964552238805971
[2m[36m(func pid=167216)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=167216)[0m f1_macro: 0.2721815015923478
[2m[36m(func pid=167216)[0m f1_weighted: 0.3561814319984296
[2m[36m(func pid=167216)[0m f1_per_class: [0.08, 0.368, 0.229, 0.406, 0.076, 0.353, 0.333, 0.47, 0.208, 0.198]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.4025186567164179
[2m[36m(func pid=167836)[0m top5: 0.9500932835820896
[2m[36m(func pid=167836)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=167836)[0m f1_macro: 0.27911558807940284
[2m[36m(func pid=167836)[0m f1_weighted: 0.3625282578574116
[2m[36m(func pid=167836)[0m f1_per_class: [0.303, 0.498, 0.471, 0.179, 0.0, 0.362, 0.551, 0.092, 0.135, 0.2]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.0930 | Steps: 2 | Val loss: 1.9404 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.8555 | Steps: 2 | Val loss: 5.0081 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 00:38:14 (running for 00:45:23.56)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.093 |      0.255 |                   72 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.063 |      0.279 |                   68 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3255597014925373
[2m[36m(func pid=167216)[0m top5: 0.8931902985074627
[2m[36m(func pid=167216)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=167216)[0m f1_macro: 0.25490786482929473
[2m[36m(func pid=167216)[0m f1_weighted: 0.3507074858125858
[2m[36m(func pid=167216)[0m f1_per_class: [0.043, 0.292, 0.188, 0.48, 0.063, 0.336, 0.3, 0.476, 0.204, 0.167]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.3185634328358209
[2m[36m(func pid=167836)[0m top5: 0.8288246268656716
[2m[36m(func pid=167836)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=167836)[0m f1_macro: 0.22078103198247478
[2m[36m(func pid=167836)[0m f1_weighted: 0.2733276829282249
[2m[36m(func pid=167836)[0m f1_per_class: [0.087, 0.016, 0.218, 0.566, 0.193, 0.372, 0.128, 0.357, 0.172, 0.098]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.0124 | Steps: 2 | Val loss: 1.9601 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9698 | Steps: 2 | Val loss: 7.9248 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:38:19 (running for 00:45:28.67)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.012 |      0.261 |                   73 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.855 |      0.221 |                   69 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3180970149253731
[2m[36m(func pid=167216)[0m top5: 0.8941231343283582
[2m[36m(func pid=167216)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=167216)[0m f1_macro: 0.2614705605090763
[2m[36m(func pid=167216)[0m f1_weighted: 0.3544906168485145
[2m[36m(func pid=167216)[0m f1_per_class: [0.08, 0.38, 0.191, 0.391, 0.062, 0.333, 0.353, 0.418, 0.208, 0.198]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.16977611940298507
[2m[36m(func pid=167836)[0m top5: 0.6972947761194029
[2m[36m(func pid=167836)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=167836)[0m f1_macro: 0.1447890657966097
[2m[36m(func pid=167836)[0m f1_weighted: 0.160842588964934
[2m[36m(func pid=167836)[0m f1_per_class: [0.0, 0.0, 0.15, 0.278, 0.055, 0.254, 0.084, 0.382, 0.14, 0.105]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.3928 | Steps: 2 | Val loss: 1.9318 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:38:24 (running for 00:45:33.88)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.25
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  1.393 |      0.271 |                   74 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.97  |      0.145 |                   70 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.32136194029850745
[2m[36m(func pid=167216)[0m top5: 0.9076492537313433
[2m[36m(func pid=167216)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=167216)[0m f1_macro: 0.27111389957007975
[2m[36m(func pid=167216)[0m f1_weighted: 0.3604039691915015
[2m[36m(func pid=167216)[0m f1_per_class: [0.189, 0.293, 0.367, 0.432, 0.066, 0.348, 0.405, 0.276, 0.115, 0.22]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.6594 | Steps: 2 | Val loss: 9.8342 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=167836)[0m top1: 0.208955223880597
[2m[36m(func pid=167836)[0m top5: 0.6203358208955224
[2m[36m(func pid=167836)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=167836)[0m f1_macro: 0.11791858216551562
[2m[36m(func pid=167836)[0m f1_weighted: 0.18523206122032543
[2m[36m(func pid=167836)[0m f1_per_class: [0.082, 0.032, 0.108, 0.003, 0.042, 0.288, 0.471, 0.0, 0.048, 0.105]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8477 | Steps: 2 | Val loss: 2.0078 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:38:30 (running for 00:45:39.11)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.251
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.848 |      0.267 |                   75 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.659 |      0.118 |                   71 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3138992537313433
[2m[36m(func pid=167216)[0m top5: 0.9123134328358209
[2m[36m(func pid=167216)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=167216)[0m f1_macro: 0.26651320600129197
[2m[36m(func pid=167216)[0m f1_weighted: 0.3541796239759433
[2m[36m(func pid=167216)[0m f1_per_class: [0.127, 0.223, 0.393, 0.377, 0.084, 0.396, 0.46, 0.304, 0.085, 0.218]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0774 | Steps: 2 | Val loss: 7.7706 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=167836)[0m top1: 0.2224813432835821
[2m[36m(func pid=167836)[0m top5: 0.7066231343283582
[2m[36m(func pid=167836)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=167836)[0m f1_macro: 0.17333588022374405
[2m[36m(func pid=167836)[0m f1_weighted: 0.2138811997157402
[2m[36m(func pid=167836)[0m f1_per_class: [0.186, 0.226, 0.125, 0.0, 0.06, 0.305, 0.413, 0.12, 0.072, 0.226]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.9101 | Steps: 2 | Val loss: 2.0083 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 00:38:35 (running for 00:45:44.72)
Memory usage on this node: 18.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.251
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.91  |      0.3   |                   76 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  2.077 |      0.173 |                   72 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.34095149253731344
[2m[36m(func pid=167216)[0m top5: 0.9123134328358209
[2m[36m(func pid=167216)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=167216)[0m f1_macro: 0.29952627005905735
[2m[36m(func pid=167216)[0m f1_weighted: 0.3852735312716044
[2m[36m(func pid=167216)[0m f1_per_class: [0.131, 0.339, 0.367, 0.373, 0.122, 0.412, 0.47, 0.387, 0.135, 0.26]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4127 | Steps: 2 | Val loss: 7.5025 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=167836)[0m top1: 0.28171641791044777
[2m[36m(func pid=167836)[0m top5: 0.8274253731343284
[2m[36m(func pid=167836)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=167836)[0m f1_macro: 0.2232006121696261
[2m[36m(func pid=167836)[0m f1_weighted: 0.16265081045064775
[2m[36m(func pid=167836)[0m f1_per_class: [0.117, 0.434, 0.218, 0.003, 0.25, 0.446, 0.0, 0.407, 0.131, 0.226]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.9726 | Steps: 2 | Val loss: 1.9434 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 4.9031 | Steps: 2 | Val loss: 6.6350 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 00:38:41 (running for 00:45:50.30)
Memory usage on this node: 18.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.251
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.973 |      0.31  |                   77 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  0.413 |      0.223 |                   73 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.36427238805970147
[2m[36m(func pid=167216)[0m top5: 0.9183768656716418
[2m[36m(func pid=167216)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=167216)[0m f1_macro: 0.30982203694101135
[2m[36m(func pid=167216)[0m f1_weighted: 0.3979515277918278
[2m[36m(func pid=167216)[0m f1_per_class: [0.177, 0.52, 0.282, 0.303, 0.159, 0.4, 0.469, 0.445, 0.1, 0.244]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167836)[0m top1: 0.28125
[2m[36m(func pid=167836)[0m top5: 0.882929104477612
[2m[36m(func pid=167836)[0m f1_micro: 0.28125
[2m[36m(func pid=167836)[0m f1_macro: 0.1850492921585384
[2m[36m(func pid=167836)[0m f1_weighted: 0.2215359370714451
[2m[36m(func pid=167836)[0m f1_per_class: [0.158, 0.23, 0.262, 0.447, 0.0, 0.129, 0.039, 0.385, 0.064, 0.137]
[2m[36m(func pid=167836)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.8518 | Steps: 2 | Val loss: 1.9008 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=167216)[0m top1: 0.36007462686567165
[2m[36m(func pid=167216)[0m top5: 0.9132462686567164
[2m[36m(func pid=167216)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=167216)[0m f1_macro: 0.29806182069339443
[2m[36m(func pid=167216)[0m f1_weighted: 0.3536322295194928
[2m[36m(func pid=167216)[0m f1_per_class: [0.24, 0.521, 0.196, 0.188, 0.175, 0.427, 0.403, 0.488, 0.142, 0.2]
[2m[36m(func pid=167216)[0m 
== Status ==
Current time: 2024-01-07 00:38:46 (running for 00:45:55.54)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.251
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.852 |      0.298 |                   78 |
| train_51d3e_00023 | RUNNING    | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  4.903 |      0.185 |                   74 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167836)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.7982 | Steps: 2 | Val loss: 10.1850 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.9219 | Steps: 2 | Val loss: 1.8941 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=167836)[0m top1: 0.18236940298507462
[2m[36m(func pid=167836)[0m top5: 0.7765858208955224
[2m[36m(func pid=167836)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=167836)[0m f1_macro: 0.1698448342622336
[2m[36m(func pid=167836)[0m f1_weighted: 0.20105979317462302
[2m[36m(func pid=167836)[0m f1_per_class: [0.075, 0.0, 0.364, 0.244, 0.389, 0.016, 0.404, 0.0, 0.077, 0.13]
== Status ==
Current time: 2024-01-07 00:38:51 (running for 00:46:00.68)
Memory usage on this node: 16.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.922 |      0.305 |                   79 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.37546641791044777
[2m[36m(func pid=167216)[0m top5: 0.9006529850746269
[2m[36m(func pid=167216)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=167216)[0m f1_macro: 0.30483256777193357
[2m[36m(func pid=167216)[0m f1_weighted: 0.36589797765631277
[2m[36m(func pid=167216)[0m f1_per_class: [0.184, 0.527, 0.189, 0.348, 0.174, 0.422, 0.289, 0.485, 0.224, 0.207]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.9207 | Steps: 2 | Val loss: 1.8791 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 00:38:56 (running for 00:46:05.71)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.922 |      0.305 |                   79 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.36613805970149255
[2m[36m(func pid=167216)[0m top5: 0.8997201492537313
[2m[36m(func pid=167216)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=167216)[0m f1_macro: 0.29228790041016006
[2m[36m(func pid=167216)[0m f1_weighted: 0.3687524542576736
[2m[36m(func pid=167216)[0m f1_per_class: [0.252, 0.396, 0.175, 0.512, 0.106, 0.39, 0.239, 0.478, 0.146, 0.23]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.7919 | Steps: 2 | Val loss: 1.9963 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 00:39:02 (running for 00:46:11.33)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.921 |      0.292 |                   80 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.32276119402985076
[2m[36m(func pid=167216)[0m top5: 0.8889925373134329
[2m[36m(func pid=167216)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=167216)[0m f1_macro: 0.2652931202395816
[2m[36m(func pid=167216)[0m f1_weighted: 0.3481500973589923
[2m[36m(func pid=167216)[0m f1_per_class: [0.196, 0.188, 0.202, 0.482, 0.075, 0.364, 0.335, 0.479, 0.122, 0.211]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.7588 | Steps: 2 | Val loss: 2.0624 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 00:39:07 (running for 00:46:16.86)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.792 |      0.265 |                   81 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.29151119402985076
[2m[36m(func pid=167216)[0m top5: 0.8917910447761194
[2m[36m(func pid=167216)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=167216)[0m f1_macro: 0.24591647888837107
[2m[36m(func pid=167216)[0m f1_weighted: 0.33621388945681985
[2m[36m(func pid=167216)[0m f1_per_class: [0.186, 0.142, 0.268, 0.387, 0.058, 0.349, 0.444, 0.36, 0.089, 0.176]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.8413 | Steps: 2 | Val loss: 2.0801 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 00:39:13 (running for 00:46:22.33)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.759 |      0.246 |                   82 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.27611940298507465
[2m[36m(func pid=167216)[0m top5: 0.8838619402985075
[2m[36m(func pid=167216)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=167216)[0m f1_macro: 0.2520162295713395
[2m[36m(func pid=167216)[0m f1_weighted: 0.3239244998425181
[2m[36m(func pid=167216)[0m f1_per_class: [0.152, 0.216, 0.344, 0.278, 0.058, 0.351, 0.452, 0.408, 0.086, 0.174]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.8326 | Steps: 2 | Val loss: 2.0651 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 00:39:18 (running for 00:46:27.91)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.841 |      0.252 |                   83 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.28451492537313433
[2m[36m(func pid=167216)[0m top5: 0.8847947761194029
[2m[36m(func pid=167216)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=167216)[0m f1_macro: 0.27454198264224905
[2m[36m(func pid=167216)[0m f1_weighted: 0.31939536202090213
[2m[36m(func pid=167216)[0m f1_per_class: [0.156, 0.371, 0.407, 0.187, 0.074, 0.377, 0.405, 0.474, 0.121, 0.174]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.8122 | Steps: 2 | Val loss: 1.9858 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 00:39:24 (running for 00:46:33.42)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.833 |      0.275 |                   84 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3218283582089552
[2m[36m(func pid=167216)[0m top5: 0.886660447761194
[2m[36m(func pid=167216)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=167216)[0m f1_macro: 0.29576733548142403
[2m[36m(func pid=167216)[0m f1_weighted: 0.3137630120197309
[2m[36m(func pid=167216)[0m f1_per_class: [0.208, 0.497, 0.423, 0.113, 0.114, 0.401, 0.362, 0.494, 0.124, 0.222]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.7594 | Steps: 2 | Val loss: 1.8509 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 00:39:30 (running for 00:46:39.01)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.812 |      0.296 |                   85 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3763992537313433
[2m[36m(func pid=167216)[0m top5: 0.9006529850746269
[2m[36m(func pid=167216)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=167216)[0m f1_macro: 0.32364722434621673
[2m[36m(func pid=167216)[0m f1_weighted: 0.375138352454917
[2m[36m(func pid=167216)[0m f1_per_class: [0.256, 0.531, 0.393, 0.312, 0.165, 0.417, 0.355, 0.491, 0.118, 0.2]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.9379 | Steps: 2 | Val loss: 1.7597 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 00:39:35 (running for 00:46:44.76)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.759 |      0.324 |                   86 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.4207089552238806
[2m[36m(func pid=167216)[0m top5: 0.9067164179104478
[2m[36m(func pid=167216)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=167216)[0m f1_macro: 0.34499951941636187
[2m[36m(func pid=167216)[0m f1_weighted: 0.4200659325924036
[2m[36m(func pid=167216)[0m f1_per_class: [0.267, 0.545, 0.361, 0.464, 0.176, 0.435, 0.343, 0.496, 0.142, 0.222]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.7440 | Steps: 2 | Val loss: 1.7285 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 00:39:41 (running for 00:46:50.34)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.938 |      0.345 |                   87 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.43050373134328357
[2m[36m(func pid=167216)[0m top5: 0.9193097014925373
[2m[36m(func pid=167216)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=167216)[0m f1_macro: 0.33997820276690394
[2m[36m(func pid=167216)[0m f1_weighted: 0.4299129170226099
[2m[36m(func pid=167216)[0m f1_per_class: [0.265, 0.47, 0.328, 0.529, 0.173, 0.447, 0.364, 0.461, 0.104, 0.258]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.8839 | Steps: 2 | Val loss: 1.8161 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 00:39:47 (running for 00:46:56.01)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.744 |      0.34  |                   88 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.39598880597014924
[2m[36m(func pid=167216)[0m top5: 0.9132462686567164
[2m[36m(func pid=167216)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=167216)[0m f1_macro: 0.31359748385666464
[2m[36m(func pid=167216)[0m f1_weighted: 0.41608638666918285
[2m[36m(func pid=167216)[0m f1_per_class: [0.229, 0.375, 0.282, 0.515, 0.118, 0.406, 0.411, 0.436, 0.106, 0.257]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.7365 | Steps: 2 | Val loss: 1.9398 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:39:52 (running for 00:47:01.78)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.884 |      0.314 |                   89 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3423507462686567
[2m[36m(func pid=167216)[0m top5: 0.8978544776119403
[2m[36m(func pid=167216)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=167216)[0m f1_macro: 0.2696884159236105
[2m[36m(func pid=167216)[0m f1_weighted: 0.3797559981863963
[2m[36m(func pid=167216)[0m f1_per_class: [0.201, 0.269, 0.262, 0.418, 0.087, 0.387, 0.482, 0.301, 0.094, 0.196]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.9477 | Steps: 2 | Val loss: 2.0250 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 00:39:58 (running for 00:47:07.04)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.737 |      0.27  |                   90 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3148320895522388
[2m[36m(func pid=167216)[0m top5: 0.8838619402985075
[2m[36m(func pid=167216)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=167216)[0m f1_macro: 0.2550644601160254
[2m[36m(func pid=167216)[0m f1_weighted: 0.35897205947758376
[2m[36m(func pid=167216)[0m f1_per_class: [0.178, 0.289, 0.25, 0.326, 0.073, 0.362, 0.499, 0.301, 0.09, 0.182]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.8246 | Steps: 2 | Val loss: 2.0078 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 00:40:03 (running for 00:47:12.85)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.948 |      0.255 |                   91 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.30736940298507465
[2m[36m(func pid=167216)[0m top5: 0.8824626865671642
[2m[36m(func pid=167216)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=167216)[0m f1_macro: 0.2578846800728662
[2m[36m(func pid=167216)[0m f1_weighted: 0.3453185513732451
[2m[36m(func pid=167216)[0m f1_per_class: [0.171, 0.337, 0.272, 0.254, 0.076, 0.365, 0.481, 0.361, 0.093, 0.171]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.7932 | Steps: 2 | Val loss: 1.9560 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 00:40:09 (running for 00:47:18.38)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.825 |      0.258 |                   92 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.32975746268656714
[2m[36m(func pid=167216)[0m top5: 0.8936567164179104
[2m[36m(func pid=167216)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=167216)[0m f1_macro: 0.28706041729549947
[2m[36m(func pid=167216)[0m f1_weighted: 0.3610741611942451
[2m[36m(func pid=167216)[0m f1_per_class: [0.195, 0.46, 0.282, 0.26, 0.089, 0.387, 0.426, 0.442, 0.104, 0.225]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.7536 | Steps: 2 | Val loss: 1.9201 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 00:40:15 (running for 00:47:24.32)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.793 |      0.287 |                   93 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3362873134328358
[2m[36m(func pid=167216)[0m top5: 0.8875932835820896
[2m[36m(func pid=167216)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=167216)[0m f1_macro: 0.3003529830809161
[2m[36m(func pid=167216)[0m f1_weighted: 0.3387577582057167
[2m[36m(func pid=167216)[0m f1_per_class: [0.221, 0.518, 0.314, 0.327, 0.13, 0.381, 0.245, 0.475, 0.117, 0.276]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.7075 | Steps: 2 | Val loss: 1.9211 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 00:40:20 (running for 00:47:29.89)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.754 |      0.3   |                   94 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.3493470149253731
[2m[36m(func pid=167216)[0m top5: 0.8745335820895522
[2m[36m(func pid=167216)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=167216)[0m f1_macro: 0.30466260726541516
[2m[36m(func pid=167216)[0m f1_weighted: 0.32444132853663593
[2m[36m(func pid=167216)[0m f1_per_class: [0.21, 0.551, 0.344, 0.4, 0.152, 0.388, 0.103, 0.472, 0.146, 0.281]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.8788 | Steps: 2 | Val loss: 1.8904 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 00:40:26 (running for 00:47:35.52)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.708 |      0.305 |                   95 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.375
[2m[36m(func pid=167216)[0m top5: 0.8833955223880597
[2m[36m(func pid=167216)[0m f1_micro: 0.375
[2m[36m(func pid=167216)[0m f1_macro: 0.3204443158120068
[2m[36m(func pid=167216)[0m f1_weighted: 0.3597579032367519
[2m[36m(func pid=167216)[0m f1_per_class: [0.216, 0.545, 0.367, 0.467, 0.16, 0.401, 0.154, 0.486, 0.147, 0.261]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.7035 | Steps: 2 | Val loss: 1.8174 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 00:40:32 (running for 00:47:41.19)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.879 |      0.32  |                   96 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.408115671641791
[2m[36m(func pid=167216)[0m top5: 0.902518656716418
[2m[36m(func pid=167216)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=167216)[0m f1_macro: 0.3315286841325248
[2m[36m(func pid=167216)[0m f1_weighted: 0.42191926149972087
[2m[36m(func pid=167216)[0m f1_per_class: [0.221, 0.451, 0.361, 0.514, 0.134, 0.406, 0.372, 0.498, 0.127, 0.231]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.7454 | Steps: 2 | Val loss: 1.8215 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 00:40:37 (running for 00:47:46.74)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.703 |      0.332 |                   97 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.4076492537313433
[2m[36m(func pid=167216)[0m top5: 0.9127798507462687
[2m[36m(func pid=167216)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=167216)[0m f1_macro: 0.31503247212154073
[2m[36m(func pid=167216)[0m f1_weighted: 0.4267694462629974
[2m[36m(func pid=167216)[0m f1_per_class: [0.227, 0.358, 0.333, 0.535, 0.112, 0.373, 0.453, 0.41, 0.136, 0.214]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.7643 | Steps: 2 | Val loss: 1.8596 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 00:40:43 (running for 00:47:52.31)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.745 |      0.315 |                   98 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=167216)[0m top1: 0.38759328358208955
[2m[36m(func pid=167216)[0m top5: 0.9029850746268657
[2m[36m(func pid=167216)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=167216)[0m f1_macro: 0.30040396073536224
[2m[36m(func pid=167216)[0m f1_weighted: 0.41849015331648565
[2m[36m(func pid=167216)[0m f1_per_class: [0.226, 0.301, 0.301, 0.516, 0.097, 0.39, 0.48, 0.37, 0.121, 0.201]
[2m[36m(func pid=167216)[0m 
[2m[36m(func pid=167216)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.8157 | Steps: 2 | Val loss: 1.9379 | Batch size: 32 | lr: 0.01 | Duration: 3.33s
== Status ==
Current time: 2024-01-07 00:40:48 (running for 00:47:57.81)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.251
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00022 | RUNNING    | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.764 |      0.3   |                   99 |
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 00:40:49 (running for 00:47:58.69)
Memory usage on this node: 15.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.251
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.54 GiB heap, 0.0/55.65 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_51d3e_00000 | TERMINATED | 192.168.7.53:68418  | 0.0001 |       0.99 |         0      |  2.202 |      0.09  |                   75 |
| train_51d3e_00001 | TERMINATED | 192.168.7.53:68792  | 0.001  |       0.99 |         0      |  0.999 |      0.278 |                  100 |
| train_51d3e_00002 | TERMINATED | 192.168.7.53:69189  | 0.01   |       0.99 |         0      |  0.486 |      0.251 |                   75 |
| train_51d3e_00003 | TERMINATED | 192.168.7.53:69627  | 0.1    |       0.99 |         0      |  3.093 |      0.216 |                   75 |
| train_51d3e_00004 | TERMINATED | 192.168.7.53:85916  | 0.0001 |       0.9  |         0      |  2.602 |      0.086 |                   75 |
| train_51d3e_00005 | TERMINATED | 192.168.7.53:86021  | 0.001  |       0.9  |         0      |  2.058 |      0.197 |                   75 |
| train_51d3e_00006 | TERMINATED | 192.168.7.53:86940  | 0.01   |       0.9  |         0      |  0.706 |      0.274 |                  100 |
| train_51d3e_00007 | TERMINATED | 192.168.7.53:91972  | 0.1    |       0.9  |         0      |  0.818 |      0.2   |                   75 |
| train_51d3e_00008 | TERMINATED | 192.168.7.53:103380 | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.088 |                   75 |
| train_51d3e_00009 | TERMINATED | 192.168.7.53:103390 | 0.001  |       0.99 |         0.0001 |  1.003 |      0.267 |                  100 |
| train_51d3e_00010 | TERMINATED | 192.168.7.53:109192 | 0.01   |       0.99 |         0.0001 |  0.727 |      0.229 |                   75 |
| train_51d3e_00011 | TERMINATED | 192.168.7.53:110192 | 0.1    |       0.99 |         0.0001 |  2.908 |      0.216 |                   75 |
| train_51d3e_00012 | TERMINATED | 192.168.7.53:120862 | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.088 |                   75 |
| train_51d3e_00013 | TERMINATED | 192.168.7.53:126068 | 0.001  |       0.9  |         0.0001 |  2.041 |      0.153 |                   75 |
| train_51d3e_00014 | TERMINATED | 192.168.7.53:127473 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.236 |                   75 |
| train_51d3e_00015 | TERMINATED | 192.168.7.53:127909 | 0.1    |       0.9  |         0.0001 |  0.142 |      0.247 |                  100 |
| train_51d3e_00016 | TERMINATED | 192.168.7.53:138124 | 0.0001 |       0.99 |         1e-05  |  2.226 |      0.108 |                   75 |
| train_51d3e_00017 | TERMINATED | 192.168.7.53:144068 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.31  |                  100 |
| train_51d3e_00018 | TERMINATED | 192.168.7.53:145070 | 0.01   |       0.99 |         1e-05  |  0.26  |      0.247 |                   75 |
| train_51d3e_00019 | TERMINATED | 192.168.7.53:150330 | 0.1    |       0.99 |         1e-05  |  1.819 |      0.191 |                   75 |
| train_51d3e_00020 | TERMINATED | 192.168.7.53:155946 | 0.0001 |       0.9  |         1e-05  |  2.606 |      0.082 |                   75 |
| train_51d3e_00021 | TERMINATED | 192.168.7.53:162129 | 0.001  |       0.9  |         1e-05  |  2.069 |      0.199 |                   75 |
| train_51d3e_00022 | TERMINATED | 192.168.7.53:167216 | 0.01   |       0.9  |         1e-05  |  0.816 |      0.283 |                  100 |
| train_51d3e_00023 | TERMINATED | 192.168.7.53:167836 | 0.1    |       0.9  |         1e-05  |  1.798 |      0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 00:40:49,790	INFO tune.py:798 -- Total run time: 2879.69 seconds (2878.67 seconds for the tuning loop).
[2m[36m(func pid=167216)[0m top1: 0.34701492537313433
[2m[36m(func pid=167216)[0m top5: 0.894589552238806
[2m[36m(func pid=167216)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=167216)[0m f1_macro: 0.2832665649910071
[2m[36m(func pid=167216)[0m f1_weighted: 0.3910979783157152
[2m[36m(func pid=167216)[0m f1_per_class: [0.24, 0.345, 0.212, 0.432, 0.094, 0.392, 0.447, 0.341, 0.118, 0.211]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341333.1 ON aap04 CANCELLED AT 2024-01-07T00:40:54 ***
srun: error: aap04: task 0: Exited with exit code 1
